{
 "awd_id": "0504064",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "SCI:   TeraGrid Resource Partner",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rudolf Eigenmann",
 "awd_eff_date": "2005-08-01",
 "awd_exp_date": "2015-03-31",
 "tot_intn_awd_amt": 14919205.0,
 "awd_amount": 35707710.0,
 "awd_min_amd_letter_date": "2005-08-05",
 "awd_max_amd_letter_date": "2015-01-28",
 "awd_abstract_narration": "The Extensible Terascale Facility (ETF) is the next stage in the evolution of NSF large-scale cyberinfrastructure for enabling high-end computational research. The ETF enables researchers to address the most challenging computational problems by utilizing the integrated resources, data collections, instruments and visualization capabilities of nine resource partners. On October 1, 2004, the ETF concluded a three-year construction effort to create this distributed environment called the TeraGrid (TG) and we are now entering the production operations phase.\r\n\r\nThe TeraGrid resource partners inclue:  the University of Chicago/Argonne National Laboratory, the San Diego Supercomputer Center at UCSD, the Texas Advanced Computing Center at UT-Austin, the National Center for Supercomputing Applications at UIUC, Indiana University, Purdue University, Oak Ridge National Laboratory, and the Pittsburgh Supercomputing Center.\r\n\r\nA separate proposal was submitted to NSF on October 19, 2004 for the TeraGrid Grid Infrastructure Group (GIG). Under the direction of Charlie Catlett at UC/ANL, in general, the GIG will be responsible for coordination of development activities for the TeraGrid with subcontracts to the partner sites. The resource partners (RP) will each have independent cooperative agreements with NSF, but will work closely with the GIG to implement the vision of the TeraGrid.\r\n\r\nThis proposal outlines the plan at NCSA to participate as a resource partner in the TeraGrid team to provide the expanding user community with ongoing access to this computational science facility. This proposal covers the period November 1, 2004 through October 31, 2009.\r\n\r\n\r\nTo fulfill NCSA's mission of enabling new scientific discoveries, NCSA brings its extensive experience and world-class leadership in supporting engineering and science research and education in a 24x7 production environment to the TeraGrid  (TG).  Our goals as a TG Resource Provider  (RP) are to a) support applications development and tuning to enable users to develop highly effective grid applications, b) offer production-quality computational, data, and visualization services to support advanced science needs, c) respond rapidly to issues raised by users, administrators, or other TeraGrid sites, and d) support the services, software, policies, and milestones of the TG Grid Integration Group  (GIG).  The result of these efforts will be a stable, robust, secure, grid-enabled high-performance computing and storage environment that supports the development and execution of grid applications, science gateways, and capability applications.  In fulfilling these goals, NCSA will leverage expertise and effort from other activities, including the NCSA/SDSC CyberInfrastrcture Partnership, to address the needs of the TG user communities to the greatest extent possible. The success of TG depends on the integration of the unique and collective strengths and capabilities of the individual sites and the GIG. NCSA's strengths are in the areas of application performance engineering, advanced grid applications support, software integration, packaging and support, distributed operations and procedures, large-scale parallel file systems and data-exchange and analysis tools, I/O and network performance analysis, security, and large-scale compute resources integrated with storage and visualization. Our hardware and software resources are backed by a knowledgeable and experienced staff well versed in working with high-end users. The integration of site-specific production services into the common TG user environment will make it possible for applications teams and user communities to increase their productivity by minimizing the effort required to access multiple or different resources while maximizing the quality and availability of those resources. This will enable current users to expand the scope and range of their scientific research, provide an easy-to-use, highly productive environment for new users and communities, and create a cyberinfrastructure that will be the basis for new scientific programs. NCSA's success within the TG will be measured by the degree to which its TG resources are effectively used to solve important scientific problems such as those being investigated by leading edge researchers.\r\n\r\nThis proposal outlines NCSA's plan to participate as a resource provider on the TG team. We will provide to the expanding TG user community ongoing access to the growing NCSA-managed TG computational science facility.  The TG RP grant will be used almost entirely to fund highly experienced NCSA personnel to support this facility and to serve the needs of TG users in coordination with NCSA's Core NSF support. Appendix A contains common text that links the nine TG RP proposals and the TG GIG proposal. \r\n\r\nThrough this RP, we will empower research communities to do their science and Engineering research by seamless use of NCSA's TG and Core resources in the context of the larger TG effort. This includes support for distributed computation, data-centric science, on-demand computing, and connection to a broad range of emerging cyberinfrastructure-based capabilities funded from a variety of sources.  Working together with scientists, engineers, and other RP sites, NCSA will help enable new research that would otherwise be much more difficult, or simply impossible, to carry out.  \r\n\r\nNCSA will build, improve, and make available a set of high-end resources in support of the national research community.  The results of this effort will lead to a hardware, software, and networking environment that can be extended to other facilities. It will also provide valuable information to others building grids that may include high-end facilities. Finally, NCSA's experienced staff will have a substantial positive impact on the quality of the TG environment and on the ability of users to work effectively in that environment. \r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Towns",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "John W Towns",
   "pi_email_addr": "jtowns@ncsa.illinois.edu",
   "nsf_id": "000400201",
   "pi_start_date": "2015-01-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "Cockerill",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy M Cockerill",
   "pi_email_addr": "cockerill@tacc.utexas.edu",
   "nsf_id": "000299304",
   "pi_start_date": "2005-08-05",
   "pi_end_date": "2007-10-31"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Towns",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "John W Towns",
   "pi_email_addr": "jtowns@ncsa.illinois.edu",
   "nsf_id": "000400201",
   "pi_start_date": "2005-08-05",
   "pi_end_date": "2007-10-31"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jay",
   "pi_last_name": "Alameda",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jay Alameda",
   "pi_email_addr": "alameda@illinois.edu",
   "nsf_id": "000308578",
   "pi_start_date": "2005-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Ruth",
   "pi_last_name": "Aydt",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Ruth A Aydt",
   "pi_email_addr": "aydt@ncsa.uiuc.edu",
   "nsf_id": "000297243",
   "pi_start_date": "2005-08-05",
   "pi_end_date": "2011-03-07"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Von",
   "pi_last_name": "Welch",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Von S Welch",
   "pi_email_addr": "vwelch@illinois.edu",
   "nsf_id": "000290743",
   "pi_start_date": "2005-08-05",
   "pi_end_date": "2011-03-07"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "Cockerill",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy M Cockerill",
   "pi_email_addr": "cockerill@tacc.utexas.edu",
   "nsf_id": "000299304",
   "pi_start_date": "2015-01-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "122100",
   "pgm_ele_name": "HEP-High Energy Physics"
  },
  {
   "pgm_ele_code": "717200",
   "pgm_ele_name": "Graduate Research Fellowship"
  },
  {
   "pgm_ele_code": "747600",
   "pgm_ele_name": "XD-Extreme Digital"
  },
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "7476",
   "pgm_ref_txt": "ETF"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0105",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0409",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04000910DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2005,
   "fund_oblg_amt": 2700000.0
  },
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 1546965.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 11589805.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 15719758.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 885421.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 842222.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 1782991.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 640548.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The TeraGrid was an open cyberinfrastructure that enabled and supported leading&shy;edge scientific discovery and promoted science and technology education. The TeraGrid comprised supercomputing and massive storage systems, visualization resources, data collections, and science gateways, connected by high&shy;bandwidth networks integrated by coordinated policies and operations, and supported by computational science and technology experts,.</p>\n<p>TeraGrid&rsquo;s objectives were accomplished via a three-pronged strategy: to support the most advanced computational science in multiple domains (deep impact), to empower new communities of users (wide impact), and to provide resources and services that can be extended to a broader cyberinfrastructure (open infrastructure). This &ldquo;deep, wide, and open&rdquo; strategy guided the development, deployment, operations, and support activities to ensure maximum impact on science research and education across communities.</p>\n<p>When it ended, TeraGrid was an integrated, national-scale computational science infrastructure operated in a partnership comprising the Grid Infrastructure Group (GIG), eleven Resource Provider (RP) institutions, and six Software Integration partners, with funding from the National Science Foundation&rsquo;s (NSF) Office of Cyberinfrastructure (OCI). Initially created as the Distributed Terascale Facility (with four partners) through a Major Research Equipment (MRE) award in 2001, the TeraGrid began providing production computing, storage, and visualization services to the national community in October 2004. In August 2005, NSF funded a five-year program to operate, enhance, and expand the capacity and capabilities of the TeraGrid to meet the growing needs of the science and engineering community through 2010, and then extended the TeraGrid an additional year into 2011 to provide an extended planning phase in preparation for TeraGrid Phase III eXtreme Digital (XD).</p>\n<p>Accomplishing this vision was crucial for the advancement of many areas of scientific discovery, ensuring US scientific leadership, and increasingly, for addressing important societal issues. TeraGrid achieves its purpose and fulfills its mission through a three?pronged strategy:&nbsp;&nbsp;</p>\n<p><strong>Deep</strong>: ensure profound impact for the most experienced users, through provision of the most powerful computational resources and advanced computational expertise and enable transformational scientific discovery through leadership in HPC for high&shy;end computational research;</p>\n<p><strong>Wide</strong>: enable scientific discovery by broader and more diverse communities of researchers and educators who can leverage TeraGrid&rsquo;s high?end resources, portals and science gateways and increase the overall impact of TeraGrid&rsquo;s advanced computational resources to larger and more diverse research and education communities through user interfaces and portals, domain specific gateways, and enhanced support that facilitate scientific discovery by people without requiring them to become high performance computing experts; and&nbsp;</p>\n<p><strong>Open</strong>: facilitate simple integration with the broader cyberinfrastructure through the use of open interfaces, partnerships with other grids, and collaborations with other science research groups delivering and supporting open cyberinfrastructure facilities.</p>\n<p>The TeraGrid&rsquo;s integrated resource portfolio evolved over the life of the project from an initial single integrated but distributed cluster to more than 20 high-performance computational (HPC) systems, several massive storage systems, and remote visualization resources, all supported by a dedicated interconnection network. This infrastructure was integrated at several levels: policy and planning, operational and user support, and software and services.</p>\n<p>The national, and global, user community that...",
  "por_txt_cntn": "\nThe TeraGrid was an open cyberinfrastructure that enabled and supported leading&shy;edge scientific discovery and promoted science and technology education. The TeraGrid comprised supercomputing and massive storage systems, visualization resources, data collections, and science gateways, connected by high&shy;bandwidth networks integrated by coordinated policies and operations, and supported by computational science and technology experts,.\n\nTeraGrid\u00c6s objectives were accomplished via a three-pronged strategy: to support the most advanced computational science in multiple domains (deep impact), to empower new communities of users (wide impact), and to provide resources and services that can be extended to a broader cyberinfrastructure (open infrastructure). This \"deep, wide, and open\" strategy guided the development, deployment, operations, and support activities to ensure maximum impact on science research and education across communities.\n\nWhen it ended, TeraGrid was an integrated, national-scale computational science infrastructure operated in a partnership comprising the Grid Infrastructure Group (GIG), eleven Resource Provider (RP) institutions, and six Software Integration partners, with funding from the National Science Foundation\u00c6s (NSF) Office of Cyberinfrastructure (OCI). Initially created as the Distributed Terascale Facility (with four partners) through a Major Research Equipment (MRE) award in 2001, the TeraGrid began providing production computing, storage, and visualization services to the national community in October 2004. In August 2005, NSF funded a five-year program to operate, enhance, and expand the capacity and capabilities of the TeraGrid to meet the growing needs of the science and engineering community through 2010, and then extended the TeraGrid an additional year into 2011 to provide an extended planning phase in preparation for TeraGrid Phase III eXtreme Digital (XD).\n\nAccomplishing this vision was crucial for the advancement of many areas of scientific discovery, ensuring US scientific leadership, and increasingly, for addressing important societal issues. TeraGrid achieves its purpose and fulfills its mission through a three?pronged strategy:  \n\nDeep: ensure profound impact for the most experienced users, through provision of the most powerful computational resources and advanced computational expertise and enable transformational scientific discovery through leadership in HPC for high&shy;end computational research;\n\nWide: enable scientific discovery by broader and more diverse communities of researchers and educators who can leverage TeraGrid\u00c6s high?end resources, portals and science gateways and increase the overall impact of TeraGrid\u00c6s advanced computational resources to larger and more diverse research and education communities through user interfaces and portals, domain specific gateways, and enhanced support that facilitate scientific discovery by people without requiring them to become high performance computing experts; and \n\nOpen: facilitate simple integration with the broader cyberinfrastructure through the use of open interfaces, partnerships with other grids, and collaborations with other science research groups delivering and supporting open cyberinfrastructure facilities.\n\nThe TeraGrid\u00c6s integrated resource portfolio evolved over the life of the project from an initial single integrated but distributed cluster to more than 20 high-performance computational (HPC) systems, several massive storage systems, and remote visualization resources, all supported by a dedicated interconnection network. This infrastructure was integrated at several levels: policy and planning, operational and user support, and software and services.\n\nThe national, and global, user community that relied on TeraGrid grew tremendously to more than 10,000 total lifetime users. To support the great diversity of research activities and their wide range in resources needs, user support and operations teams leveraged..."
 }
}