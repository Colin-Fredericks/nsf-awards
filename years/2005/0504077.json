{
 "awd_id": "0504077",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "SCI: TeraGrid Resource Partners",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rudolf Eigenmann",
 "awd_eff_date": "2005-08-01",
 "awd_exp_date": "2013-12-31",
 "tot_intn_awd_amt": 7735890.0,
 "awd_amount": 10180504.0,
 "awd_min_amd_letter_date": "2005-07-29",
 "awd_max_amd_letter_date": "2013-09-17",
 "awd_abstract_narration": "The proposal requests funding to establish the participation of the Texas Advanced Computing Center (TACC) as a Resource Provider (RP) in the Extensible Terascale Facility, also referred to as the ETF or the TeraGrid. ETF is an integrated heterogeneous computing-communication-information system designed to provide the national science and engineering community with unparalleled access to secure state-of-the-art cyberinfrastructure resources and services. ETF seamlessly integrates the highest-end computing resources available to the open science community, including powerful and innovative systems at TACC, sophisticated scientific instruments, and diverse data collections, using software tools and services that enable their effective use.\r\n\r\nAmong large-scale comprehensive cyberinfrastructure projects, ETF pioneers the integration of state-of-the-art software services with the policies and procedures of autonomous open national computing centers and universities. As ETF moves into its five-year operations phase, its partner institutions will seek to: deliver the promise of convenient, reliable, secure, persistent computing, data storage, data collection, and real-time instrument capabilities;support user priorities that include new software services such as co-scheduling, meta-scheduling, parameter sweep tools, and advanced data management and handling; and\r\nimplement and support science gateway services that engage a much larger number of the nation's scientists and engineers in computing-enabled research and education.\r\n\r\nWorking alongside TACC are eight other ETF partner organizations: Argonne National Laboratory,  Indiana University, the National Center for Supercomputing Applications (NCSA), Oak Ridge National Laboratory, Pittsburgh Supercomputing Center (PSC), Purdue University, and the San Diego Supercomputer Center (SDSC).\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Boisseau",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "John R Boisseau",
   "pi_email_addr": "boisseau@tacc.utexas.edu",
   "nsf_id": "000240621",
   "pi_start_date": "2005-07-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kelly",
   "pi_last_name": "Gaither",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Kelly P Gaither",
   "pi_email_addr": "kelly@tacc.utexas.edu",
   "nsf_id": "000356197",
   "pi_start_date": "2005-07-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Barbara",
   "pi_last_name": "Fossum",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Barbara M Fossum",
   "pi_email_addr": "bfossum@purdue.edu",
   "nsf_id": "000299381",
   "pi_start_date": "2005-07-29",
   "pi_end_date": "2013-09-17"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Edward",
   "pi_last_name": "Walker",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Edward Walker",
   "pi_email_addr": "ewalker544@gmail.com",
   "nsf_id": "000493059",
   "pi_start_date": "2005-07-29",
   "pi_end_date": "2013-09-17"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Tommy",
   "pi_last_name": "Minyard",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Tommy K Minyard",
   "pi_email_addr": "minyard@tacc.utexas.edu",
   "nsf_id": "000371000",
   "pi_start_date": "2005-07-29",
   "pi_end_date": "2013-09-17"
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "110 INNER CAMPUS DR",
  "perf_city_name": "AUSTIN",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121139",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "747600",
   "pgm_ele_name": "XD-Extreme Digital"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7476",
   "pgm_ref_txt": "ETF"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0105",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2005,
   "fund_oblg_amt": 1399999.0
  },
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 802130.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 3388384.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 3294499.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 618015.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 474094.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 24655.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 178728.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Over a more than 8 &nbsp;year period, this &nbsp;award funded two distinct projects.&nbsp; The original award, consuming most of the funding, was for the Texas Advanced Computing Center&rsquo;s (TACC) support of the TeraGrid project.&nbsp; The award funded several large-scale production resources to support scientific research including part of an HPC system, a remote visualization platform and high-speed networking hardware and connections.&nbsp;&nbsp; In addition to the hardware resources, the project also provided funding for support staff to operate these high-end resources and develop new novel interfaces to make it easier for researchers to use the NSF funded large-scale systems including the first TeraGrid User Portal.&nbsp;&nbsp; In the second phase on the project, additional funding was provided and the nature of the work shifted to a small team of investigators conducting research in computational analysis and visual analytics on large archival data sets from the National Archives and Records Administration.&nbsp; This report covers both phases of the project.</p>\n<p>The TeraGrid portion of the award supported a large High Performance Computing (HPC) system, Lonestar 3, which was one of the first x86-based 64-bit platforms, using the then new low-latency, high-bandwidth InfiniBand interconnect, that are now common for HPC clustered system.&nbsp; The system ran for over four years supporting thousands of researchers in hundreds of projects and delivered tens of millions of CPU hours to the NSF community.&nbsp;&nbsp; The first large-scale system designed primarily for visualization for NSF researchers, Maverick, was also supported on this grant.&nbsp; This visualization system provided a 512GB shared-memory platform with 16 high performance GPUs to perform large scale rendering and remote visualization.&nbsp;&nbsp; This system resulted in the development of remote visualization tools and techniques that have been refined and are still in use on recent visualization resources.</p>\n<p>This project also included 10Gigabit Ethernet networking to connect all of the TeraGrid-funded sites together with high-bandwidth pipes to facilitate large data transfers between the systems.&nbsp; An existing Force10 switch was supported from the project funds and acted as the core switch connecting all of the TACC large-scale resources and archive library to the other of the TeraGrid sites.&nbsp; This connectivity ran across the National Lambda Rail network and funds from this project were used to cover a portion of the costs to connect to NLR.</p>\n<p>Several software products also resulted from this provided funding, most importantly, the TeraGrid User Portal.&nbsp; This was the first portal which allowed researchers with NSF allocations to manage resource allocations across all of the TeraGrid sites.&nbsp; This portal has been greatly enhanced over the years with additional functionality and now operates as the XSEDE User Portal.</p>\n<p>Beginning in 2008 &nbsp;until December of 2013, the project added a new focus and new funding, and a multidisciplinary team at TACC conducted research in computational analysis and visual analytics for big archives processing with support from the National Archives and Records Administration. This research is specifically relevant today, in the era of big data, if we consider that the possibilities of making unprecedented discoveries through data-intensive science on all walks of knowledge are based on the existence of organized, readily available, and documented data and records collections. This research is also relevant to government accountability and the possibility for the public to find the documents and data produced by the federal and state governments. These are the topics occupy archivists and records managers and our work addressed solutions into this pressing problem: <em>imagining and exploring next generation of methods and too...",
  "por_txt_cntn": "\nOver a more than 8  year period, this  award funded two distinct projects.  The original award, consuming most of the funding, was for the Texas Advanced Computing Center\u00c6s (TACC) support of the TeraGrid project.  The award funded several large-scale production resources to support scientific research including part of an HPC system, a remote visualization platform and high-speed networking hardware and connections.   In addition to the hardware resources, the project also provided funding for support staff to operate these high-end resources and develop new novel interfaces to make it easier for researchers to use the NSF funded large-scale systems including the first TeraGrid User Portal.   In the second phase on the project, additional funding was provided and the nature of the work shifted to a small team of investigators conducting research in computational analysis and visual analytics on large archival data sets from the National Archives and Records Administration.  This report covers both phases of the project.\n\nThe TeraGrid portion of the award supported a large High Performance Computing (HPC) system, Lonestar 3, which was one of the first x86-based 64-bit platforms, using the then new low-latency, high-bandwidth InfiniBand interconnect, that are now common for HPC clustered system.  The system ran for over four years supporting thousands of researchers in hundreds of projects and delivered tens of millions of CPU hours to the NSF community.   The first large-scale system designed primarily for visualization for NSF researchers, Maverick, was also supported on this grant.  This visualization system provided a 512GB shared-memory platform with 16 high performance GPUs to perform large scale rendering and remote visualization.   This system resulted in the development of remote visualization tools and techniques that have been refined and are still in use on recent visualization resources.\n\nThis project also included 10Gigabit Ethernet networking to connect all of the TeraGrid-funded sites together with high-bandwidth pipes to facilitate large data transfers between the systems.  An existing Force10 switch was supported from the project funds and acted as the core switch connecting all of the TACC large-scale resources and archive library to the other of the TeraGrid sites.  This connectivity ran across the National Lambda Rail network and funds from this project were used to cover a portion of the costs to connect to NLR.\n\nSeveral software products also resulted from this provided funding, most importantly, the TeraGrid User Portal.  This was the first portal which allowed researchers with NSF allocations to manage resource allocations across all of the TeraGrid sites.  This portal has been greatly enhanced over the years with additional functionality and now operates as the XSEDE User Portal.\n\nBeginning in 2008  until December of 2013, the project added a new focus and new funding, and a multidisciplinary team at TACC conducted research in computational analysis and visual analytics for big archives processing with support from the National Archives and Records Administration. This research is specifically relevant today, in the era of big data, if we consider that the possibilities of making unprecedented discoveries through data-intensive science on all walks of knowledge are based on the existence of organized, readily available, and documented data and records collections. This research is also relevant to government accountability and the possibility for the public to find the documents and data produced by the federal and state governments. These are the topics occupy archivists and records managers and our work addressed solutions into this pressing problem: imagining and exploring next generation of methods and tools to tackle big digital archives are. To address this problem we built a visual analytics framework through which we tested different archives and data analysis functions including collection\u00c6s c..."
 }
}