{
 "awd_id": "0512003",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Proposal:    Functional Near Infrared Imaging for Communication and Control",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2005-07-01",
 "awd_exp_date": "2007-06-30",
 "tot_intn_awd_amt": 468110.0,
 "awd_amount": 468110.0,
 "awd_min_amd_letter_date": "2005-06-30",
 "awd_max_amd_letter_date": "2005-06-30",
 "awd_abstract_narration": "Severe motor disabilities significantly impact the quality of life for millions of people worldwide. The most profound motor disability, so-called locked-in syndrome, describes people who are completely paralyzed and unable to speak - they are intelligent and alert, but unable to communicate even their most basic needs.  Several technologies, based on detecting minute electrical changes in brain signals, have been assessed for whether they can provide a channel of communication for people with locked-in syndrome; although the results from these studies are encouraging, the interfaces are slow, highly error prone, and often require weeks or months of training before control is achieved.  Functional Near-Infrared (fNIR) imaging is a new and promising brain-imaging technology that measures small changes in blood volume and oxygenation in the brain.  The technology has been explored for augmented cognition and for diagnosis, but has not been investigated for its control potential.  Initial studies in able-bodied and locked-in subjects suggest that fNIR control is more accurate, easier to activate, and does not require any training.  The overall goal of this research is to fully characterize and test fNIR imaging for control application.  To this end, the PIs will first conduct a comprehensive study to determine optimal fNIR activation methods (e.g., determination of mental tasks that can be easily performed and that result in detectable brain activations). The product of this study will be a screening protocol that can be used to determine the most controllable brain area and device configuration for an individual user.  The PIs will also work to improve fNIR imaging methods, by conducting offline and online studies to determine optimal sensitivity, cortical depth, filters, and signal processing algorithms for fNIR control.  The product of this study will be an fNIR imaging device that is as accurate, sensitive, and robust as possible, as well as a set of heuristics for optimally configuring the device for a particular user.  Finally, the PIs will demonstrate fNIR control in real-world applications by determining optimal mappings of fNIR signals to traditional assistive technology control interfaces such as scanning or logical interfaces, cursor movement, and direct selection.  The results from all of these studies will be validated by combining and incorporating the findings into a comprehensive test of the fNIR system, by implementing a system for in-home use to control devices such as light switches, a television, and an MP3 player, which will be tested with five locked-in subjects.\r\n\r\nBroader Impacts:  For people with severe motor disabilities, the implications of researching and improving access to assistive technologies are profound.  This research will make a significant contribution to the area of brain-computer interfaces by introducing a new, unexplored brain imaging method for control.  It will also add to the body of knowledge for assistive technology and human-computer interfaces, by establishing protocols and mappings between an fNIR device and control interfaces.  There are many people with less profound motor impairments (e.g., palsy), who might also be helped by an fNIR input device.  Further research in fNIR control could lead to control of prosthetics that could restore movement in paralyzed limbs.  Additionally, interface strategies for low-bandwidth, high error rate contexts may have significant application in other domains such as mobile and wearable computing systems and hands-free device operation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dennis",
   "pi_last_name": "Proffitt",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Dennis R Proffitt",
   "pi_email_addr": "drp@virginia.edu",
   "nsf_id": "000216642",
   "pi_start_date": "2005-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia Main Campus",
  "perf_str_addr": "1001 EMMET ST N",
  "perf_city_name": "CHARLOTTESVILLE",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229034833",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "684600",
   "pgm_ele_name": "UNIVERSAL ACCESS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0105",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0105",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2005,
   "fund_oblg_amt": 468110.0
  }
 ],
 "por": null
}