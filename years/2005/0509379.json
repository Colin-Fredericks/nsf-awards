{
 "awd_id": "0509379",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CSR-SMA: Toward Reliable and Efficient Message Passing Software Through Formal Analysis",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Krishna Kant",
 "awd_eff_date": "2005-07-01",
 "awd_exp_date": "2011-06-30",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 449996.0,
 "awd_min_amd_letter_date": "2005-06-17",
 "awd_max_amd_letter_date": "2010-07-07",
 "awd_abstract_narration": "The quest for high performance drives parallel scientific computing software design. Well over 60% of the high-performance computing (HPC) community writes programs using the MPI library; to gain performance, they are known to perform many manual optimizations. Even groups that generate MPI from high level descriptions ultimately seem to generate MPI code, due to its eminent portability. However, since performance does not port, manual tweaks are inevitable. This, together with the vastness and evolving nature of the MPI standard, and the innate complexity of concurrent programming introduces costly bugs.\r\n\r\nFormal methods are enjoying an explosive growth precisely to help eliminate these kinds of bugs. Already they  find applications in diverse areas ranging from formally verifying optimizing compiler transformations, formally debugging device driver codes, and solidifying industrial standards.   The project will investigate and  employ a number of complementary formal approaches to HPC software design: erect formal standards for MPI so that designers are properly educated, take advantage of the standards and write comprehensive MPI platform tests, extract finite-state models from MPI programs and analyze them automatically for deadlocks and race conditions, and  will instrument the MPI-based program with correctness assertions that can be checked at run-time.\r\n\r\nThe project will advance the state of the art in formal methods by developing algorithms that take advantage of semantic properties of communication libraries. It will help advance the state of the art in parallel scientific programming by encouraging the use of formal assertions, and encouraging the use of formal analysis in lieu of brute-force execution.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ganesh",
   "pi_last_name": "Gopalakrishnan",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Ganesh L Gopalakrishnan",
   "pi_email_addr": "ganesh@cs.utah.edu",
   "nsf_id": "000160895",
   "pi_start_date": "2005-06-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Kirby",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Robert M Kirby",
   "pi_email_addr": "kirby@cs.utah.edu",
   "nsf_id": "000484303",
   "pi_start_date": "2005-06-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Utah",
  "inst_street_address": "201 PRESIDENTS CIR",
  "inst_street_address_2": "",
  "inst_city_name": "SALT LAKE CITY",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8015816903",
  "inst_zip_code": "841129049",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "UT01",
  "org_lgl_bus_name": "UNIVERSITY OF UTAH",
  "org_prnt_uei_num": "",
  "org_uei_num": "LL8GLEVH6MG3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Utah",
  "perf_str_addr": "201 PRESIDENTS CIR",
  "perf_city_name": "SALT LAKE CITY",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "841129049",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "UT01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "2884",
   "pgm_ref_txt": "NEXT GENERATION SOFTWARE PROGR"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0105",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0105",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2005,
   "fund_oblg_amt": 199998.0
  },
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 6000.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 99999.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 111999.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High performance computing is of strategic importance for every nation,providing the basic ability to conduct scientific research. While nations keep leapfrogging each other in terms of the highest achieved computational speeds, they must also ensure that in the more practical realms of&nbsp;performance, they have approaches to sustain predictable compute performance,&nbsp;cost-effective methodologies to develop robust HPC software, and&nbsp;adequate resources to train future HPC programmers. One of the most widely used software libraries in this regard is the Message Passing Interface (MPI).</p>\n<p>This project was proposed directly in response to the fact that MPI programmers are known to struggle to debug their programs. A perfectly working MPI program suddenly and inexplicably stops working when ported to a new platform. The root-cause was identified to be a lack of understanding of the formal principles ('programming laws') governing the semantics of MPI programs. &nbsp;It was identified that unless one develops a machine-independent semantics for MPI programs, these debugging challenges would persist, causing a severe productivity loss in our ability to conduct science and engineering.</p>\n<p>&nbsp;</p>\n<p>To give the public a perspective of how scientific research using MPI is done, consider the problem of determining the reason(s) for antibiotic resistance in bacteria. Work by the Sanbonmatsu team at Los Alamos National Laboratory shows that by simulating the atoms within an mRNA (messenger RNA) according to Newton's Laws, one can recreate, within a computer, the behavior of these mRNA - one of life's fundamental building blocks. One can further probe into the functioning of Riboswitches (http://en.wikipedia.org/wiki/Riboswitch) which then reveals drug resistance properties. These simulations are written using molecular dynamics packages based such as NAMD, and expressed in MPI derivatives such as Charm++. &nbsp;The supercomputing machines on which these simulations are written cost a few billion dollars to build and deploy, consume several million dollars of electricity a year to operate, and are obsolete in about six years. In this setting, one truly can appreciate the extent of damage that a nasty software defect that takes weeks (typical) to debug. Our work is geared to very quickly unearth these bugs, and not let them crash a supercomputing simulation after a month of its running.</p>\n<p>&nbsp;</p>\n<p>We now present the scientific outcomes of this work under the Intellectual Merit and Broader Impact categories. These results were obtained over a time period from 2007 to date. One can go to our website (http://www.cs.utah.edu/fv) and obtain a chronologically accurate view of our contributions each year.</p>\n<p>&nbsp;</p>\n<p>INTELLECTUAL MERIT: This research has lead to the discovery of the fundamental programming laws governing the behavior of MPI commands. By understanding these laws, we can accurately compute the possible execution outcomes of a given MPI program even on supercomputers that have not been built or one does not have access to. This enables a division of labor fundamental to all science and engineering; namely that one can develop MPI-based programs in isolation fully confident of the fact that when future supercomputers are delivered, one can run these programs and encounter no nasty MPI-related programming errors (\"bugs\").</p>\n<p>&nbsp;</p>\n<p>BROADER IMPACT: This research has helped with the training of five doctoral students who have graduated: Drs. Robert Palmer, Yu Yang, Xiaofang Chen, Sarvani Vakkalanka, and Anh Vo. It has also helped with the training of several undergraduate and Masters Degree level students. It has lead to the release of a highly capable MPI program analysis tool called In Situ Partial order analysis (ISP). It has fed into the National Tool Infrastructure through integration into the Eclipse Parallel Tools Plat...",
  "por_txt_cntn": "\nHigh performance computing is of strategic importance for every nation,providing the basic ability to conduct scientific research. While nations keep leapfrogging each other in terms of the highest achieved computational speeds, they must also ensure that in the more practical realms of performance, they have approaches to sustain predictable compute performance, cost-effective methodologies to develop robust HPC software, and adequate resources to train future HPC programmers. One of the most widely used software libraries in this regard is the Message Passing Interface (MPI).\n\nThis project was proposed directly in response to the fact that MPI programmers are known to struggle to debug their programs. A perfectly working MPI program suddenly and inexplicably stops working when ported to a new platform. The root-cause was identified to be a lack of understanding of the formal principles ('programming laws') governing the semantics of MPI programs.  It was identified that unless one develops a machine-independent semantics for MPI programs, these debugging challenges would persist, causing a severe productivity loss in our ability to conduct science and engineering.\n\n \n\nTo give the public a perspective of how scientific research using MPI is done, consider the problem of determining the reason(s) for antibiotic resistance in bacteria. Work by the Sanbonmatsu team at Los Alamos National Laboratory shows that by simulating the atoms within an mRNA (messenger RNA) according to Newton's Laws, one can recreate, within a computer, the behavior of these mRNA - one of life's fundamental building blocks. One can further probe into the functioning of Riboswitches (http://en.wikipedia.org/wiki/Riboswitch) which then reveals drug resistance properties. These simulations are written using molecular dynamics packages based such as NAMD, and expressed in MPI derivatives such as Charm++.  The supercomputing machines on which these simulations are written cost a few billion dollars to build and deploy, consume several million dollars of electricity a year to operate, and are obsolete in about six years. In this setting, one truly can appreciate the extent of damage that a nasty software defect that takes weeks (typical) to debug. Our work is geared to very quickly unearth these bugs, and not let them crash a supercomputing simulation after a month of its running.\n\n \n\nWe now present the scientific outcomes of this work under the Intellectual Merit and Broader Impact categories. These results were obtained over a time period from 2007 to date. One can go to our website (http://www.cs.utah.edu/fv) and obtain a chronologically accurate view of our contributions each year.\n\n \n\nINTELLECTUAL MERIT: This research has lead to the discovery of the fundamental programming laws governing the behavior of MPI commands. By understanding these laws, we can accurately compute the possible execution outcomes of a given MPI program even on supercomputers that have not been built or one does not have access to. This enables a division of labor fundamental to all science and engineering; namely that one can develop MPI-based programs in isolation fully confident of the fact that when future supercomputers are delivered, one can run these programs and encounter no nasty MPI-related programming errors (\"bugs\").\n\n \n\nBROADER IMPACT: This research has helped with the training of five doctoral students who have graduated: Drs. Robert Palmer, Yu Yang, Xiaofang Chen, Sarvani Vakkalanka, and Anh Vo. It has also helped with the training of several undergraduate and Masters Degree level students. It has lead to the release of a highly capable MPI program analysis tool called In Situ Partial order analysis (ISP). It has fed into the National Tool Infrastructure through integration into the Eclipse Parallel Tools Platform (PTP). Our students enjoyed valuable mentorship and training from scientists involved in developing PTP and currently working for IBM Research.  Over 25 pap..."
 }
}