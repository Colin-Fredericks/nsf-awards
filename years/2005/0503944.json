{
 "awd_id": "0503944",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "SCI:    TeraGrid Resource Partners",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rudolf Eigenmann",
 "awd_eff_date": "2005-08-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 14919205.0,
 "awd_amount": 28608933.0,
 "awd_min_amd_letter_date": "2005-08-01",
 "awd_max_amd_letter_date": "2014-02-24",
 "awd_abstract_narration": "The Extensible Terascale Facility (ETF) is the next stage in the evolution of NSF large-scale cyberinfrastructure for enabling high-end computational research. The ETF enables researchers to address the most challenging computational problems by utilizing the integrated resources, data collections, instruments and visualization capabilities of nine resource partners. On October 1, 2004, the ETF concluded a three-year construction effort to create this distributed environment called the TeraGrid (TG) and we are now entering the production operations phase.\r\n\r\nThe TeraGrid resource partners include: the University of Chicago/Argonne National Laboratory, the San Diego Supercomputer Center at UCSD, the Texas Advanced Computing Center at UT-Austin, the National Center for Supercomputing Applications at UIUC, Indiana University, Purdue University, Oak Ridge National Laboratory, and the Pittsburgh Supercomputing Center.\r\n\r\nA separate proposal was submitted to NSF on October 19, 2004 for the TeraGrid Grid Infrastructure Group (GIG). Under the direction of Charlie Catlett at UC/ANL, in general, the GIG will be responsible for coordination of development activities for the TeraGrid with subcontracts to the partner sites. The resource partners (RP) will each have independent cooperative agreements with NSF, but will work closely with the GIG to implement the vision of the TeraGrid.\r\n\r\nThis proposal outlines the plan at SDSC to participate as a resource partner in the TeraGrid team to provide the expanding user community with ongoing access to this computational science facility. This proposal covers the period November 1, 2004 through October 31, 2009.\r\n\r\nSDSC will bring a powerful set of compute and storage resources to the TeraGrid and will focus on the data-intensive computing needs of TeraGrid scientists. SDSC's current TeraGrid resources are summarized with those of other RP sites and include a 3.1 Tflop IA- 64 system, a 1.1 Tflop Power4 system (part of the larger DataStar system), and an experimental 1.3 Tflop IA-64 system. Every SDSC compute node can access data at 200 MBps or more and can use its DB2 client to address structured databases via leading edge servers. The central Storage Area Network (SAN) has over 0.5 PB of disk and a 6 PB archival tape capacity available to all nodes and servers, and SDSC plans to export its capabilities across the TeraGrid Wide Area Network (WAN) [16, 17] using a true global file system. SDSC's 540 TB SAN of online disk will make possible new modes of handling data across the TeraGrid. Generating data locally, competing with others for disk space and saving results to archival storage through an often slow interface all serve as restrictions to scientists. The ability to write directly to a very large file system, where post-processing can occur and where results are automatically archived, is a significant improvement in capability for scientists. For example, the Southern California Earthquake Center (SCEC) [6] has performed an earthquake simulation analyzing the resulting damage from a potential earthquake in the Southern California region.\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Sheddon",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Mark E Sheddon",
   "pi_email_addr": "sheddon@sdsc.edu",
   "nsf_id": "000453844",
   "pi_start_date": "2005-08-01",
   "pi_end_date": "2008-10-22"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Moore",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Richard L Moore",
   "pi_email_addr": "rlm@sdsc.edu",
   "nsf_id": "000292321",
   "pi_start_date": "2008-10-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "178700",
   "pgm_ele_name": "SERVICE ENTERPRISE SYSTEMS"
  },
  {
   "pgm_ele_code": "140300",
   "pgm_ele_name": "Proc Sys, Reac Eng & Mol Therm"
  },
  {
   "pgm_ele_code": "140700",
   "pgm_ele_name": "CFS-Combustion & Fire Systems"
  },
  {
   "pgm_ele_code": "144300",
   "pgm_ele_name": "FD-Fluid Dynamics"
  },
  {
   "pgm_ele_code": "146400",
   "pgm_ele_name": "ESD-Eng & Systems Design"
  },
  {
   "pgm_ele_code": "178600",
   "pgm_ele_name": "MANFG ENTERPRISE SYSTEMS"
  },
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  },
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  },
  {
   "pgm_ele_code": "747600",
   "pgm_ele_name": "XD-Extreme Digital"
  },
  {
   "pgm_ele_code": "761900",
   "pgm_ele_name": "Innovative HPC"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7476",
   "pgm_ref_txt": "ETF"
  },
  {
   "pgm_ref_code": "7556",
   "pgm_ref_txt": "CONFERENCE AND WORKSHOPS"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0105",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2005,
   "fund_oblg_amt": 2700000.0
  },
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 1546965.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 9760784.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 11681552.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 2919632.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has delivered high-performance computational resources to advance the scientific research of academic researchers across the country. We have supported supercomputing resources used by ~10,000 scientists and engineers, resulting in thousands of scientific publications.</p>\n<p>Under this award we have acquired and supported, in whole or in part, the following resources:</p>\n<ul>\n<li><em>DataStar </em>&ndash; an IBM Power4 system with 15 Teraflops (TF) computing capacity.</li>\n<li>Itanium cluster(s) &ndash; Intel systems based on Itanium processors with 4 TF capacity.</li>\n<li><em>BlueGene/L</em> &ndash; an innovative IBM system with 17 TF capacity with a large number of low-power processors. </li>\n<li><em>Trestles</em> &ndash; a Cray system with 100 TF capacity. </li>\n<li><em>GPFS-WAN</em> &ndash; the first-ever production &lsquo;wide-area&rsquo; storage system.</li>\n<li>Archival storage &ndash; a tape-based repository, with 25 petabytes (PB) data capacity.</li>\n</ul>\n<p>Operation of these systems included system administration of cutting-edge hardware, front-line user support, advanced support in helping users optimize their applications, and extensive training, education and outreach efforts.</p>\n<p>Some key technical innovations include:</p>\n<ul>\n<li>Developed the software for a secure global file system (<em>GPFS-WAN</em>), accessible by authorized computers anywhere in the world as if the storage system was local, helping users be more productive. The capability is now widely incorporated into parallel file system software by IBM (GPFS) and the Lustre community.</li>\n<li>In 2006, we established <em>Data Central</em>, providing users dedicated data repositories readily accessible by researchers worldwide. This capability preceded by many years the emerging mandates by federal agencies to make researcher data broadly available. </li>\n<li><em>Trestles</em> was the first NSF supercomputer targeted to the &lsquo;long tail of science&rsquo; &ndash; the large number of modest-scale users rather than the elite few users at the top of the &lsquo;Branscomb pyramid.&rsquo; This laid the groundwork for <em>Comet,</em> which was successfully deployed in 2015 in response to NSF&rsquo;s solicitation for a system to support a more inclusive user base. </li>\n</ul>\n<p>We have supported thousands of students at the high school and college level with classes and summer intern programs, including programs targeting women and under-represented minorities. We have hosted hundreds of computational science training events and annual in-depth summer institutes. With <em>Trestles</em>, we championed computation for new and non-traditional computational users. And we have supported science gateways, which democratize access to scientific tools. For example, with no assistance from us, a high school student won the Massachusetts science fair using the CIPRES phylogenetics gateway running on the <em>Trestles</em> supercomputer.</p>\n<p>This project provides the computational infrastructure needed by scientists to conduct their research. As such, our greatest scientific impact and intellectual merit lies in the science produced by our users. We have supported ~10,000 academic researchers across virtually all fields of science, and those researchers have published ~2,000 papers that relied on our resources.</p>\n<p>&nbsp;</p>\n<p>The following are just a few examples of users&rsquo; research.</p>\n<ul>\n<li>The Southern California Earthquake Center (SCEC) has pioneered physics-based simulations of earthquake-induced ground motion. Such simulations underpin improved seismic hazard estimates, better building codes, and safer structural designs, potentially saving lives, property and costs. Many of SCEC&rsquo;s initial simulations ran on SDSC resources, and SCEC is now a major user of the largest DOE supercomputers. </li>\n<li>New drug targets were desperately needed circa 2007 to combat ...",
  "por_txt_cntn": "\nThis project has delivered high-performance computational resources to advance the scientific research of academic researchers across the country. We have supported supercomputing resources used by ~10,000 scientists and engineers, resulting in thousands of scientific publications.\n\nUnder this award we have acquired and supported, in whole or in part, the following resources:\n\nDataStar &ndash; an IBM Power4 system with 15 Teraflops (TF) computing capacity.\nItanium cluster(s) &ndash; Intel systems based on Itanium processors with 4 TF capacity.\nBlueGene/L &ndash; an innovative IBM system with 17 TF capacity with a large number of low-power processors. \nTrestles &ndash; a Cray system with 100 TF capacity. \nGPFS-WAN &ndash; the first-ever production \u00e6wide-area\u00c6 storage system.\nArchival storage &ndash; a tape-based repository, with 25 petabytes (PB) data capacity.\n\n\nOperation of these systems included system administration of cutting-edge hardware, front-line user support, advanced support in helping users optimize their applications, and extensive training, education and outreach efforts.\n\nSome key technical innovations include:\n\nDeveloped the software for a secure global file system (GPFS-WAN), accessible by authorized computers anywhere in the world as if the storage system was local, helping users be more productive. The capability is now widely incorporated into parallel file system software by IBM (GPFS) and the Lustre community.\nIn 2006, we established Data Central, providing users dedicated data repositories readily accessible by researchers worldwide. This capability preceded by many years the emerging mandates by federal agencies to make researcher data broadly available. \nTrestles was the first NSF supercomputer targeted to the \u00e6long tail of science\u00c6 &ndash; the large number of modest-scale users rather than the elite few users at the top of the \u00e6Branscomb pyramid.\u00c6 This laid the groundwork for Comet, which was successfully deployed in 2015 in response to NSF\u00c6s solicitation for a system to support a more inclusive user base. \n\n\nWe have supported thousands of students at the high school and college level with classes and summer intern programs, including programs targeting women and under-represented minorities. We have hosted hundreds of computational science training events and annual in-depth summer institutes. With Trestles, we championed computation for new and non-traditional computational users. And we have supported science gateways, which democratize access to scientific tools. For example, with no assistance from us, a high school student won the Massachusetts science fair using the CIPRES phylogenetics gateway running on the Trestles supercomputer.\n\nThis project provides the computational infrastructure needed by scientists to conduct their research. As such, our greatest scientific impact and intellectual merit lies in the science produced by our users. We have supported ~10,000 academic researchers across virtually all fields of science, and those researchers have published ~2,000 papers that relied on our resources.\n\n \n\nThe following are just a few examples of users\u00c6 research.\n\nThe Southern California Earthquake Center (SCEC) has pioneered physics-based simulations of earthquake-induced ground motion. Such simulations underpin improved seismic hazard estimates, better building codes, and safer structural designs, potentially saving lives, property and costs. Many of SCEC\u00c6s initial simulations ran on SDSC resources, and SCEC is now a major user of the largest DOE supercomputers. \nNew drug targets were desperately needed circa 2007 to combat the deadly bird flu virus, with the only drugs then available &ndash; Tamiflu and Relenza &ndash; in limited supply and some virus strains already resistant. Andy McCammon and collaborators at UCSD used our supercomputers to identify promising new drug targets. In addition, the group identified several potential inhibitors that might represent new drugs against avian flu..."
 }
}