{
 "awd_id": "9972464",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Model and Motion Libraries from Video",
 "cfda_num": "47.070",
 "org_code": "05010200",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Robert B Grafton",
 "awd_eff_date": "1999-07-15",
 "awd_exp_date": "2004-06-30",
 "tot_intn_awd_amt": 387000.0,
 "awd_amount": 387000.0,
 "awd_min_amd_letter_date": "1999-03-16",
 "awd_max_amd_letter_date": "2003-03-17",
 "awd_abstract_narration": "9972464   Wilhelms, Jane\r\nThis project investigates new methods of extracting general, three-dimensional human and animal models and their motion from  digitized video,  in order to create an accurate and realistic virtual library.  Realistic, computer-animated humans and animals are important in computer graphics and many other fields, including biomechanics, biology, surgical planning, sports medicine, engineering, education, training, and ergonomics.  Natural human movement simulation is important in the ergonomic design of environments such as cars, planes, and work places.  While much work has been done in the anthropomorphic study of the range of human body types, less is known about the natural range of comfortable motion among individuals in specific environments.  Locomotion studies of humans and animals would profit from three-dimensional representations that allow quantitative comparisons between recorded motion of different individuals and species.  Humans and animals lend verisimilitude and interest to any computer-generated scene.  Videotape already records massive amounts of information on the free movement of humans and many animal species;  what is needed are ways to extract quantitative information from these records.  However, generating the realistic motion of complex articulated structures is extremely difficult.  Trained animators can produce realistic motion using laborious keyframing techniques, but results are not useful for scientific study.  Considerable research has been done in the last 15 years in attempting to use constraints, physical simulation, and higher-level procedural specification to partially automate the process with some encouraging but limited results.  Studio motion capture techniques can provide exact motion specification for single individuals, at considerable cost, but work has just begun on generalizing these results.  Computer vision is developing automatic recognition approaches, but at this point results are only reliable for simplified cases and environments.  Methods of extracting accurate three-dimensional motion from video filmed using a single camera in any general environment will provide the necessary general tool for turning recorded motion into a quantitatively useful form.  This project will bring together several research areas to attack the problem of extracting and representing articulated motion.  The process will combine parametric, hierarchical, and constraint-based methods from computer graphics, physiological insights from biology and biomechanics, and model-based image-by-synthesis approaches from computer vision.  The process will be partially automated, but user input will provide necessary initial conditions and alterations when necessary.\r\nThe result will be a library of accurate human and animal models accompanied by realistic parameterized, hierarchical motion descriptions.  Scientists in other research areas, such as biology, biomechanics, anthropology, and vision, both at the University of California, Santa Cruz, and elsewhere will be involved.  The virtual library will be made available, and software will be modular to facilitate integration with existing systems.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jane",
   "pi_last_name": "Wilhelms",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jane Wilhelms",
   "pi_email_addr": "wilhelms@cse.ucsc.edu",
   "nsf_id": "000334351",
   "pi_start_date": "1999-03-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Cruz",
  "inst_street_address": "1156 HIGH ST",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA CRUZ",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8314595278",
  "inst_zip_code": "950641077",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "CA19",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA SANTA CRUZ",
  "org_prnt_uei_num": "",
  "org_uei_num": "VXUFPE4MCZH5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Cruz",
  "perf_str_addr": "1156 HIGH ST",
  "perf_city_name": "SANTA CRUZ",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "950641077",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "CA19",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "192000",
   "pgm_ele_name": "RESEARCH/TESTBEDS"
  },
  {
   "pgm_ele_code": "286500",
   "pgm_ele_name": "NUMERIC, SYMBOLIC & GEO COMPUT"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0198",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0198",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0199",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0199",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 1999,
   "fund_oblg_amt": 387000.0
  }
 ],
 "por": null
}