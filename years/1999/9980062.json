{
 "awd_id": "9980062",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "KDI: Temporal Abstraction in Reinforcement Learning",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Radhakisan Baheti",
 "awd_eff_date": "1999-09-15",
 "awd_exp_date": "2003-08-31",
 "tot_intn_awd_amt": 560000.0,
 "awd_amount": 560000.0,
 "awd_min_amd_letter_date": "1999-08-18",
 "awd_max_amd_letter_date": "1999-08-18",
 "awd_abstract_narration": "This project investigates a new approach to learning, planning, and representing knowledge at multiple levels of temporal abstraction.  It develops methods by which an artificial reinforcement learning system can model and reason about persistent courses of action and perceive its environment in corresponding terms, and it develops and examines the validity of models of animal behavior related to this approach.  The project's objectives are to develop the mathematical theory of the approach, to refine, extend, and conduct validation studies of related models of animal behavior, to examine the theory's relationship to control theory and artificial intelligence, and to demonstrate its effectiveness in a number of simulated learning tasks.\r\n\r\nMost current reinforcement learning (RL) research uses a framework in which an agent has to take a sequence of actions paced by a single, fixed time step: actions take one step to complete, and their immediate consequences become available after one step (modeled as a Markov decision process, or MDP).  This makes it difficult to learn and plan at different time scales. Some RL research instead uses a generalization of this framework (semi-Markov decision processes, or SMDPs) in which actions take varying amounts of time to complete, and the existing theory specifies how to model the results of these actions and how to plan with them.  However, this approach is limited because temporally extended actions are treated as indivisible and unknown units.  For the greatest flexibility and best performance, it is necessary to look inside temporally extended actions to examine or modify how they are comprised of lower-level actions, which is not considered in existing approaches.\r\n\r\nThis project, by contrast, will model extended courses of action as SMDP actions overlaid upon a base MDP.  These courses of action, called options, can then be treated as if they were primitive actions; existing RL can be applied almost unchanged.  This approach enables options to be analyzed at both the MDP and SMDP levels and introduces new issues at the interface between the levels.  This approach is appealing because of its simplicity, similarity to previous approaches using primitive actions, and its solid mathematical foundation in MDP and SMDP theory.  This is being developed further into a general approach to hierarchical and multi-time-scale planning and learning.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Barto",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew G Barto",
   "pi_email_addr": "barto@cs.umass.edu",
   "nsf_id": "000201472",
   "pi_start_date": "1999-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Moore",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "John W Moore",
   "pi_email_addr": "jwmoore@psych.umass.edu",
   "nsf_id": "000301764",
   "pi_start_date": "1999-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "101 COMMONWEALTH AVE",
  "perf_city_name": "AMHERST",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039252",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "887700",
   "pgm_ele_name": "KDI-COMPETITION"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1347",
   "pgm_ref_txt": "KDI/LIS"
  },
  {
   "pgm_ref_code": "8877",
   "pgm_ref_txt": "KDI-COMPETITION"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0199",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0199",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 1999,
   "fund_oblg_amt": 560000.0
  }
 ],
 "por": null
}