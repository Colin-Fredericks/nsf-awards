{
 "awd_id": "1003228",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "An Application Driven I/O Optimization Approach for PetaScale Systems and Scientific Discoveries",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-06-01",
 "awd_exp_date": "2014-05-31",
 "tot_intn_awd_amt": 299863.0,
 "awd_amount": 299863.0,
 "awd_min_amd_letter_date": "2010-04-21",
 "awd_max_amd_letter_date": "2013-05-23",
 "awd_abstract_narration": "Application workflows typically involve large-scale simulations, applications, and subsequent analysis, verification and validation. One of the most important requirements shared by various applications running on petascale systems is fast, portable, scalable I/O which is componentized, metadata rich and easy to use. The Adaptable I/O System (ADIOS) serves as a high-level I/O interface for an application to select which I/O libraries and formats to use without changing the application program. Codes such as GTC generate hundreds of TBs of data on hundreds of thousands of cores in a twenty four hour period. One must therefore optimize the I/O both for fast output in the generation phase and for fast input in the analysis phase. Both the writing and reading efficiency of I/O are critical for knowledge discovery. Development of a high level software infrastructure to allow optimization of I/O for entire workflows (including High-Performance I/O when reading data with different patterns) would greatly improve end-to-end performance in the knowledge discovery cycle. \r\nThis project plans to develop efficient I/O methods which will enable application scientists to optimize data for writing, and which will be able to re-organize the data to obtain optimal performance for common reading patterns used by scientists.  This project directly impacts the I/O performance of many petascale applications, including the GTC, GTS, XGC-1, Chimera, and S3D codes, and work directly with these teams to optimize the I/O in all stages of their scientific workflow.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Scott",
   "pi_last_name": "Klasky",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Scott A Klasky",
   "pi_email_addr": "klasky@tennessee.edu",
   "nsf_id": "000529121",
   "pi_start_date": "2010-04-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Micah",
   "pi_last_name": "Beck",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Micah D Beck",
   "pi_email_addr": "mbeck@utk.edu",
   "nsf_id": "000340028",
   "pi_start_date": "2010-04-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "201 ANDY HOLT TOWER",
  "perf_city_name": "KNOXVILLE",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "747600",
   "pgm_ele_name": "XD-Extreme Digital"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 299863.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"text-align: left;\">The main goals of this project was to research and develop an application-driven approach to enable large-scale Input/Ouput (I/O) that is fast, portable, scalable, metadata rich, and easy to use for scientific workflows. We focused on applications which read and write petabytes of data in a year, during both the checkpoint/restart phase of the large scale simulations, as well as for data that was used for post-processing. My students and postdocs were key members of our 2013 R&amp;D 100 award for the Adaptable I/O System (ADIOS). We produced five papers from this project, one nominated for the best student paper at SC 2012, and one which one my student first place in the ACM Graduate Student research competition in 2012.</p>\n<p style=\"text-align: left;\">&nbsp;</p>\n<p style=\"text-align: left;\">The key to our work was in speeding up the I/O of many simulations and experiments, which is shown in (<a href=\"http://science.energy.gov/~media/ascr/images/ADIOS.jpg\">http://science.energy.gov/~media/ascr/images/ADIOS.jpg</a>). For almost every simulation which we worked with, we were able to speed up the I/O by over 10X from other state-of-the-art techniques, including MPI-IO, pnetcdf, and parallel HDF5. In this project, we worked with many codes, including climate and weather codes used by NASA and Chinese researchers (GEOS5, GRAPES), and we were able to show that by laying out the data on parallel file systems using a Hilbert curve, over the space-time elements, we could dramatically reduce the speed of reading the data during analysis and visualization. Furthermore, we used machine learning techniques (Deterministic Annealing) to learn the I/O access patterns when users were visualizing their data, and then pre-fetch their data to eliminate much of the reading overhead during interactive visualization.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/13/2014<br>\n\t\t\t\t\tModified by: Scott&nbsp;A&nbsp;Klasky</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The main goals of this project was to research and develop an application-driven approach to enable large-scale Input/Ouput (I/O) that is fast, portable, scalable, metadata rich, and easy to use for scientific workflows. We focused on applications which read and write petabytes of data in a year, during both the checkpoint/restart phase of the large scale simulations, as well as for data that was used for post-processing. My students and postdocs were key members of our 2013 R&amp;D 100 award for the Adaptable I/O System (ADIOS). We produced five papers from this project, one nominated for the best student paper at SC 2012, and one which one my student first place in the ACM Graduate Student research competition in 2012.\n \nThe key to our work was in speeding up the I/O of many simulations and experiments, which is shown in (http://science.energy.gov/~media/ascr/images/ADIOS.jpg). For almost every simulation which we worked with, we were able to speed up the I/O by over 10X from other state-of-the-art techniques, including MPI-IO, pnetcdf, and parallel HDF5. In this project, we worked with many codes, including climate and weather codes used by NASA and Chinese researchers (GEOS5, GRAPES), and we were able to show that by laying out the data on parallel file systems using a Hilbert curve, over the space-time elements, we could dramatically reduce the speed of reading the data during analysis and visualization. Furthermore, we used machine learning techniques (Deterministic Annealing) to learn the I/O access patterns when users were visualizing their data, and then pre-fetch their data to eliminate much of the reading overhead during interactive visualization.\n\n\t\t\t\t\tLast Modified: 08/13/2014\n\n\t\t\t\t\tSubmitted by: Scott A Klasky"
 }
}