{
 "awd_id": "1047879",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SI2-SSI: Accelerating the Pace of Research through Implicitly Parallel Programming",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2010-10-01",
 "awd_exp_date": "2016-03-31",
 "tot_intn_awd_amt": 1740214.0,
 "awd_amount": 1740214.0,
 "awd_min_amd_letter_date": "2010-09-13",
 "awd_max_amd_letter_date": "2010-09-13",
 "awd_abstract_narration": "Today, two trends conspire to slow down the pace of science, engineering, and academic research progress in general.  First, researchers increasingly rely on computation to process ever larger data sets and to perform ever more computationally-intensive simulations.  Second, individual processor speeds are no longer increasing with every computer chip generation as they once were.  To compensate, processor manufacturers have moved to including more processors, or cores, on a chip with each generation.  To obtain peak performance on these multicore chips, software must be implemented so that it can execute in parallel and thereby use the additional processor cores. Unfortunately, writing efficient, explicitly parallel software programs using today's software-development tools takes advanced training in computer science, and even with such training, the task remains extremely difficult, error-prone, and time consuming.  This project will create a new high-level programming platform, called Implicit Parallel Programming (IPP), designed to bring the performance promises of modern multicore machines to scientists and engineers without the costs associated with having to teach these users how to write explicitly parallel programs.  In the short term, this research will provide direct and immediate benefit to researchers in several areas of science as the PIs will pair computer science graduate students with non-computer science graduate students to study, analyze, and develop high-value scientific applications.  In the long term, this research has the potential to fundamentally change the way scientists obtain performance from parallel machines, improve their productivity, and accelerate the overall pace of science.  This work will also have major educational impact by developing courseware and tutorial materials, useable by all scientists and engineers, on the topics of explicit and implicit parallel computing.\r\n\r\nIPP will operate by allowing users to write ordinary sequential programs and then to augment them with logical specifications that expand (or abstract) the set of sequential program behaviors.  This capacity for abstraction will provide parallelizing compilers with the flexibility to more aggressively optimize programs than would otherwise be possible.  In fact, it will enable effective parallelization techniques where they were impossible before.  The language design and compiler implementation will be accompanied by formal semantic analysis that will be used to judge the correctness of compiler transformations, provide a foundation for about reasoning programs, and guide the creation of static analysis and program defect detection algorithms.  Moreover since existing programs and languages can be viewed as (degenerately) implicitly parallel, decades of investment in human expertise, languages, compilers, methods, tools, and applications is preserved.  In particular, it will be possible to upgrade old legacy programs or libraries from slow sequential versions without overhauling the entire system architecture, but merely by adding a few auxiliary specifications.  Compiler technology will help guide scientists and engineers through this process, further simplifying the task.  Conceptually, IPP restores an important layer of abstraction, freeing programmers to write high-level code, designed to be easy to understand, rather than low-level code, architected according to the specific demands of a particular parallel machine.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "August",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "David I August",
   "pi_email_addr": "august@cs.princeton.edu",
   "nsf_id": "000192343",
   "pi_start_date": "2010-09-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Walker",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Walker",
   "pi_email_addr": "dpw@cs.princeton.edu",
   "nsf_id": "000167001",
   "pi_start_date": "2010-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "1 NASSAU HALL",
  "perf_city_name": "PRINCETON",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "2878",
   "pgm_ref_txt": "SPECIAL PROJECTS - CCF"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 1740214.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><blockquote>\n<div>\n<blockquote>\n<p>The goal of this project is to accelerate the pace of research through implicitly parallel programming. The project began with an extensive survey and analysis of the practice of computer science by computational scientists and researchers at a highly-ranked research university. &nbsp;As part of the survey, project personnel interviewed 114 researchers from diverse fields of natural science, engineering, interdisciplinary sciences, and social sciences. The survey covered three major themes central to scientific computing: (a) programming practices, (b) computational time and resource use, and (c) performance-enhancing methods.</p>\n<p>The survey yielded several important findings. The most important insight was that scientists typically don't have the time, desire, or skills to write parallel applications. &nbsp;This is a problem because the majority of the sequential applications they do write to conduct research take days, weeks, or months to execute. Consequently, the pace of research is often limited by slow application execution time. &nbsp;The survey revealed that most researchers run their programs only on desktops, even when clusters were available to them, and that almost all of them run single threaded code on these desktops despite the presence of multicore processors. The findings of the survey guided the automatic parallelization techniques developed during this project.</p>\n<p>Following the survey, the project focused on the development of automatic parallelizations tools that transformed unoptimized sequential programs into efficient, parallel programs. These tools and methods enabled researchers to focus on the core of their algorithm without having to worry about the low-level details of the program, thereby improving their productivity.</p>\n<p>The project also resulted in the integration of several new techniques into a maturing automatic parallelization framework. The Fast DAG_SCC algorithm improved the scalability of key algorithms in parallelizing compilers, enabling the compiler to analyze larger program scopes and more efficiently use stronger, more computationally expensive program analyses. The concept of context-sensitive speculation allowed for more efficient parallelization of complex programs. Development of runtime and hardware support for multi-threaded transactions enabled support for efficient speculation on both desktops and clusters. This allowed for faster and more scalable parallelization of existing workloads. Another new automatic technique called CGCM (CPU-GPU Communication Manager) enhanced GPU execution by optimizing CPU-GPU communication. Finally, the development of the implicitly parallel programming model enabled the compiler to leverage programmer insights to transform sequential code into well-performing parallel versions.</p>\n<p>The project team applied various combinations of these techniques to real programs used by researchers to make a broader impact. For instance, the survey found the use of scripting languages to be very common among researchers. Consequently, project personnel developed and applied aggressive optimizations to frequently used scripting languages such as Python, Perl, and Lua. The results, deployed in real programs, helped researchers in various disciples such as epidemiology, hardware verification, and computational biology. For example, the application of these techniques to epidemiology enabled scientists there to better gain an understanding of the spread of diseases and ways to control them.</p>\n<p>Beyond the application of project artifacts to scientific research, the project also contributed in several ways to human resource development. &nbsp;A new graduate course on parallelism taught students how to develop high-performance applications using state-of-the-art language and compiler technology--an often under-taught part of common computer science curricula. Additionally, the project also funded several outreach activities that aimed to popularize the appeal of STEM fields among elementary school students.</p>\n</blockquote>\n</div>\n</blockquote><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/18/2016<br>\n\t\t\t\t\tModified by: David&nbsp;I&nbsp;August</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nThe goal of this project is to accelerate the pace of research through implicitly parallel programming. The project began with an extensive survey and analysis of the practice of computer science by computational scientists and researchers at a highly-ranked research university.  As part of the survey, project personnel interviewed 114 researchers from diverse fields of natural science, engineering, interdisciplinary sciences, and social sciences. The survey covered three major themes central to scientific computing: (a) programming practices, (b) computational time and resource use, and (c) performance-enhancing methods.\n\nThe survey yielded several important findings. The most important insight was that scientists typically don't have the time, desire, or skills to write parallel applications.  This is a problem because the majority of the sequential applications they do write to conduct research take days, weeks, or months to execute. Consequently, the pace of research is often limited by slow application execution time.  The survey revealed that most researchers run their programs only on desktops, even when clusters were available to them, and that almost all of them run single threaded code on these desktops despite the presence of multicore processors. The findings of the survey guided the automatic parallelization techniques developed during this project.\n\nFollowing the survey, the project focused on the development of automatic parallelizations tools that transformed unoptimized sequential programs into efficient, parallel programs. These tools and methods enabled researchers to focus on the core of their algorithm without having to worry about the low-level details of the program, thereby improving their productivity.\n\nThe project also resulted in the integration of several new techniques into a maturing automatic parallelization framework. The Fast DAG_SCC algorithm improved the scalability of key algorithms in parallelizing compilers, enabling the compiler to analyze larger program scopes and more efficiently use stronger, more computationally expensive program analyses. The concept of context-sensitive speculation allowed for more efficient parallelization of complex programs. Development of runtime and hardware support for multi-threaded transactions enabled support for efficient speculation on both desktops and clusters. This allowed for faster and more scalable parallelization of existing workloads. Another new automatic technique called CGCM (CPU-GPU Communication Manager) enhanced GPU execution by optimizing CPU-GPU communication. Finally, the development of the implicitly parallel programming model enabled the compiler to leverage programmer insights to transform sequential code into well-performing parallel versions.\n\nThe project team applied various combinations of these techniques to real programs used by researchers to make a broader impact. For instance, the survey found the use of scripting languages to be very common among researchers. Consequently, project personnel developed and applied aggressive optimizations to frequently used scripting languages such as Python, Perl, and Lua. The results, deployed in real programs, helped researchers in various disciples such as epidemiology, hardware verification, and computational biology. For example, the application of these techniques to epidemiology enabled scientists there to better gain an understanding of the spread of diseases and ways to control them.\n\nBeyond the application of project artifacts to scientific research, the project also contributed in several ways to human resource development.  A new graduate course on parallelism taught students how to develop high-performance applications using state-of-the-art language and compiler technology--an often under-taught part of common computer science curricula. Additionally, the project also funded several outreach activities that aimed to popularize the appeal of STEM fields among elementary school students.\n\n\n\n\n\t\t\t\t\tLast Modified: 08/18/2016\n\n\t\t\t\t\tSubmitted by: David I August"
 }
}