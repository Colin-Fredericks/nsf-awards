{
 "awd_id": "1017134",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI:  Small:  Vision-Based Mobile Manipulation and Navigation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2010-08-15",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 449271.0,
 "awd_amount": 449271.0,
 "awd_min_amd_letter_date": "2010-08-19",
 "awd_max_amd_letter_date": "2015-08-31",
 "awd_abstract_narration": "This project focuses on tackling a critical barrier to long-term autonomy for robotic systems, namely the lack of theoretically well-founded self-calibration methods for inertial and vision-based sensors, commonly found on sophisticated robots. The project is motivated by the vision of power-up-and-go robotic systems that are able to operate autonomously for long periods without requiring tedious manual sensor calibration. The research team addresses this problem in the context of vision-based mobile manipulation and navigation. The core foci of the work are: 1. the development of a unified mathematical theory of anytime, automatic calibration for visual-inertial systems, and 2. an experimental characterization of the resulting algorithms with state-of-the-art, sophisticated robots of significant diversity (humanoids performing mobile manipulation and autonomous ground vehicles navigating outdoors). Inertial sensing is critically important for humanoid balance control, while visual sensing relates the 3D world to the robot's body coordinates thereby enabling manipulation.  In the case of autonomous ground vehicles, monocular and stereo camera calibration is still commonly performed manually using a known calibration target. The project obviates the need for this requirement. The expected outcomes of the project are: 1. a theoretical foundation for humanoid robots to function autonomously in unstructured environments over significant periods of time, and 2. new navigation algorithms for ground vehicles allowing them to see further with greater acuity. The project explicitly incorporates undergraduate research in cooperation with an REU site currently operational at the USC Computer Science Department.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gaurav",
   "pi_last_name": "Sukhatme",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gaurav Sukhatme",
   "pi_email_addr": "gaurav@cs.usc.edu",
   "nsf_id": "000489305",
   "pi_start_date": "2010-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stefan",
   "pi_last_name": "Schaal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefan Schaal",
   "pi_email_addr": "sschaal@usc.edu",
   "nsf_id": "000106410",
   "pi_start_date": "2010-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S FLOWER ST FL 3",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "90033",
  "perf_ctry_code": "US",
  "perf_cong_dist": "34",
  "perf_st_cong_dist": "CA34",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 449271.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our vision is to enable power-up-and-go robotic systems that are able to operate autonomously for long periods without requiring tedious manual sensor (re-) calibration. We work to accomplishing this for a diverse set of platforms ranging from humanoids to outdoor autonomous ground vehicles. Superficially, the problems associated with these vehicles may appear disparate. However, we have argued that the underlying fundamental capability for the long-term successful functioning of almost all robots is their ability to keep their sensor systems calibrated, both intrinsically and relative to each other. Our focus here is thus two-fold. We are working on a unified mathematical theory of anytime, automatic calibration for visual-inertial systems, and experimentally characterizing the resulting algorithms with state-of-the-art, sophisticated robots of significant diversity.</p>\n<p>In terms of the complexity of the dynamics, humanoid robots are one of the most challenging platforms for accurate navigation and manipulation. Inertial sensing is critically important for bipedal balancing and reactive control, while visual sensing relates the 3D world to body coordinates and enables manipulation. Thus, accurate pose estimation and accurate registration of the visual world relative to this pose is critical. In the case of mobile robots, there are several examples of visual-inertial systems that have been built. However monocular and stereo camera calibration was still commonly performed manually using a known calibration target. This need not be the case. We focus on developing and extending target-free calibration to incorporate the camera intrinsic and distortion parameters.&nbsp;</p>\n<p>Starting more from the viewpoint of calibration of sensors for robotics, this project made tremendous progress over the funding period, towards a new topics of robotics, called interactive pereption. It really became clear that we need to address the complete preception-action-learning loop to understand how to calibrate sensors, how to use sensory information for action generation, and how to action generation to improve sensor calibration and perception. &nbsp;It is really this intergrated perception-action-learning loop that has become the coherent theme of our research, and that warrants future research and funding. It has also become very clear that education of students in robotics cannot focus solely on robotics topics anymore, but rather focus on a broad education on robot perception, robot learning with machine learning techniques, and robot control that is inherently coupled to perception and learning topics.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/07/2016<br>\n\t\t\t\t\tModified by: Stefan&nbsp;Schaal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur vision is to enable power-up-and-go robotic systems that are able to operate autonomously for long periods without requiring tedious manual sensor (re-) calibration. We work to accomplishing this for a diverse set of platforms ranging from humanoids to outdoor autonomous ground vehicles. Superficially, the problems associated with these vehicles may appear disparate. However, we have argued that the underlying fundamental capability for the long-term successful functioning of almost all robots is their ability to keep their sensor systems calibrated, both intrinsically and relative to each other. Our focus here is thus two-fold. We are working on a unified mathematical theory of anytime, automatic calibration for visual-inertial systems, and experimentally characterizing the resulting algorithms with state-of-the-art, sophisticated robots of significant diversity.\n\nIn terms of the complexity of the dynamics, humanoid robots are one of the most challenging platforms for accurate navigation and manipulation. Inertial sensing is critically important for bipedal balancing and reactive control, while visual sensing relates the 3D world to body coordinates and enables manipulation. Thus, accurate pose estimation and accurate registration of the visual world relative to this pose is critical. In the case of mobile robots, there are several examples of visual-inertial systems that have been built. However monocular and stereo camera calibration was still commonly performed manually using a known calibration target. This need not be the case. We focus on developing and extending target-free calibration to incorporate the camera intrinsic and distortion parameters. \n\nStarting more from the viewpoint of calibration of sensors for robotics, this project made tremendous progress over the funding period, towards a new topics of robotics, called interactive pereption. It really became clear that we need to address the complete preception-action-learning loop to understand how to calibrate sensors, how to use sensory information for action generation, and how to action generation to improve sensor calibration and perception.  It is really this intergrated perception-action-learning loop that has become the coherent theme of our research, and that warrants future research and funding. It has also become very clear that education of students in robotics cannot focus solely on robotics topics anymore, but rather focus on a broad education on robot perception, robot learning with machine learning techniques, and robot control that is inherently coupled to perception and learning topics. \n\n \n\n\t\t\t\t\tLast Modified: 12/07/2016\n\n\t\t\t\t\tSubmitted by: Stefan Schaal"
 }
}