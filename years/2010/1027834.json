{
 "awd_id": "1027834",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  CDI-Type I:  Computational Models for the Automatic Recognition of Non-Human Primate Social Behaviors",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032928759",
 "po_email": "jyellen@nsf.gov",
 "po_sign_block_name": "John Yellen",
 "awd_eff_date": "2010-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 577773.0,
 "awd_amount": 577773.0,
 "awd_min_amd_letter_date": "2010-09-27",
 "awd_max_amd_letter_date": "2015-10-20",
 "awd_abstract_narration": "The goal of this project is to develop methods that will permit researchers to remotely and automatically monitor behavior of primates and other highly social animals. The PIs will collect behavioral data from cameras and microphones. They will then develop statistical models and computational algorithms to track the individuals in the group and to recognize facial expressions and vocalizations. Patterns in movements, expressions, and vocalizations will be used to develop behavior-identifying algorithms that will recognize different behaviors such as aggression, submission, grooming, eating and sleeping. The project is a collaboration between computer scientists and primatologists. A key element of this project is the observation that complex social interactions can often be regarded as being composed of sequences of elementary behaviors which occur frequently and consist of relatively simple and distinct gestures. Thus, the task of modeling complex social interactions can be broken down into two regimes - elementary behaviors spanning short duration, and their stochastic sequences spanning relatively longer time duration. \r\n\r\nApart from advancing computational science, the new methods for recording behavior unobtrusively and analyzing them at a high data rate are likely to be of interest to behavioral ecologists, socio-biologists and neuroscientists in studies of primates and other highly social animals. With these new tools, scientists can study and understand behavior, for example, in the context of planning conservation efforts for threatened species, building accurate animal models for health research, and supporting animal husbandry decisions in zoos. The project will provide an extensive, annotated data repository and associated algorithms and will also fund graduate students who will gain hands-on training in all aspects of the project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Izhak",
   "pi_last_name": "Shafran",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Izhak Shafran",
   "pi_email_addr": "zakshafran@gmail.com",
   "nsf_id": "000096166",
   "pi_start_date": "2010-09-27",
   "pi_end_date": "2014-04-02"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Kain",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander Kain",
   "pi_email_addr": "kaina@ohsu.edu",
   "nsf_id": "000296505",
   "pi_start_date": "2014-04-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kristine",
   "pi_last_name": "Coleman",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Kristine D Coleman",
   "pi_email_addr": "colemank@ohsu.edu",
   "nsf_id": "000156985",
   "pi_start_date": "2010-09-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Klaus",
   "pi_last_name": "Miczek",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Klaus Miczek",
   "pi_email_addr": "klaus.miczek@tufts.edu",
   "nsf_id": "000081152",
   "pi_start_date": "2010-09-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kathleen",
   "pi_last_name": "Grant",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Kathleen A Grant",
   "pi_email_addr": "grantka@ohsu.edu",
   "nsf_id": "000520011",
   "pi_start_date": "2010-09-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon Health & Science University",
  "inst_street_address": "3181 SW SAM JACKSON PARK RD",
  "inst_street_address_2": "",
  "inst_city_name": "PORTLAND",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5034947784",
  "inst_zip_code": "972393011",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OR01",
  "org_lgl_bus_name": "OREGON HEALTH & SCIENCE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPSNT86JKN51"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon Health & Science University",
  "perf_str_addr": "3181 SW SAM JACKSON PARK RD",
  "perf_city_name": "PORTLAND",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "972393011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OR01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "775000",
   "pgm_ele_name": "CDI TYPE I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7721",
   "pgm_ref_txt": "FROM DATA TO KNOWLEDGE"
  },
  {
   "pgm_ref_code": "7722",
   "pgm_ref_txt": "COMPLEXITY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 577773.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Enabling machines to unobtrusively observe and model social interactions effectively has the potential to transform our understanding of behaviors of highly social animals. Currently, observation of social behaviors in animals is achieved by having a human identify observations. Having a human present can greatly alter the behavior of the subjects, and even highly trained observers do not always catch every behavior, particularly complex behaviors that may occur simultaneously. Even with video recordings, coding behaviors can be prohibitively time and labor intensive, limiting the ability to record animals continuously for days. Thus, one of the key factors limiting progress in animal behavior research is the rate at which data can be gathered and analyzed. Understanding and modeling behaviors such as dominance, mating and courtship behavior, and parental behaviors is critically important in improving conservation efforts for threatened species, building accurate animal models for health research, and supporting animal husbandry decisions in zoos. </span></p>\n<p><span>The goals of this project were: (a) to develop an audio-visual platform for recording the activities of groups of macaques with sufficient fidelity to allow both humans and machines to subsequently recognize their behaviors, (b) to automatically recognize the vocalization of the individuals in the group, (c) to track the movements of the individuals, and (d) to recognize their behaviors automatically using machines. </span></p>\n<p><span>Dr. Grant and Dr. Coleman, the primatologists on this project, designed the animal study, which consisted of observing 4-6 monkeys in a group over a period of a week as they established their social hierarchy. This was followed by an additional week of observation where the hierarchy was mildly perturbed by unfamiliar human presence, food treats and removing one of the members. Dr. Shafran, with help from Dr. Erdogmus and Dr. Hunt, led the effort in developing a data collection platform. </span></p>\n<p><span>The recording setup consisted of a special pen (12' long x 7' deep x 7' tall) whose two sides and ceiling were made of metal wire mesh, on which we mounted four cameras (GC1380CH, 2/3&rsquo; CCD) in a manner that minimized occlusion ? three with wide-aperture lenses (Optron 5mm f/2.8) near the vertical edges and one with wide-angle fisheye lens (Edmunds Optics NT62-274, focal length 1.8mm, F1.4,185 x 185 degrees) on the ceiling, as illustrated in the figure.&nbsp;</span></p>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 2\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>The vocalizations were captured using a tiny programmable recorder (EDIC B21 Mini Recorder, about 40 x 15 x 10 mm in size that was concealed in the collar of each monkey. In all, we collected contains 80 days of social behaviors of groups of monkeys. </span></p>\n<p><span>The approach we adopted was to learn the necessary models for recognizing behaviors automatically from the recorded data. This task is somewhat simplified by the observation that complex social interactions can be broken down into relatively simple elementary behaviors stitched together in different sequences. Thus, we focused on recognizing a set of elementary behaviors specified by the primatologists, including but not limited to aggression, chasing, fear grimace, grooming and threat screams. </span></p>\n<p><span>Dr Shafran and his lab developed models and algorithms to process the audio recordings of the monkeys behaviors (in all 3800 hours of audio, each session 12 hours long). For efficiency, the audio was processed in multiple stages. The signal was enhanced in the spectral domain by subtracting the stationary or slowly varying background noise such as hum of the electrical equipments, and then segments with potential vocalization were identified. Then, we extracted various cues from these segments and then were able to successfully identify vocalization with an accuracy of about 88.9%. The recordings from the audio recorders worn by all the monkeys in the group were aligned with each other compared to identify which monkey vocalized. In this process of tackling the difference in the recording, we also developed and published a class of novel (multivariate copula) models that can be more widely applicable. </span></p>\n<p><span>Dr Erdogmus and his lab developed models and algorithms to track the individual monkeys in the video recordings. They demonstrated that the in- dividual monkeys can be detected with precision and recall of about 0.8 and&nbsp;</span>0.75 respectively. For tracking, they devised and compared two approaches, a Kalman filter and a particle filter, and demonstrated that certain behaviors such as chasing, locomotion, displacement and stationary can be recognized with a precision and recall of about 0.7-0.9 and 0.7-0.8, respectively.</p>\n<div class=\"page\" title=\"Page 3\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>In addition to the outcomes mentioned above, the project established a new collaboration between computer scientists and primatologists to investigate social behavior in more depth than current methods for studying behavior allowed. The project created an extensive, annotated data repository and associated algorithms for the two respective communities. Finally, the project partially funded two doctoral students and fully funded two additional graduate students, all of whom gained hands-on training in different aspects of the project, thus honed their skills to cross disciplinary boundaries.</span></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/01/2016<br>\n\t\t\t\t\tModified by: Alexander&nbsp;Kain</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1027834/1027834_10049981_1480609793033_figure--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1027834/1027834_10049981_1480609793033_figure--rgov-800width.jpg\" title=\"Setup\"><img src=\"/por/images/Reports/POR/2016/1027834/1027834_10049981_1480609793033_figure--rgov-66x44.jpg\" alt=\"Setup\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Pen Setup</div>\n<div class=\"imageCredit\">Shafran</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;Kain</div>\n<div class=\"imageTitle\">Setup</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\n\n\nEnabling machines to unobtrusively observe and model social interactions effectively has the potential to transform our understanding of behaviors of highly social animals. Currently, observation of social behaviors in animals is achieved by having a human identify observations. Having a human present can greatly alter the behavior of the subjects, and even highly trained observers do not always catch every behavior, particularly complex behaviors that may occur simultaneously. Even with video recordings, coding behaviors can be prohibitively time and labor intensive, limiting the ability to record animals continuously for days. Thus, one of the key factors limiting progress in animal behavior research is the rate at which data can be gathered and analyzed. Understanding and modeling behaviors such as dominance, mating and courtship behavior, and parental behaviors is critically important in improving conservation efforts for threatened species, building accurate animal models for health research, and supporting animal husbandry decisions in zoos. \n\nThe goals of this project were: (a) to develop an audio-visual platform for recording the activities of groups of macaques with sufficient fidelity to allow both humans and machines to subsequently recognize their behaviors, (b) to automatically recognize the vocalization of the individuals in the group, (c) to track the movements of the individuals, and (d) to recognize their behaviors automatically using machines. \n\nDr. Grant and Dr. Coleman, the primatologists on this project, designed the animal study, which consisted of observing 4-6 monkeys in a group over a period of a week as they established their social hierarchy. This was followed by an additional week of observation where the hierarchy was mildly perturbed by unfamiliar human presence, food treats and removing one of the members. Dr. Shafran, with help from Dr. Erdogmus and Dr. Hunt, led the effort in developing a data collection platform. \n\nThe recording setup consisted of a special pen (12' long x 7' deep x 7' tall) whose two sides and ceiling were made of metal wire mesh, on which we mounted four cameras (GC1380CH, 2/3? CCD) in a manner that minimized occlusion ? three with wide-aperture lenses (Optron 5mm f/2.8) near the vertical edges and one with wide-angle fisheye lens (Edmunds Optics NT62-274, focal length 1.8mm, F1.4,185 x 185 degrees) on the ceiling, as illustrated in the figure. \n\n\n\n\n\n\n\nThe vocalizations were captured using a tiny programmable recorder (EDIC B21 Mini Recorder, about 40 x 15 x 10 mm in size that was concealed in the collar of each monkey. In all, we collected contains 80 days of social behaviors of groups of monkeys. \n\nThe approach we adopted was to learn the necessary models for recognizing behaviors automatically from the recorded data. This task is somewhat simplified by the observation that complex social interactions can be broken down into relatively simple elementary behaviors stitched together in different sequences. Thus, we focused on recognizing a set of elementary behaviors specified by the primatologists, including but not limited to aggression, chasing, fear grimace, grooming and threat screams. \n\nDr Shafran and his lab developed models and algorithms to process the audio recordings of the monkeys behaviors (in all 3800 hours of audio, each session 12 hours long). For efficiency, the audio was processed in multiple stages. The signal was enhanced in the spectral domain by subtracting the stationary or slowly varying background noise such as hum of the electrical equipments, and then segments with potential vocalization were identified. Then, we extracted various cues from these segments and then were able to successfully identify vocalization with an accuracy of about 88.9%. The recordings from the audio recorders worn by all the monkeys in the group were aligned with each other compared to identify which monkey vocalized. In this process of tackling the difference in the recording, we also developed and published a class of novel (multivariate copula) models that can be more widely applicable. \n\nDr Erdogmus and his lab developed models and algorithms to track the individual monkeys in the video recordings. They demonstrated that the in- dividual monkeys can be detected with precision and recall of about 0.8 and 0.75 respectively. For tracking, they devised and compared two approaches, a Kalman filter and a particle filter, and demonstrated that certain behaviors such as chasing, locomotion, displacement and stationary can be recognized with a precision and recall of about 0.7-0.9 and 0.7-0.8, respectively.\n\n\n\n\nIn addition to the outcomes mentioned above, the project established a new collaboration between computer scientists and primatologists to investigate social behavior in more depth than current methods for studying behavior allowed. The project created an extensive, annotated data repository and associated algorithms for the two respective communities. Finally, the project partially funded two doctoral students and fully funded two additional graduate students, all of whom gained hands-on training in different aspects of the project, thus honed their skills to cross disciplinary boundaries.\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 12/01/2016\n\n\t\t\t\t\tSubmitted by: Alexander Kain"
 }
}