{
 "awd_id": "1016285",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF:  Small:  Confidence in Manycore/Multi-Core Modeling and Simulation",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-08-01",
 "awd_exp_date": "2013-07-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2010-07-19",
 "awd_max_amd_letter_date": "2010-07-19",
 "awd_abstract_narration": "Computer designers use techniques to accelerate the simulation of their new computer designs while ignoring the errors of the acceleration methods. This is common because the entire simulation of modern computer programs is intractable. Basing the performance of a design only on fragments of program execution can yield results that are misleading, and result in far from optimal computer hardware designs. Without bounds on confidence, the relative performance of two architectures is impossible to compare with any statistical validity. For computer design to advance to an engineering science with reproducible results, the status quo must change. This research remedies the situation by introducing confidence bounding to fast manycore simulation.\r\n\r\nThe Georgia Tech research team has made significant and lasting contributions to adding confidence to single-threaded processor simulation, and is adapting and discovering new techniques for the manycore simulation. The team is investigating several approaches to thread slip problem (where the relative execution order of threads is unknown at the start of the simulation of a cluster of events) and are developing models that can be used by designers.  Another class of problems is state reconstruction problems unique to manycore simulation.  For example, global ordering of thread executions and their sharing patterns greatly impact the coherence state of cache blocks.  Coherence information, in addition to directory contents, indicate the current owner of a line, which must be reconstructed for measured cache latencies and interconnect network flow to be representative of unsampled execution.  Other specific problems involve recovering the directory state, the state of the interconnection network (e.g., flit buffers, conflicts for routes, etc), and cache state.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Conte",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas M Conte",
   "pi_email_addr": "conte@cc.gatech.edu",
   "nsf_id": "000238429",
   "pi_start_date": "2010-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 NORTH AVE NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "794200",
   "pgm_ele_name": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Computers are slow and consume a lot of power. &nbsp;Engineers usually optimize designs by simulating future designs. &nbsp;But simulating computers is difficult. &nbsp;This is because each \"instruction\" that a computer executes in the simulation requires many, many actual instructions in the simulator software. &nbsp;One solution to this problem is to not simulate the whole workload (i.e., the whole benchmark program) for a future computer but instead to only simulate pieces of it. &nbsp;We developed techniques to choose these \"pieces\" using sound statistical techniques developed for the life sciences a century ago. &nbsp;Applying these techniques to computing is not straight-forward. &nbsp;The reason is that when you skip some work a (simulated) computer should have been executing, you do not know the status (i.e., the \"brain contents\") of the computer when you turn back on the simulation. &nbsp;We have previously developed technologies to fix these problems. &nbsp;But in doing so, we found that <span>multicore</span> computers-- computers with more than one \"CPU\" core-- provided new challenges to simulation. &nbsp;In these computers, a program is comprised of multiple \"threads\" of work that run in parallel. &nbsp;If you skip some of this work, then when you turn back on the simulation, you do not know how the parallel threads line up relative to each other. &nbsp;Our solution to this problem was to stand the problem on its head: instead of interrupting when a <span>multicore</span> is running in parallel, wait until it gets all of the threads in <span>synch</span>. &nbsp;This occurs because programmers need to <span>synch</span> threads from time to time to make sure the threads are \"on the same page.\" &nbsp;Each of the work between these simulation points is a \"block\" of parallel work, or what we call a barrier interval. &nbsp;The innovation here is to simulate all of the blocks in parallel using a <span>multicore</span> to simulate a <span>multicore</span>! &nbsp;The result is a speedup of hundreds of times for a computer simulation. &nbsp;That means we can evaluate new computer designs that heretofore we didn't have the ability to evaluate. &nbsp;And that, in turn means that new computers will be faster and more power efficient.</span>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/08/2014<br>\n\t\t\t\t\tModified by: Thomas&nbsp;M&nbsp;Conte</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nComputers are slow and consume a lot of power.  Engineers usually optimize designs by simulating future designs.  But simulating computers is difficult.  This is because each \"instruction\" that a computer executes in the simulation requires many, many actual instructions in the simulator software.  One solution to this problem is to not simulate the whole workload (i.e., the whole benchmark program) for a future computer but instead to only simulate pieces of it.  We developed techniques to choose these \"pieces\" using sound statistical techniques developed for the life sciences a century ago.  Applying these techniques to computing is not straight-forward.  The reason is that when you skip some work a (simulated) computer should have been executing, you do not know the status (i.e., the \"brain contents\") of the computer when you turn back on the simulation.  We have previously developed technologies to fix these problems.  But in doing so, we found that multicore computers-- computers with more than one \"CPU\" core-- provided new challenges to simulation.  In these computers, a program is comprised of multiple \"threads\" of work that run in parallel.  If you skip some of this work, then when you turn back on the simulation, you do not know how the parallel threads line up relative to each other.  Our solution to this problem was to stand the problem on its head: instead of interrupting when a multicore is running in parallel, wait until it gets all of the threads in synch.  This occurs because programmers need to synch threads from time to time to make sure the threads are \"on the same page.\"  Each of the work between these simulation points is a \"block\" of parallel work, or what we call a barrier interval.  The innovation here is to simulate all of the blocks in parallel using a multicore to simulate a multicore!  The result is a speedup of hundreds of times for a computer simulation.  That means we can evaluate new computer designs that heretofore we didn't have the ability to evaluate.  And that, in turn means that new computers will be faster and more power efficient. \n\n\t\t\t\t\tLast Modified: 01/08/2014\n\n\t\t\t\t\tSubmitted by: Thomas M Conte"
 }
}