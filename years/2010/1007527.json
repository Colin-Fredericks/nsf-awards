{
 "awd_id": "1007527",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "High Dimensional Statistical Theory for Sparse Regularization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2010-07-01",
 "awd_exp_date": "2014-06-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2010-05-10",
 "awd_max_amd_letter_date": "2012-05-03",
 "awd_abstract_narration": "High Dimensional Statistical Theory for Sparse Regularization\r\n\r\n\r\nThe investigator studies statistical machine learning with sparse regularization in the setting of high dimensional statistical estimation. A number of research directions will be explored, including improved performance bounds for sparse regularization, new sparse learning formulations, and the statistical theory for several important computational algorithms.\r\n\r\n\r\nIn the information age, more and more data become available electronically, and these data need to be automatically analyzed by computers in order to filter out the most important information. Statistical machine learning is the main technical tool for analyzing electronic data. Many modern applications involve data in very high dimension that cannot be handled by traditional algorithms. Sparse regularization is an important new statistical machine learning technique that can deal with this issue by effectively identifying the most significant patterns from a vast amount of available information. This research develops new sparse regularization algorithms that will significantly enhance the capability for modern computer systems to find critical information from available electronic data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tong",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tong Zhang",
   "pi_email_addr": "tozhang@illinois.edu",
   "nsf_id": "000102616",
   "pi_start_date": "2010-05-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "3 RUTGERS PLZ",
  "perf_city_name": "NEW BRUNSWICK",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "089018559",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 38663.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 102087.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 109250.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many real world applications involve data with very high dimensionality. To extract values from these data, we have to employ machine learning algorithms using &nbsp;complex statistical models with many parameters. However, these problems are difficult because the number of parameters is much larger than the number of data points. A modern solution to overcome this difficulty is by using sparse regularization, in which the number of nonzero parameters is much smaller than the data dimensionality.&nbsp;<br />The proposed research is to gain better understanding of sparse regularization by studying sparse learning algorithms in the setting of high dimensionalstatistical estimation. A number of research directions have been explored, including improved theoretical understanding for sparse regularization, new sparse learning formulations, and the statistical theory for several important computational algorithms. We have published more than ten scientific papers and disseminated our results via academic conferences, web sites, courses and lectures. Software based on this research has been distributed publicly, and used by many people to analyze real world data. Our methods lead to more accuracy prediction models and faster computational time in a wide range of data analysis applications. These techniques have been used in the industry, and have made significant impact on our modern society where big data and thus methods to analyzing these data become more and more important.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/29/2014<br>\n\t\t\t\t\tModified by: Tong&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany real world applications involve data with very high dimensionality. To extract values from these data, we have to employ machine learning algorithms using  complex statistical models with many parameters. However, these problems are difficult because the number of parameters is much larger than the number of data points. A modern solution to overcome this difficulty is by using sparse regularization, in which the number of nonzero parameters is much smaller than the data dimensionality. \nThe proposed research is to gain better understanding of sparse regularization by studying sparse learning algorithms in the setting of high dimensionalstatistical estimation. A number of research directions have been explored, including improved theoretical understanding for sparse regularization, new sparse learning formulations, and the statistical theory for several important computational algorithms. We have published more than ten scientific papers and disseminated our results via academic conferences, web sites, courses and lectures. Software based on this research has been distributed publicly, and used by many people to analyze real world data. Our methods lead to more accuracy prediction models and faster computational time in a wide range of data analysis applications. These techniques have been used in the industry, and have made significant impact on our modern society where big data and thus methods to analyzing these data become more and more important. \n\n\t\t\t\t\tLast Modified: 07/29/2014\n\n\t\t\t\t\tSubmitted by: Tong Zhang"
 }
}