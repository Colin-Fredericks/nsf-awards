{
 "awd_id": "1018691",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: RI: Small: A Scalable Architecture for Image Interpretation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2010-09-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 158946.0,
 "awd_amount": 158946.0,
 "awd_min_amd_letter_date": "2010-09-13",
 "awd_max_amd_letter_date": "2010-09-13",
 "awd_abstract_narration": "Seamless understanding of the meaning of visual images is a key property of human cognition that is far beyond the abilities of current computer vision programs.  The purpose of this project is to build a computational system that captures the dynamical and interactive aspects of human vision by integrating higher-level concepts with lower-level visual perception.  If successful, this system will be able to interpret visual scenes in a way that scales well with the complexity of the scene. Current computer vision systems typically rely on relatively low-level visual information (e.g., color, texture, shape) to classify objects or determine the overall category of a scene.   Such categorization is typically done in a \"bottom-up\" fashion, in which the vision system extracts lower-level features from all parts of the scene, and subsequently analyzes the extracted features to determine which parts of the scene contain objects of interest and how those objects should be categorized.  Such systems lack the abilities to scale to large numbers of visual categories and to identify more complex visual concepts that involve spatial and abstract relationships among object categories.  Visual perception by humans is known to be a temporal process with feedback, in which lower-level visual features serve to activate higher-level concepts (or knowledge).  These active concepts, in turn, guide the perception of and attention given to lower-level visual features.  Moreover, activated concepts can spread activation to semantically related concepts (e.g., \"wheels\" might activate \"car\" or \"bicycle\"; \"bicycle\" might activate \"road\" or \"rider\").    In this way there is a continual interaction between the lower and higher levels of vision, which allows the viewer to focus on and connect important aspects of a complex scene in order to perceive its meaning, without having to pay equal attention to every detail of the scene.   The system proposed here will model these aspects of human visual perception.  \r\n\r\nThe proposed system, called Petacat, will integrate and build on two existing projects:  the HMAX model of object recognition originally developed by Riesenhuber and Poggio, and the Copycat model of high-level perception and analogy-making, developed by Hofstadter and Mitchell.    HMAX models the \"what\" pathway of mammalian visual cortex via a feed-forward network that extracts increasingly complex textural and shape features from an image. (HMAX has been reimplemented, as the \"Petascale Artificial Neural Network\" or PANN, by the Synthetic Vision Group at Los Alamos to allow for high-performance computing on large numbers of neurons.)  Copycat implements a process of interaction between high-level concepts and lower-level perception, and has been used to model focus of attention, conceptual slippage, and analogy-making in several non-visual domains.  This project will marry the feature extraction abilities of HMAX/PANN with the higher-level interactive perceptual abilities of Copycat to build the Petacat architecture. The image interpretation abilities of Petacat will be evaluated on families of related semantic visual recognition tasks (e.g., recognizing, in a flexible, human-like way, instances of \"walking a dog\").    The evaluation part of the project will involve the creation of image databases for benchmarking semantic image-understanding systems.  The Petacat source code and benchmarking databases will be made publically available via the web.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Garrett",
   "pi_last_name": "Kenyon",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Garrett T Kenyon",
   "pi_email_addr": "gkenyon@lanl.gov",
   "nsf_id": "000460690",
   "pi_start_date": "2010-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New Mexico Consortium",
  "inst_street_address": "4200 W JEMEZ RD STE 301",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ALAMOS",
  "inst_state_code": "NM",
  "inst_state_name": "New Mexico",
  "inst_phone_num": "5054124200",
  "inst_zip_code": "875442587",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "NM03",
  "org_lgl_bus_name": "NMC, INC.",
  "org_prnt_uei_num": "",
  "org_uei_num": "TUMCX1C2C4B3"
 },
 "perf_inst": {
  "perf_inst_name": "New Mexico Consortium",
  "perf_str_addr": "4200 W JEMEZ RD STE 301",
  "perf_city_name": "LOS ALAMOS",
  "perf_st_code": "NM",
  "perf_st_name": "New Mexico",
  "perf_zip_code": "875442587",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "NM03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 158946.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>&nbsp;</strong></p>\n<p>Our work on this project has three major goals:</p>\n<p>&nbsp;</p>\n<p><strong>Goal 1:</strong> To understand and improve the performance of hierarchical network models that learn features for object recognition.&nbsp; The hierarchical networks we have focused on so far are the HMAX architecture of Serre et al. and the Sparse Localized Features model of Mutch and Lowe.&nbsp; In the final year of our grant we plan to investigate convolutional networks (LeCun et al. and Hinton et al.)</p>\n<p>&nbsp;</p>\n<p><strong>Goal 2:</strong> To develop Petacat, a novel architecture that integrates the object recognition abilities of hierarchical networks with the high-level perception functions of Hofstadter et al.'s active symbol architectures.&nbsp; The goal of our work on Petacat is to develop an architecture that can interpret visual \"situations\", consisting of multiple objects and their spatial and semantic relationships, that add up to a known visual concept (e.g., \"walking a dog\").</p>\n<p>&nbsp;</p>\n<p><strong>Goal 3:</strong> To construct a new, publicly available benchmark dataset for visual situation understanding, in order to test different architectures, and to promote research in this area.</p>\n<p>&nbsp;</p>\n<p><strong>* What was accomplished under these goals?</strong></p>\n<p>Major Activities:</p>\n<p><strong>I. Completion and release of Glimpse implementation.</strong></p>\n<p>We completed the Glimpse open-source project, a framework for easily expressing and applying hierarchical visual models.&nbsp; Glimpse is the architecture we used to perform exeriments on the hierarchical networks described below. &nbsp;Our open-source code is freely available at http://pythonhosted.org/glimpse/ .<br /> <br /> <strong>II.&nbsp; Interpretable machine learning:</strong></p>\n<p>Contribution propagation In order to explain and visualize the classification decisions in hierarchical networks, we have developed and implemented an algorithm based on the propagation of the classifier's confidence backward through the network. The result is an informative explanation of the degree to which each input (each pixel, in the case of an image) contributed to the model's classification, either towards a positive or negative classification.<br /> <br /><br /> I<strong>II.&nbsp; Investigating the Role of Prototypes in Hierarchical Networks</strong><br /> <br /> We found that sparse, random prototypes account for the performance previously reported for dictionaries learned by \"imprinting\". This result held for every reference corpora we considered, which included all of those commonly used to study this family of model.</p>\n<p>This work was presented at the International Joint Conference on Neural Networks, which was made possible by grant funds.</p>\n<p><strong>IV. Improving Prototype Learning for Object Recognition</strong><br /> <br />We found that HMAX-type models are very sensitive to the choice of learning method and its parameters. For example, we found that simple clustering techniques provide little benefit over the common \"imprinting\" method.&nbsp; The same clustering techniques have been found to work well in convolutional-style networks (an approach closely related to HMAX). The reason for this disparity is currently unclear.<br /> <strong>&nbsp;</strong></p>\n<p><strong>V. Learning sparse representations for image reconstruction</strong></p>\n<p><br /> During this grant period we also focused on generative learning of sparse representations in order to optimize image reconstruction. Deep, sparse models encompass a powerful set of techniques for representing the hierarchical structure of the visual world.&nbsp; However, most existing implementations only enforce sparseness locally at each processing stage. In our work, we employed top-down competition to explore how a globally-sparse representation that spans multiple layers can be learned simultaneously acro...",
  "por_txt_cntn": "\n \n\nOur work on this project has three major goals:\n\n \n\nGoal 1: To understand and improve the performance of hierarchical network models that learn features for object recognition.  The hierarchical networks we have focused on so far are the HMAX architecture of Serre et al. and the Sparse Localized Features model of Mutch and Lowe.  In the final year of our grant we plan to investigate convolutional networks (LeCun et al. and Hinton et al.)\n\n \n\nGoal 2: To develop Petacat, a novel architecture that integrates the object recognition abilities of hierarchical networks with the high-level perception functions of Hofstadter et al.'s active symbol architectures.  The goal of our work on Petacat is to develop an architecture that can interpret visual \"situations\", consisting of multiple objects and their spatial and semantic relationships, that add up to a known visual concept (e.g., \"walking a dog\").\n\n \n\nGoal 3: To construct a new, publicly available benchmark dataset for visual situation understanding, in order to test different architectures, and to promote research in this area.\n\n \n\n* What was accomplished under these goals?\n\nMajor Activities:\n\nI. Completion and release of Glimpse implementation.\n\nWe completed the Glimpse open-source project, a framework for easily expressing and applying hierarchical visual models.  Glimpse is the architecture we used to perform exeriments on the hierarchical networks described below.  Our open-source code is freely available at http://pythonhosted.org/glimpse/ .\n \n II.  Interpretable machine learning:\n\nContribution propagation In order to explain and visualize the classification decisions in hierarchical networks, we have developed and implemented an algorithm based on the propagation of the classifier's confidence backward through the network. The result is an informative explanation of the degree to which each input (each pixel, in the case of an image) contributed to the model's classification, either towards a positive or negative classification.\n \n\n III.  Investigating the Role of Prototypes in Hierarchical Networks\n \n We found that sparse, random prototypes account for the performance previously reported for dictionaries learned by \"imprinting\". This result held for every reference corpora we considered, which included all of those commonly used to study this family of model.\n\nThis work was presented at the International Joint Conference on Neural Networks, which was made possible by grant funds.\n\nIV. Improving Prototype Learning for Object Recognition\n \nWe found that HMAX-type models are very sensitive to the choice of learning method and its parameters. For example, we found that simple clustering techniques provide little benefit over the common \"imprinting\" method.  The same clustering techniques have been found to work well in convolutional-style networks (an approach closely related to HMAX). The reason for this disparity is currently unclear.\n  \n\nV. Learning sparse representations for image reconstruction\n\n\n During this grant period we also focused on generative learning of sparse representations in order to optimize image reconstruction. Deep, sparse models encompass a powerful set of techniques for representing the hierarchical structure of the visual world.  However, most existing implementations only enforce sparseness locally at each processing stage. In our work, we employed top-down competition to explore how a globally-sparse representation that spans multiple layers can be learned simultaneously across all hierarchical levels.  A modified locally competitive algorithm, employing both lateral and top-down inhibition and a combination of soft and hard (firm) thresholding, was used to promote global sparsity, with additional algorithmic extensions for learning dictionaries that optimize sparse reconstruction.  Our results indicate that top-down competition enforces a globally sparse solution in which all levels in the hierarchy act in concert to encode each image or video fr..."
 }
}