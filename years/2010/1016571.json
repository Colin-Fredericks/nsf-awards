{
 "awd_id": "1016571",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Fast First-Order Methods for Large-Scale Structured and Sparse Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924488",
 "po_email": "jwang@nsf.gov",
 "po_sign_block_name": "Junping Wang",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2010-08-16",
 "awd_max_amd_letter_date": "2010-08-16",
 "awd_abstract_narration": "Algorithms for large-scale optimization have traditionally exploited\r\nsparsity and structure in problem data. Many important optimization \r\nproblems today, such as those that arise in  statistical machine \r\nlearning (ML) and in compressive sensing (CS) are extremely large-scale convex\r\nproblems with completely dense and/or unstructured problem data. \r\nHowever, there is often sparsity and structure in the solutions to \r\nthese problems. The goal of this research project is the development of\r\nfirst-order algorithms, including gradient methods for non-smooth functions,\r\nsmoothed penalty methods for constrained problems, multiple splitting methods,\r\nalternating-direction augmented-Lagrangian methods, and\r\nblock coordinate descent methods, for extremely large-scale convex \r\noptimization problems that take advantage of solution structure and/or \r\nsparsity. Rigorous convergence analysis for these methods will be provided and\r\nrobust software implementations will be developed. Although these \r\nmethods are expected to have wide applicability, the focus will be on \r\napplications in CS and ML. Specifically, the investigators propose to \r\ndevelop  and analyze new scalable algorithms for (i) CS signal \r\nrecovery, including algorithms that are able to exploit more detailed \r\na priori knowledge in addition to sparsity; (ii) matrix rank minimization, the \r\nmatrix analog of CS, and its variants; and (iii) a broad array of ML problems that exploit \r\nthe special sparsity/structure of the solutions to these \r\nproblems.\r\n\r\nThe research that is proposed under this grant is focused on the development of \r\nalgorithms with provable performance guarantees that are capable of \r\nsolving extremely large scale optimization problems whose solutions are \r\neither sparse or have special structure.  Such problems arise under the paradigm of \r\ncompressive sensing, which allows signals (e.g., radar) and images \r\n(e.g., CT and MRI scans) to be obtained with far fewer measurements \r\nthan predicted by traditional theory, various extensions of CS, and in \r\na broad array of problems in machine learning. All of these problems are \r\naimed at extracting a \"sparse\" or low-dimensional true model from a \r\nhigh dimensional or dense empirical model or data. They have important applications \r\nin extracting information from surveillance video and hyper-spectral images, face \r\nrecognition, medical imaging and data mining,as well as many other areas \r\nof strategic interest such as national security and biotechnology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Donald",
   "pi_last_name": "Goldfarb",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Donald Goldfarb",
   "pi_email_addr": "goldfarb@columbia.edu",
   "nsf_id": "000118999",
   "pi_start_date": "2010-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Garud",
   "pi_last_name": "Iyengar",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Garud N Iyengar",
   "pi_email_addr": "garud@ieor.columbia.edu",
   "nsf_id": "000487025",
   "pi_start_date": "2010-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Katya",
   "pi_last_name": "Scheinberg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Katya Scheinberg",
   "pi_email_addr": "katyascheinberg@gmail.com",
   "nsf_id": "000544723",
   "pi_start_date": "2010-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "615 W 131ST ST",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  },
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Intellectual Merit.<br /><br />Algorithms for large-scale optimization have traditionally leveraged problem structure by explicitly tailoring the algorithm to the underlying functions and efficiently handling sparsity in problem data. The optimization problems arising in&nbsp; machine learning (ML)&nbsp; and in compressive sensing (CS) are extremely large-scale convex problems with completely dense problem data, but&nbsp; often with sparsity and structure in the solutions. In this&nbsp; project we&nbsp; developed first-order algorithms for extremely large-scale convex optimization problems that take advantage of solution structure and&nbsp; sparsity. We provided rigorous convergence analysis for these methods, and produced robust software implementations. In particular we focused on optimization problems arising in CS and ML. Examples include sparse signal recovery, collaborative filtering, linear classification, graphical models, image denoising, just to name a few important large-scale applications. <br /><br />&nbsp; We applied compressed sensing and random matrix theory to show that one can recover quadratic interpolation models of functions whose Hessian are sparse using fewer interpolation points than was previously considered necessary. We have applied these ideas in the context of derivative-free optimization and developed an algorithm and a Matlab implementation. <br /><br />&nbsp;&nbsp;&nbsp; We developed a second-order algorithm for large-scale sparse optimization, which constructs Lasso-type models of the objective function using L-BFGS Hessian approximations and minimizes the models inexactly using coordinate descent. We have provided convergence rates. Our C++ and Matlab implementations outperform state-of-the-art software for sparse logistic regression and sparse inverse covariance selection.<br /><br />&nbsp; We&nbsp; developed a new non-convex model for risk-parity portfolio selection which we solve very efficiently by&nbsp; a sequence of convex quadratic optimization problems. We proved convergence and provided convergence rates&nbsp; for these new methods. <br /><br />&nbsp;&nbsp;&nbsp; We have combined a fixed-point continuation (FPC) method with active set identification and subspace optimization to develop an extremely effective and efficient method for the basis pursuit problem. We proved global R-linear convergence and showed that the method recovers signals with very large dynamic ranges in CS applications. We have also developed an FPC method and a Bregman iterative algorithm for matrix rank minimization problems and analyzed their ability to recover low-rank solutions. We have developed accelerated versions of the linearized Bregman method, proved that their iteration complexity is reduced from O(1/&epsilon;) to O(1/&radic;&epsilon;) and applied them to CS and matrix completion problems.<br /><br />&nbsp;&nbsp;&nbsp; We developed alternating linearization methods (ALMs) for solving convex optimizaion problems that often arise as tight convex relaxations of nonconvex structured optimization problems. Our methods solve problems of the form: min{F(x,y)&equiv;f(x)+g(y): Ax+y=b}. Under the assumption that both f and g are convex functions with Lipschitz continuous gradients, we proved that our methods require O(1/&epsilon;) iterations to obtain an &epsilon;-optimal solution (O(1/&radic;&epsilon;), for accelerated versions),&nbsp; while requiring essentially the same computational effort at each iteration. We developed specialized versions of these algorithms to solve various high dimensional problems in ML. Specifically, we applied them to Gaussian graphical models and to overlapping group LASSO problems involving appropriate sparsity-inducing norm regularizers. We developed an&nbsp; efficient block-coordinate descent approach for&nbsp; group LASSO problems, line search versions of our accelerated ALMs and the fast prox-gradient FISTA method t...",
  "por_txt_cntn": "\nIntellectual Merit.\n\nAlgorithms for large-scale optimization have traditionally leveraged problem structure by explicitly tailoring the algorithm to the underlying functions and efficiently handling sparsity in problem data. The optimization problems arising in  machine learning (ML)  and in compressive sensing (CS) are extremely large-scale convex problems with completely dense problem data, but  often with sparsity and structure in the solutions. In this  project we  developed first-order algorithms for extremely large-scale convex optimization problems that take advantage of solution structure and  sparsity. We provided rigorous convergence analysis for these methods, and produced robust software implementations. In particular we focused on optimization problems arising in CS and ML. Examples include sparse signal recovery, collaborative filtering, linear classification, graphical models, image denoising, just to name a few important large-scale applications. \n\n  We applied compressed sensing and random matrix theory to show that one can recover quadratic interpolation models of functions whose Hessian are sparse using fewer interpolation points than was previously considered necessary. We have applied these ideas in the context of derivative-free optimization and developed an algorithm and a Matlab implementation. \n\n    We developed a second-order algorithm for large-scale sparse optimization, which constructs Lasso-type models of the objective function using L-BFGS Hessian approximations and minimizes the models inexactly using coordinate descent. We have provided convergence rates. Our C++ and Matlab implementations outperform state-of-the-art software for sparse logistic regression and sparse inverse covariance selection.\n\n  We  developed a new non-convex model for risk-parity portfolio selection which we solve very efficiently by  a sequence of convex quadratic optimization problems. We proved convergence and provided convergence rates  for these new methods. \n\n    We have combined a fixed-point continuation (FPC) method with active set identification and subspace optimization to develop an extremely effective and efficient method for the basis pursuit problem. We proved global R-linear convergence and showed that the method recovers signals with very large dynamic ranges in CS applications. We have also developed an FPC method and a Bregman iterative algorithm for matrix rank minimization problems and analyzed their ability to recover low-rank solutions. We have developed accelerated versions of the linearized Bregman method, proved that their iteration complexity is reduced from O(1/&epsilon;) to O(1/&radic;&epsilon;) and applied them to CS and matrix completion problems.\n\n    We developed alternating linearization methods (ALMs) for solving convex optimizaion problems that often arise as tight convex relaxations of nonconvex structured optimization problems. Our methods solve problems of the form: min{F(x,y)&equiv;f(x)+g(y): Ax+y=b}. Under the assumption that both f and g are convex functions with Lipschitz continuous gradients, we proved that our methods require O(1/&epsilon;) iterations to obtain an &epsilon;-optimal solution (O(1/&radic;&epsilon;), for accelerated versions),  while requiring essentially the same computational effort at each iteration. We developed specialized versions of these algorithms to solve various high dimensional problems in ML. Specifically, we applied them to Gaussian graphical models and to overlapping group LASSO problems involving appropriate sparsity-inducing norm regularizers. We developed an  efficient block-coordinate descent approach for  group LASSO problems, line search versions of our accelerated ALMs and the fast prox-gradient FISTA method that preserve these methods\u00c6  fast iteration complexity (and improve performance) and specialized versions of ALMs and prox-gradient methods for  robust and stable principle component pursuit problems. We also developed  an extremely fast..."
 }
}