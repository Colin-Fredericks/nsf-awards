{
 "awd_id": "0952425",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Advanced System-Level Support for Hybrid Multi-Tasking Computing",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2010-03-01",
 "awd_exp_date": "2016-02-29",
 "tot_intn_awd_amt": 407086.0,
 "awd_amount": 407086.0,
 "awd_min_amd_letter_date": "2010-02-18",
 "awd_max_amd_letter_date": "2014-04-30",
 "awd_abstract_narration": "This work investigates new methods to provide operating system support for hybrid computing systems. A hybrid system includes one or more instruction-based processors coupled with accelerator logic that exploits application parallelism to increase performance. The work focuses on accelerators constructed from reconfigurable hardware, such as field-programmable gate array (FPGA) logic. Reconfigurable hardware also reduces energy by directly implementing the needed computation instead of fetching, decoding, and executing instructions. Future work will extend the investigation to other types of accelerators, such as general-purpose graphics processing units (GPGPUs). In particular, the research studies the interdependence of accelerator management and software thread scheduling in multi-tasking hybrid systems, and proposes integrating these two system-level functions to increase the overall performance and energy-efficiency. This will encourage industry to further embrace hybrid computing in the development of new products, and thus, enable more capable and energy-efficient mobile devices; quiet multi-function household information appliances; portable medical devices; and more flexible, capable, and robust infrastructure systems in transportation, healthcare, and other key industries. \r\n\r\nThis project also targets to improve teaching, learning, and diversity in Computer Engineering. Activities include developing curriculum and teaching aids, outreach activities, undergraduate research, and involvement in education-related committees. As a part of this project, participants will create new instructional materials for Computer Engineering topics at a variety of levels of understanding.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Katherine",
   "pi_last_name": "Morrow",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Katherine Morrow",
   "pi_email_addr": "kati@engr.wisc.edu",
   "nsf_id": "000304888",
   "pi_start_date": "2010-02-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 N PARK ST STE 6301",
  "perf_city_name": "MADISON",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "794100",
   "pgm_ele_name": "COMPUTER ARCHITECTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 80101.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 77007.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 80089.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 83283.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 86606.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Various highly-parallel architectures have proposed as replacements for or assistants to traditional processors for highly-parallel computations such as graphic processing, scientific data processing, signal processing, encryption, and others. Although processor design has made great strides improving performance and reducing power, processors are still fundamentally \"jack of all trades\" devices. In many respects, they are more complex than necessary yet not specialized for these applications (i.e., not as fast or energy-efficient as possible). Traditional processors are still critical for their flexibility, but off-loading these computations to more specialized hardware provides significant performance and energy benefits.</p>\n<p>A challenge in incorporating accelerators, whether configurable hardware that implements tasks as high-performance circuits, or General-Purpose Graphics Processing Units (GPGPUs) that are a large set of (relatively) simpler processors, is the coordination of these resources within the operating system of a multi-tasking system.</p>\n<p>This project examined scheduling within multitasking heterogeneous systems and issues related to managing \"spatial\" multitasking of accelerators (allocating a subset of the accelerator to each of multiple applications simultaneously) instead of only \"temporal\" multitasking (applications taking turns using all hardware resources at once). We devloped a number of effective techniques that incorporate information regarding the nature of tasks and the hardware to make dynamic resource allocation decisions that effectively maximize performance, meet deadlines, minimize power, ensure fairness, or some combination of these.</p>\n<p>First, we developed algorithms for scheduling tasks in heterogeneous real-time systems. We examined systems with firm-deadline tasks, where a deadline is an expiration time for the \"usefulness\" of a computation. For example, if a frame in a video cannot be decoded quickly enough to play it at the correct time, it is often best to \"drop\" (not show) the frame rather than slow the video. In this work, each task required a different time to complete on each resource, and in some cases the \"preferred\" resource for each task (the one on which it executes most quickly) differed for different tasks (modeling different affinities of tasks to type of resources). We created scheduling methodologies that reduced deadline misses for these systems.</p>\n<p>We also created scheduling techniques for mixed-deadline heterogeneous real-time systems that execute a combination of periodic hard-deadline tasks (i.e., known tasks whose deadlines must not be missed or the system fails) and aperiodic soft-deadline tasks (cannot predict these prior to run-time, but they still require responsiveness). This models a mixed-use system such as a smartphone. Our methods increased the ability to complete the aperiodic tasks within a reasonable time period while ensuring that all hard deadlines were met.</p>\n<p>Other aspects of this work examined spatial multitasking within GPGPU systems. First we examined allocating GPGPU resources to tasks based on meeting quality-of-service (QoS) requirements (e.g., meeting minimum video playback speed). We presented two possible alternatives: (1) disabling \"extra\" resources (not required to meet QoS) to save power, and (2) reallocating \"extra\" resources to \"best-effort\" applications (without a QoS requirement) to improve their performance. This method was able to measure performance and dynamically adjust the allocation as needed. We also demonstrated significant power savings or performance improvements depending on the chosen method.</p>\n<p>Another study took advantage of process variation within GPGPUs to improve performance and energy. The manufacturing process results in some chip regions having the potential to run faster than others. Normally a chip's \"speed\" is limited by the sl...",
  "por_txt_cntn": "\nVarious highly-parallel architectures have proposed as replacements for or assistants to traditional processors for highly-parallel computations such as graphic processing, scientific data processing, signal processing, encryption, and others. Although processor design has made great strides improving performance and reducing power, processors are still fundamentally \"jack of all trades\" devices. In many respects, they are more complex than necessary yet not specialized for these applications (i.e., not as fast or energy-efficient as possible). Traditional processors are still critical for their flexibility, but off-loading these computations to more specialized hardware provides significant performance and energy benefits.\n\nA challenge in incorporating accelerators, whether configurable hardware that implements tasks as high-performance circuits, or General-Purpose Graphics Processing Units (GPGPUs) that are a large set of (relatively) simpler processors, is the coordination of these resources within the operating system of a multi-tasking system.\n\nThis project examined scheduling within multitasking heterogeneous systems and issues related to managing \"spatial\" multitasking of accelerators (allocating a subset of the accelerator to each of multiple applications simultaneously) instead of only \"temporal\" multitasking (applications taking turns using all hardware resources at once). We devloped a number of effective techniques that incorporate information regarding the nature of tasks and the hardware to make dynamic resource allocation decisions that effectively maximize performance, meet deadlines, minimize power, ensure fairness, or some combination of these.\n\nFirst, we developed algorithms for scheduling tasks in heterogeneous real-time systems. We examined systems with firm-deadline tasks, where a deadline is an expiration time for the \"usefulness\" of a computation. For example, if a frame in a video cannot be decoded quickly enough to play it at the correct time, it is often best to \"drop\" (not show) the frame rather than slow the video. In this work, each task required a different time to complete on each resource, and in some cases the \"preferred\" resource for each task (the one on which it executes most quickly) differed for different tasks (modeling different affinities of tasks to type of resources). We created scheduling methodologies that reduced deadline misses for these systems.\n\nWe also created scheduling techniques for mixed-deadline heterogeneous real-time systems that execute a combination of periodic hard-deadline tasks (i.e., known tasks whose deadlines must not be missed or the system fails) and aperiodic soft-deadline tasks (cannot predict these prior to run-time, but they still require responsiveness). This models a mixed-use system such as a smartphone. Our methods increased the ability to complete the aperiodic tasks within a reasonable time period while ensuring that all hard deadlines were met.\n\nOther aspects of this work examined spatial multitasking within GPGPU systems. First we examined allocating GPGPU resources to tasks based on meeting quality-of-service (QoS) requirements (e.g., meeting minimum video playback speed). We presented two possible alternatives: (1) disabling \"extra\" resources (not required to meet QoS) to save power, and (2) reallocating \"extra\" resources to \"best-effort\" applications (without a QoS requirement) to improve their performance. This method was able to measure performance and dynamically adjust the allocation as needed. We also demonstrated significant power savings or performance improvements depending on the chosen method.\n\nAnother study took advantage of process variation within GPGPUs to improve performance and energy. The manufacturing process results in some chip regions having the potential to run faster than others. Normally a chip's \"speed\" is limited by the slowest region. In this work, we assumed we could clock each GPGPU compute unit at its own maximum f..."
 }
}