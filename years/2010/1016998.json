{
 "awd_id": "1016998",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC:Small: Tactile communication in human-computer Interactions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2010-08-15",
 "awd_exp_date": "2013-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2010-08-01",
 "awd_max_amd_letter_date": "2010-08-01",
 "awd_abstract_narration": "Tactile communication systems represent a promising arena for enhancing human-computer interactions by using a relatively underused sensory system, the sense of touch, to present information. The goal of the proposed research is to understand how the sense of touch can be used to facilitate human-computer interactions by presenting information using tactile cues that is usually presented in the visual or auditory modalities. The tactile signals are similar to those produced by a vibrating cell phone or pager. One of the challenges in using a tactile display that has an array of vibrating motors distributed over the skin is in determining what type of information can be presented tactually, which aspects of vibrotactile stimulation can be used to convey this information effectively, and what tasks benefit from tactile cues. One objective of the proposed research is to develop a conceptual framework that provides guidance to computer-interface designers on how tactile communication systems can be created that reduce work load, decrease errors associated with information overload and diminish the time taken to complete tasks.\r\n\r\nThe broader impact and application of this research extends well beyond computer interfaces, to all tactile interfaces that are used to display information to users, particularly those with visual and auditory impairments. Tactile displays are being developed to aid navigation in the visually impaired and to assist the hearing impaired in learning to lip read; the proposed research directly impacts the development of these displays by determining the optimal properties of vibrotactile signals that users can easily learn and identify. In addition to these applications, the present research is highly relevant to the implementation of tactile feedback in mobile devices and provides guidance as to the types of signals that are processed most efficiently by users.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lynette",
   "pi_last_name": "Jones",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Lynette A Jones",
   "pi_email_addr": "LJones@MIT.edu",
   "nsf_id": "000092937",
   "pi_start_date": "2010-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research is focused on understanding how a relatively underutilized sensory system, the sense of touch, can be used to present information to people interacting with computers or mobile devices and to determine what properties of tactile signals are easily learned and perceived. In many situations the visual and auditory systems are overloaded and so tactile communication systems represent a promising arena for presenting information. These systems typically comprise vibrating motors that can be activated at different frequencies and amplitudes and attached at different locations on the body. Over the past decade, tactile display technologies have become more sophisticated, less intrusive and thus more effective and acceptable to users. The objective of our research has been to determine how tactile vocabularies can be created so that users can readily identify the meaning of tactile patterns presented on their skin. By analyzing which properties of tactile signals are reliably perceived and how the different types of motors used to vibrate the skin affect the ability to identify different tactile patterns, guidance can be provided to designers of tactile interfaces in different application domains. We have demonstrated that a tactile vocabulary of around 10 &ldquo;words&rdquo; can be readily learned when presented on the forearm, hand or finger and that a larger vocabulary is feasible if the location at which a tactile signal is presented (e.g. on the hand, forearm or shoulder)&nbsp;can be used as an additional cue to encode information. The results also indicated that it was surprisingly difficult for participants in the research studies to identify tactile signals that varied only in frequency and amplitude, and that for these dimensions it appears that&nbsp;only two or three values&nbsp;should be used to create tactile vocabularies.&nbsp;Tactile displays may be part of a hand-held device such as a mobile&nbsp;phone, or mounted in a seat in a vehicle or&nbsp;in a sleeve or wrist&nbsp;band that is worn. The information transmitted by the display may change&nbsp;depending on where it makes contact with the body, and so we analyzed how the mechanical properties of skin at different sites on the body influenced the&nbsp;tactile information transmitted. The vibrotactile signals were found to change in both frequency and amplitude as a function of location, with the main difference&nbsp;being between the skin on the palm of the hand and&nbsp;the other locations tested&nbsp;(forearm and thigh). This result means that tactile vocabularies used at one location on the body will not necessarily be perceived to be the same when presented at another site. The application of this research extends well beyond users of mobile devices and other computing platforms for which tactile displays are becoming an increasingly important component. People with visual impairments can use wearable tactile displays to assist in navigation and receive simple messages; these displays have also been shown to assist in speech comprehension for those with auditory impairments. There are many other application areas of this research ranging from assisting people working in hazardous environments where visual and auditory information may be absent or unreliable to helping drivers deal with visual information&nbsp;overload by presenting salient information such as warnings and navigation cues via a tactile display.&nbsp;This research has been presented to a number of companies interested in incorporating tactile displays in their products&nbsp;and has been featured in a range of on-line science and engineering websites.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/14/2013<br>\n\t\t\t\t\tModified by: Lynette&nbsp;A&nbsp;Jones</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research is focused on understanding how a relatively underutilized sensory system, the sense of touch, can be used to present information to people interacting with computers or mobile devices and to determine what properties of tactile signals are easily learned and perceived. In many situations the visual and auditory systems are overloaded and so tactile communication systems represent a promising arena for presenting information. These systems typically comprise vibrating motors that can be activated at different frequencies and amplitudes and attached at different locations on the body. Over the past decade, tactile display technologies have become more sophisticated, less intrusive and thus more effective and acceptable to users. The objective of our research has been to determine how tactile vocabularies can be created so that users can readily identify the meaning of tactile patterns presented on their skin. By analyzing which properties of tactile signals are reliably perceived and how the different types of motors used to vibrate the skin affect the ability to identify different tactile patterns, guidance can be provided to designers of tactile interfaces in different application domains. We have demonstrated that a tactile vocabulary of around 10 \"words\" can be readily learned when presented on the forearm, hand or finger and that a larger vocabulary is feasible if the location at which a tactile signal is presented (e.g. on the hand, forearm or shoulder) can be used as an additional cue to encode information. The results also indicated that it was surprisingly difficult for participants in the research studies to identify tactile signals that varied only in frequency and amplitude, and that for these dimensions it appears that only two or three values should be used to create tactile vocabularies. Tactile displays may be part of a hand-held device such as a mobile phone, or mounted in a seat in a vehicle or in a sleeve or wrist band that is worn. The information transmitted by the display may change depending on where it makes contact with the body, and so we analyzed how the mechanical properties of skin at different sites on the body influenced the tactile information transmitted. The vibrotactile signals were found to change in both frequency and amplitude as a function of location, with the main difference being between the skin on the palm of the hand and the other locations tested (forearm and thigh). This result means that tactile vocabularies used at one location on the body will not necessarily be perceived to be the same when presented at another site. The application of this research extends well beyond users of mobile devices and other computing platforms for which tactile displays are becoming an increasingly important component. People with visual impairments can use wearable tactile displays to assist in navigation and receive simple messages; these displays have also been shown to assist in speech comprehension for those with auditory impairments. There are many other application areas of this research ranging from assisting people working in hazardous environments where visual and auditory information may be absent or unreliable to helping drivers deal with visual information overload by presenting salient information such as warnings and navigation cues via a tactile display. This research has been presented to a number of companies interested in incorporating tactile displays in their products and has been featured in a range of on-line science and engineering websites.\n\n\t\t\t\t\tLast Modified: 08/14/2013\n\n\t\t\t\t\tSubmitted by: Lynette A Jones"
 }
}