{
 "awd_id": "1018840",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Breaking the Address Translation Barrier in Large Memory Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 489171.0,
 "awd_amount": 496671.0,
 "awd_min_amd_letter_date": "2010-08-02",
 "awd_max_amd_letter_date": "2012-08-21",
 "awd_abstract_narration": "Today, inexpensive computer systems based on commodity, off-the-shelf components can support hundreds of gigabytes of memory. Traditionally, the demand for large-memory systems came predominantly from the operators of databases for applications such as high-volume transaction processing.  Today, however, a much wider variety of applications drive the demand for such large-memory systems, ranging from server consolidation using virtualization to infrastructure for Web 2.0 applications. At a scale of 100GB or more, for many of these applications, virtual memory access becomes a bottleneck.  Specifically, the overhead of address translation increases dramatically.  Large pages can mitigate this problem by significantly increasing translation look-aside buffer (TLB) coverage.  However, all too often these applications exhibit poor temporal and/or spatial locality of reference.  Thus, even with large pages, the TLB hit rate is very low.\r\n\r\nThis research will develop novel architectural mechanisms and operating systems support to mitigate the cost of address translation. Effective approaches to this problem include caching internal levels of the page table in dedicated hardware, providing hardware support to exploit physically contiguous memory reservations within the operating system, and re-examining page table organizations for large address spaces.  This research will explore all of these techniques and carefully consider the interactions between memory allocation and management in the operating system and address translation overhead in the hardware.\r\n\r\nThis research will transform the way in which address translation is performed on future systems, enabling the effective use of hundreds of gigabytes of memory.  Currently, large memory machines suffer from address translation bottlenecks, limiting overall performance.  As these machines consume significant amounts of power, this leads to poor power efficiency, wasting both energy and money.  More efficient address translation will lead to significant improvements, especially in datacenters with numerous large memory machines.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Scott",
   "pi_last_name": "Rixner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Scott Rixner",
   "pi_email_addr": "rixner@rice.edu",
   "nsf_id": "000102366",
   "pi_start_date": "2010-08-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Cox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan Cox",
   "pi_email_addr": "alc@rice.edu",
   "nsf_id": "000277789",
   "pi_start_date": "2010-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "6100 MAIN ST",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "794100",
   "pgm_ele_name": "COMPUTER ARCHITECTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7796",
   "pgm_ref_txt": "ALGORITHMIC FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 489171.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 7500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Today, inexpensive computer systems based on commodity, off-the-shelf<br />components can support 100s of gigabytes of memory.&nbsp; Traditionally,<br />the demand for large-memory systems came predominantly from the<br />operators of databases for applications such as high-volume<br />transaction processing.&nbsp; Now, however, applications that require<br />such large-memory systems include a much wider variety of<br />applications, ranging from server consolidation using virtualization<br />to infrastructure for Web 2.0 applications.<br /><br />For many of these applications, at a scale of 100GB or more, access to<br />data in virtual memory becomes a bottleneck.&nbsp; Specifically, the<br />overhead of address translation increases dramatically.&nbsp; Large pages<br />can mitigate this problem.&nbsp; In particular, they significantly increase<br />TLB coverage.&nbsp; However, all too often these applications exhibit poor<br />temporal and/or spatial locality of reference.&nbsp; For example, the<br />memory access pattern of a database hash join is essentially random.<br />Thus, even with large pages, the TLB hit rate can be very low.<br /><br />The overall objective of this project has been to explore approaches<br />and mechanisms to improve performance and reduce energy in large-scale<br />memory systems.&nbsp; Over the course of this project, we have explored a<br />wide range of issues with modern memory systems spanning the stack of<br />layers from hardware (TLBs, nested TLBs, and memory controllers) to<br />system software (memory-based key-value stores and memory allocation).<br />This holistic approach to memory system design has helped to<br />illuminate where the performance bottlenecks are in modern large-scale<br />memory systems.<br /><br />We have shown that address translation can have a large impact on<br />system performance, both with and without virtualization.&nbsp; We have<br />taken a fresh look at address translation and developed several<br />innovative hardware mechanisms to better match and exploit the<br />allocation policies of modern operating systems.&nbsp; Our innovative TLB<br />architectures reduce address translation latencies without requiring<br />any modifications to the system software.&nbsp; We have also shown that by<br />modifying the system software, we can gain further benefits.&nbsp; We have<br />developed page clustering policies that strike a balance between the<br />address translation benefits of large pages and the costs of<br />performing unnecessary, speculative I/O.&nbsp; This work benefits<br />applications by reducing the latency of both code and data accesses<br />without increasing the amount of I/O.<br /><br />We have also shown that replacement cost is a critical factor in the<br />design of large-scale, memory-based, key-value stores.&nbsp; We have<br />demonstrated that Web 2.0 applications can benefit from providing<br />their cost of computing a value to the key-value store.&nbsp; We have also<br />developed an innovative replacement policy that makes replacement<br />decisions by integrating the locality of access to key-value pairs<br />with the cost of later reinserting a key-value pair after its removal.<br />This replacement policy has amortized constant time, making its<br />overhead competitive with existing policies, while yielding much<br />better replacement decisions.&nbsp; This work will not only improve the<br />performance of existing key-value stores, but will hopefully drive<br />further innovation in the area.<br /><br />As the demand for memory capacity continues to increase at an<br />incredible rate, it is becoming critical to minimize address<br />translation overheads.&nbsp; Currently, large memory machines suffer from<br />address translation bottlenecks, limiting overall performance.&nbsp; As<br />these machines consume significant amounts of power, this leads to<br />poor po...",
  "por_txt_cntn": "\nToday, inexpensive computer systems based on commodity, off-the-shelf\ncomponents can support 100s of gigabytes of memory.  Traditionally,\nthe demand for large-memory systems came predominantly from the\noperators of databases for applications such as high-volume\ntransaction processing.  Now, however, applications that require\nsuch large-memory systems include a much wider variety of\napplications, ranging from server consolidation using virtualization\nto infrastructure for Web 2.0 applications.\n\nFor many of these applications, at a scale of 100GB or more, access to\ndata in virtual memory becomes a bottleneck.  Specifically, the\noverhead of address translation increases dramatically.  Large pages\ncan mitigate this problem.  In particular, they significantly increase\nTLB coverage.  However, all too often these applications exhibit poor\ntemporal and/or spatial locality of reference.  For example, the\nmemory access pattern of a database hash join is essentially random.\nThus, even with large pages, the TLB hit rate can be very low.\n\nThe overall objective of this project has been to explore approaches\nand mechanisms to improve performance and reduce energy in large-scale\nmemory systems.  Over the course of this project, we have explored a\nwide range of issues with modern memory systems spanning the stack of\nlayers from hardware (TLBs, nested TLBs, and memory controllers) to\nsystem software (memory-based key-value stores and memory allocation).\nThis holistic approach to memory system design has helped to\nilluminate where the performance bottlenecks are in modern large-scale\nmemory systems.\n\nWe have shown that address translation can have a large impact on\nsystem performance, both with and without virtualization.  We have\ntaken a fresh look at address translation and developed several\ninnovative hardware mechanisms to better match and exploit the\nallocation policies of modern operating systems.  Our innovative TLB\narchitectures reduce address translation latencies without requiring\nany modifications to the system software.  We have also shown that by\nmodifying the system software, we can gain further benefits.  We have\ndeveloped page clustering policies that strike a balance between the\naddress translation benefits of large pages and the costs of\nperforming unnecessary, speculative I/O.  This work benefits\napplications by reducing the latency of both code and data accesses\nwithout increasing the amount of I/O.\n\nWe have also shown that replacement cost is a critical factor in the\ndesign of large-scale, memory-based, key-value stores.  We have\ndemonstrated that Web 2.0 applications can benefit from providing\ntheir cost of computing a value to the key-value store.  We have also\ndeveloped an innovative replacement policy that makes replacement\ndecisions by integrating the locality of access to key-value pairs\nwith the cost of later reinserting a key-value pair after its removal.\nThis replacement policy has amortized constant time, making its\noverhead competitive with existing policies, while yielding much\nbetter replacement decisions.  This work will not only improve the\nperformance of existing key-value stores, but will hopefully drive\nfurther innovation in the area.\n\nAs the demand for memory capacity continues to increase at an\nincredible rate, it is becoming critical to minimize address\ntranslation overheads.  Currently, large memory machines suffer from\naddress translation bottlenecks, limiting overall performance.  As\nthese machines consume significant amounts of power, this leads to\npoor power efficiency, wasting both energy and money.  This research\nhas advanced the state-of-the-art in address translation, lowering the\noverheads for future large-scale memory systems.  More efficient\naddress translation will lead to significant improvements, especially\nin data centers with numerous large memory machines.\n\n\t\t\t\t\tLast Modified: 12/19/2014\n\n\t\t\t\t\tSubmitted by: Scott Rixner"
 }
}