{
 "awd_id": "0964420",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research:  Recognition of Materials",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2010-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 392638.0,
 "awd_amount": 392638.0,
 "awd_min_amd_letter_date": "2010-06-28",
 "awd_max_amd_letter_date": "2013-06-18",
 "awd_abstract_narration": "We live in a world made of diverse materials whose variations in appearance enrich our visual experience. It is also this variability of materials that adds daunting complexity to image understanding. This research program aims to establish the theoretical and computational foundation for automatic visual understanding and recognition of real-world materials. The program tackles this challenging problem from three key aspects, namely, deriving 1) novel hybrid physically-based and data-driven representations of the spatial, angular, spectral, temporal, and scale variations of material appearance, 2) active and passive methods for estimating the values of physically-based parameters that govern material appearance, and 3) single-image material recognition methods that leverage physically-based optical parameters as priors or invariants to guide machine learning techniques. These research thrusts lead to a comprehensive set of computational tools to recognize materials in real-world images despite their complex appearance variations, such as recognizing rusted metals, discerning soft cloth from hard concrete, identifying different fat content of milks, and labeling image regions with material traits like soft, hard, rough, and heavy.\r\n\r\nThe capabilities resulting from this program are crucial to a broad range of scenarios, for instance, to enable humanoid robots to understand that it should not squeeze the soft hands of a child, autonomous vehicles to understand what regions to avoid in a rugged terrain, visual analyses of tissues to help medical diagnosis, and automated inspection systems to reliably discover sub-standard quality food to prevent ill-health. The PIs work with research groups in these specific application areas to closely integrate the results from this project into their efforts. The results from this research are also broadly disseminated via publications, websites, databases, new courses and symposiums.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ko",
   "pi_last_name": "Nishino",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ko Nishino",
   "pi_email_addr": "kon@drexel.edu",
   "nsf_id": "000429327",
   "pi_start_date": "2010-06-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Drexel University",
  "inst_street_address": "3141 CHESTNUT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158956342",
  "inst_zip_code": "191042875",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "DREXEL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "XF3XM9642N96"
 },
 "perf_inst": {
  "perf_inst_name": "Drexel University",
  "perf_str_addr": "3141 CHESTNUT ST",
  "perf_city_name": "PHILADELPHIA",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191042875",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 90887.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 95703.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 100511.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 105537.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this research program was to derive computational methods for recognizing materials from images, that is, automatically identify what things captured in an image are made of. Humans can tell what materials constitute their surroundings with relative ease, but this is a challenging task for computers as it requires complex reasoning based on various contextual cues. We tackled this difficult task with two distinct approaches. The first was to estimate the reflectance of object surfaces. Surface reflectance (i.e., the way light scatters at object surfaces) is dependent on the micro geometry of the surface and its characteristics are more or less dictated by its underlying material. Our aim was to &nbsp;derive methods to recover this reflectance from arbitrary images, which requires decomposition of the scene appearance into its physical constituents, namely the geometry, illumination, and reflectance. We showed that we can indeed reliably estimate reflectance from as few as a single image taken under complex, natural illumination, and of intricate geometry. The other approach was to directly classify each pixel into different material categories. This is a difficult task even for human vision as global contextual information cannot be used. We focus on this local material recognition as we would like to recognize a wool glove as the looks indicate that it's made of wool, not because the computer recognizes it as a glove (then we will have trouble recognizing a leather glove). We achieved this by first training computers to recognize the characteristic looks that are distinctive of certain material cateogries, which we call visual materia traits. These are visual attributes, such as rough, smooth, and shiny that collecitvely enables the computer to discern different material categories (e.g., wool is rough and fuzzy but not shiny). We showed that we can successfully train visual material trait classifiers which in turn enables reliable local material recognition. These results constitute the computational foundation for exploiting rich visual information pertaining to what materials underly a scene captured in images, which would likely find applications in a wide range of areas. For instance, a robot will be able to tell the difference of a paper cup and a ceramic mug and plan ahead on how to pick them up. We may query photos based on materials, e.g., find that image that had my satin chair in it. They may also lead to better medical diagnosis, for instance, for describing dermatological lesions. The computational methods also provide insights into how we, human beings, perceive materials in our visual system. Recognizing material from images alone already has such significant implications, which can also easily be combined with other sensory modalities, most important tactile sensing, and opens new avenues of research.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/29/2015<br>\n\t\t\t\t\tModified by: Ko&nbsp;Nishino</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2015/0964420/0964420_10017738_1443579417550_refandill--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2015/0964420/0964420_10017738_1443579417550_refandill--rgov-800width.jpg\" title=\"Joint estimation of reflectance and illumination from a single image\"><img src=\"/por/images/Reports/POR/2015/0964420/0964420_10017738_1443579417550_refandill--rgov-66x44.jpg\" alt=\"Joint estimation of reflectance and illumination fr...",
  "por_txt_cntn": "\nThe goal of this research program was to derive computational methods for recognizing materials from images, that is, automatically identify what things captured in an image are made of. Humans can tell what materials constitute their surroundings with relative ease, but this is a challenging task for computers as it requires complex reasoning based on various contextual cues. We tackled this difficult task with two distinct approaches. The first was to estimate the reflectance of object surfaces. Surface reflectance (i.e., the way light scatters at object surfaces) is dependent on the micro geometry of the surface and its characteristics are more or less dictated by its underlying material. Our aim was to  derive methods to recover this reflectance from arbitrary images, which requires decomposition of the scene appearance into its physical constituents, namely the geometry, illumination, and reflectance. We showed that we can indeed reliably estimate reflectance from as few as a single image taken under complex, natural illumination, and of intricate geometry. The other approach was to directly classify each pixel into different material categories. This is a difficult task even for human vision as global contextual information cannot be used. We focus on this local material recognition as we would like to recognize a wool glove as the looks indicate that it's made of wool, not because the computer recognizes it as a glove (then we will have trouble recognizing a leather glove). We achieved this by first training computers to recognize the characteristic looks that are distinctive of certain material cateogries, which we call visual materia traits. These are visual attributes, such as rough, smooth, and shiny that collecitvely enables the computer to discern different material categories (e.g., wool is rough and fuzzy but not shiny). We showed that we can successfully train visual material trait classifiers which in turn enables reliable local material recognition. These results constitute the computational foundation for exploiting rich visual information pertaining to what materials underly a scene captured in images, which would likely find applications in a wide range of areas. For instance, a robot will be able to tell the difference of a paper cup and a ceramic mug and plan ahead on how to pick them up. We may query photos based on materials, e.g., find that image that had my satin chair in it. They may also lead to better medical diagnosis, for instance, for describing dermatological lesions. The computational methods also provide insights into how we, human beings, perceive materials in our visual system. Recognizing material from images alone already has such significant implications, which can also easily be combined with other sensory modalities, most important tactile sensing, and opens new avenues of research. \n\n\t\t\t\t\tLast Modified: 09/29/2015\n\n\t\t\t\t\tSubmitted by: Ko Nishino"
 }
}