{
 "awd_id": "0964705",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI:  Medium:  Collaborative Research:  Optimizing Policies for Service Organizations in Complex Structured Domains",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927215",
 "po_email": "tleen@nsf.gov",
 "po_sign_block_name": "Todd Leen",
 "awd_eff_date": "2010-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 581681.0,
 "awd_amount": 588401.0,
 "awd_min_amd_letter_date": "2010-07-08",
 "awd_max_amd_letter_date": "2013-06-15",
 "awd_abstract_narration": "This project studies an important class of complex structured planning \r\ndomains called ``service domains'' using simulators and probabilistic\r\naction models. Examples of service domains include optimizing emergency \r\nresponse in a typical city, scheduling doctors and nurses in a \r\nhospital, administering tasks in a typical office, optimally delivering \r\nproducts to shops from distribution centers. These domains \r\nshare many characteristics such as relational structure, parallel actions, \r\nmulti-time-scale decision making, exogenous events, and the need for \r\nhuman interpretable solutions that make them highly challenging.\r\nThe project develops scalable and principled planning algorithms for \r\nservice domains through a variety of techniques including a novel\r\nhierarchical framework of multi-time-scale optimization, new\r\nmodel-free simulation-based planning algorithms, and model-based \r\nplanning via composition of first-order decision diagrams. These \r\ntechniques are applied to the real-world problem of optimizing \r\nthe fire and emergency response in cities through a collaboration\r\nwith the fire department of Corvallis, Oregon. \r\n\r\n\r\nThe results of the project include new algorithms and frameworks to solve \r\nservice domains, prototype implementations of the algorithms\r\nin the emeregency response domain, and new testbeds of service domains \r\nfor research. The broader impact of the work includes more cost-effective \r\nemergency response systems, and development of new research-oriented courses, \r\ntutorials and special workshops on the next generation decision support \r\nsystems for service domains.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Prasad",
   "pi_last_name": "Tadepalli",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Prasad H Tadepalli",
   "pi_email_addr": "tadepall@eecs.orst.edu",
   "nsf_id": "000101828",
   "pi_start_date": "2010-07-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Fern",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan Fern",
   "pi_email_addr": "afern@eecs.oregonstate.edu",
   "nsf_id": "000088242",
   "pi_start_date": "2010-07-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "1500 SW JEFFERSON AVE",
  "perf_city_name": "CORVALLIS",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973318655",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 150122.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 141315.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 169627.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 127337.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Automated Planning</strong> seeks to develop algorithms for taking optimal actions to achieve desired long-term objectives. Example applications of planning <br />includes optimizing emergency response in a typical city, conservation <br />planning, scheduling doctors and nurses in a hospital, administering tasks <br />in a typical office, and controlling the inventories of distribution centers.<br />These domains are problematic for state of the art planners due to the <br />presence of highly parallel actions and the high degree of uncertainty in the <br />action outcomes. All these domains are formalized as Markov Decision Processes (MDPs) with large state and action spaces. The current project developed&nbsp; several planning algorithms based on the following approaches. <br /><br />1. <strong>Symbolic Dynamic Programming (SDP).</strong> This class of algorithms are <br />distinguished by employing symbolic reasoning to derive symbolic descriptions of value functions and policies. The current project extended previous work on symbolic value iteration and policy iteration algorithms in <br />the following directions.</p>\n<p>a. Symbolic value iteration was extended to <em>factored actions</em> by enhancing<br />the representations of policies and value functions to include action <br />variables. A new version of the symbolic value iteration algorithm that <br />smoothly trades off space for time, and a novel approximate algorithm that <br />significantly reduces planning time with little loss in solution quality were <br />also developed and were shown to achieve state of the art performance.</p>\n<p>b. A new algorithm called <em>optimistic policy iteration</em> was developed, which is <br />a hybrid between the value iteration and the policy iteration and guarantees <br />space-bounded generalization for factored state and action Markov decision <br />processes. <br /><br />c. Multiple versions of <em>symbolic real-time dynamic programming</em> algorithms <br />were also developed that take advantage of the initial state to generalize<br />incrementally. All algorithms were shown to perform competitively on the <br />benchmark planning domains and often significantly outperformed the prior <br />state of the art. <br /><br />d. A <em>relational</em> version of <em>symbolic value iteration</em> algorithm was developed, <br />which exploited first order decision diagrams and novel reduction operators <br />to solve relational MDPs with additive rewards and exogenous events. The <br />algorithm is guaranteed to achieve a lower bound on the optimal performance under some technical conditions. A practical variant was also developed that takes advantage of a set of sampled states to derive an approximate solution.</p>\n<p>2. <strong>Integer Linear Programming</strong>. It is known that deterministic planning <br />problems can be reduced to integer linear programming (ILP) for which fast <br />algorithms are available. In this work we formulate probabilistic planning in<br />the ILP framework by integrating it with two innovations. The first idea, hindsight approximation, replaces the probabilistic actions with deterministic actions by prior sampling of their outcomes. Although hindsight approximation is known to be optimistic in general, it has been found to work well in practice. The second idea is to facilitate efficient encoding of parallel actions by introducing decision variables that correspond to action factors. The overall idea is to reduce the probabilistic planning to solving a sample of deterministic planning problems with a shared first action via ILP. The resulting approach has been shown to be very effective and competitive with state of the art planners in many benchmark planning domains despite the optimistic hindsight approximation. It has also been successfully demonstrated in a bird conservation application.</p>\n<p>3. <strong>Factored Monte-...",
  "por_txt_cntn": "\nAutomated Planning seeks to develop algorithms for taking optimal actions to achieve desired long-term objectives. Example applications of planning \nincludes optimizing emergency response in a typical city, conservation \nplanning, scheduling doctors and nurses in a hospital, administering tasks \nin a typical office, and controlling the inventories of distribution centers.\nThese domains are problematic for state of the art planners due to the \npresence of highly parallel actions and the high degree of uncertainty in the \naction outcomes. All these domains are formalized as Markov Decision Processes (MDPs) with large state and action spaces. The current project developed  several planning algorithms based on the following approaches. \n\n1. Symbolic Dynamic Programming (SDP). This class of algorithms are \ndistinguished by employing symbolic reasoning to derive symbolic descriptions of value functions and policies. The current project extended previous work on symbolic value iteration and policy iteration algorithms in \nthe following directions.\n\na. Symbolic value iteration was extended to factored actions by enhancing\nthe representations of policies and value functions to include action \nvariables. A new version of the symbolic value iteration algorithm that \nsmoothly trades off space for time, and a novel approximate algorithm that \nsignificantly reduces planning time with little loss in solution quality were \nalso developed and were shown to achieve state of the art performance.\n\nb. A new algorithm called optimistic policy iteration was developed, which is \na hybrid between the value iteration and the policy iteration and guarantees \nspace-bounded generalization for factored state and action Markov decision \nprocesses. \n\nc. Multiple versions of symbolic real-time dynamic programming algorithms \nwere also developed that take advantage of the initial state to generalize\nincrementally. All algorithms were shown to perform competitively on the \nbenchmark planning domains and often significantly outperformed the prior \nstate of the art. \n\nd. A relational version of symbolic value iteration algorithm was developed, \nwhich exploited first order decision diagrams and novel reduction operators \nto solve relational MDPs with additive rewards and exogenous events. The \nalgorithm is guaranteed to achieve a lower bound on the optimal performance under some technical conditions. A practical variant was also developed that takes advantage of a set of sampled states to derive an approximate solution.\n\n2. Integer Linear Programming. It is known that deterministic planning \nproblems can be reduced to integer linear programming (ILP) for which fast \nalgorithms are available. In this work we formulate probabilistic planning in\nthe ILP framework by integrating it with two innovations. The first idea, hindsight approximation, replaces the probabilistic actions with deterministic actions by prior sampling of their outcomes. Although hindsight approximation is known to be optimistic in general, it has been found to work well in practice. The second idea is to facilitate efficient encoding of parallel actions by introducing decision variables that correspond to action factors. The overall idea is to reduce the probabilistic planning to solving a sample of deterministic planning problems with a shared first action via ILP. The resulting approach has been shown to be very effective and competitive with state of the art planners in many benchmark planning domains despite the optimistic hindsight approximation. It has also been successfully demonstrated in a bird conservation application.\n\n3. Factored Monte-Carlo Tree Search. Monte-Carlo Tree Search (MCTS) is a popular method of choice in probabilistic planning. However, MCTS is found to degrade very rapidly when the problem size is increased. The main reason for this is that MCTS searches in the space of atomic states, and fails to take advantage of their internal structure. The current project explored a..."
 }
}