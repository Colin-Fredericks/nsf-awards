{
 "awd_id": "0953100",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:  Autotuning Foundations for Exascale Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-04-15",
 "awd_exp_date": "2015-03-31",
 "tot_intn_awd_amt": 460000.0,
 "awd_amount": 460000.0,
 "awd_min_amd_letter_date": "2010-04-21",
 "awd_max_amd_letter_date": "2014-06-01",
 "awd_abstract_narration": "The goal of this research is to discover novel foundational principles\r\nfor developing highly-efficient and reliable software that can achieve\r\nsustainable performance on the exascale computing platforms expected by\r\n2020. Such platforms will deliver three orders of magnitude beyond\r\ntoday?s systems; harnessing this raw computational power could\r\nrevolutionize our modeling and understanding of critical phenomena in\r\nareas like climate modeling, energy, medicine, sustainability,\r\ncosmology, engineering design, and massive-scale data analytics. Yet,\r\ndeveloping software for exascale systems is a tremendous challenge\r\nbecause the hardware is complex and it is not believed that the most\r\nproductive ?high-level? software development environments (e.g.,\r\nprogramming languages and libraries) will be able to effectively exploit\r\nthese exascale systems.\r\n\r\nThe investigator aims to address this challenge by using automated\r\ntuning (autotuning) to eliminate the low performance traditionally\r\nassociated with high-level programming models. This research (a)\r\ndevelops new model-driven frameworks for tuning parallel algorithms and\r\ndata structures, going beyond existing techniques that focus on\r\nlow-level code tuning; and (b) studies autotuning for programs expressed\r\nin high-level programming models, with the aim of eliminating the\r\nperformance gap. Concomitant with this research, the PI will create a\r\nnew practicum course: The HPC Garage. The HPC Garage physically\r\nco-locates interdisciplinary teams in a social collaborative lab space;\r\nthe teams engage in a year-long competition, called the XD Prize, to\r\ndevelop highly scalable algorithms and software for NSF TeraGrid?s\r\nnext-generation XD facilities. The HPC Garage also hosts summer interns\r\nin Georgia Tech?s Computing Research Undergraduate Intern Summer\r\nExperience (CRUISE) program, whose mission is to encourage students,\r\nespecially those from underrepresented groups, to pursue graduate\r\ndegrees in computing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Vuduc",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Richard W Vuduc",
   "pi_email_addr": "richie@cc.gatech.edu",
   "nsf_id": "000080331",
   "pi_start_date": "2010-04-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "926 DALNEY ST NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303186395",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  },
  {
   "pgm_ele_code": "732900",
   "pgm_ele_name": "COMPILERS"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 120437.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 81830.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 83830.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 85890.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 88013.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this CAREER project was to discover fundamental principles of designing automatically tuned algorithms and software for next-generation high-performance computing (or <em>supercomputing</em>) systems. In this context, &ldquo;automatically tuned&rdquo; (<em>autotuned</em>) refers to the idea of an algorithm or software adjusting itself to run as efficiently as possible on a given machine. The motivation for autotuned systems is that supercomputer-class systems are become so complex that it is become harder to predict how efficiently an algorithm or piece of software will run. Consequently, high-performance software development is becoming more costly, as is the cost of running the machines, in terms of all the metrics of time, energy, and power to execute a program.</p>\n<p><br />The <strong>intellectual merit </strong>of this CAREER project has been to produce a number of research results that advance our understanding of how to build autotuned systems for supercomputers.</p>\n<p><br />The first set of research results concerns a number of new principles for designing algorithms and improving the performance of software on current and future supercomputers that contain, as one of their key components, <em>manycore co-processors</em>. These principles highlight not just the benefits of using such co-processors, but also their limitations, which is critical to our understanding of such systems. One highlight of these results was the collaborative development of a blood flow simulation code that, in 2010, received the Gordon Bell Prize, an award for setting a new high-water mark in achieved supercomputer performance.</p>\n<p><br />The second research set of results concerns new models for predicting how well a program will perform. Models are critical to developing autotuned systems in the following way: a &ldquo;tunable&rdquo; or &ldquo;adaptive&rdquo; program will having a number of parameters that need to be set correctly for a given machine; and <em>models</em>&nbsp;can narrow the set of parameters that need to be considered.</p>\n<p>One of these models, referred to as the <em>energy roofline model</em>, permits reasoning not just about the execution time of an algorithm or program, but also its execution <em>energy</em> and&nbsp;<em>power</em>. These latter metrics are becoming as or arguably more important costs to consider than time as the scale of supercomputers increases. A new NSF project to extend this model is underway.</p>\n<p><br />One culmination of these research results is the development of an autotuning framework for a class of computations known as <em>sparse direct solvers</em>. Such solvers lie at the heart of a number of computational science and engineering applications. As this project comes to a close, this prototype is being integrated into a widely used open source solver package.</p>\n<p><br />Beyond its research outcomes, the <strong>broader impact</strong> of this CAREER project has been to advance several educational outcomes aimed at broadening interest in and the use of high-performance computing. These outcomes include the development of a new massively open online course on high-performance computing, which is to become part of Georgia Tech&rsquo;s experimental low-cost Online Masters in Computer Science degree program; as well a new Research Experience for Undergraduate (REU) sub-project that aims to bring high-performance computing to the Web.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/07/2015<br>\n\t\t\t\t\tModified by: Richard&nbsp;W&nbsp;Vuduc</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this CAREER project was to discover fundamental principles of designing automatically tuned algorithms and software for next-generation high-performance computing (or supercomputing) systems. In this context, \"automatically tuned\" (autotuned) refers to the idea of an algorithm or software adjusting itself to run as efficiently as possible on a given machine. The motivation for autotuned systems is that supercomputer-class systems are become so complex that it is become harder to predict how efficiently an algorithm or piece of software will run. Consequently, high-performance software development is becoming more costly, as is the cost of running the machines, in terms of all the metrics of time, energy, and power to execute a program.\n\n\nThe intellectual merit of this CAREER project has been to produce a number of research results that advance our understanding of how to build autotuned systems for supercomputers.\n\n\nThe first set of research results concerns a number of new principles for designing algorithms and improving the performance of software on current and future supercomputers that contain, as one of their key components, manycore co-processors. These principles highlight not just the benefits of using such co-processors, but also their limitations, which is critical to our understanding of such systems. One highlight of these results was the collaborative development of a blood flow simulation code that, in 2010, received the Gordon Bell Prize, an award for setting a new high-water mark in achieved supercomputer performance.\n\n\nThe second research set of results concerns new models for predicting how well a program will perform. Models are critical to developing autotuned systems in the following way: a \"tunable\" or \"adaptive\" program will having a number of parameters that need to be set correctly for a given machine; and models can narrow the set of parameters that need to be considered.\n\nOne of these models, referred to as the energy roofline model, permits reasoning not just about the execution time of an algorithm or program, but also its execution energy and power. These latter metrics are becoming as or arguably more important costs to consider than time as the scale of supercomputers increases. A new NSF project to extend this model is underway.\n\n\nOne culmination of these research results is the development of an autotuning framework for a class of computations known as sparse direct solvers. Such solvers lie at the heart of a number of computational science and engineering applications. As this project comes to a close, this prototype is being integrated into a widely used open source solver package.\n\n\nBeyond its research outcomes, the broader impact of this CAREER project has been to advance several educational outcomes aimed at broadening interest in and the use of high-performance computing. These outcomes include the development of a new massively open online course on high-performance computing, which is to become part of Georgia Tech\u00c6s experimental low-cost Online Masters in Computer Science degree program; as well a new Research Experience for Undergraduate (REU) sub-project that aims to bring high-performance computing to the Web.\n\n\t\t\t\t\tLast Modified: 07/07/2015\n\n\t\t\t\t\tSubmitted by: Richard W Vuduc"
 }
}