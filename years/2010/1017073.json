{
 "awd_id": "1017073",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "DC: Small: Collaborative Research: DARE: Declarative and Scalable Recovery",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Hong Jiang",
 "awd_eff_date": "2010-09-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 190000.0,
 "awd_amount": 190000.0,
 "awd_min_amd_letter_date": "2010-09-15",
 "awd_max_amd_letter_date": "2010-09-15",
 "awd_abstract_narration": "One dominant characteristic of today's large-scale computing systems\r\nis the prevalence of large storage clusters.  Storage clusters at the\r\nscale of hundreds or thousands of commodity machines are\r\nincreasingly being deployed. At companies like Amazon, Google, Yahoo,\r\nand others, thousands of nodes are managed as a single system.\r\n\r\nAs large clusters have brought many benefits, they also bring a new\r\nchallenge: a growing number and frequency of failures that must be\r\nmanaged. Bits, sectors, disks, machines, racks, and many other\r\ncomponents fail.  With millions of servers and hundreds of data\r\ncenters, there are millions of opportunities for these components to\r\nfail. Failing to deal with failures will directly impact the\r\nreliability and availability of data and jobs.\r\n\r\nUnfortunately, we still hear data-loss stories even recently. For\r\nexample, in March 2009, Facebook lost millions of photos due to\r\nsimultaneous disk failures that \"should\" rarely happen at the same\r\ntime (but it happened); in July 2009, a large bank was fined a record\r\ntotal of 3 millions pounds after losing data on thousands of its\r\ncustomers; more recently, in October 2009, T-Mobile Sidekick, which\r\nuses Microsoft's cloud service, also lost its customer data.  These\r\nincidents have shown that existing large-scale storage systems are\r\nstill fragile to failures.\r\n\r\nTo address the challenges of large-scale recovery, the goal of this\r\nproject is to: (1) seek the fundamental problems of recovery in\r\ntoday's scalable world of computing, (2) improve the reliability,\r\nperformance, and scalability of existing large-scale recovery, and (3)\r\nexplore formally grounded languages to empower rigorous specification\r\nof recovery properties and behaviors.  Our vision is to build systems\r\nthat \"DARE to fail\": systems that deliberately fail themselves,\r\nexercise recovery routinely, and enable easy and correct deployment of\r\nnew recovery policies.\r\n\r\nFor more information, please visit this website:\r\nhttp://boom.cs.berkeley.edu/dare/",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Arpaci-Dusseau",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea C Arpaci-Dusseau",
   "pi_email_addr": "dusseau@cs.wisc.edu",
   "nsf_id": "000275887",
   "pi_start_date": "2010-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 N PARK ST STE 6301",
  "perf_city_name": "MADISON",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779300",
   "pgm_ele_name": "DATA-INTENSIVE COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7793",
   "pgm_ref_txt": "DATA-INTENSIVE COMPUTING"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 190000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Large-scale computing and data storage systems in the cloud are an important platform for much of our society's infrastrcture. A critical factor in the availability, reliability, and performance of cloud services is how they react to failure. &nbsp;While many cloud services are able to handle a single \"fail-stop\" fault, how they react to other types of emerging faults are less understood. &nbsp;Other types of faults that we now must consider include when multiple components fail simultaneously, when a component misbehaves and fails silently, and when a component exhibits performance that is many times slower thanusual. &nbsp;Thus, the goals of the DARE project are to understand how existing cloud-based services react to these three new types of faults and to develop services that are more robust and scalable.</p>\n<p>We have analyzed a wide range of existing, widely-used cloud services (e.g., Hadoop, HDFS, ZooKeeper, Cassandra, HBase, and Dropbox) to understand how they react to these three types of failures. &nbsp;By systematically searching through the extremely large space of failures, we have identified numerous implementation and design flaws. &nbsp;For example, even in the relatively well-understood implementation of speculative execution of map-reduce jobs in Hadoop, we uncovered three problems that can lead to a collapse of the entire cluster. &nbsp;First, if all tasks in a map-reduce job are slow, then there is &ldquo;no&rdquo; stragglerthat can be identified. Second, imprecise accounting can cause a slow map node to be instead blamed on an otherwise normal reducer node. Finally, a backup task can be improperly restarted on the same problematic components due to memoryless retry.</p>\n<p>To fix these types of general problems, we have devised, implemented, and evaluated a number of novel solutions. &nbsp;For example, we have developed Fracture, a framework that enables an existing, complex application to be divided into individual mini-processes; each of these mini-processes can be isolated from one another, run in its own environment, and sampled, restarted, or replicated on its own. Second, we proposed an approach for robust RAID storage systems built with flash-based storage devices, Warped Mirrors; Warped Mirrors increase the liklihood that flash devices will not wear out at similar times before a faulty device can be replaced and thus, preserve the independence of device failures within a RAID array. &nbsp;Third, we implemented a robust version of a cloud-based file synchronization service, ViewBox; most importantly, ViewBox will not forward corruptions from one client machine to the cloud version or to other clients and will not propagate inconsistent file system images from a client (such as those that occur after a crash).</p>\n<p>We describe one of our general solutions in more detail. &nbsp;We propose that distributed systems be built with selective and lightweight versioning (SLEEVE), which can detect silent faults in select subsystems in a lightweight manner (with little space and performance overhead). For example, a developer can pick some important functionality (e.g.,file-system namespace management) and protect that functionality by developing a second lightweight implementation of thefunctionality. This approach essentially transforms a target system into an efficient two-version form.</p>\n<p>Using the SLEEVE approach, we hardened three pieces of HDFS functionality: namespace management, replica management, and theread/write protocol. &nbsp;Our experimental results show that while the orginal HDFS code silently misbehaves in many cases, HardFS isolates faulty behavior so that it remains within a single node. In particular, HardFS handles 90% of the fail-silent faults that result from random memory corruption and correctly detects and recovers from 100% of 78 targeted corruptions and 5 real-world bugs. &nbsp;Since errors do not propagate to p...",
  "por_txt_cntn": "\nLarge-scale computing and data storage systems in the cloud are an important platform for much of our society's infrastrcture. A critical factor in the availability, reliability, and performance of cloud services is how they react to failure.  While many cloud services are able to handle a single \"fail-stop\" fault, how they react to other types of emerging faults are less understood.  Other types of faults that we now must consider include when multiple components fail simultaneously, when a component misbehaves and fails silently, and when a component exhibits performance that is many times slower thanusual.  Thus, the goals of the DARE project are to understand how existing cloud-based services react to these three new types of faults and to develop services that are more robust and scalable.\n\nWe have analyzed a wide range of existing, widely-used cloud services (e.g., Hadoop, HDFS, ZooKeeper, Cassandra, HBase, and Dropbox) to understand how they react to these three types of failures.  By systematically searching through the extremely large space of failures, we have identified numerous implementation and design flaws.  For example, even in the relatively well-understood implementation of speculative execution of map-reduce jobs in Hadoop, we uncovered three problems that can lead to a collapse of the entire cluster.  First, if all tasks in a map-reduce job are slow, then there is \"no\" stragglerthat can be identified. Second, imprecise accounting can cause a slow map node to be instead blamed on an otherwise normal reducer node. Finally, a backup task can be improperly restarted on the same problematic components due to memoryless retry.\n\nTo fix these types of general problems, we have devised, implemented, and evaluated a number of novel solutions.  For example, we have developed Fracture, a framework that enables an existing, complex application to be divided into individual mini-processes; each of these mini-processes can be isolated from one another, run in its own environment, and sampled, restarted, or replicated on its own. Second, we proposed an approach for robust RAID storage systems built with flash-based storage devices, Warped Mirrors; Warped Mirrors increase the liklihood that flash devices will not wear out at similar times before a faulty device can be replaced and thus, preserve the independence of device failures within a RAID array.  Third, we implemented a robust version of a cloud-based file synchronization service, ViewBox; most importantly, ViewBox will not forward corruptions from one client machine to the cloud version or to other clients and will not propagate inconsistent file system images from a client (such as those that occur after a crash).\n\nWe describe one of our general solutions in more detail.  We propose that distributed systems be built with selective and lightweight versioning (SLEEVE), which can detect silent faults in select subsystems in a lightweight manner (with little space and performance overhead). For example, a developer can pick some important functionality (e.g.,file-system namespace management) and protect that functionality by developing a second lightweight implementation of thefunctionality. This approach essentially transforms a target system into an efficient two-version form.\n\nUsing the SLEEVE approach, we hardened three pieces of HDFS functionality: namespace management, replica management, and theread/write protocol.  Our experimental results show that while the orginal HDFS code silently misbehaves in many cases, HardFS isolates faulty behavior so that it remains within a single node. In particular, HardFS handles 90% of the fail-silent faults that result from random memory corruption and correctly detects and recovers from 100% of 78 targeted corruptions and 5 real-world bugs.  Since errors do not propagate to persistent storage or other nodes, previously fail-silent errors are transformed into fail-stop errors, enabling the use of standard recovery mechanisms s..."
 }
}