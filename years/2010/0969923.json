{
 "awd_id": "0969923",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Mathematical Programming for Streaming Data",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Sheldon Jacobson",
 "awd_eff_date": "2010-06-01",
 "awd_exp_date": "2013-05-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2010-05-05",
 "awd_max_amd_letter_date": "2010-05-05",
 "awd_abstract_narration": "A large amount of data is now easily accessible in real-time in a streaming fashion: news, traffic, temperature or other physical measurements sent by sensors on cell phones. Applying statistical and machine learning methods to these streaming data sets represents tremendous opportunities for a better real-time understanding of complex physical, social or economic phenomena. These algorithms could be used, for example, to understand trends in how news media cover certain topics, and how these trends evolve over time, or to track incidents in transportation networks.\r\n\r\nUnfortunately, most algorithms for large-scale data analysis are not designed for streaming data; typically, adding data points (representing, say, today's batch of news articles from the Associated Press) requires re-solving the entire problem. In addition, many of these algorithms require the whole data set under consideration to be stored in one place. These constraints make classical methods impractical for modern, live data sets.\r\n\r\nThis project's focus is on optimization algorithms designed to work in online mode, allowing for faster, possibly real-time, updating of solutions when new data or constraints are added to the problem. Efficient online algorithms are currently known for just a few special cases. Using homotopy methods and related ideas, this work will seek to allow online updating for a host of modern data analysis problems. A special emphasis will be put on problems involving sparsity or grouping constraints; such constraints are important for example to understand how a few key features in the data set that explain most of the changes in the data. These new online algorithms will be amenable to distributed implementations to allow for parts of the data to be stored on different servers.\r\n\r\nThese methods will be applied to streaming news data coming from major US media, and also to the problem of online detection, which arises when tracking some important signal over, say, a communication network, in an online fashion.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Laurent",
   "pi_last_name": "El Ghaoui",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Laurent El Ghaoui",
   "pi_email_addr": "elghaoui@eecs.berkeley.edu",
   "nsf_id": "000487972",
   "pi_start_date": "2010-05-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "1608 4TH ST STE 201",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947101749",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "9147",
   "pgm_ref_txt": "GENERIC TECHNOL FOR MANUFACTURING CELLS"
  },
  {
   "pgm_ref_code": "MANU",
   "pgm_ref_txt": "MANUFACTURING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focussed on the solution of large optimization problems that arise in machine learning and more generally \"big data\" applications. These problems arise in a variety pf applications: deciding for example, which few features determine health outcomes for patients in an emergency unit in a hospital, or the likelihood of bankrupcy of businesses; or which keywords make an email spam or not spam. In this age of big data, such problems become very large. This project has introduced novel methods that allow one to drastically reduce the size of the problem with a computationally fast, but rigorous test. We have also examined the case when the data comes in as a stream, and demonstrated how to re-use previous computations. Again, the goal was to speed up ad scale up.</p>\n<p><br />In our driving application, which is text analytics, we are facing very large collections of text documents, in the millions of scientific articles for example; and we would like to analyze how a certain topic (e.g., \"cervical cancer\") is treated, by discovering which keywords \"trigger\" the appearance of that topic in any unit of the texts. We have developed a web-based text analytics platform, StatNews, that displays the result of our analyses in real-time, in a visually intuitive way. The technology developed in this project is what made possible for us to solve very large problems in real-time.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/30/2013<br>\n\t\t\t\t\tModified by: Laurent&nbsp;El Ghaoui</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focussed on the solution of large optimization problems that arise in machine learning and more generally \"big data\" applications. These problems arise in a variety pf applications: deciding for example, which few features determine health outcomes for patients in an emergency unit in a hospital, or the likelihood of bankrupcy of businesses; or which keywords make an email spam or not spam. In this age of big data, such problems become very large. This project has introduced novel methods that allow one to drastically reduce the size of the problem with a computationally fast, but rigorous test. We have also examined the case when the data comes in as a stream, and demonstrated how to re-use previous computations. Again, the goal was to speed up ad scale up.\n\n\nIn our driving application, which is text analytics, we are facing very large collections of text documents, in the millions of scientific articles for example; and we would like to analyze how a certain topic (e.g., \"cervical cancer\") is treated, by discovering which keywords \"trigger\" the appearance of that topic in any unit of the texts. We have developed a web-based text analytics platform, StatNews, that displays the result of our analyses in real-time, in a visually intuitive way. The technology developed in this project is what made possible for us to solve very large problems in real-time.\n\n\t\t\t\t\tLast Modified: 07/30/2013\n\n\t\t\t\t\tSubmitted by: Laurent El Ghaoui"
 }
}