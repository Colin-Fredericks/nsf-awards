{
 "awd_id": "1018509",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF:  Small:  Gossiping, Intermittency, and Kalman Filtering",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "John Cozzens",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 493778.0,
 "awd_amount": 493778.0,
 "awd_min_amd_letter_date": "2010-07-24",
 "awd_max_amd_letter_date": "2010-07-24",
 "awd_abstract_narration": "This research considers large scale sparsely networked systems as they arise in large critical infrastructures, e.g., the power grid, or a large transportation system, or when monitoring large physical spaces instrumented with ad-hoc networks of sensors, as in environmental applications. The research will develop a completely novel distributed estimator to process the measurements collected by the networked sensors. A significant problem is the intermittent availability of the measurements because the underlying communication infrastructure among the sensors may exhibit random failures (e.g., due to packet loss in switched networks) or may communicate through a random protocol (gossip or variations.) Due to this intermittency, the estimator equations are random and the filter equations switch at random times between two modes of behaviors; in particular, the filter Riccati equation switches randomly between a linear algebraic matrix equation (Lyapounov) and a quadratic algebraic matrix equation. This research develops an appropriate distributed estimation algorithm, establishes under what structural conditions and for what rates of measurement collection does the filtering error stay bounded, and aims at determining the probability of the filtering error growing unbounded. To study these issues, this work develops new, potentially transformative, powerful methodologies that draw from the theory of random dynamical systems and (moderate) large deviations principle. The work will define appropriate conditions under which the sequences of the filter random covariances converge, determine their limiting invariant distributions, define appropriate rare events, and develop a moderate deviations principle that determines appropriate best rates at which the probability of rare events vanishes.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jose",
   "pi_last_name": "Moura",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Jose M Moura",
   "pi_email_addr": "moura@ece.cmu.edu",
   "nsf_id": "000101574",
   "pi_start_date": "2010-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "793600",
   "pgm_ele_name": "SIGNAL PROCESSING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 493778.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research considered very large systems that are the interconnect of similar components (or agents). The agents cooperate among themselves to achieve a common goal. This cooperation is through a sparse network of interconnections among the agents. Each agent collects data from the outside world &nbsp;that we refer to as observations or measurements. A common goal is to learn about the state of the world, for example, to compute the average temperature of the environment from the individual (local) temperature measurements of the agents. This could be done if every agent sent their temperature reading to a centralized location who would then average all received readings and rebroadcast the average temperature to all the agents. This is called a centralized processor. Centralized processing has numerous disadvantages, including single point of failure and overcrowding certain parts of the system. The cooperative distributed processing we considered achieves the same goal (in our simple example, computing the average temperature) but with only local communication, i.e., each agent only interchanging its information with a few selected neighbors. We consider these types of problems by extending in significant directions the standard consensus algorithm. In particular, we extended the algorithm to consider that each agent makes a new measurement each time it also cooperates with its neighbors. We proposed a new class of algorithms that we call consensus + innovations, where, at each time step, agents interchange their current state with neighbors and assimilate their new measurement; in other words, at each time step, each agent updates its state by processing the information about the state of their neighbors and their current new measurement.</p>\n<p>Intellectual merit: We considered problems of distributed inference where agents detect an event about the world or estimate characteristics or parameters of the world by cooperating among themselves and processing their local data as they receive it sequentially in time. We proposed a new class of distributed algorithms, consensus + innovations, to address these distributed inference problems. The effect of the consensus term is to compute locally a global average; the effect of the innovations term is to assimilate locally the current observation of the agent. &nbsp;The copnsensus+innovations algorithms that we considered address distributed inference in a very broad framework: the measurements may be nonlinearly related to the desired world parameters; there may be noise in the communications among agents; due to bandwidth constraints, the communication among agents needs to be quantized; the agents may fail at random intermittent times; the communications among agents may fail at random times; and the protocol used for distributed communications may be random (of the gossip type). Under this broad set of conditions we showed that the distributed consensus + innovations class of algorithms exhibits performance that is asymptotically, i.e., in the sense of a large number of processing steps, equivalent to the performance of the centralized processor. For example, for distributed estimation, we show that the consensus + innovations distributed estimator is asymptotically unbiased, consistent, asymptotically efficient, and converges in time at the same rate as the centralized processor. These results are obtained by carefully designing two vanishing weight sequences, one weigthing the consensus term and the other weighting the innovations term. The techniques introduced to prove these results extend in very significant way current methodologies to analyse performance of distributed processing algorithms.</p>\n<p>&nbsp;</p>\n<p>Broad impact: Our results have broad applicability in very diverse domains from large scale critical networked infrastructures like the power grid to social networks where rumours or fads spread and diffuse across communities through social contacts. &nbsp;Our results were broadly disseminated among the technical community; we published seventeen journal papers and eighteen conference papers. Our results generated broad interest and we gave numerous keynotes and plenaries at major technical international meetings and conferences. We also gave numerous distinguished seminars at Universities and Laboratories across the world. There were four PhD students whose work was directly impacted by this research, one a female student.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/09/2017<br>\n\t\t\t\t\tModified by: Jose&nbsp;M&nbsp;Moura</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research considered very large systems that are the interconnect of similar components (or agents). The agents cooperate among themselves to achieve a common goal. This cooperation is through a sparse network of interconnections among the agents. Each agent collects data from the outside world  that we refer to as observations or measurements. A common goal is to learn about the state of the world, for example, to compute the average temperature of the environment from the individual (local) temperature measurements of the agents. This could be done if every agent sent their temperature reading to a centralized location who would then average all received readings and rebroadcast the average temperature to all the agents. This is called a centralized processor. Centralized processing has numerous disadvantages, including single point of failure and overcrowding certain parts of the system. The cooperative distributed processing we considered achieves the same goal (in our simple example, computing the average temperature) but with only local communication, i.e., each agent only interchanging its information with a few selected neighbors. We consider these types of problems by extending in significant directions the standard consensus algorithm. In particular, we extended the algorithm to consider that each agent makes a new measurement each time it also cooperates with its neighbors. We proposed a new class of algorithms that we call consensus + innovations, where, at each time step, agents interchange their current state with neighbors and assimilate their new measurement; in other words, at each time step, each agent updates its state by processing the information about the state of their neighbors and their current new measurement.\n\nIntellectual merit: We considered problems of distributed inference where agents detect an event about the world or estimate characteristics or parameters of the world by cooperating among themselves and processing their local data as they receive it sequentially in time. We proposed a new class of distributed algorithms, consensus + innovations, to address these distributed inference problems. The effect of the consensus term is to compute locally a global average; the effect of the innovations term is to assimilate locally the current observation of the agent.  The copnsensus+innovations algorithms that we considered address distributed inference in a very broad framework: the measurements may be nonlinearly related to the desired world parameters; there may be noise in the communications among agents; due to bandwidth constraints, the communication among agents needs to be quantized; the agents may fail at random intermittent times; the communications among agents may fail at random times; and the protocol used for distributed communications may be random (of the gossip type). Under this broad set of conditions we showed that the distributed consensus + innovations class of algorithms exhibits performance that is asymptotically, i.e., in the sense of a large number of processing steps, equivalent to the performance of the centralized processor. For example, for distributed estimation, we show that the consensus + innovations distributed estimator is asymptotically unbiased, consistent, asymptotically efficient, and converges in time at the same rate as the centralized processor. These results are obtained by carefully designing two vanishing weight sequences, one weigthing the consensus term and the other weighting the innovations term. The techniques introduced to prove these results extend in very significant way current methodologies to analyse performance of distributed processing algorithms.\n\n \n\nBroad impact: Our results have broad applicability in very diverse domains from large scale critical networked infrastructures like the power grid to social networks where rumours or fads spread and diffuse across communities through social contacts.  Our results were broadly disseminated among the technical community; we published seventeen journal papers and eighteen conference papers. Our results generated broad interest and we gave numerous keynotes and plenaries at major technical international meetings and conferences. We also gave numerous distinguished seminars at Universities and Laboratories across the world. There were four PhD students whose work was directly impacted by this research, one a female student.\n\n\t\t\t\t\tLast Modified: 06/09/2017\n\n\t\t\t\t\tSubmitted by: Jose M Moura"
 }
}