{
 "awd_id": "1058779",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: RESYST: Resilience via Synergistic Redundancy and Fault Tolerance for High-End Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 376219.0,
 "awd_amount": 376219.0,
 "awd_min_amd_letter_date": "2010-09-22",
 "awd_max_amd_letter_date": "2015-08-31",
 "awd_abstract_narration": "In High-End Computing (HEC), faults have become the norm rather than the exception for parallel computation on clusters with 10s/100s of thousands of cores. As the core count increases, so does the overhead for fault-tolerant techniques relying on checkpoint/restart(C/R) mechanisms. At 50% overheads, redundancy is a viable alternative to fault recovery and actually scales, which makes the approach attractive for HEC.\r\n\r\nThe objective of this work to the develop a synergistic approach by combining C/R-based fault tolerance with redundancy in HEC installations to achieve high levels of resilience.\r\n\r\nThis work alleviates scalability limitations of current fault tolerant practices. It contributes to fault modeling as well as fault detection and recovery in significantly advancing existing techniques by controlling levels of redundancy and checkpointing intervals in the presence of faults. It is transformative in providing a model where users select a target failure probability at the price of using additional resources.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Mueller",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frank Mueller",
   "pi_email_addr": "fmuelle@ncsu.edu",
   "nsf_id": "000484031",
   "pi_start_date": "2010-09-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "2601 WOLF VILLAGE WAY",
  "perf_city_name": "RALEIGH",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "794200",
   "pgm_ele_name": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 376219.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In High-End Computing (HEC), faults have become the norm rather than<br />the exception for parallel computation on clusters with 10s/100s of<br />thousands of cores. As the core count increases, so does the overhead<br />for fault-tolerant techniques relying on checkpoint/restart (C/R)<br />mechanisms. At 50% overheads, redundancy is a viable alternative to<br />fault recovery and actually scales, which makes the approach<br />attractive for HEC.<br /><br />The objective of this work is to develop a synergistic approach by<br />combining C/R-based fault tolerance with redundancy in HEC<br />installations to achieve high levels of resilience.<br /><br />This work alleviates scalability limitations of current fault tolerant<br />practices. It contributes to fault modeling as well as fault detection<br />and recovery in significantly advancing existing techniques by<br />controlling levels of redundancy and checkpointing intervals in the<br />presence of faults. It is transformative in providing a model where<br />users select a target failure probability at the price of using<br />additional resources.<br /><br />Our work shows that redundancy-based fault tolerance can be used in<br />synergy with checkpoint/restart-based fault tolerance to achieve<br />better application performance for large-scale HPC applications than<br />can be achieved by any of the two techniques alone, which has been<br />analytically modeled and experimentally confirmed.<br /><br />We further assessed the feasibility and effectiveness of SDC detection<br />and correction at the MPI layer via redundancy.&nbsp; We develped two<br />consistency protocols, explored the unique challenges in creating a<br />deterministic MPI environment for replication purposes, investigated<br />the effects of fault injection in to our framework, analyzed the<br />costs and showed the benefits of SDC protection via redundancy.<br /><br />We also studied Single Event Upsets (SEUs) in floating-point data. We<br />show that SEUs produce predictable, non-uniform errors that can be<br />bounded using analytical modeling of perturbed dot-products for<br />elementary linear algebra constructs, and by analyzing convergence<br />theory of first-order (stationary) iterative linear solvers.<br />Convergence for stationary iterative methods is provable, and the<br />performance impact (increased iteration count) of an SEU in data is<br />predictable with low error.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/10/2016<br>\n\t\t\t\t\tModified by: Frank&nbsp;Mueller</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn High-End Computing (HEC), faults have become the norm rather than\nthe exception for parallel computation on clusters with 10s/100s of\nthousands of cores. As the core count increases, so does the overhead\nfor fault-tolerant techniques relying on checkpoint/restart (C/R)\nmechanisms. At 50% overheads, redundancy is a viable alternative to\nfault recovery and actually scales, which makes the approach\nattractive for HEC.\n\nThe objective of this work is to develop a synergistic approach by\ncombining C/R-based fault tolerance with redundancy in HEC\ninstallations to achieve high levels of resilience.\n\nThis work alleviates scalability limitations of current fault tolerant\npractices. It contributes to fault modeling as well as fault detection\nand recovery in significantly advancing existing techniques by\ncontrolling levels of redundancy and checkpointing intervals in the\npresence of faults. It is transformative in providing a model where\nusers select a target failure probability at the price of using\nadditional resources.\n\nOur work shows that redundancy-based fault tolerance can be used in\nsynergy with checkpoint/restart-based fault tolerance to achieve\nbetter application performance for large-scale HPC applications than\ncan be achieved by any of the two techniques alone, which has been\nanalytically modeled and experimentally confirmed.\n\nWe further assessed the feasibility and effectiveness of SDC detection\nand correction at the MPI layer via redundancy.  We develped two\nconsistency protocols, explored the unique challenges in creating a\ndeterministic MPI environment for replication purposes, investigated\nthe effects of fault injection in to our framework, analyzed the\ncosts and showed the benefits of SDC protection via redundancy.\n\nWe also studied Single Event Upsets (SEUs) in floating-point data. We\nshow that SEUs produce predictable, non-uniform errors that can be\nbounded using analytical modeling of perturbed dot-products for\nelementary linear algebra constructs, and by analyzing convergence\ntheory of first-order (stationary) iterative linear solvers.\nConvergence for stationary iterative methods is provable, and the\nperformance impact (increased iteration count) of an SEU in data is\npredictable with low error.\n\n\t\t\t\t\tLast Modified: 11/10/2016\n\n\t\t\t\t\tSubmitted by: Frank Mueller"
 }
}