{
 "awd_id": "1017234",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NetSE: Small: Network Memory for the Future Internet",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Darleen Fisher",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 497577.0,
 "awd_amount": 497577.0,
 "awd_min_amd_letter_date": "2010-07-14",
 "awd_max_amd_letter_date": "2010-07-14",
 "awd_abstract_narration": "Several studies including those by the PIs have inferred the presence of considerable amounts of redundancy in Internet traffic content (ranging up to 99%). These studies clearly establish the tremendous scope for enhancing communication performance through appropriate exploitation of the redundancies.  In this research, the investigators study and develop a network memory solution for the future Internet. Network memory, in its simplest form, involves a memory at every network element that allows store/retrieve operations. The store operation is used when a particular data flows through the element for the first time. The retrieve operation is used when the same data needs to be retrieved from that element at a later point. The primary goal of using such a network memory is to minimize or eliminate any redundant traffic when delivering any content from its server to any client, and hence, reducing the bandwidth requirements. \r\n\r\nIntellectual Merit: The underlying fabric of the Internet consisting of the content Servers, routers, and clients perform very little, if any, memorization and hence re-use of the memorized content. The focus of this research is to rethink this aspect of the Internet for the design of the future Internet architecture. The investigators introduce network memory as a new layer 3.5 for the future Internet, residing beneath the transport layer and above the network layer. The overall benefits of using network memory are better network delivery performance and higher network utilization levels through the exploitation of redundancies. Previous techniques such as web-caching, CDNs, and P2P applications, while in principle try to leverage redundancy, either do not harness the redundancy available or are too narrow in their scope to even attempt leveraging redundancies along the various (temporal and spatial) dimensions. A subset of the questions that the investigators plan to address through the work includes: 1. What is the granularity at which data should be memorized in the network memory? Should it be sub-packet, packet-level, or even super-packet level granularity? 2. Who are the participants in the network memory? Is it the clients, content-servers, or routers, or perhaps all of them? 3. How does the network memory work? How is the memory location determined for any given data? How is addressing of the network memory performed? 4. Should the network memory support sophisticated operations beyond just store and retrieve? The investigators will also explore fundamental issues with the network memory such as the nature of traffic redundancies and the concept of network compression. Finally, the research will address several key systems issues pertaining to the network memory including overheads of realization, protocol and header formats, and impact on other Internet protocols. \r\n\r\nBroader Impact: In addition to graduate and undergraduate training opportunities, the research will be aimed at the standardization and technology transfer for the network-memory-layer solutions. The broader impact also includes: (a) Undergraduate curriculum development through senior undergraduate classes taught by the PIs, (b) Graduate curriculum development through two graduate-level classes on networking and information theory taught by the PIs, (c) Support for minority students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Raghupathy",
   "pi_last_name": "Sivakumar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Raghupathy Sivakumar",
   "pi_email_addr": "siva@ece.gatech.edu",
   "nsf_id": "000310027",
   "pi_start_date": "2010-07-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Faramarz",
   "pi_last_name": "Fekri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Faramarz Fekri",
   "pi_email_addr": "fekri@ece.gatech.edu",
   "nsf_id": "000261381",
   "pi_start_date": "2010-07-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "926 DALNEY ST NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303186395",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779400",
   "pgm_ele_name": "NETWORK SCIENCE & ENGINEERING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 497577.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this research, we study, design, and develop a network memory solution for the future Internet. Network memory, in its simplest form, involves a memory at every network element that allows store/retrieve operations. The store operation is used when a particular data flows through the element for the first time. The retrieve operation is used when the same data needs to be retrieved from that element at a later point. The primary goal of using such a network memory is to minimize the bit-hop measure for delivering any content from its server to any client, and thereby increase performance and lower cost of delivering that content. We develop network memory as a new layer 3.5 for the future Internet, residing beneath the transport layer and above the network layer. Briefly, the key outcomes of our research have been both along fundamental information theoretic results for network memory assisted compression and practical algorithms and systems for using network memory solutions in real-life environments. The results of the research have been widely disseminated through publications in peer-reviewed conferences and journals. Intellectual property rights are being pursued through patent applications.&nbsp;Highlights of the research outcomes are provided below:</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Intelligent network memory solutions for several network scenarios including: (a) when there is knowledge asymmetry between the sender and the receiver straddling a wireless link; (b) mobile to mobile remote computing, and (c) when there is computational power asymmetry between the sender and receiver. For example, Network deduplication (dedup) is a class of solutions that exploits redundancies in network traffic to improve network performance. Dedup faces a unique challenge when used in mobile environments. There exist several mobile scenarios where the sender is likely to have knowledge of only a small subset of the cache at the mobile. The solution developed through this research - asymmetric caching - allows for the cache at the receiver to be larger than that at the sender, but still allow the dedup efficiency at the sender to approach the ideal performance.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Smart prefetching solutions that amplify the benefits that network memory can provide in the future. For example, traditional web prefetching solutions, which rely on the URLs visited in the past by a user for predicting future access, are ineffective in today&rsquo;s dynamic, interactive and personalized web. The solution developed through this research - Precog - addresses this issue by remembering, not the exact URL accessed in the past, but the actions performed on a particular website. The actions are remembered as interactions with the content layout, which stays consistent over a long period of time. Unlike prior offloading solutions, which require concurrent cellular and WiFi connectivity, Precog offloads cellular content over time-shifted WiFi access.</p>\n<p>- &nbsp; The concept of Network Compression via Network Memory. Network compression via memory is a new module residing at the network layer. As such, memory enabled nodes can learn the network data statistics which can then be used (as side information) toward reducing the cost of describing the packet in compression. Our recent works at GIT have shown that memory-assisted universal compression technique holds a signi?cant promise for reducing the amount of tra?c in the networks. The network memory generically forms the side information needed to remove all three types of redundancy described earlier. Most importantly, it establishes the tremendous scope for performance improvements achievable through appropriate exploitation of the redundancies.&nbsp; We reported a memorization gain (defined as the additional compression gain on top and over what can be achieved by the existing compress...",
  "por_txt_cntn": "\nIn this research, we study, design, and develop a network memory solution for the future Internet. Network memory, in its simplest form, involves a memory at every network element that allows store/retrieve operations. The store operation is used when a particular data flows through the element for the first time. The retrieve operation is used when the same data needs to be retrieved from that element at a later point. The primary goal of using such a network memory is to minimize the bit-hop measure for delivering any content from its server to any client, and thereby increase performance and lower cost of delivering that content. We develop network memory as a new layer 3.5 for the future Internet, residing beneath the transport layer and above the network layer. Briefly, the key outcomes of our research have been both along fundamental information theoretic results for network memory assisted compression and practical algorithms and systems for using network memory solutions in real-life environments. The results of the research have been widely disseminated through publications in peer-reviewed conferences and journals. Intellectual property rights are being pursued through patent applications. Highlights of the research outcomes are provided below:\n\n-        Intelligent network memory solutions for several network scenarios including: (a) when there is knowledge asymmetry between the sender and the receiver straddling a wireless link; (b) mobile to mobile remote computing, and (c) when there is computational power asymmetry between the sender and receiver. For example, Network deduplication (dedup) is a class of solutions that exploits redundancies in network traffic to improve network performance. Dedup faces a unique challenge when used in mobile environments. There exist several mobile scenarios where the sender is likely to have knowledge of only a small subset of the cache at the mobile. The solution developed through this research - asymmetric caching - allows for the cache at the receiver to be larger than that at the sender, but still allow the dedup efficiency at the sender to approach the ideal performance.\n\n-        Smart prefetching solutions that amplify the benefits that network memory can provide in the future. For example, traditional web prefetching solutions, which rely on the URLs visited in the past by a user for predicting future access, are ineffective in today\u00c6s dynamic, interactive and personalized web. The solution developed through this research - Precog - addresses this issue by remembering, not the exact URL accessed in the past, but the actions performed on a particular website. The actions are remembered as interactions with the content layout, which stays consistent over a long period of time. Unlike prior offloading solutions, which require concurrent cellular and WiFi connectivity, Precog offloads cellular content over time-shifted WiFi access.\n\n-   The concept of Network Compression via Network Memory. Network compression via memory is a new module residing at the network layer. As such, memory enabled nodes can learn the network data statistics which can then be used (as side information) toward reducing the cost of describing the packet in compression. Our recent works at GIT have shown that memory-assisted universal compression technique holds a signi?cant promise for reducing the amount of tra?c in the networks. The network memory generically forms the side information needed to remove all three types of redundancy described earlier. Most importantly, it establishes the tremendous scope for performance improvements achievable through appropriate exploitation of the redundancies.  We reported a memorization gain (defined as the additional compression gain on top and over what can be achieved by the existing compression techniques) of order close to three when our algorithms are tested on the real Internet traffic traces. This aspect of our research also included several fundamental q..."
 }
}