{
 "awd_id": "1012205",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI-Large: Activity Learning and Recognition for a   Cognitive Assistant",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2010-08-15",
 "awd_exp_date": "2014-01-31",
 "tot_intn_awd_amt": 750000.0,
 "awd_amount": 750000.0,
 "awd_min_amd_letter_date": "2010-08-07",
 "awd_max_amd_letter_date": "2012-08-03",
 "awd_abstract_narration": "This project addresses a key problem in advancing the state of the art in cognitive assistant systems that can interact naturally with humans in order to help them perform everyday tasks more effectively. Such a system would help not only people with cognitive disabilities but all individuals as they perform complex tasks they are unfamiliar with. The research focuses on structured activities of daily living that lend themselves to practical experimentation, such as meal preparation and other kitchen activities.\r\n\r\nSpecifically, the core focus of the research is activity recognition, i.e., systems that can identify the goals and individual actions a person is performing as they work on a task. Key innovations of this work are 1) that the activity models are learned from the user via intuitive natural demonstration, and 2) that the system is able to reason over activity models to generalize and adapt them. In contrast, current practice requires specialized training supervised by the researchers and supports no reasoning over the models. This advance is accomplished by integrating capabilities that are typically studied separately, including activity recognition, knowledge representation and reasoning, natural language understanding and machine learning. The work addresses a significant step towards the goal of building practical and flexible in-home automated assistants.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Allen",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "James F Allen",
   "pi_email_addr": "jallen@ihmc.us",
   "nsf_id": "000167435",
   "pi_start_date": "2010-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Florida Institute for Human and Machine Cognition, Inc.",
  "inst_street_address": "40 S ALCANIZ ST",
  "inst_street_address_2": "",
  "inst_city_name": "PENSACOLA",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "8502024404",
  "inst_zip_code": "325026008",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "FL01",
  "org_lgl_bus_name": "FLORIDA INSTITUTE FOR HUMAN & MACHINE COGNITION INC",
  "org_prnt_uei_num": "EMXDXET56K76",
  "org_uei_num": "EMXDXET56K76"
 },
 "perf_inst": {
  "perf_inst_name": "Florida Institute for Human and Machine Cognition, Inc.",
  "perf_str_addr": "40 S ALCANIZ ST",
  "perf_city_name": "PENSACOLA",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "325026008",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "FL01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 556113.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 193887.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Activity Recognition and Learning for a Cognitive Agent</strong></p>\n<p><span>The ability to automatically recognize human activity has a wide range of potential applications, from automated assistants for those with cognitive impairments to human-robot collaborative activity. This project focused on a key problem of how systems can learn the underlying models of activity automatically from natural language demonstrations. We have developed a range of innovative learning techniques that have been tested in a variety of domains, including assisting in kitchen-based activities such as cooking, monitoring activity in wet-labs to ensure compliance, and automatically learning new procedures in text-editing tasks.</span></p>\n<p><span>A unique focus of this work was the use of language understanding, both for rapid learning of new tasks, and for improving activity recognition when language is provided. A key finding was the development of a general model of activity recognition that can take advantage of input at varying levels of detail (from low level vision to high level action descriptions in language). This required the development of a new temporal model of action that supports probabilistic inference (using Markov Logic Networks), and demonstrating the effectiveness of this model in integrating perception and language for activity recognition. We also demonstrated how a system can rapidly learn the meaning of new words, including adjectives, grounding these terms in its perceptual system. In addition, we developed a new technique for learning procedures in a text-editing domain, where the system can rapidly learn new procedures from natural language instruction plus concrete examples. Related to these efforts, we also developed a new formalism encoding and resolving scope ambiguities in natural language, which greatly expands the range of scoping phenomena that can be effectively resolved. Finally, in the performance of this research, it became clear that rapid learning of activity models from language requires extensive common-sense knowledge in order to link the language to the observed activities being demonstrated. To meet this need, we developed new techniques for acquiring commonsense knowledge from reading natural definitions, specifying those found in the lexical dictionary WordNet.&nbsp;</span></p>\n<p><span>We supported 14 PhD students at various stages of their work, combing our efforts between both IHMC and the University of Rochester. Four of these have completed their PhD&rsquo;s already, and many more will finish in the coming two years.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/15/2014<br>\n\t\t\t\t\tModified by: James&nbsp;F&nbsp;Allen</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2014/1012205/1012205_10019962_1400174166085_hierarchicalplans--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2014/1012205/1012205_10019962_1400174166085_hierarchicalplans--rgov-800width.jpg\" title=\"hierarchical plans\"><img src=\"/por/images/Reports/POR/2014/1012205/1012205_10019962_1400174166085_hierarchicalplans--rgov-66x44.jpg\" alt=\"hierarchical plans\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">System observes a human performing a cooking activity and describing what they are doing, and learns a hierarchical plan and perceptual models that can be used for activity recognition</div>\n<div class=\"imageCredit\">University of Rochester</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div ...",
  "por_txt_cntn": "\nActivity Recognition and Learning for a Cognitive Agent\n\nThe ability to automatically recognize human activity has a wide range of potential applications, from automated assistants for those with cognitive impairments to human-robot collaborative activity. This project focused on a key problem of how systems can learn the underlying models of activity automatically from natural language demonstrations. We have developed a range of innovative learning techniques that have been tested in a variety of domains, including assisting in kitchen-based activities such as cooking, monitoring activity in wet-labs to ensure compliance, and automatically learning new procedures in text-editing tasks.\n\nA unique focus of this work was the use of language understanding, both for rapid learning of new tasks, and for improving activity recognition when language is provided. A key finding was the development of a general model of activity recognition that can take advantage of input at varying levels of detail (from low level vision to high level action descriptions in language). This required the development of a new temporal model of action that supports probabilistic inference (using Markov Logic Networks), and demonstrating the effectiveness of this model in integrating perception and language for activity recognition. We also demonstrated how a system can rapidly learn the meaning of new words, including adjectives, grounding these terms in its perceptual system. In addition, we developed a new technique for learning procedures in a text-editing domain, where the system can rapidly learn new procedures from natural language instruction plus concrete examples. Related to these efforts, we also developed a new formalism encoding and resolving scope ambiguities in natural language, which greatly expands the range of scoping phenomena that can be effectively resolved. Finally, in the performance of this research, it became clear that rapid learning of activity models from language requires extensive common-sense knowledge in order to link the language to the observed activities being demonstrated. To meet this need, we developed new techniques for acquiring commonsense knowledge from reading natural definitions, specifying those found in the lexical dictionary WordNet. \n\nWe supported 14 PhD students at various stages of their work, combing our efforts between both IHMC and the University of Rochester. Four of these have completed their PhD\u00c6s already, and many more will finish in the coming two years.\n\n \n\n\t\t\t\t\tLast Modified: 05/15/2014\n\n\t\t\t\t\tSubmitted by: James F Allen"
 }
}