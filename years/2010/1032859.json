{
 "awd_id": "1032859",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SDCI: HPC: Improvement: Infrastructure for Multi-Node Manycore Computing",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924863",
 "po_email": "edwalker@nsf.gov",
 "po_sign_block_name": "Edward Walker",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 391859.0,
 "awd_amount": 397622.0,
 "awd_min_amd_letter_date": "2010-08-16",
 "awd_max_amd_letter_date": "2015-04-30",
 "awd_abstract_narration": "One of the major challenges facing manycore computing today is to make parallelism accessible to the mainstream programmer. In today?s era, where nearly every computer contains a manycore processor in the form of the GPU, we have not yet successfully built the primitives and techniques that we require to make manycore processing readily available to the entire computing community.\r\n\r\nOne solution to this challenge is the construction of libraries that encapsulate common programming patterns and idioms. Libraries are self-contained and thus easily added to existing projects; can easily be upgraded; and are well-suited for development and maintenance by academic groups such as ours.\r\n\r\nHigh-performance computing is extensively supported by such libraries on the CPU side, but there are few in the manycore world. Our CUDPP (CUDA Data-Parallel Primitives) library is widely used in the GPU computing community, albeit only on single-node (largely non-HPC) systems. We believe the next-generation OpenCL programming environment is the future of manycore programming and thus target our future work toward this standard.\r\n\r\nOur plan is to extend and support CUDPP for use in the high-performance computing community, which is increasingly adopting manycore processors as core computational engines. We will add single- and many-node primitives to CUDPP and the OpenCL-based CLDPP library, primitives we consider to be vital to the growth of the manycore HPC community. We also plan to define best practices for manycore libraries and to build not just a library but also tools to help others build libraries.\r\n\r\nThe intellectual merit of this proposal lies in the development and improvement of core data structures and algorithms targeted to manycore computing environments in general and HPC computing environments specifically. Other interesting outcomes from this project include multi-node data structures and algorithms and the software engineering of tools to build manycore libraries.\r\n\r\nLike its predecessor, CUDPP, the library that will result from this work will be open-sourced and widely used in the manycore programming community. The optimization techniques and tools will also find widespread use. We will work with the OpenCL consortium and our industry partners to bring libraries into the OpenCL standard, directly impacting the entire manycore community. The PI will also continue to introduce undergraduates to research through manycore computing, including collaborating with our industrial partners to host Google Summer of Code engineers on this project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Owens",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "John D Owens",
   "pi_email_addr": "jowens@ece.ucdavis.edu",
   "nsf_id": "000377403",
   "pi_start_date": "2010-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "1850 RESEARCH PARK DR STE 300",
  "perf_city_name": "DAVIS",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186153",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  },
  {
   "pgm_ele_code": "768300",
   "pgm_ele_name": "SOFTWARE DEVELOPEMENT FOR CI"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7683",
   "pgm_ref_txt": "SOFTWARE DEVELOPEMENT FOR CI"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 391859.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 5763.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our work focuses on techniques for efficient parallel computing. Over its history, the dominant form of computing has been serial computing, where a computer gets a series of instructions and executes them in sequence, one after another. Recently, the progress in building faster serial computers has significantly slowed; computers and programs today can run faster not by running serial programs faster but instead by running pieces of their programs simultaneously (\"in parallel\").</p>\n<p>This move to parallel computing comes with numerous challenges. Our research group primarily focuses on a particular parallel processor called the graphics processing unit, or GPU, which has historically been used to render images quickly, but can also be used to process a wide range of computing problems faster and more efficiently than serial computers. These processors are now ubiquitous in modern computing, present in nearly all cell phones and tablets, computers, and increasingly powering the most powerful supercomputers in the world. From a power and cost perspective, GPUs are superior to CPUs for the programs they can address, and if we can use them efficiently, they will be the centerpiece of much of next-generation computing.</p>\n<p>However, the GPU and other \"manycore\" processors are quite challenging to program. In our field, we talk about the \"ninja gap\", the performance gap between the code that experts (\"ninjas\") can write and what an average programmer might write. The ninja gap is large. Closing the ninja gap is vital for making the most of these processors.</p>\n<p>In this project we built open-source libraries for the GPU. These are building blocks that other developers can use in their own applications. We chose a liberal license for these libraries so that both industry and academic programmers can use them. In this way we ensure our work has the broadest impact.</p>\n<p>Our first library, CUDPP, the \"CUDA Data Parallel Primitives\", provides code for numerous algorithms and data structures that are broadly useful across a variety of different application domains. These data-parallel building blocks are the foundation of GPU computing. Our code is used (and used as a point of comparison) in dozens of projects around the world.</p>\n<p>Our second (and newer) library is Gunrock, which targets graph analytics: computations on graphs. Graphs are often visible to the general public in terms of their social networks, but are also used across a wide variety of applications such as epidemiology, genomics, networking, and many others. Compared to prior work, Gunrock makes advances in both performance and programmability, making it much easier for others to integrate high-performance graph analytics into their applications.</p>\n<p>Our work has both produced novel research results in parallel data structures, algorithms, and graph analytics (our \"intellectual merit\") as well as made our these research results available to the world in the form of our open-source libraries (our chief \"broader impact\").</p>\n<p>Thank you to the taxpayers of the United States of America for funding our work and to our NSF program directors for their leadership and guidance.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/01/2016<br>\n\t\t\t\t\tModified by: John&nbsp;D&nbsp;Owens</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur work focuses on techniques for efficient parallel computing. Over its history, the dominant form of computing has been serial computing, where a computer gets a series of instructions and executes them in sequence, one after another. Recently, the progress in building faster serial computers has significantly slowed; computers and programs today can run faster not by running serial programs faster but instead by running pieces of their programs simultaneously (\"in parallel\").\n\nThis move to parallel computing comes with numerous challenges. Our research group primarily focuses on a particular parallel processor called the graphics processing unit, or GPU, which has historically been used to render images quickly, but can also be used to process a wide range of computing problems faster and more efficiently than serial computers. These processors are now ubiquitous in modern computing, present in nearly all cell phones and tablets, computers, and increasingly powering the most powerful supercomputers in the world. From a power and cost perspective, GPUs are superior to CPUs for the programs they can address, and if we can use them efficiently, they will be the centerpiece of much of next-generation computing.\n\nHowever, the GPU and other \"manycore\" processors are quite challenging to program. In our field, we talk about the \"ninja gap\", the performance gap between the code that experts (\"ninjas\") can write and what an average programmer might write. The ninja gap is large. Closing the ninja gap is vital for making the most of these processors.\n\nIn this project we built open-source libraries for the GPU. These are building blocks that other developers can use in their own applications. We chose a liberal license for these libraries so that both industry and academic programmers can use them. In this way we ensure our work has the broadest impact.\n\nOur first library, CUDPP, the \"CUDA Data Parallel Primitives\", provides code for numerous algorithms and data structures that are broadly useful across a variety of different application domains. These data-parallel building blocks are the foundation of GPU computing. Our code is used (and used as a point of comparison) in dozens of projects around the world.\n\nOur second (and newer) library is Gunrock, which targets graph analytics: computations on graphs. Graphs are often visible to the general public in terms of their social networks, but are also used across a wide variety of applications such as epidemiology, genomics, networking, and many others. Compared to prior work, Gunrock makes advances in both performance and programmability, making it much easier for others to integrate high-performance graph analytics into their applications.\n\nOur work has both produced novel research results in parallel data structures, algorithms, and graph analytics (our \"intellectual merit\") as well as made our these research results available to the world in the form of our open-source libraries (our chief \"broader impact\").\n\nThank you to the taxpayers of the United States of America for funding our work and to our NSF program directors for their leadership and guidance.\n\n \n\n\t\t\t\t\tLast Modified: 12/01/2016\n\n\t\t\t\t\tSubmitted by: John D Owens"
 }
}