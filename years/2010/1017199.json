{
 "awd_id": "1017199",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: 3D Nonrigid Object Reconstruction from Large-Scale Unorganized 2D Images",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 216000.0,
 "awd_min_amd_letter_date": "2010-09-05",
 "awd_max_amd_letter_date": "2011-02-18",
 "awd_abstract_narration": "Reconstructing the 3D shape of an object from multiple 2D images is a fundamental problem in computer vision. Prior work on this problem usually requires the object of interest to be rigid or the available 2D images to be well organized, such as consecutive frames in a video. This project investigates the challenging problem of reconstructing a nonrigid 3D object from a large number of unorganized 2D images, which may be taken at different times, with different backgrounds, from different perspectives, under different lighting conditions, and/or using different cameras.\r\n\r\nThe research team develops new algorithms of combining object localization, feature matching, and partial shape matching across the images to segment the 2D object of interest from the input images. The segmented 2D objects are organized into clusters to recover the underlying 3D nonrigid deformation. Pieces of the 3D object are reconstructed from these clusters and finally assembled to obtain the complete 3D object by removing the in-between nonrigid deformations. An image database with 2D images of selected nonrigid objects is constructed for performance evaluation.\r\n\r\nThis research benefits many applications in computer vision, computer graphics, computer gaming, zoology, microbiology, marine science, and medical research, which all involve the modeling of 3D norigid objects. Progress made on object localization, feature matching and partial shape matching has immediate applications in object detection, object recognition, image search, surveillance, tracking, and segmentation. This research also provides an excellent setting for the training of both undergraduate and graduate students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Song",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Song Wang",
   "pi_email_addr": "songwang@cec.sc.edu",
   "nsf_id": "000312736",
   "pi_start_date": "2010-09-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University South Carolina Research Foundation",
  "inst_street_address": "915 BULL ST",
  "inst_street_address_2": "STE 202",
  "inst_city_name": "COLUMBIA",
  "inst_state_code": "SC",
  "inst_state_name": "South Carolina",
  "inst_phone_num": "8037777093",
  "inst_zip_code": "292084009",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "SC06",
  "org_lgl_bus_name": "SOUTH CAROLINA RESEARCH FOUNDATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "ELBVJ1KYX976"
 },
 "perf_inst": {
  "perf_inst_name": "University of South Carolina at Columbia",
  "perf_str_addr": "1600 HAMPTON ST",
  "perf_city_name": "COLUMBIA",
  "perf_st_code": "SC",
  "perf_st_name": "South Carolina",
  "perf_zip_code": "292083403",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "SC06",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project is focused on developing effective models and algorithms to reconstruct 3D objects from images that are taken from different views. In particular, new approaches were developed to reconstruct 3D human poses from 2D poses (2D human joints) and estimate 2D poses from images.</p>\n<p>To reconstruct the 3D human pose from the locations of 2D joints, a typical approach is to consider the sparsity in the 3D pose space -- the underlying 3D pose can be sparsely represented by the linear combination of a small number of basis poses that are learned from a set of training 3D pose samples. In this project the importance of locality of these basis poses in this reconstruction problem was investigated-- the selected basis poses must be drawn from subspaces that are compact and similar to each other. For this purpose, a hierarchical pose tree is constructed where each node represents a pose subspace and the parent subspace is divided into more compact child subspaces. Based on the pose tree, the locality is enforced by drawing basis poses only from a small number of subspaces that are near to each other in the tree. Experiments on the CMU MOCAP dataset show that the accuracy of 3D human pose reconstruction can be substantially improved by enforcing the locality.</p>\n<p>To estimate 2D human pose from an image, a Dual-Source Convolutional Neural Networks (DS-CNN) model is developed by combining both the local part appearance in image patches and the holistic view of the full human body. By taking both the image patch and its holistic view as the inputs, the proposed DS-CNN performs a unified learning to achieve both joint detection, which determines whether an object proposal contains a body joint, and joint localization, which finds the exact location of the joint in the object proposal. The finding is that such combined inputs can help discriminate the joints with similar appearances, such as left and right knees, in an image and lead to better 2D human pose estimation. In addition, the deep learning framework, such as the convolutional neural networks, is appropriate for combining such multiple source inputs.</p>\n<p>In this project, several other important issues related to image-based 3D object/human reconstruction were also investigated, including multiscale superpixel and supervoxel segmentation, partial shape matching, visual attention from images/videos, and object detection by combining boundary and appearance information. The outcome of this project includes more than 40 publications in related journals and conferences, which are all accessible by public. Seven Ph.D. students, including one female student, have graduated with their dissertation research partially or fully supported by this grant and four more current Ph.D. students were also benefited from this research. Two undergraduate students were trained through this project as NSF REU students. The research results from this project have been extended to address problems in document image processing, material-science image segmentation and medical imaging.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2015<br>\n\t\t\t\t\tModified by: Song&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2015/1017199/1017199_10037173_1448805911753_DS-CNN--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2015/1017199/1017199_10037173_1448805911753_DS-CNN--rgov-800width.jpg\" title=\"DS-CNN-Pose-Estimation\"><img src=\"/por/images/Repor...",
  "por_txt_cntn": "\nThis project is focused on developing effective models and algorithms to reconstruct 3D objects from images that are taken from different views. In particular, new approaches were developed to reconstruct 3D human poses from 2D poses (2D human joints) and estimate 2D poses from images.\n\nTo reconstruct the 3D human pose from the locations of 2D joints, a typical approach is to consider the sparsity in the 3D pose space -- the underlying 3D pose can be sparsely represented by the linear combination of a small number of basis poses that are learned from a set of training 3D pose samples. In this project the importance of locality of these basis poses in this reconstruction problem was investigated-- the selected basis poses must be drawn from subspaces that are compact and similar to each other. For this purpose, a hierarchical pose tree is constructed where each node represents a pose subspace and the parent subspace is divided into more compact child subspaces. Based on the pose tree, the locality is enforced by drawing basis poses only from a small number of subspaces that are near to each other in the tree. Experiments on the CMU MOCAP dataset show that the accuracy of 3D human pose reconstruction can be substantially improved by enforcing the locality.\n\nTo estimate 2D human pose from an image, a Dual-Source Convolutional Neural Networks (DS-CNN) model is developed by combining both the local part appearance in image patches and the holistic view of the full human body. By taking both the image patch and its holistic view as the inputs, the proposed DS-CNN performs a unified learning to achieve both joint detection, which determines whether an object proposal contains a body joint, and joint localization, which finds the exact location of the joint in the object proposal. The finding is that such combined inputs can help discriminate the joints with similar appearances, such as left and right knees, in an image and lead to better 2D human pose estimation. In addition, the deep learning framework, such as the convolutional neural networks, is appropriate for combining such multiple source inputs.\n\nIn this project, several other important issues related to image-based 3D object/human reconstruction were also investigated, including multiscale superpixel and supervoxel segmentation, partial shape matching, visual attention from images/videos, and object detection by combining boundary and appearance information. The outcome of this project includes more than 40 publications in related journals and conferences, which are all accessible by public. Seven Ph.D. students, including one female student, have graduated with their dissertation research partially or fully supported by this grant and four more current Ph.D. students were also benefited from this research. Two undergraduate students were trained through this project as NSF REU students. The research results from this project have been extended to address problems in document image processing, material-science image segmentation and medical imaging.\n\n\t\t\t\t\tLast Modified: 11/29/2015\n\n\t\t\t\t\tSubmitted by: Song Wang"
 }
}