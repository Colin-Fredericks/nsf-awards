{
 "awd_id": "1018188",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF:  AF:  Small:  Locality with Dynamic Parallelism",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-08-01",
 "awd_exp_date": "2014-01-31",
 "tot_intn_awd_amt": 449055.0,
 "awd_amount": 449055.0,
 "awd_min_amd_letter_date": "2010-07-28",
 "awd_max_amd_letter_date": "2012-06-05",
 "awd_abstract_narration": "With the recent dominance of computers with many parallel cores, and the widespread use of large data centers, the need to supply high-level, simple and general approaches to developing parallel codes for these machines has become critical.  There are many challenges to effectively developing such parallel codes, but certainly a principle difficulty is dealing with communication costs - or when looked at from the other side, taking advantage of locality.  Unfortunately this challenge has only become more difficult on modern parallel machines that have many forms of locality - network latency and bandwidth, shared and distributed caches, partitioned memories, and secondary storage in a variety of organizations.\r\n\r\nTo address this problem this project is developing an approach for programmers to understand locality in their parallel code without needing to know any details of the particular parallel machine they are using.  In the approach programmers express the full dynamic parallelism of their algorithm without describing how it is mapped onto processors, and are given a simple high-level model for analyzing locality.  The research is based on the conjecture that locality should be viewed by the programmer as a property of the algorithm or code and not the machine.  To ensure that real machines can take proper advantage of the locality analyzed in the model, the research is developing scheduling approaches that map the \"algorithm locality\" onto various forms of machine locality, including shared caches, distributed caches, trees of caches, and distributed memory machines. The results of the research include both theoretical bounds for such schedulers on specific machine organizations, and experimental validation on a set of benchmark applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Guy",
   "pi_last_name": "Blelloch",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Guy E Blelloch",
   "pi_email_addr": "guyb@cs.cmu.edu",
   "nsf_id": "000196851",
   "pi_start_date": "2010-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  },
  {
   "pgm_ele_code": "793400",
   "pgm_ele_name": "PARAL/DISTRIBUTED ALGORITHMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 143425.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 149591.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 156039.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Because of the high cost of communication in parallel machines<br />locality has always played a key role in the performance of parallel<br />applications.&nbsp; However, programming for locality has proven to be very<br />difficult---having the programmer control the layout, synchronization<br />and scheduling of the computation and data to deal with locality can<br />be extremely cumbersome, dominating the time required to develop,<br />analyze, verify, maintain, and update codes.</p>\n<p>The outcome of the research were</p>\n<ol>\n<li>We developed cost models for easily understanding the locality in parallel algorithms without having to understand the details of any particular machine.&nbsp;&nbsp; The key model is what we call the Parallel Cache Model.&nbsp; It assigns a cost to computations described in a high-level nested parallel programming model, but captures both \"spacial\" and \"temporal\" locality.</li>\n<li>We developed a variety of algorithms in the model.&nbsp;&nbsp; We showed that some standard algorithms are already efficient in the model (e.g. matrix multiply) but also developed some new algorithms (e.g. a variant of sample sort).</li>\n<li>We developed theoretically efficient schedulers that map the high level Parallel Cache model onto a variety of parallel machine configurations with hierarchical memory organizations.&nbsp;&nbsp; We were able to show some strong bounds on how the high-level costs map onto the low-level costs.</li>\n<li>We implemented a scheduler that incorporates many of the theoretical ideas and experimented with a variety of algorithms using the scheduler.&nbsp;&nbsp; Our results show that indeed the cost model does capture locality well, and that the schedulers improve performance over standard schedulers (e.g. work-stealing schedulers).</li>\n</ol>\n<p>As part of the work we published several papers on the topic, and developed a scheduler that we have made public.&nbsp;&nbsp; The work has lead to newer ongoing work on incorporating automatic memory management into the model and scheduler.</p>\n<p>We also developed course material on \"cache efficient parallel algorithms\" that has been added to a graduate course on applied algorithms.&nbsp;&nbsp; We have presented the work at a variety of venues.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/05/2014<br>\n\t\t\t\t\tModified by: Guy&nbsp;E&nbsp;Blelloch</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBecause of the high cost of communication in parallel machines\nlocality has always played a key role in the performance of parallel\napplications.  However, programming for locality has proven to be very\ndifficult---having the programmer control the layout, synchronization\nand scheduling of the computation and data to deal with locality can\nbe extremely cumbersome, dominating the time required to develop,\nanalyze, verify, maintain, and update codes.\n\nThe outcome of the research were\n\nWe developed cost models for easily understanding the locality in parallel algorithms without having to understand the details of any particular machine.   The key model is what we call the Parallel Cache Model.  It assigns a cost to computations described in a high-level nested parallel programming model, but captures both \"spacial\" and \"temporal\" locality.\nWe developed a variety of algorithms in the model.   We showed that some standard algorithms are already efficient in the model (e.g. matrix multiply) but also developed some new algorithms (e.g. a variant of sample sort).\nWe developed theoretically efficient schedulers that map the high level Parallel Cache model onto a variety of parallel machine configurations with hierarchical memory organizations.   We were able to show some strong bounds on how the high-level costs map onto the low-level costs.\nWe implemented a scheduler that incorporates many of the theoretical ideas and experimented with a variety of algorithms using the scheduler.   Our results show that indeed the cost model does capture locality well, and that the schedulers improve performance over standard schedulers (e.g. work-stealing schedulers).\n\n\nAs part of the work we published several papers on the topic, and developed a scheduler that we have made public.   The work has lead to newer ongoing work on incorporating automatic memory management into the model and scheduler.\n\nWe also developed course material on \"cache efficient parallel algorithms\" that has been added to a graduate course on applied algorithms.   We have presented the work at a variety of venues.\n\n\t\t\t\t\tLast Modified: 05/05/2014\n\n\t\t\t\t\tSubmitted by: Guy E Blelloch"
 }
}