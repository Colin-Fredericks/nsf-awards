{
 "awd_id": "0963904",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Medium: Collaborative Research:  Geometric Network Analysis Tools:  Algorithmic Methods for Identifying Structure in Large Informatics Graphs",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frank Olken",
 "awd_eff_date": "2010-07-01",
 "awd_exp_date": "2013-06-30",
 "tot_intn_awd_amt": 418011.0,
 "awd_amount": 418011.0,
 "awd_min_amd_letter_date": "2010-04-28",
 "awd_max_amd_letter_date": "2012-07-24",
 "awd_abstract_narration": "There has been an enormous amount of work in recent years directed\r\ntoward understanding the structural and dynamical properties of\r\n\"informatics graphs\" or \"complex networks.\" Most of this work has been\r\non small to medium-sized networks, and it has led to an improved\r\nunderstanding of the properties of networks arising in many graph\r\nmining applications.  In spite of this, formulating appropriate models\r\nfor and answering even basic questions about larger informatics\r\ngraphs remains challenging.  For instance, recent work has shown that\r\ndynamic properties as well as basic structural properties of large\r\ninformatics graphs are not reproduced even qualitatively by popular\r\nnetwork generative models.\r\n\r\nThe proposed work will use traditional and recently-developed\r\napproximation algorithms for the graph partitioning problem as\r\n\"experimental probes\" of large informatics graphs in order to\r\ncharacterize in a more robust and scalable manner the structural and\r\ndynamic properties of very large informatics graphs.  This will\r\ninclude extending and implementing recently-developed algorithms such\r\nas \"local\" spectral methods and algorithms that intuitively\r\n\"interpolate\" between spectral and flow-based methods, as well as\r\nrevisiting in light of new applications traditional methods such as\r\nthe global spectral method and ideas underlying the popular package\r\nMetis.  A central goal will be to provide the analyst with tools that\r\nhave sufficient algorithmic and statistical flexibility to\r\ncharacterize the local and global structures of large networks in a\r\nrich and robust way.\r\n\r\nThe Intellectual Merit of the proposed work lies in extending recent\r\ntheoretical and algorithmic developments and applying them to very\r\nreal-world problems.  The Broader Impact of the project lies in\r\nenhancing interdisciplinary education at Berkeley and Stanford and\r\nmore generally.  This will involve the organization of meetings and\r\ncourses that will include the opportunity for research projects,\r\nincluding by students from underrepresented groups, that focus on\r\nbridging theoretical methods and real-world applications.  For\r\nfurther information see the project web page:\r\nURL:  http://cs.stanford.edu/people/mmahoney/graphmining/",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Satish",
   "pi_last_name": "Rao",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Satish B Rao",
   "pi_email_addr": "satishr@cs.berkeley.edu",
   "nsf_id": "000373507",
   "pi_start_date": "2010-04-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "1608 4TH ST STE 201",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947101749",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "792600",
   "pgm_ele_name": "ALGORITHMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 145305.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 136676.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 136030.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Given a corpus of document, what are the topics covered in the corpus?<br />Which topics are covered in each document? Automatically discovering<br />using a computer is termed topic modelling.&nbsp;&nbsp;&nbsp; Numerous approaches<br />for this problem have been proposed in the last few years. This<br />project undertook the task of evaluating these techniques in<br />a common framework and in coming up with more effective evaluation<br />techniques as well as more effective methods.<br /><br />In this project, we decided just looking at the topics that<br />were output was interesting but hard to evaluate. Instead, we<br />regarded a set of topics to be useful, if it could help us<br />predict something new about a document; for example, other related<br />topics, or related language that might be useful.&nbsp; Our framework<br />thus, dropped half of a document and tried to recover information<br />about the dropped half the topic models found using the various<br />approaches. We found that recent sophisticated approaches were<br />actually inferior to older, more established, but more<br />straightforward approaches.&nbsp; We then examined various difficult<br />cases, and derived an approach which were intuitive variations<br />on the traditional effective approaches to get still better<br />methods for topic modelling.<br /><br />In sum, we took a fresh look at topic modelling as a tool for<br />prediction, found that this view reflected negatively on currently<br />popular methods for this task, and found an improved method<br />which works better with this view of the world.<br /><br /><br /><br /><br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/06/2014<br>\n\t\t\t\t\tModified by: Satish&nbsp;B&nbsp;Rao</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nGiven a corpus of document, what are the topics covered in the corpus?\nWhich topics are covered in each document? Automatically discovering\nusing a computer is termed topic modelling.    Numerous approaches\nfor this problem have been proposed in the last few years. This\nproject undertook the task of evaluating these techniques in\na common framework and in coming up with more effective evaluation\ntechniques as well as more effective methods.\n\nIn this project, we decided just looking at the topics that\nwere output was interesting but hard to evaluate. Instead, we\nregarded a set of topics to be useful, if it could help us\npredict something new about a document; for example, other related\ntopics, or related language that might be useful.  Our framework\nthus, dropped half of a document and tried to recover information\nabout the dropped half the topic models found using the various\napproaches. We found that recent sophisticated approaches were\nactually inferior to older, more established, but more\nstraightforward approaches.  We then examined various difficult\ncases, and derived an approach which were intuitive variations\non the traditional effective approaches to get still better\nmethods for topic modelling.\n\nIn sum, we took a fresh look at topic modelling as a tool for\nprediction, found that this view reflected negatively on currently\npopular methods for this task, and found an improved method\nwhich works better with this view of the world.\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 01/06/2014\n\n\t\t\t\t\tSubmitted by: Satish B Rao"
 }
}