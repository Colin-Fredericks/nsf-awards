{
 "awd_id": "0956881",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:   iGlasses: An Appliance for Improving Speech Understanding in Face-to-Face  Communication and Classroom Situations",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Glenn H. Larsen",
 "awd_eff_date": "2010-02-01",
 "awd_exp_date": "2012-01-31",
 "tot_intn_awd_amt": 499843.0,
 "awd_amount": 561843.0,
 "awd_min_amd_letter_date": "2010-01-21",
 "awd_max_amd_letter_date": "2010-06-10",
 "awd_abstract_narration": "This Small Business Innovation Research (SBIR) Phase II project will complete the development of technology to supplement ordinary face-to-face language interaction for the millions of individuals who are deaf or hard of hearing or face other speech/language challenges. The goal of the project is to enable such individuals to fully participate in the spoken language community. The need for language and speech intelligibility aids is pervasive in today's world. Millions of individuals live with language and speech challenges (such as 36 million Americans with hearing deficits), and these individuals require additional support for communication and language learning. The Phase I research developed and tested the behavioral science and technology for iGlasses. Building on this research, the proposed research is to complete and bring to market an innovative intervention that can bring spoken language and culture into the lives of individuals who are currently marginalized because of hearing loss or other speech/language challenges. The proposed research will advance the state of the art in human machine interaction, speech, machine learning, and assistive technologies. \r\n\r\nThe broader/commercial impact of this project will benefit the deaf and hard-of-hearing populations as well as the scientific community by providing a research and theoretical foundation for a speech aid that would be naturally available to almost all individuals at a very low cost. It does not require literate users because no written information is presented as would be the case in a captioning system; it is age-independent in that it might be used by toddlers, adolescents, and throughout the lifespan; it is functional for all languages because it is language independent given that all languages share the same phonetic features with highly similar corresponding acoustic characteristics; it would provide significant help for people with hearing aids and cochlear implants; and it would be beneficial for many individuals with language challenges and even for children learning to read. Finally, regardless of the advances or lack of advances in speech recognition technology, it will always be more accurate and effective to pick off the fundamental acoustic features of speech than it is to recognize entire phonemes which are more complex combinations of these basic properties.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Cohen",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Michael M Cohen",
   "pi_email_addr": "michael@animatedspeech.com",
   "nsf_id": "000506277",
   "pi_start_date": "2010-01-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Animated Speech Corporation",
  "inst_street_address": "851 Burlway Road",
  "inst_street_address_2": "Suite 506",
  "inst_city_name": "Burlingame",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8182122913",
  "inst_zip_code": "940101715",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "CA15",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": null
 },
 "perf_inst": {
  "perf_inst_name": "Animated Speech Corporation",
  "perf_str_addr": "851 Burlway Road",
  "perf_city_name": "Burlingame",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "940101715",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "CA15",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "115E",
   "pgm_ref_txt": "RESEARCH EXP FOR TEACHERS"
  },
  {
   "pgm_ref_code": "1658",
   "pgm_ref_txt": "SOFTWARE"
  },
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "9261",
   "pgm_ref_txt": "RESRCH ASSIST-MINORITY H.S. ST"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 561843.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Enhanced Speech Comprehension for Hearing-Impaired People </strong></p>\n<p><strong>&nbsp;</strong>&nbsp; &nbsp; &nbsp;The goal of this project is to enhance the ability of hearing-impaired and deaf people to understand conversational and classroom speech. About 36 million people nationwide live with hearing deficits and confront extraordinary difficulty participating in spoken interaction. While many individuals rely on lip-reading, cued speech, cochlear implants, or hearing aids to help them perceive spoken language, none of these restore communication completely.</p>\n<p>&nbsp; &nbsp; &nbsp;The initial plan was to track the interlocutor&rsquo;s speech and process basic properties of the auditory speech signal (voicing, frication, and nasality information) in near-real time. We developed sophisticated signal processing of the speech and designed and trained artificial neural networks (ANNs) and Hidden Markov Models that accurately learned and tracked the acoustic/phonetic properties of the incoming speech. These properties were transformed into visual cues to supplement lip-reading and whatever hearing was available. Given this technological development, the three cues were presented on three LEDs placed in the periphery of a lens of our prototype eyeglasses.</p>\n<p>&nbsp; &nbsp; &nbsp;The iGlasses were envisioned to be worn as a regular pair of eyeglasses, but were designed to contain two small microphones and three colored LEDs. The wearer was intended to look at the interlocutor while the microphones delivered the interlocutor&rsquo;s speech to a processing device such as an iPhone, for processing the acoustic input. We developed a way to analyze the input for low frequency voicing information, high frequency frication energy, and nasal resonance that are associated with the acoustic/phonetic properties of voicing, frication, and nasality. The three properties were then transformed in near real-time into simple visual cues displayed on the three vertically mounted LEDs.</p>\n<p>&nbsp; &nbsp; &nbsp;The particular phonetic properties were chosen because they are fairly easy to track in the speech signal, and more importantly, because they distinguish instances within a viseme category (i.e., subsets of phonemes that are highly confusable in speechreading). These cues also require no literacy, which is a benefit in that it widens the demographic to include pre-literate children and other non-readers.</p>\n<p>&nbsp; &nbsp; &nbsp;Although the speech processing was reasonably accurate, we found that decoding this information and integrating with other information was not adequately learned even with extensive practice. This led us to seek a different approach to improving speech recognition for hard of hearing people.</p>\n<p>&nbsp; &nbsp; &nbsp;Because of dramatic improvement in automated speech recognition since our project was originally planned, and because our perceivers had difficulties learning the LED cues, we identified a different technology pairing, automated speech recognition with easily-readable text, that would be a more effective solution to the problem. Since text rather than simple LEDs are to be used for the output, eyeglasses are not feasible for the output at this stage. The technology for presenting text on eyeglasses is currently cost prohibitive. Thus, our research direction changed to implement a different solution. The new strategy uses automatic speech recognition (ASR) to translate the interlocutor&rsquo;s speech into text. The user sees the interlocutor speech and then reads the text and a screen of a portable device such as an iPhone, iPod, or iPad. This initial solution, called Read What I Say, is available in the Apple app store and is a proof-of-concept for facilitating face-to-face communication.</p>\n<p>&nbsp; &nbsp; &nbsp;The hard of hearing individual has his/her interlocutors talk into an iPhone or iPad, which presents the tex...",
  "por_txt_cntn": "\nEnhanced Speech Comprehension for Hearing-Impaired People \n\n      The goal of this project is to enhance the ability of hearing-impaired and deaf people to understand conversational and classroom speech. About 36 million people nationwide live with hearing deficits and confront extraordinary difficulty participating in spoken interaction. While many individuals rely on lip-reading, cued speech, cochlear implants, or hearing aids to help them perceive spoken language, none of these restore communication completely.\n\n     The initial plan was to track the interlocutor\u00c6s speech and process basic properties of the auditory speech signal (voicing, frication, and nasality information) in near-real time. We developed sophisticated signal processing of the speech and designed and trained artificial neural networks (ANNs) and Hidden Markov Models that accurately learned and tracked the acoustic/phonetic properties of the incoming speech. These properties were transformed into visual cues to supplement lip-reading and whatever hearing was available. Given this technological development, the three cues were presented on three LEDs placed in the periphery of a lens of our prototype eyeglasses.\n\n     The iGlasses were envisioned to be worn as a regular pair of eyeglasses, but were designed to contain two small microphones and three colored LEDs. The wearer was intended to look at the interlocutor while the microphones delivered the interlocutor\u00c6s speech to a processing device such as an iPhone, for processing the acoustic input. We developed a way to analyze the input for low frequency voicing information, high frequency frication energy, and nasal resonance that are associated with the acoustic/phonetic properties of voicing, frication, and nasality. The three properties were then transformed in near real-time into simple visual cues displayed on the three vertically mounted LEDs.\n\n     The particular phonetic properties were chosen because they are fairly easy to track in the speech signal, and more importantly, because they distinguish instances within a viseme category (i.e., subsets of phonemes that are highly confusable in speechreading). These cues also require no literacy, which is a benefit in that it widens the demographic to include pre-literate children and other non-readers.\n\n     Although the speech processing was reasonably accurate, we found that decoding this information and integrating with other information was not adequately learned even with extensive practice. This led us to seek a different approach to improving speech recognition for hard of hearing people.\n\n     Because of dramatic improvement in automated speech recognition since our project was originally planned, and because our perceivers had difficulties learning the LED cues, we identified a different technology pairing, automated speech recognition with easily-readable text, that would be a more effective solution to the problem. Since text rather than simple LEDs are to be used for the output, eyeglasses are not feasible for the output at this stage. The technology for presenting text on eyeglasses is currently cost prohibitive. Thus, our research direction changed to implement a different solution. The new strategy uses automatic speech recognition (ASR) to translate the interlocutor\u00c6s speech into text. The user sees the interlocutor speech and then reads the text and a screen of a portable device such as an iPhone, iPod, or iPad. This initial solution, called Read What I Say, is available in the Apple app store and is a proof-of-concept for facilitating face-to-face communication.\n\n     The hard of hearing individual has his/her interlocutors talk into an iPhone or iPad, which presents the text of what they said. Thus, the user can read what was said even if it wasn\u00c6t heard completely. Because the user first sees the person talking and then sees the corresponding text, both elements can be used together to understand the message.\n\n \n\nProducts\n\nRead What ..."
 }
}