{
 "awd_id": "1017766",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Large-Scale Web Crawling and Spam Avoidance in Search-Engine Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 400105.0,
 "awd_amount": 415105.0,
 "awd_min_amd_letter_date": "2010-07-21",
 "awd_max_amd_letter_date": "2011-07-26",
 "awd_abstract_narration": "Search engines and various data-mining applications commonly rely on web crawlers to navigate the web, discover valuable content, and keep it fresh. However, the enormous volume of available information and sophisticated spam techniques commonly used to deceive search engines present significant challenges in web crawling, especially in non-commercial applications such as research. The first part of this project designs efficient real-time graph-manipulation algorithms and builds a high-performance distributed web-crawler architecture that seamlessly couples the various components of Internet-scale networking, information retrieval, and graph theory. The second part creates probabilistic techniques for quick estimation of domain reputation and explores various ranking techniques to achieve better robustness against spam. The third part designs advanced budgeting mechanisms to control the crawl rate of different parts of the web at multiple levels of granularity. The project is expected to engage students at Texas A&M in research-intensive education in cross-disciplinary fields (such as data-intensive computing, networking, graph theory, distributed systems, parallel architectures, and modeling), broaden integration of web research into classroom teaching, attract undergraduate students to REU, extend participation of minority groups in engineering, stimulate collaboration among students and sharing of ideas, and permit web-related research at other institutions through publicly shared outcomes of our work.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dmitri",
   "pi_last_name": "Loguinov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dmitri Loguinov",
   "pi_email_addr": "dmitri@cs.tamu.edu",
   "nsf_id": "000410005",
   "pi_start_date": "2010-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "3124 TAMU",
  "perf_city_name": "COLLEGE STATION",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433124",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 400105.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 15000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Users often find online information and services through search engines, which in turn rely on <em>web crawlers</em> to discover valuable pages and maintain a current picture of the web. Competition for high placement in search results has created a financial incentive for many unethical Internet practices intended to deceive (i.e., spam) search engines and manipulate their ranking algorithms. To address these issues, we performed the first comprehensive study of domain-ranking algorithms and proposed a novel ranking technique whose performance significantly surpassed that of existing methods. Although this method was superior in every comparison, it required an enormous amount of processing, which made it difficult to incorporate into a high performance web crawler. To overcome this problem, we created a highly accurate ranking method that approximated the exact algorithm using four orders of magnitude less overhead. We then formalized the problem of topological ranking using finite/infinite resources and showed that traditional techniques, which work over a single infinite graph, often failed to produce good ranking lists. We solved this problem by proposing a framework that relied on cross-linking of multiple graphs, at least one of which was finite. We applied these ideas to the Internet host graph and found the resulting structure to be indeed resistant to spammer inflation.</p>\n<p>Web crawlers are often tasked with real-time detection of mirrored (or otherwise duplicate) content and classification of auto-generated templates (e.g., parked domains, blogs/forums). Many of these problems involve <em>similarity matching</em>, which is a process of finding pairs of documents that are similar in some sense to each other. Since exact algorithms are infeasible for collections larger than a few thousand pages, we developed an approach that relied on a family of <em>approximate</em> algorithms, improving other such techniques in both memory usage and runtime.</p>\n<p>When this project started, prior crawls in the literature were often poorly documented and difficult to interpret. As the field stood, there existed no standard methodology for examining web crawls and comparing their performance against one another. To improve the knowledge in the field, we presented new IRLbot implementation details, proposed a novel methodology for documenting large-scale crawls, and used it to deliver a massive amount of previously undocumented information about the IRLbot experiment. We also derived a model for extrapolating the size of various structures as a function of crawl size, confirming the colloquial notion that the space of URLs and hostnames is infinite, and estimated the number of remaining domains in larger IRLbot crawls. We also proposed several methods for assessing Internet-wide crawl coverage, examined the budget function of IRLbot, and suggested avenues for improvement.</p>\n<p>Many Big Data applications (e.g., search engines, web crawlers) process streams of random key-value records that follow highly skewed frequency distributions. To characterizing their performance, we created an accurate analytical framework for modeling random data streams. We demonstrated that these models were applicable to&nbsp;not just synthetically generated streams, but also real workloads stemming from caching and external-memory processing of real-world graphs. Besides helping determine the amount of disk activity, these results enable accurate modeling of the runtime and selection of best parameters that optimize large-scale computation.</p>\n<p>With the ever-growing volume and speed of Internet traffic, network applications place higher demand on packet I/O rates. Although 1-Gbps and even 10-Gbps Ethernet are widely adopted, achieving wire rate with small packets is still hindered by bottlenecks inside the TCP/IP stack. Improvements have been made for Linux, but there is limited work in ...",
  "por_txt_cntn": "\nUsers often find online information and services through search engines, which in turn rely on web crawlers to discover valuable pages and maintain a current picture of the web. Competition for high placement in search results has created a financial incentive for many unethical Internet practices intended to deceive (i.e., spam) search engines and manipulate their ranking algorithms. To address these issues, we performed the first comprehensive study of domain-ranking algorithms and proposed a novel ranking technique whose performance significantly surpassed that of existing methods. Although this method was superior in every comparison, it required an enormous amount of processing, which made it difficult to incorporate into a high performance web crawler. To overcome this problem, we created a highly accurate ranking method that approximated the exact algorithm using four orders of magnitude less overhead. We then formalized the problem of topological ranking using finite/infinite resources and showed that traditional techniques, which work over a single infinite graph, often failed to produce good ranking lists. We solved this problem by proposing a framework that relied on cross-linking of multiple graphs, at least one of which was finite. We applied these ideas to the Internet host graph and found the resulting structure to be indeed resistant to spammer inflation.\n\nWeb crawlers are often tasked with real-time detection of mirrored (or otherwise duplicate) content and classification of auto-generated templates (e.g., parked domains, blogs/forums). Many of these problems involve similarity matching, which is a process of finding pairs of documents that are similar in some sense to each other. Since exact algorithms are infeasible for collections larger than a few thousand pages, we developed an approach that relied on a family of approximate algorithms, improving other such techniques in both memory usage and runtime.\n\nWhen this project started, prior crawls in the literature were often poorly documented and difficult to interpret. As the field stood, there existed no standard methodology for examining web crawls and comparing their performance against one another. To improve the knowledge in the field, we presented new IRLbot implementation details, proposed a novel methodology for documenting large-scale crawls, and used it to deliver a massive amount of previously undocumented information about the IRLbot experiment. We also derived a model for extrapolating the size of various structures as a function of crawl size, confirming the colloquial notion that the space of URLs and hostnames is infinite, and estimated the number of remaining domains in larger IRLbot crawls. We also proposed several methods for assessing Internet-wide crawl coverage, examined the budget function of IRLbot, and suggested avenues for improvement.\n\nMany Big Data applications (e.g., search engines, web crawlers) process streams of random key-value records that follow highly skewed frequency distributions. To characterizing their performance, we created an accurate analytical framework for modeling random data streams. We demonstrated that these models were applicable to not just synthetically generated streams, but also real workloads stemming from caching and external-memory processing of real-world graphs. Besides helping determine the amount of disk activity, these results enable accurate modeling of the runtime and selection of best parameters that optimize large-scale computation.\n\nWith the ever-growing volume and speed of Internet traffic, network applications place higher demand on packet I/O rates. Although 1-Gbps and even 10-Gbps Ethernet are widely adopted, achieving wire rate with small packets is still hindered by bottlenecks inside the TCP/IP stack. Improvements have been made for Linux, but there is limited work in Windows. To bridge this gap, we proposed a set of new algorithms and provided a corresponding implementation that impr..."
 }
}