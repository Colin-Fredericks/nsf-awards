{
 "awd_id": "1016501",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF:  Small:  Fast and Efficient Randomized Algorithms for Solving Laplacian Systems of Linear Equations and Sparse Least Squares Problems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Anita La Salle",
 "awd_eff_date": "2010-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 322731.0,
 "awd_amount": 322731.0,
 "awd_min_amd_letter_date": "2010-07-22",
 "awd_max_amd_letter_date": "2013-05-06",
 "awd_abstract_narration": "Randomization in the context of linear-algebraic algorithms is an exciting and innovative idea. In recent years, a large body of work has focused on provably accurate randomized algorithms for  regression problems, with a particular emphasis on least-squares regression. Fast algorithms for such problems are of continuous interest due to their broad applicability in scientific computing and statistical data analysis, where increasingly larger input matrices appear. The PI seeks to theoretically and numerically investigate provably accurate and practically useful randomized algorithms for such problems when (i) the constraint matrix of the regression problem is Laplacian, or (ii) the regression problem is under- or over-constrained and sparse. Thus, the PI seeks to address the alarming gap between recent breakthrough theoretical results of Spielman, Teng, and collaborators and their practical applicability, as well as the lack of efficient algorithms dealing with over- or under-constrained regression problems with sparse input matrices. In order to bridge the gap between theory and applications in this line of research, a number of novel theoretical results are necessary and will be investigated. The practical usefulness of the proposed research will be numerically evaluated using data matrices from scientific applications.\r\n\r\n\r\nEfficiently solving large systems of linear equations is perhaps the most fundamental question in numerical analysis and linear algebra, mainly because such systems are ubiquitous in scientific computing applications. The proposed work seeks to bring the theoretical breakthroughs of the recent work of Spielman, Teng, and collaborators on solving systems of linear equations with Laplacian input matrices closer to practice. Towards that end, both theoretical as well as numerical results will be derived. This research paradigm can subsequently be used as a starting point in order to spark further research efforts on broader classes of massive systems of linear equations. A second aspect of the impact of the proposed work has to do with the considerable overlap between Theoretical Computer Science and Numerical Linear Algebra approaches that will be explored. As randomization becomes increasingly useful in the context of linear algebra, the PI expects that the next generation of researchers in this domain will need solid training in both areas, which is exactly what the proposed work will provide to graduate students. Finally, a third aspect of the impact of the proposed work will emerge from the dissemination of our results via workshops, tutorials, and mini-symposia in high-profile relevant conferences.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Malik",
   "pi_last_name": "Magdon-Ismail",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Malik Magdon-Ismail",
   "pi_email_addr": "magdon@rpi.edu",
   "nsf_id": "000111881",
   "pi_start_date": "2010-10-07",
   "pi_end_date": "2013-05-06"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Petros",
   "pi_last_name": "Drineas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Petros Drineas",
   "pi_email_addr": "pdrineas@purdue.edu",
   "nsf_id": "000117416",
   "pi_start_date": "2013-05-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rensselaer Polytechnic Institute",
  "inst_street_address": "110 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "TROY",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5182766000",
  "inst_zip_code": "121803590",
  "inst_country_name": "United States",
  "cong_dist_code": "20",
  "st_cong_dist_code": "NY20",
  "org_lgl_bus_name": "RENSSELAER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "U5WBFKEBLMX3"
 },
 "perf_inst": {
  "perf_inst_name": "Rensselaer Polytechnic Institute",
  "perf_str_addr": "110 8TH ST",
  "perf_city_name": "TROY",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "121803590",
  "perf_ctry_code": "US",
  "perf_cong_dist": "20",
  "perf_st_cong_dist": "NY20",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "793300",
   "pgm_ele_name": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 322731.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div>In this project, we investigated the use of Randomized Numerical Linear Algebra (RandNLA) approaches to solving least-squares problems, including the special case of systems of linear equations with Laplacian matrices as inputs. The project contributed a number of novel results that we are describing in more detail below.</div>\n<div>1. Leverage scores: We investigated the connections between leverage scores and effective resistances and came up with fast, practical algorithms for estimating the leverage scores. Prior to this research, existing state-of-the-art algorithms did not immediately imply new and improved algorithms for approximating the leverage scores of arbitrary matrices. Leverage scores essentially correspond to eigenvector computations, a topic that has been well-explored in the numerical analysis literature. Our team, in the context of this project, developed simple, provably accurate, easily implementable algorithms that approximate statistical leverage scores in approximately linear time in the input size exist for arbitrary input matrices.</div>\n<div>2. Connections between graph sparsification and large-scale regression problems: our team investigated connections between leverage scores and graph sparsification as well as large-scale regression problems. More specifically, the existence of almost linear graph sparsifiers that preserve the eigenvalues of the original graph Laplacian matrix up to constant factor accuracy was known. The connections between such approaches and leverage score were not known; it was also not known whether the running time of prior methods could be improved using techniques such as the ones that were investigated in our leverage score research. We were also able to extend the notion of leverage scores to general regression problems and demonstrate that they can be solved faster using approximations to such scores.</div>\n<div>3. Connections with the column subset selection problem:&nbsp; Graph sparsification, at least to some extent, might be thought of as column subset selection from the edge-incidence matrix of a graph. Prior results argued that every matrix has a sufficiently large submatrix consisting of columns of the original matrix whose spectral norm and, even more impressively, whose condition number is bounded from above by reasonably small constants. The proofs of these theorems were, until recently, existential. We demonstrated connections between these theorems and our work on the column subset selection problem and graph sparsification.</div>\n<div>5. Numerical evaluations: Finally, our team performed experimental evaluations of the proposed methods for graph sparsification and regression in data mining and scientific computing applications.</div>\n<div>This project trained one PhD student. The PI was an organizer and a lecturer of the 2015 Gene Golub SIAM Summer School on the topic on Randomization in Numerical Linear Algebra. This massive outreach activity attracted 50 students from US and European universities and exposed this line of research to a broad audience.</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2015<br>\n\t\t\t\t\tModified by: Petros&nbsp;Drineas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nIn this project, we investigated the use of Randomized Numerical Linear Algebra (RandNLA) approaches to solving least-squares problems, including the special case of systems of linear equations with Laplacian matrices as inputs. The project contributed a number of novel results that we are describing in more detail below.\n1. Leverage scores: We investigated the connections between leverage scores and effective resistances and came up with fast, practical algorithms for estimating the leverage scores. Prior to this research, existing state-of-the-art algorithms did not immediately imply new and improved algorithms for approximating the leverage scores of arbitrary matrices. Leverage scores essentially correspond to eigenvector computations, a topic that has been well-explored in the numerical analysis literature. Our team, in the context of this project, developed simple, provably accurate, easily implementable algorithms that approximate statistical leverage scores in approximately linear time in the input size exist for arbitrary input matrices.\n2. Connections between graph sparsification and large-scale regression problems: our team investigated connections between leverage scores and graph sparsification as well as large-scale regression problems. More specifically, the existence of almost linear graph sparsifiers that preserve the eigenvalues of the original graph Laplacian matrix up to constant factor accuracy was known. The connections between such approaches and leverage score were not known; it was also not known whether the running time of prior methods could be improved using techniques such as the ones that were investigated in our leverage score research. We were also able to extend the notion of leverage scores to general regression problems and demonstrate that they can be solved faster using approximations to such scores.\n3. Connections with the column subset selection problem:  Graph sparsification, at least to some extent, might be thought of as column subset selection from the edge-incidence matrix of a graph. Prior results argued that every matrix has a sufficiently large submatrix consisting of columns of the original matrix whose spectral norm and, even more impressively, whose condition number is bounded from above by reasonably small constants. The proofs of these theorems were, until recently, existential. We demonstrated connections between these theorems and our work on the column subset selection problem and graph sparsification.\n5. Numerical evaluations: Finally, our team performed experimental evaluations of the proposed methods for graph sparsification and regression in data mining and scientific computing applications.\nThis project trained one PhD student. The PI was an organizer and a lecturer of the 2015 Gene Golub SIAM Summer School on the topic on Randomization in Numerical Linear Algebra. This massive outreach activity attracted 50 students from US and European universities and exposed this line of research to a broad audience.\n\n \n\n\t\t\t\t\tLast Modified: 09/01/2015\n\n\t\t\t\t\tSubmitted by: Petros Drineas"
 }
}