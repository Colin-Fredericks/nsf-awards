{
 "awd_id": "1036462",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Frontiers of Activity Recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2010-05-15",
 "awd_exp_date": "2011-04-30",
 "tot_intn_awd_amt": 20000.0,
 "awd_amount": 20000.0,
 "awd_min_amd_letter_date": "2010-05-19",
 "awd_max_amd_letter_date": "2010-05-19",
 "awd_abstract_narration": "This award is made in support of a collaborative project called \"Frontiers in Activity Recognition\" whereby a group of experts from different fields of computer science, engineering, mathematics and statistics convene in a workshop to be held in the vicinity of UCLA.  \r\nOne component of the workshop consists in interactive break-out sessions where different approaches to activity representation (descriptors) and recognition will be analyzed. A second component consists in a competition, announced to the broad public ahead of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), whereby an extensive dataset provided by a third party will be released, with benchmarks, and contestants will be invited to submit their best results in the detection of a number of action categories. The proposers of high-ranking approaches will be invited to the workshop to present their results and discuss it in the context of the analysis of the state of the art to be performed as part of the field assessment. The workshop can have broad impact to many applications ranging from security (surveillance, monitoring) to environmental science (habitat monitoring, global warming), to industrial operations (factory floor optimization), to multi-media and information retrieval (content-based video meta-data extraction), to entertainment (input devices for games), and to transportation (driver assistance).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefano",
   "pi_last_name": "Soatto",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefano Soatto",
   "pi_email_addr": "soatto@ucla.edu",
   "nsf_id": "000489719",
   "pi_start_date": "2010-05-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "10889 WILSHIRE BLVD STE 700",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900244200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 20000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Automated analysis of video is becoming increasingly important in applications ranging from security to environmental monitoring, to industrial engineering, to multi-media and content-based retrieval, entertainment, transportation, just to mention a few. In particular, the analysis of actions and events has emerged as a particularly challenging domain, where significant activity is underway, and yet progress has been elusive in many scenarios other than the most structured ones.<br />The Workshop on Frontiers of Activity Recognition was held on June 29-30, 2010, in Los Angeles, with the goal of bringing together the foremost experts on Activity Recognition in Video, to assess the state of the art, identify challenges and opportunities, and chart the path ahead. In addition to a number of invited participants, a Challenge and Dataset were issued and circulated worldwide prior to the Workshop, inviting researchers to submit their results on a new benchmark dataset recently released under the aegis of DARPA's VIRAT Program. The dataset was made publicly available, and several contestants entered their results, from which a number of entries were selected for presentation in the Workshop.<br />Among the key issues in need for development, representational ambiguity was identified as the most challenging, as significant within-class variability has to be captured and modeled: The same action can manifest itself in a wide variety of ways depending on the actor, the viewer, environmental conditions including clutter, illumination and other nuisance factors. Many actions occur at different time scales, and involve the interaction among multiple people, or people and objects. &nbsp;Also the absence of a clear taxonomy and unambiguos classification of actions makes it difficult to develop benchmarks, to evaluate existing schemes, and to quantify progress.<br />During the workshop, the current state of the art was surveyed and critiqued, and failures and promising developments identified. Following parallel progress in the field of object recognition, it was determined that progressive challenges, where researchers compete on benchmarks of increasing difficulty, are desirable, although the limitations of such challenges were evident in the results of the preliminary competition on the dataset released. In addition to empirical evaluation of progress based on benchmark datasets, that is susceptible to the limitations of said datasets, analytical work characterizing the properties of spatio-temporal descriptors for action was also auspicated.<br />A detailed report of the findings of the workshop is available through NSF.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/15/2011<br>\n\t\t\t\t\tModified by: Stefano&nbsp;Soatto</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAutomated analysis of video is becoming increasingly important in applications ranging from security to environmental monitoring, to industrial engineering, to multi-media and content-based retrieval, entertainment, transportation, just to mention a few. In particular, the analysis of actions and events has emerged as a particularly challenging domain, where significant activity is underway, and yet progress has been elusive in many scenarios other than the most structured ones.\nThe Workshop on Frontiers of Activity Recognition was held on June 29-30, 2010, in Los Angeles, with the goal of bringing together the foremost experts on Activity Recognition in Video, to assess the state of the art, identify challenges and opportunities, and chart the path ahead. In addition to a number of invited participants, a Challenge and Dataset were issued and circulated worldwide prior to the Workshop, inviting researchers to submit their results on a new benchmark dataset recently released under the aegis of DARPA's VIRAT Program. The dataset was made publicly available, and several contestants entered their results, from which a number of entries were selected for presentation in the Workshop.\nAmong the key issues in need for development, representational ambiguity was identified as the most challenging, as significant within-class variability has to be captured and modeled: The same action can manifest itself in a wide variety of ways depending on the actor, the viewer, environmental conditions including clutter, illumination and other nuisance factors. Many actions occur at different time scales, and involve the interaction among multiple people, or people and objects.  Also the absence of a clear taxonomy and unambiguos classification of actions makes it difficult to develop benchmarks, to evaluate existing schemes, and to quantify progress.\nDuring the workshop, the current state of the art was surveyed and critiqued, and failures and promising developments identified. Following parallel progress in the field of object recognition, it was determined that progressive challenges, where researchers compete on benchmarks of increasing difficulty, are desirable, although the limitations of such challenges were evident in the results of the preliminary competition on the dataset released. In addition to empirical evaluation of progress based on benchmark datasets, that is susceptible to the limitations of said datasets, analytical work characterizing the properties of spatio-temporal descriptors for action was also auspicated.\nA detailed report of the findings of the workshop is available through NSF.\n\n\t\t\t\t\tLast Modified: 05/15/2011\n\n\t\t\t\t\tSubmitted by: Stefano Soatto"
 }
}