{
 "awd_id": "1007593",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Conditional Modeling and Conditional Inference",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2010-09-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 245999.0,
 "awd_amount": 245999.0,
 "awd_min_amd_letter_date": "2010-09-09",
 "awd_max_amd_letter_date": "2010-09-09",
 "awd_abstract_narration": "In many applications, the complexity and dimensionality of the data preclude nonparametric inference, despite the availability of massive data sets. At the same time, it is usually true that too little is known about the detailed mechanisms generating the data to meaningfully specify parametric models. Conditional modeling and conditional inference are semi-parametric approaches to complex high-dimensional data in which attention is focused on manageable low-dimensional statistical modeling and estimation. Applications include efficient feature estimation and data classification (e.g. in computer vision), exact tests for broad and scientifically relevant hypotheses (e.g. in the statistical analysis of multi-electrode neuronal recordings), exploration of time scale in non-stationary processes (e.g. in the study of market dynamics), and construction of complex distributions through successive low-dimensional perturbations (e.g. in the study of probabilistic context-sensitive grammars). \r\n \r\nHigh-dimensional data are ubiquitous. Sources include molecular biology, finance, neurophysiological recordings, and the imagery and text of the Internet. Despite the availability of almost unlimited amounts of these data, their complexity and high dimensionality challenge existing statistical models and represent a bottleneck to successful applications. Oftentimes the complexity and dimensionality can be finessed through mathematical methods that select and focus on a collection of low-dimensional characteristics of the data. The approach avoids untenable or un-testable model assumptions without necessarily compromising the information content and power of the data. The research is at the interface between statistical theory and scientific application, with potential impact in technology (e.g. through computer vision) and, more broadly, society (e.g. through neuroscience and better financial modeling).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stuart",
   "pi_last_name": "Geman",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Stuart A Geman",
   "pi_email_addr": "Stuart_Geman@Brown.edu",
   "nsf_id": "000316539",
   "pi_start_date": "2010-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Harrison",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew T Harrison",
   "pi_email_addr": "Matthew_Harrison@brown.edu",
   "nsf_id": "000245423",
   "pi_start_date": "2010-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "1 PROSPECT ST",
  "perf_city_name": "PROVIDENCE",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 245999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Increasingly, scientists are faced with the tasks of modeling and analyzing extremely high-dimensional data, such as financial time series, multi-channel neurophysiological recordings, and the large amounts of text and imagery available through the Internet. Despite the availability of large quantities of data, developing useful models for complex high-dimensional systems remains a challenge and oftentimes a bottleneck to successful applications.&nbsp; The timing of important market movements remains a mystery, computer vision is nowhere near as versatile or effective as biological vision, and newer and better ways to record neural activities in the brains of humans and animals have not resolved the most basic questions about representation and computation in the neural circuitry.</p>\n<p>Complex systems cannot be understood without models.&nbsp; Oftentimes, high-dimensional systems can be understood with low-dimensional models, through a kind of statistical focusing process called conditioning.&nbsp; Through conditioning, a sequence of simple models can be constructed to successively approximate a high-dimensional complex system.</p>\n<p>A variety of successful applications have emerged, including a method for the analysis of the fine-temporal structure in the communication between neurons, an approach to exploring the time scale of the departure of market data from financial models, an approach to feature selection for image classification, and methods for managing complexity in models of scenes and in the applications of these models to automated image analysis.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/18/2013<br>\n\t\t\t\t\tModified by: Stuart&nbsp;A&nbsp;Geman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIncreasingly, scientists are faced with the tasks of modeling and analyzing extremely high-dimensional data, such as financial time series, multi-channel neurophysiological recordings, and the large amounts of text and imagery available through the Internet. Despite the availability of large quantities of data, developing useful models for complex high-dimensional systems remains a challenge and oftentimes a bottleneck to successful applications.  The timing of important market movements remains a mystery, computer vision is nowhere near as versatile or effective as biological vision, and newer and better ways to record neural activities in the brains of humans and animals have not resolved the most basic questions about representation and computation in the neural circuitry.\n\nComplex systems cannot be understood without models.  Oftentimes, high-dimensional systems can be understood with low-dimensional models, through a kind of statistical focusing process called conditioning.  Through conditioning, a sequence of simple models can be constructed to successively approximate a high-dimensional complex system.\n\nA variety of successful applications have emerged, including a method for the analysis of the fine-temporal structure in the communication between neurons, an approach to exploring the time scale of the departure of market data from financial models, an approach to feature selection for image classification, and methods for managing complexity in models of scenes and in the applications of these models to automated image analysis. \n\n \n\n\t\t\t\t\tLast Modified: 12/18/2013\n\n\t\t\t\t\tSubmitted by: Stuart A Geman"
 }
}