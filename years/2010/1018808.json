{
 "awd_id": "1018808",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: High-Fidelity Datacenter Emulation",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2010-07-21",
 "awd_max_amd_letter_date": "2010-07-21",
 "awd_abstract_narration": "This research addresses a fundamental computer systems challenge presented by the rise of datacenter computing as the dominant platform for ``cloud computing.''  Today, datacenters host mission-critical services for IT, health, and financial institutions using complex systems consisting of large ensembles of machines spread across multiple physical networks and geographic regions.  These sophisticated services must meet stringent design and performance requirements such as horizontal scalability, fault tolerance, self adaptation, and security while leveraging low-cost commodity hardware.  A key challenge is to quantify the impact of hardware  changes, software designs, and energy management policies.  Critically, such evaluations require the real code, workloads, component interactions, heterogeneous hardware, and high-load conditions to accurately predict performance.  \r\n\r\nThis proposal investigates techniques and architectures for building a high-fidelity datacenter emulation platform to transform datacenter network and service design from a black art into a rigorous, accurate, and repeatable (scientific) process.  Such a facility allows researchers and architects to develop the principles and best practices for next-generation datacenter design.  By accurately emulating realistic datacenter scales (10,000+ machines, 100+switches, multiple cooperating services) with a modest cluster of a few hundred machines, the proposed work aims to place datacenter experimentation within the reach of students, academics, and businesses without the financial reach of large IT firms.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kenneth",
   "pi_last_name": "Yocum",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Kenneth G Yocum",
   "pi_email_addr": "kyocum@cs.ucsd.edu",
   "nsf_id": "000367876",
   "pi_start_date": "2010-07-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Amin",
   "pi_last_name": "Vahdat",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Amin M Vahdat",
   "pi_email_addr": "vahdat@cs.ucsd.edu",
   "nsf_id": "000438703",
   "pi_start_date": "2010-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 6\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<div class=\"page\" title=\"Page 2\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>This research addresses a fundamental computer systems challenge presented by the rise of datacenter computing as the dominant platform for providing ``cloud computing.'' &nbsp; Datacenters are the primary environments that provide sufficient computing power to run both consumer-facing compute services (e.g., maps, web search, social networks) and mission-critical business (CRM), health, financial, and eScience applications. Today, datacenters deliver these services using complex systems consisting of&nbsp;large ensembles of machines spread across multiple physical networks and geographic regions. These sophisticated services must meet stringent design and performance requirements such as horizontal scalability, fault tolerance, self adaptation, and security while leveraging low-cost commodity hardware. At the same time, these applications, such as large-scale data processing and machine virtualization, place incredible demands on the datacenter network, resulting in numerous alternative network architectures.</p>\n<p>The primary goal of this research is to provide techniques, tools, and accurate workloads to allow both researchers and industry experts to build next-generation data center networks and applications. &nbsp; However, a&nbsp;fundamental bottleneck to designing the next generation of datacenter applications and networks is the inability to test and measure these systems at scale. &nbsp;It is often difficult to design next generation data center networks without being able to test those designs on expensive and limited production environments with real-world traffic patterns. &nbsp;</p>\n<p>This research made four primary contributions: &nbsp;</p>\n<ol>\n<li>In-situ MapReduce: &nbsp;a scalable monitoring framework for processing logs produced in data centers. &nbsp;The analysis of logs is an increasingly important component of running&nbsp;large Internet sites and services. However, the traditional centralized approach to log processing fundamentally limits its scale and timeliness as a data center can produce terabytes of log data daily. &nbsp;&nbsp;In contrast, In-situ MapReduce processes log data on the servers &nbsp;themselves. &nbsp;This architecture runs continuous analytics written in the popular MapReduce programming model, and intelligently adapts to changes in available CPU and memory by sampling the incoming data. &nbsp;&nbsp;Ultimately, continuous in-situ processing improves system scalability and reduces analysis times for data center log analytics.&nbsp;&nbsp;</li>\n<li>Quantify limits of data center network emulation: &nbsp;A key method for testing new data center designs is to test them in small-scale network \"emulators\" that try to reproduce conditions seen in larger deployments. Such emulators should faithfully reproduce the behavior of actual hardware. &nbsp; This research found specific conditions and quantified the errors introduced when shrinking an emulated software defined network to test new network hardware and software designs.</li>\n<li>Accurate switch emulation: &nbsp; Emulation platforms must accomodate the intricacies of real-world network hardware, such as software-defined network (SDN) switches. &nbsp; This research introduced a new technique that captures a fingerprint for a particular vendor's network switch and can then apply the fingerprint to a generic SDN switch emulator to re-create switch vendor artifacts.</li>\n<li>Facebook data center traffic study: &nbsp; The research produced a detailed analytis of the network workloads from within Facebook&rsquo;s datacenters. &nbsp; The work illustrates how prior traffic studies may not accurately capture Facebook&rsquo;s network demands. &nbsp;Thus the assumptions made by previous network fabric and protocol design research...",
  "por_txt_cntn": "\n\n\n\n\n\n\nThis research addresses a fundamental computer systems challenge presented by the rise of datacenter computing as the dominant platform for providing ``cloud computing.''   Datacenters are the primary environments that provide sufficient computing power to run both consumer-facing compute services (e.g., maps, web search, social networks) and mission-critical business (CRM), health, financial, and eScience applications. Today, datacenters deliver these services using complex systems consisting of large ensembles of machines spread across multiple physical networks and geographic regions. These sophisticated services must meet stringent design and performance requirements such as horizontal scalability, fault tolerance, self adaptation, and security while leveraging low-cost commodity hardware. At the same time, these applications, such as large-scale data processing and machine virtualization, place incredible demands on the datacenter network, resulting in numerous alternative network architectures.\n\nThe primary goal of this research is to provide techniques, tools, and accurate workloads to allow both researchers and industry experts to build next-generation data center networks and applications.   However, a fundamental bottleneck to designing the next generation of datacenter applications and networks is the inability to test and measure these systems at scale.  It is often difficult to design next generation data center networks without being able to test those designs on expensive and limited production environments with real-world traffic patterns.  \n\nThis research made four primary contributions:  \n\nIn-situ MapReduce:  a scalable monitoring framework for processing logs produced in data centers.  The analysis of logs is an increasingly important component of running large Internet sites and services. However, the traditional centralized approach to log processing fundamentally limits its scale and timeliness as a data center can produce terabytes of log data daily.   In contrast, In-situ MapReduce processes log data on the servers  themselves.  This architecture runs continuous analytics written in the popular MapReduce programming model, and intelligently adapts to changes in available CPU and memory by sampling the incoming data.   Ultimately, continuous in-situ processing improves system scalability and reduces analysis times for data center log analytics.  \nQuantify limits of data center network emulation:  A key method for testing new data center designs is to test them in small-scale network \"emulators\" that try to reproduce conditions seen in larger deployments. Such emulators should faithfully reproduce the behavior of actual hardware.   This research found specific conditions and quantified the errors introduced when shrinking an emulated software defined network to test new network hardware and software designs.\nAccurate switch emulation:   Emulation platforms must accomodate the intricacies of real-world network hardware, such as software-defined network (SDN) switches.   This research introduced a new technique that captures a fingerprint for a particular vendor's network switch and can then apply the fingerprint to a generic SDN switch emulator to re-create switch vendor artifacts.\nFacebook data center traffic study:   The research produced a detailed analytis of the network workloads from within Facebook\u00c6s datacenters.   The work illustrates how prior traffic studies may not accurately capture Facebook\u00c6s network demands.  Thus the assumptions made by previous network fabric and protocol design research proposals may not apply to current large-scale datacenter deployments.    These findings promise to influence how the academic and industrial communities design the next generation of networking equipment, protocols, and data center architectures. \n\n\nFinally this project has helped fund and provide mentoring for the careers of six graduate students.  The students have joined the global workforce in c..."
 }
}