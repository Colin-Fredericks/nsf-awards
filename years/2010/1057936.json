{
 "awd_id": "1057936",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TC: Workshop on Real-World Cybersecurity Data for Research",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2011-08-31",
 "tot_intn_awd_amt": 93582.0,
 "awd_amount": 93582.0,
 "awd_min_amd_letter_date": "2010-08-30",
 "awd_max_amd_letter_date": "2010-08-30",
 "awd_abstract_narration": "As a consequence of the 2009 White House 60-Day Review, it became clear that the academic community working on cyber-security is in dire need of real data. The PREDICT portal is an effort to catalog and house the increasingly available research data, and per NSF's request, the needs of the academia for data have been documented.  On the other hand, agencies, companies and organizations that do have data are seeking research innovations in the ?arms race? against cyber-attacks.  The  ?Cyber-security Data for Experimentation? will bring together people from companies/organizations, academia, and government agencies to discuss (1) models of engagement that will allow the research community to conduct experiments with real-world data sets, (2) how to share research results, and (3) how funding agencies can facilitate the process. A guiding principle of the workshop is that the resulting models should be feasible with the built-in incentives to all parties involved, and the guarantees against violations of privacy or other regulations. The workshop will be organized around panel discussions of three topics: (1) Data availability and use conditions, (2) Research that can greatly benefit from available data to make much needed progresses in cyber-defense, and (3) Engagement models and related IP issues. This will be a one-day workshop, in the DC metro area, held on August 27th, 2010.  The workshop will be broadcast to the community at large and will be open to questions and suggestions from the listeners.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicholas",
   "pi_last_name": "Feamster",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Nicholas G Feamster",
   "pi_email_addr": "feamster@uchicago.edu",
   "nsf_id": "000489704",
   "pi_start_date": "2010-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "926 DALNEY ST NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303186395",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779500",
   "pgm_ele_name": "TRUSTWORTHY COMPUTING"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 93582.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\">The &ldquo;60-Day Review&rdquo; pointed out that academic community working on cybersecurity is in dire need of real data. The PREDICT portal is an effort to catalog and house the increasingly available research data, and per NSF&rsquo;s request, the needs of the academia for data have been documented.</p>\n<p class=\"p1\">On the other hand, agencies, companies, and organizations who do have data are seeking research in- novations in the &ldquo;arms race&rdquo; against cyber-attacks. To bridge the two sides, we are organizing a workshop on &ldquo;Real-world Cybersecurity Data Research&rdquo; that brought together people from companies/organizations, academia, and government agencies. The goal of the workshop is to discuss (1) models of engagement that allowed the research community to conduct experiments with real-world data sets, (2) how to share research results, and (3) how funding agencies can facilitate the process. This workshop is sponsored by NSF and is in collaboration with DHS, ONR, and the Treasury (we are currently seeking more collaborators). The workshop has attracted companies that are committed to participate and eager to share their data or to engage the research community in other ways concerning real-world data for cybersecurity research.</p>\n<p class=\"p1\">A guiding principle of the workshop is that the resulting models should be feasible with the built-in incentives to all parties involved, and the guarantees against violations of privacy or other regulations. The workshop was organized around panel discussions of three topics: (1) Data availability and use conditions, (2) Research that can greatly benefit from available data to make much needed progresses in cyber-defense, and (3) Engagement models and related IP issues. This was a one-day workshop, in the DC metro area, held on August 27th, 2010.</p>\n<p class=\"p1\">We asked attendees from research and industry to prepare respective material. We asked academics to write one paragraph describing a research project or idea that they are currently working on where real-world cybersecurity data could help them answer their questions better. We asked industry to list one question they would like answered, or one problem that they would like solved where the use of their data could be brought to bear in solving the problem.</p>\n<p class=\"p1\">The workshop had a keynote speech on why data is so important for academics. The remainder of the day consisted of three panels: (1) a panel from industry, discussing the availability of various data, as well as how it can be analyzed; (2) a panel from academics describing the&nbsp;data that is needed; (3) a panel from various participants discussing the mechanics and policies of data sharing.</p>\n<p class=\"p1\">Roger Dingledine shared the following insights from the workshop, which best summarized the outcomes and insights:</p>\n<p class=\"p1\">1. Researchers already have data, it&rsquo;s just not the data they think they want. Either they need to clean / understand / better analyze what they already have, or they need to figure out where they can gather the data themselves (universities sure have lots of users), or they can turn to organizations like PREDICT (or Tor) that are aggregating data sets for the purpose of making them available for researchers.&nbsp;</p>\n<p class=\"p1\">2. Data preservation questions. When a student moves on, it&rsquo;s common that nobody knows how to continue using what gets left behind. The industry side similarly finds internships too short to be a reliable investment: interns disappear right about the time they start to provide value.</p>\n<p class=\"p1\">3. Standardized data sets vs specialized data sets. On the one hand, we want standardized public open data sets (think traces for voice recognition), so everybody is doing research from the same starting point.&nbsp;</p>\n<p class=\"p1\">4. Existing databases need labelling &ndash; fo...",
  "por_txt_cntn": "The \"60-Day Review\" pointed out that academic community working on cybersecurity is in dire need of real data. The PREDICT portal is an effort to catalog and house the increasingly available research data, and per NSF\u00c6s request, the needs of the academia for data have been documented.\nOn the other hand, agencies, companies, and organizations who do have data are seeking research in- novations in the \"arms race\" against cyber-attacks. To bridge the two sides, we are organizing a workshop on \"Real-world Cybersecurity Data Research\" that brought together people from companies/organizations, academia, and government agencies. The goal of the workshop is to discuss (1) models of engagement that allowed the research community to conduct experiments with real-world data sets, (2) how to share research results, and (3) how funding agencies can facilitate the process. This workshop is sponsored by NSF and is in collaboration with DHS, ONR, and the Treasury (we are currently seeking more collaborators). The workshop has attracted companies that are committed to participate and eager to share their data or to engage the research community in other ways concerning real-world data for cybersecurity research.\nA guiding principle of the workshop is that the resulting models should be feasible with the built-in incentives to all parties involved, and the guarantees against violations of privacy or other regulations. The workshop was organized around panel discussions of three topics: (1) Data availability and use conditions, (2) Research that can greatly benefit from available data to make much needed progresses in cyber-defense, and (3) Engagement models and related IP issues. This was a one-day workshop, in the DC metro area, held on August 27th, 2010.\nWe asked attendees from research and industry to prepare respective material. We asked academics to write one paragraph describing a research project or idea that they are currently working on where real-world cybersecurity data could help them answer their questions better. We asked industry to list one question they would like answered, or one problem that they would like solved where the use of their data could be brought to bear in solving the problem.\nThe workshop had a keynote speech on why data is so important for academics. The remainder of the day consisted of three panels: (1) a panel from industry, discussing the availability of various data, as well as how it can be analyzed; (2) a panel from academics describing the data that is needed; (3) a panel from various participants discussing the mechanics and policies of data sharing.\nRoger Dingledine shared the following insights from the workshop, which best summarized the outcomes and insights:\n1. Researchers already have data, it\u00c6s just not the data they think they want. Either they need to clean / understand / better analyze what they already have, or they need to figure out where they can gather the data themselves (universities sure have lots of users), or they can turn to organizations like PREDICT (or Tor) that are aggregating data sets for the purpose of making them available for researchers. \n2. Data preservation questions. When a student moves on, it\u00c6s common that nobody knows how to continue using what gets left behind. The industry side similarly finds internships too short to be a reliable investment: interns disappear right about the time they start to provide value.\n3. Standardized data sets vs specialized data sets. On the one hand, we want standardized public open data sets (think traces for voice recognition), so everybody is doing research from the same starting point. \n4. Existing databases need labelling &ndash; for example, if you\u00c6re looking at a traffic flow and you want to identify what protocols were in use, you need somebody to annotate it with the ground truth (what protocols actually were in use) or you\u00c6ll never know if your algorithms are producing useful answers. Metadata is critical, but it\u00c6s expensive t..."
 }
}