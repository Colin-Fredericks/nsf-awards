{
 "awd_id": "1050004",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Shared Gaze in Collaborative Referring",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2010-08-13",
 "awd_max_amd_letter_date": "2010-08-13",
 "awd_abstract_narration": "In situated dialogue, although artificial agents and their human partners are copresent in a shared environment, their knowledge and representation of the shared world are significantly different. When a shared basis of the environment is missing, communication between partners become more challenging. Language alone can be difficult and inefficient for partners to ground objects of interest. Motivated by previous empirical findings on eye gaze in joint attention, in collaboration, and in human language processing, our hypothesis is that eye gaze plays an important role in coordinating the collaborative referring process, especially between partners who have mismatched representations of their shared environment. Based on this hypothesis, the objective of this exploratory project is to examine the role of shared gaze in the collaborative referring process.\r\n\r\nThis EArly Grant for Exploratory Research aims to generate new findings on how shared gaze coordinates the collaborative referring behaviors between partners with mismatched representation of the shared environment. These findings will provide insight to computational approaches and systems that combine gaze modeling with the collaborative discourse to ground references. The collected data will support many in-depth studies on language processing in situated dialogue.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joyce",
   "pi_last_name": "Chai",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "Joyce Y Chai",
   "pi_email_addr": "chaijy@umich.edu",
   "nsf_id": "000477137",
   "pi_start_date": "2010-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan State University",
  "inst_street_address": "426 AUDITORIUM RD RM 2",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "5173555040",
  "inst_zip_code": "488242600",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MICHIGAN STATE UNIVERSITY",
  "org_prnt_uei_num": "VJKZC4D1JN36",
  "org_uei_num": "R28EKN92ZTZ9"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan State University",
  "perf_str_addr": "426 AUDITORIUM RD RM 2",
  "perf_city_name": "EAST LANSING",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488242600",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In situated interaction, humans and agents often have mismatched capabilities in perceiving the shared environment. When the shared perceptual basis is missing, communication between humans and agents becomes difficult. &nbsp;In such situation, language alone will be insufficient in grounding references to the environment. Other non-verbal modalities will play an important role.&nbsp; To address this issue, this project investigated the role of shared gaze in the collaborative process of referring in situated dialogue, especially between partners with mismatched perceptual capabilities.</p>\n<p>&nbsp;&nbsp;&nbsp; We first developed a system that relies on actual processing errors from computer vision algorithms to simulate lowered visual perceptual capability of a human. On one computer screen, a director (who is assumed to have normal human perception) can see the true image of the original scene. On the other computer screen, a matcher (who is assumed to have lowered perceptual capability) is only able to see an impoverished image (processed by computer vision algorithms) of the same original scene. Given this setup, the director and the matcher collaborate with each other to complete an object naming task: the director needs to communicate with the matcher the secret names of some objects so that the matcher can correctly identify which object has which name. &nbsp;Depending on the experimental conditions, either the director&rsquo;s eye gaze or the matcher&rsquo;s eye gaze is made available to his partner during interaction. We designed a set of experiments to investigate the effects of two factors (mismatched capabilities and shared gaze) on the task performance as well as the interactions between the two factors. A 2 by 2<sup> </sup>factorial experiment design was applied.</p>\n<p>&nbsp;&nbsp;&nbsp; Our experimental results have shown that when the shared perceptual basis is missing, the average time of accomplishing the naming task is significantly longer. This implies it is more difficult for partners with mismatched capabilities to reach a common ground. The effect of the director&rsquo;s gaze is significant. When the director&rsquo;s gaze was shared to the matcher during interaction, their collaboration became significantly more efficient. The interaction effect between the two factors is also significant. The shared gaze is more helpful under mismatched views compared to matched views. The effect of the matcher&rsquo;s gaze is marginally significant. One observation is that the matchers had a tendency of exploring the interface. Therefore their eye gaze may not be directly linked to the task at hand. These findings indicate that, tracking human eye gaze can be particularly beneficial in mediating shared perceptual basis in situated interaction. It provides important cues for the agent to recognize its own limitation in perception (e.g., recognition errors and segmentation errors) and facilitate effective collaboration between humans and agents. These findings have important implications in developing artificial agents that can interact with humans in the real world.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2012<br>\n\t\t\t\t\tModified by: Joyce&nbsp;Y&nbsp;Chai</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn situated interaction, humans and agents often have mismatched capabilities in perceiving the shared environment. When the shared perceptual basis is missing, communication between humans and agents becomes difficult.  In such situation, language alone will be insufficient in grounding references to the environment. Other non-verbal modalities will play an important role.  To address this issue, this project investigated the role of shared gaze in the collaborative process of referring in situated dialogue, especially between partners with mismatched perceptual capabilities.\n\n    We first developed a system that relies on actual processing errors from computer vision algorithms to simulate lowered visual perceptual capability of a human. On one computer screen, a director (who is assumed to have normal human perception) can see the true image of the original scene. On the other computer screen, a matcher (who is assumed to have lowered perceptual capability) is only able to see an impoverished image (processed by computer vision algorithms) of the same original scene. Given this setup, the director and the matcher collaborate with each other to complete an object naming task: the director needs to communicate with the matcher the secret names of some objects so that the matcher can correctly identify which object has which name.  Depending on the experimental conditions, either the director\u00c6s eye gaze or the matcher\u00c6s eye gaze is made available to his partner during interaction. We designed a set of experiments to investigate the effects of two factors (mismatched capabilities and shared gaze) on the task performance as well as the interactions between the two factors. A 2 by 2 factorial experiment design was applied.\n\n    Our experimental results have shown that when the shared perceptual basis is missing, the average time of accomplishing the naming task is significantly longer. This implies it is more difficult for partners with mismatched capabilities to reach a common ground. The effect of the director\u00c6s gaze is significant. When the director\u00c6s gaze was shared to the matcher during interaction, their collaboration became significantly more efficient. The interaction effect between the two factors is also significant. The shared gaze is more helpful under mismatched views compared to matched views. The effect of the matcher\u00c6s gaze is marginally significant. One observation is that the matchers had a tendency of exploring the interface. Therefore their eye gaze may not be directly linked to the task at hand. These findings indicate that, tracking human eye gaze can be particularly beneficial in mediating shared perceptual basis in situated interaction. It provides important cues for the agent to recognize its own limitation in perception (e.g., recognition errors and segmentation errors) and facilitate effective collaboration between humans and agents. These findings have important implications in developing artificial agents that can interact with humans in the real world.\n\n\t\t\t\t\tLast Modified: 11/30/2012\n\n\t\t\t\t\tSubmitted by: Joyce Y Chai"
 }
}