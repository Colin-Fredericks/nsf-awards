{
 "awd_id": "0953667",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Career: An Adaptive Compiler for Multi-core Environments",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-03-01",
 "awd_exp_date": "2015-02-28",
 "tot_intn_awd_amt": 416709.0,
 "awd_amount": 484909.0,
 "awd_min_amd_letter_date": "2010-03-05",
 "awd_max_amd_letter_date": "2014-07-16",
 "awd_abstract_narration": "Compilers are a critical component between the software developer and the computer. They translate application written by software developers into machine code that is processed by the computer. An important task of a compiler is to optimize applications so that they run efficiently.  Traditional methods to develop optimizing compilers are ad-hoc, labor-intensive, and ineffective.  As a consequence, optimizing compilers for a new processor often produces code that achieves only a fraction of the machine?s available performance. This is especially true for today's multi-core architectures, which are parallel processors on a single chip.  This research will involve investigating techniques from the artificial intelligence community that will allow a compiler to automatically adapt and tune to new architectures.  In effect, this research will replace hand-tuning with self-tuning compilers that adapt software automatically to match the performance characteristics of each target architecture.\r\n\r\nIn this project, the PI proposes to explore the viability of developing adaptive compilers for multi-core environments (ACME) to allow application portability while still achieving high performance.  The PI will create a statistical auto-tuning framework to support the probabilistic representation of the following features: the benefit analysis of optimizations, the identification and prediction of the appropriate runtime environment for different optimizations, and the generation of executables that efficiently combine several optimized code versions. He will invent components to measure accurately the characteristics of applications and targeted computing systems. The PI hopes to discover techniques to replace ?traditional? optimization benefit analysis with powerful machine learning models. These models will address the broad spectrum of parallel applications and multi-core environments, and they will be able to analyze and predict benefit under different dynamic contexts.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Cavazos",
   "pi_mid_init": "",
   "pi_sufx_name": "Dr",
   "pi_full_name": "John Cavazos",
   "pi_email_addr": "cavazos@cis.udel.edu",
   "nsf_id": "000500366",
   "pi_start_date": "2010-03-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Delaware",
  "inst_street_address": "550 S COLLEGE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "DE",
  "inst_state_name": "Delaware",
  "inst_phone_num": "3028312136",
  "inst_zip_code": "197131324",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DE00",
  "org_lgl_bus_name": "UNIVERSITY OF DELAWARE",
  "org_prnt_uei_num": "",
  "org_uei_num": "T72NHKM259N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Delaware",
  "perf_str_addr": "550 S COLLEGE AVE",
  "perf_city_name": "NEWARK",
  "perf_st_code": "DE",
  "perf_st_name": "Delaware",
  "perf_zip_code": "197131324",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DE00",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "729800",
   "pgm_ele_name": "International Research Collab"
  },
  {
   "pgm_ele_code": "732900",
   "pgm_ele_name": "COMPILERS"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "5918",
   "pgm_ref_txt": "FRANCE"
  },
  {
   "pgm_ref_code": "5980",
   "pgm_ref_txt": "WESTERN EUROPE PROGRAM"
  },
  {
   "pgm_ref_code": "7329",
   "pgm_ref_txt": "COMPILERS"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9151",
   "pgm_ref_txt": "EPSCOR OUTREACH"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 262871.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 38700.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 75659.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 91679.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research done for this grant involved the application of machine learning to hard compiler problems. &nbsp;</p>\n<p>PART 1: The first part of this project involved using machine learning to drive the \"right\" set of optimization given the characteristics of the code being optimized, and we looked at the following three problems in this area.</p>\n<p dir=\"ltr\"><strong>1. Phase Ordering</strong></p>\n<p dir=\"ltr\"><span>Phase ordering is the process of experimenting with different orders of compiler optimizations. This is a long-standing compiler problem that has been difficult to solve because compiler optimizations can impact one another. &nbsp;To solve this problem, we used neural networks to predict the best optimization to apply next to code being optimized based on the current state of the optimized code. This solution gave us an elegant way to predict the best optimization sequence for any piece of code being optimized. &nbsp;We achieved significant improvements over a well-tuned compiler by using source code characteristics and the neural networks to perform compiler phase ordering. &nbsp;We included a figure depicting our solution titled \"Phase Ordering\".</span></p>\n<p dir=\"ltr\"><strong>2. Optimization Selection</strong></p>\n<p dir=\"ltr\"><span>In this compiler problem, the machine learning algorithm predicts the set of compiler optimizations that should be applied to a particular code being compiled. &nbsp; In this problem, we have a fixed ordering of optimizations, and we simply want to choose the optimizations that should be turned on and off. &nbsp;We experimented with several problems including several real-world applications from a large financial institution. &nbsp;We used genetic algorithms (GAs) to prune out optimizations that would not improve the performance of code being compiled. &nbsp;Using GAs allowed us to quickly find the right set of optimizations for each program being compiled. &nbsp;We were able to achieve an improvement over already optimized applications. &nbsp;See the image titled \"Optimization Selection\" &nbsp;for more details. </span></p>\n<p><strong>3. Method Inlining</strong></p>\n<p dir=\"ltr\"><span>We also applied machine learning the problem of tuning the inliner in a compiler. &nbsp;An inliner is used to suggest which callee functions should be copied into the caller functions instead of calling these functions. &nbsp;Copying the body of a callee function can decrease the overhead of function calling, but too much inlining can significantly increase the size of application. &nbsp;We used features of the code being compiled to measure the suitability of the code to be inlined in each specific instance. &nbsp;We were able to achieve significant speedups over a well-tuned method inliner. The details of the training phase is shown in the figure attached titled \"Method Inlining\"</span></p>\n<p dir=\"ltr\">PART 2: The second part of this grant involved experimenting with a new prediction model and two different ways of characterizating code. &nbsp;Solving these problems are very important in the application of machine learning to compilers.</p>\n<p dir=\"ltr\"><strong>1.&nbsp;Tournament predictor&nbsp;</strong></p>\n<p dir=\"ltr\">First, we invented a new modeling technique, we called the tournament predictor. In our tournament predictor, we trained a model to predict which combination of optimizations from a pair of combinations of optimizations would perform best. We compared this new modeling technique to two previous state-of-the-art techniques, a sequence predictor and a speedup predictor. &nbsp;Our tournament prediction modeling technique outperformed both these previously state-of-the-art predictors in our evaluation by achieving 70% of the best possible achievable speedup in our selected optimization space. The details of the model is presented in the image titled \"Tournament Predictor\".</p>\n<p dir=\"ltr\"><strong>2. Graph-based Program Cha...",
  "por_txt_cntn": "\nThe research done for this grant involved the application of machine learning to hard compiler problems.  \n\nPART 1: The first part of this project involved using machine learning to drive the \"right\" set of optimization given the characteristics of the code being optimized, and we looked at the following three problems in this area.\n1. Phase Ordering\nPhase ordering is the process of experimenting with different orders of compiler optimizations. This is a long-standing compiler problem that has been difficult to solve because compiler optimizations can impact one another.  To solve this problem, we used neural networks to predict the best optimization to apply next to code being optimized based on the current state of the optimized code. This solution gave us an elegant way to predict the best optimization sequence for any piece of code being optimized.  We achieved significant improvements over a well-tuned compiler by using source code characteristics and the neural networks to perform compiler phase ordering.  We included a figure depicting our solution titled \"Phase Ordering\".\n2. Optimization Selection\nIn this compiler problem, the machine learning algorithm predicts the set of compiler optimizations that should be applied to a particular code being compiled.   In this problem, we have a fixed ordering of optimizations, and we simply want to choose the optimizations that should be turned on and off.  We experimented with several problems including several real-world applications from a large financial institution.  We used genetic algorithms (GAs) to prune out optimizations that would not improve the performance of code being compiled.  Using GAs allowed us to quickly find the right set of optimizations for each program being compiled.  We were able to achieve an improvement over already optimized applications.  See the image titled \"Optimization Selection\"  for more details. \n\n3. Method Inlining\nWe also applied machine learning the problem of tuning the inliner in a compiler.  An inliner is used to suggest which callee functions should be copied into the caller functions instead of calling these functions.  Copying the body of a callee function can decrease the overhead of function calling, but too much inlining can significantly increase the size of application.  We used features of the code being compiled to measure the suitability of the code to be inlined in each specific instance.  We were able to achieve significant speedups over a well-tuned method inliner. The details of the training phase is shown in the figure attached titled \"Method Inlining\"\nPART 2: The second part of this grant involved experimenting with a new prediction model and two different ways of characterizating code.  Solving these problems are very important in the application of machine learning to compilers.\n1. Tournament predictor \nFirst, we invented a new modeling technique, we called the tournament predictor. In our tournament predictor, we trained a model to predict which combination of optimizations from a pair of combinations of optimizations would perform best. We compared this new modeling technique to two previous state-of-the-art techniques, a sequence predictor and a speedup predictor.  Our tournament prediction modeling technique outperformed both these previously state-of-the-art predictors in our evaluation by achieving 70% of the best possible achievable speedup in our selected optimization space. The details of the model is presented in the image titled \"Tournament Predictor\".\n2. Graph-based Program Characterization Technique\n\n\n\n\nWe introduced a novel static program characterization technique, we called graph-based characterization, that improves the predictive power of machine learning models while retaining practical applicability.  We used control flow graphs (CFG) to characterize programs and then fed as input these graph characterizations into algorithms called graph kernels.  Graph kernels are machine learning algorithms that..."
 }
}