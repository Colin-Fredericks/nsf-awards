{
 "awd_id": "1018055",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Small: Enabling and Exploring Natural Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 499541.0,
 "awd_amount": 499541.0,
 "awd_min_amd_letter_date": "2010-08-04",
 "awd_max_amd_letter_date": "2010-08-04",
 "awd_abstract_narration": "Advances in technology are making it feasible to explore novel approaches to human-computer interaction in a wide variety of devices and settings, in an effort to achieve interaction that feels more natural.  While valuable from both a user and commercial perspective, use of new technology is not well understood from a more principled system design or cognitive science perspective.  Establishing even the basic elements of a set of design principles would both produce better designs and increase our confidence in using the technology in high-risk/high-reward domains, such as first responder planning and control.  The PI's long-term goal is to develop a set of principles specifying how to design systems that both enable natural human-computer interaction and are informed by an understanding of human factors.  Natural interaction refers to the cognitively transparent, effortless multimodal communication that can happen between people; this work aims to make that possible in human-computer interaction.  Designs informed by human factors take into account an understanding of human capabilities (e.g., attention, use of multiple information channels, etc.), so that the final system is a good impedance match to human information processing.  This research will involve building systems designed in this spirit and articulating principles for their design that, in turn, will facilitate future designs by making explicit both the task conditions under which one or another modality is appropriate (e.g., when to draw, when to talk), and the ways in which multiple modalities can effectively be used simultaneously in human-computer communication.  The project is set in the context of a tabletop-based system that assists with planning and coordination in the command center of an urban search and rescue (USAR) operation.  The work will proceed by leveraging and combining the team's experience in building novel interaction technologies and in human factors.  They will extend the current version of the PI's tabletop system, which permits basic pen-based interaction, so as to give it the ability to handle the kinds of sketching, freehand gestures and speech used in real-world USAR work, thereby providing a far more natural style of interaction.  The additional interaction modalities will make the system more powerful, while the real-world, time-pressured character of the task offers a good platform for studying the human factors aspects.  This will allow the team to understand how, when and why various modalities are useful, providing the data from which system design principles can be articulated.  To help ensure breadth of applicability of project outcomes, the PI will explore the same issues in a second domain, software design with UML diagrams.\r\n\r\nIntellectual Merit: This research will provide insight into a model of multimodal interaction by producing empirical data about modality selection, and by articulating a widely useful set of principles for interface design that make explicit the conditions for both modality selection and cognitively effective modality combination.  Such principles offer the possibility of transformative change to multimodal interface design, changing it from the current largely ad hoc practice to a design process guided by testable principles.  Pursuing the research in two task domains will help to ensure a useful degree of generality to the principles derived.  The work will provide benefit to society to the extent it can improve the effectiveness of first responder teams.  The ability to handle non-traditional interaction modalities (e.g., gesture) will ultimately make computer interaction more accessible to physically disadvantaged users.   The PI will take care, to the extent possible, to use and build upon open source tools, so that he can make available to the community all of the research software produced during the course of the project, thereby adding to the supply of next-generation research and education platforms.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Randall",
   "pi_last_name": "Davis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Randall Davis",
   "pi_email_addr": "davis@csail.mit.edu",
   "nsf_id": "000173701",
   "pi_start_date": "2010-08-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mary",
   "pi_last_name": "Cummings",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Mary L Cummings",
   "pi_email_addr": "cummings@gmu.edu",
   "nsf_id": "000346607",
   "pi_start_date": "2010-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 499541.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project aimed at moving beyond the keyboard and mouse typically used to interact with computers, and instead developed new ways for people and computers to interact, with the overall goal of making human-computer interaction as natural and effective as human-human interaction. We designed and enabled several kinds of <em>multi-modal</em> communication, i.e., communication that combines simultaneous gesture, drawing, and speech. We showed how these could be used effectively in engineering design and in education (e.g., explaining software behavior). We also designed and demonstrated several varieties of <em>symmetric multi-modal</em> communication, in which both the human user and the computer communicate by combining gesture, drawing and speech.</p>\n<p>Specific technical outcomes from the work include a real-time continuous gesture recognition system that distinguishes gestures defined by their path (e.g., a circular motion of the hand) vs those defined by hand pose (e.g., a thumbs-up pose) and that provided a single framework for dealing with both. This work also addressed the question of when the system should respond, distinguishing continuous gestures that should be responded to continuously as the gesture is being made (e.g., a circular gesture meaning &ldquo;scroll the document down&rdquo;) vs discrete gestures that should be responded to only when the gesture is complete (e.g., a back and forth wave that means close this document).</p>\n<p>The work produced a study of the relative efficiency of hand drawing symbols on a map vs. selecting them from a menu, in order to understand the appropriate design for a map annotation system for use in helping to plan deployment of first responders in a disaster.</p>\n<p>The work developed two new algorithms for analyzing and understanding human action and human behavior, working from videos of people in action. One algorithm learns the structure of human action by creating successively more general descriptions of that behavior, enabling it to, for example, recognizing the aircraft handling gestures used by the US Navy aboard aircraft carriers. This demonstrates the ability to learn complex gestures from examples alone, an effective tool for building new systems. The second algorithm focused on efficient ways to combine multimodal information to infer interesting elements of human behavior. As one example it detected and combined elements of facial expression, body motion, and tone of voice to effectively predict people&rsquo;s impression of a speaker&rsquo;s personality from a brief video of the speaker. This has clear applications in building computer systems that might be able to detect, e.g., a user&rsquo;s frustration and respond in a much more useful fashion.</p>\n<p>By advancing the art of multi-modal interaction generally and gesture understanding in particular, the work offers a means of diversifying the population capable of interacting with computers, potentially including those with disabilities that limit their use of traditional mouse and keyboard. The work contributed to diversifying the research community by providing support for three women (one faculty member and two graduate students). The work contributed to the research community as a whole by making the software developed and database accumulated available for open use.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/02/2015<br>\n\t\t\t\t\tModified by: Randall&nbsp;Davis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project aimed at moving beyond the keyboard and mouse typically used to interact with computers, and instead developed new ways for people and computers to interact, with the overall goal of making human-computer interaction as natural and effective as human-human interaction. We designed and enabled several kinds of multi-modal communication, i.e., communication that combines simultaneous gesture, drawing, and speech. We showed how these could be used effectively in engineering design and in education (e.g., explaining software behavior). We also designed and demonstrated several varieties of symmetric multi-modal communication, in which both the human user and the computer communicate by combining gesture, drawing and speech.\n\nSpecific technical outcomes from the work include a real-time continuous gesture recognition system that distinguishes gestures defined by their path (e.g., a circular motion of the hand) vs those defined by hand pose (e.g., a thumbs-up pose) and that provided a single framework for dealing with both. This work also addressed the question of when the system should respond, distinguishing continuous gestures that should be responded to continuously as the gesture is being made (e.g., a circular gesture meaning \"scroll the document down\") vs discrete gestures that should be responded to only when the gesture is complete (e.g., a back and forth wave that means close this document).\n\nThe work produced a study of the relative efficiency of hand drawing symbols on a map vs. selecting them from a menu, in order to understand the appropriate design for a map annotation system for use in helping to plan deployment of first responders in a disaster.\n\nThe work developed two new algorithms for analyzing and understanding human action and human behavior, working from videos of people in action. One algorithm learns the structure of human action by creating successively more general descriptions of that behavior, enabling it to, for example, recognizing the aircraft handling gestures used by the US Navy aboard aircraft carriers. This demonstrates the ability to learn complex gestures from examples alone, an effective tool for building new systems. The second algorithm focused on efficient ways to combine multimodal information to infer interesting elements of human behavior. As one example it detected and combined elements of facial expression, body motion, and tone of voice to effectively predict people\u00c6s impression of a speaker\u00c6s personality from a brief video of the speaker. This has clear applications in building computer systems that might be able to detect, e.g., a user\u00c6s frustration and respond in a much more useful fashion.\n\nBy advancing the art of multi-modal interaction generally and gesture understanding in particular, the work offers a means of diversifying the population capable of interacting with computers, potentially including those with disabilities that limit their use of traditional mouse and keyboard. The work contributed to diversifying the research community by providing support for three women (one faculty member and two graduate students). The work contributed to the research community as a whole by making the software developed and database accumulated available for open use.\n\n \n\n\t\t\t\t\tLast Modified: 11/02/2015\n\n\t\t\t\t\tSubmitted by: Randall Davis"
 }
}