{
 "awd_id": "1029430",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  Computational Behavioral Science: Modeling, Analysis, and Visualization of Social and Communicative Behavior",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 749826.0,
 "awd_amount": 749826.0,
 "awd_min_amd_letter_date": "2010-08-19",
 "awd_max_amd_letter_date": "2012-09-11",
 "awd_abstract_narration": "Computational Behavioral Science: Modeling, Analysis, and Visualization of Social and \r\nCommunicative Behavior\r\nLead PI/Institution: James M. Rehg, Georgia Institute of Technology\r\nThis Expedition will develop novel computational methods for measuring and analyzing the behavior of children and adults during face-to-face social interactions. Social behavior plays a key role in the acquisition of social and communicative skills during childhood. Children with developmental disorders, such as autism, face great challenges in acquiring these skills, resulting in substantial lifetime risks. Current best practices for evaluating behavior and assessing risk are based on direct observation by highly-trained specialists, and cannot be easily scaled to the large number of individuals who need evaluation and treatment. For example, autism affects 1 in 110 children in the U.S., with a lifetime cost of care of $3.2 million per person. By developing methods to automatically collect fine-grained behavioral data, this project will enable large-scale objective screening and more effective delivery and assessment of therapy. Going beyond the treatment of disorders, this technology will make it possible to automatically measure behavior over long periods of time for large numbers of individuals in a wide range of settings. Many disciplines, such as education, advertising, and customer relations, could benefit from a quantitative, data-drive approach to behavioral analysis. \r\nHuman behavior is inherently multi-modal, and individuals use eye gaze, hand gestures, facial expressions, body posture, and tone of voice along with speech to convey engagement and regulate social interactions.  This project will develop multiple sensing technologies, including vision, speech, and wearable sensors, to obtain a comprehensive, integrated portrait of expressed behavior. Cameras and microphones provide an inexpensive, noninvasive means for measuring eye, face, and body movements along with speech and nonspeech utterances. Wearable sensors can measure physiological variables such as heart-rate and skin conductivity, which contain important cues about levels of internal stress and arousal that are linked to expressed behavior. This project is developing unique capabilities for synchronizing multiple sensor streams, correlating these streams to measure behavioral variables such as affect and attention, and modeling extended interactions between two or more individuals. In addition, novel behavior visualization methods are being developed to enable real-time decision support for interventions and the effective use of repositories of behavioral data. Methods are also under development for reflecting the capture and analysis process to users of the technology.\r\nThe long-term goal of this project is the creation of a new scientific discipline of computational behavioral science, which draws equally from computer science and psychology in order to transform the study of human behavior. A comprehensive education plan supports this goal through the creation of an interdisciplinary summer school for young researchers and the development of new courses in computational behavior. Outreach activities include significant and on-going collaborations with major autism research centers in Atlanta, Boston, Pittsburgh, Urbana-Champaign, and Los Angeles.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stan",
   "pi_last_name": "Sclaroff",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stan Sclaroff",
   "pi_email_addr": "sclaroff@bu.edu",
   "nsf_id": "000200256",
   "pi_start_date": "2010-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "1 SILBER WAY",
  "perf_city_name": "BOSTON",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151703",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "7723",
   "pgm_ref_txt": "EXPERIMENTAL EXPEDITIONS"
  },
  {
   "pgm_ref_code": "7969",
   "pgm_ref_txt": "FY 2010 Funding for PTR"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 436073.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 313753.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research effort addressed foundational issues in video-based analysis of human communicative behavior, tracking the body motion during communicative behaviors, for instance hand gestures, head and facial expressions, posture of the body, movements of the arms, eye gaze.&nbsp; This effort also produced new methods for data mining and retrieval of communicative behavior patterns, forecasting behaviors, and methods for combining visual observations with audio observations.&nbsp; New methods were developed that can automatically learn and recognize specific human gestures, producing state-of-the-art accuracy on widely-used benchmarks for gesture recognition performance.&nbsp; In addition, machine learning algorithms were developed that can better personalize gesture recognition by computer. A number of improved methods for deep neural network learning of human motion and activity in video were formulated, implemented, and deployed as publicly available software.&nbsp; Computer vision methods were developed and tested for analysis and modeling of motion trajectories of children, including their interaction with parents and exploratory behaviors. New methods were produced for discovery of correlated events in parallel video streams, where there may time delays between events, and demonstrated in the analysis of interactions between children and adults during structured play. The results of this project can also enable faster and more reliable tracking, classification, and forecasting of human motion in other settings, including in video retrieval based on human activities and behaviors, sign language tracking and recognition, and video-based surveillance.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/13/2017<br>\n\t\t\t\t\tModified by: Stan&nbsp;Sclaroff</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research effort addressed foundational issues in video-based analysis of human communicative behavior, tracking the body motion during communicative behaviors, for instance hand gestures, head and facial expressions, posture of the body, movements of the arms, eye gaze.  This effort also produced new methods for data mining and retrieval of communicative behavior patterns, forecasting behaviors, and methods for combining visual observations with audio observations.  New methods were developed that can automatically learn and recognize specific human gestures, producing state-of-the-art accuracy on widely-used benchmarks for gesture recognition performance.  In addition, machine learning algorithms were developed that can better personalize gesture recognition by computer. A number of improved methods for deep neural network learning of human motion and activity in video were formulated, implemented, and deployed as publicly available software.  Computer vision methods were developed and tested for analysis and modeling of motion trajectories of children, including their interaction with parents and exploratory behaviors. New methods were produced for discovery of correlated events in parallel video streams, where there may time delays between events, and demonstrated in the analysis of interactions between children and adults during structured play. The results of this project can also enable faster and more reliable tracking, classification, and forecasting of human motion in other settings, including in video retrieval based on human activities and behaviors, sign language tracking and recognition, and video-based surveillance.\n\n \n\n\t\t\t\t\tLast Modified: 10/13/2017\n\n\t\t\t\t\tSubmitted by: Stan Sclaroff"
 }
}