{
 "awd_id": "0964106",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium:  Intelligent and Efficient Data Movement for Multicore Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Hong Jiang",
 "awd_eff_date": "2010-06-01",
 "awd_exp_date": "2015-05-31",
 "tot_intn_awd_amt": 1080000.0,
 "awd_amount": 1080000.0,
 "awd_min_amd_letter_date": "2010-06-06",
 "awd_max_amd_letter_date": "2012-07-11",
 "awd_abstract_narration": "Multicore chips with hundreds of cores will likely be available soon. Current trends suggest that cores will be relatively simple, that on-chip memory will be partitioned into per-core caches, and that each cache will be relatively small. Furthermore, chips will continue to be pin-limited and therefore DRAM bandwidth won't scale with the number of cores.\r\n\r\nParallel software that is data intensive has the potential to run well on multicore processors due to the large total amount of on-chip cache, but requires effective use of the distributed on-chip caches. The dangers are that data used by many cores will be duplicated in many cores' caches, decreasing the total amount of distinct data that can be cached; and that some cores may incur DRAM loads trying to access more data than fits in their caches, while other cores have spare cache space.\r\n\r\nAs the number of cores per DRAM interface and the latencies between caches on a chip increase, it is important for software to manage the distributed caches well.  A collaborative approach that involves architects and systems researchers to extend the memory interface to provide control over data placement and enable on-chip efficient data movement provides a holistic solution.  Specifically, this work proposes data-movement control (DMC) interface, which includes support for cache-to-cache transfers, batching of several cache lines, sending a message to a core that is close to a cache that holds a particular data item, and data monitoring.  The proposal also includes techniques to route data over the  interconnect that will provide high performance for the DMC interface.\r\n\r\nIf successful, DMC will help data-intensive applications avoid memory bottlenecks and allow them to run well on future multicore processors.  A full-system simulator will allow other researchers to explore intelligent and efficient data management and enable research projects in MIT's graduate computer architecture class, educating students about the challenges of multicore computing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marinus",
   "pi_last_name": "Kaashoek",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Marinus F Kaashoek",
   "pi_email_addr": "kaashoek@lcs.mit.edu",
   "nsf_id": "000098539",
   "pi_start_date": "2010-06-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Srini",
   "pi_last_name": "Devadas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Srini Devadas",
   "pi_email_addr": "devadas@mit.edu",
   "nsf_id": "000451511",
   "pi_start_date": "2010-06-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Morris",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Robert T Morris",
   "pi_email_addr": "rtm@csail.mit.edu",
   "nsf_id": "000471357",
   "pi_start_date": "2010-06-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nickolai",
   "pi_last_name": "Zeldovich",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nickolai Zeldovich",
   "pi_email_addr": "nickolai@csail.mit.edu",
   "nsf_id": "000520788",
   "pi_start_date": "2010-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "794100",
   "pgm_ele_name": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ele_code": "794500",
   "pgm_ele_name": "DES AUTO FOR MICRO & NANO SYST"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 360000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 360000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 360000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern computers have many processors, called cores, that run in parallel. In order to fully utilize these processors, software has to be parallel, i.e., comprise of many tasks that can be run in parallel. &nbsp;Each core has associated fast memory called a cache that supplies data to the task running on that core. However, since cores are geographically distributed across the chip, so are the caches. &nbsp;Sometimes a task running on a core wishes to access data that is stored not on the cache right next to it, but in a remote cache. This access takes much longer than an access to the local cache. &nbsp;If too many of these remote cache accesses are required, the software will run slowly even though there are many procesors in the system. &nbsp;This project addresses the problem of ensuring that software is scalable, that is, it can exploit parallelism in the computer by managing the caches well.<br />One way to ensure that remote cache accesses are minimized, is to enable the task to move from one core to another, through a process of task migration. The migration of a task means that the instructions comprising the task are moved from one core to another. If there are relatively few instructions that define a task, but the task needs to access a vast quantity of data, task migration can be substantially more efficient than conventional data migration, which is what remote cache accesses require. &nbsp;First, energy consumption is lower because fewer bits of information are transferred inside the chip. Second, once the task is migrated, all the data that was initially remote now becomes local.<br />During the course of this project, we produced a detailed design and implementation of hardware-level instruction-granularity task migration in a 110-core chip multiprocessor called the Execution Migration Machine. Implemented in 45nm fabrication technology, the chip occupies 100 square millimeters, and comprises a 10 by 11 grid of processing cores. &nbsp;Our implementation provides end-to-end migration latency of 4 cycles between neighboring cores for a task, and 33 cycles between the farthest cores. &nbsp;Through detailed simulation of the implementation and testing of the prototype chip, we have demonstrated that task migration can reduce on-chip data movement by up to 14 times at a relatively small area cost of 23%.<br />For software to scale well on multicore processors it must be conflict free: that is, one core shouldn't read a cache line read by another core, because the core reading will be stalled until it receives the cache line. &nbsp;Such an access can be come scalability bottleneck if many cores are contending for the cache line. &nbsp;This project contributed a new tool, mtrace, to find scalability problems due to conflicting accesses. &nbsp;We used mtrace to find scalability problems in Linux, to design new operating subsystems that scale better, and to articulate the scalable commutativity rule, which makes it possible for developers to make software scalable by design.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/02/2015<br>\n\t\t\t\t\tModified by: M. Frans&nbsp;Kaashoek</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern computers have many processors, called cores, that run in parallel. In order to fully utilize these processors, software has to be parallel, i.e., comprise of many tasks that can be run in parallel.  Each core has associated fast memory called a cache that supplies data to the task running on that core. However, since cores are geographically distributed across the chip, so are the caches.  Sometimes a task running on a core wishes to access data that is stored not on the cache right next to it, but in a remote cache. This access takes much longer than an access to the local cache.  If too many of these remote cache accesses are required, the software will run slowly even though there are many procesors in the system.  This project addresses the problem of ensuring that software is scalable, that is, it can exploit parallelism in the computer by managing the caches well.\nOne way to ensure that remote cache accesses are minimized, is to enable the task to move from one core to another, through a process of task migration. The migration of a task means that the instructions comprising the task are moved from one core to another. If there are relatively few instructions that define a task, but the task needs to access a vast quantity of data, task migration can be substantially more efficient than conventional data migration, which is what remote cache accesses require.  First, energy consumption is lower because fewer bits of information are transferred inside the chip. Second, once the task is migrated, all the data that was initially remote now becomes local.\nDuring the course of this project, we produced a detailed design and implementation of hardware-level instruction-granularity task migration in a 110-core chip multiprocessor called the Execution Migration Machine. Implemented in 45nm fabrication technology, the chip occupies 100 square millimeters, and comprises a 10 by 11 grid of processing cores.  Our implementation provides end-to-end migration latency of 4 cycles between neighboring cores for a task, and 33 cycles between the farthest cores.  Through detailed simulation of the implementation and testing of the prototype chip, we have demonstrated that task migration can reduce on-chip data movement by up to 14 times at a relatively small area cost of 23%.\nFor software to scale well on multicore processors it must be conflict free: that is, one core shouldn't read a cache line read by another core, because the core reading will be stalled until it receives the cache line.  Such an access can be come scalability bottleneck if many cores are contending for the cache line.  This project contributed a new tool, mtrace, to find scalability problems due to conflicting accesses.  We used mtrace to find scalability problems in Linux, to design new operating subsystems that scale better, and to articulate the scalable commutativity rule, which makes it possible for developers to make software scalable by design.\n\n\t\t\t\t\tLast Modified: 07/02/2015\n\n\t\t\t\t\tSubmitted by: M. Frans Kaashoek"
 }
}