{
 "awd_id": "1016284",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Closing the Gap Between Matrix and Tensor Computation",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924488",
 "po_email": "jwang@nsf.gov",
 "po_sign_block_name": "Junping Wang",
 "awd_eff_date": "2010-08-15",
 "awd_exp_date": "2013-07-31",
 "tot_intn_awd_amt": 250506.0,
 "awd_amount": 250506.0,
 "awd_min_amd_letter_date": "2010-08-04",
 "awd_max_amd_letter_date": "2010-08-04",
 "awd_abstract_narration": "A strong case can be made that tensor computation is the ``next big thing'' in numerical analysis. High-dimensional modeling is becoming commonplace and it requires the manipulation and analysis of huge multidimensional arrays. The investigator and his colleagues will enrich the interplay between matrix computations and tensor computations by pursuing four basic directions of research. They will\r\n(1) develop  tensor approximation techniques based on matrices that have low Kronecker product rank, (2) implement a pair of basic tensor algebra subprograms, one that showcases a new contraction-level generalization of Strassen multiplication and one that demonstrates how to compute effectively  contractions between tensors that have symmetry, (3) analyze the data sparse representation of huge vectors through tensor networks, and (4) develop a unifying framework for SVD-like tensor decompositions through an embedding idea that involves symmetric tensors.\r\n  \r\n\r\nA table of data is 2-dimensional and many matrix computation techniques exist for extracting information from the numbers that appear in the rows and columns. A tensor can be thought of as a table whose entries are other tables. For example, a table having 10 rows and 8 columns has 80 ``cells''. If each of those cells is itself a 5-by-4 table, then the entire data set can be thought of as a 10-by-8-by-5-by-4 tensor. Data sets of this variety are increasingly prominent in engineering and the sciences because it is the natural way to structure the information associated with a model that depends upon many factors.\r\nThe research plan is to help build an infrastructure for the scientific community that makes tensor-based  computation as natural and easy as matrix-based computation. The successful problem-solving and problem-analysis tools provided by the matrix computation field will  be broadened and generalized. The outreach agenda includes the production of educational materials that will help ensure the development of a tensor-savvy scientific community. \r\nThese materials include an online, ten-lecture short course on tensor computation, participation in a Visiting Lecturer program that targets 4-year colleges, and the addition of a tensor computation chapter in the upcoming fourth edition of  the highly-cited textbook on matrix computations by Golub and Van Loan.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Van Loan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Charles Van Loan",
   "pi_email_addr": "cv@cs.cornell.edu",
   "nsf_id": "000329400",
   "pi_start_date": "2010-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "341 PINE TREE RD",
  "perf_city_name": "ITHACA",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "287800",
   "pgm_ele_name": "Special Projects - CCF"
  },
  {
   "pgm_ele_code": "745400",
   "pgm_ele_name": "MSPA-INTERDISCIPLINARY"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 250506.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A \"matrix\" in mathematics is a 2-dimensional table of numbers with rows and columns. A \"tensor\" is a higher dimensional analog of this idea. For example, a 3-dimensional tensor can be thought of as a \"Rubik's cube of data''. It has rows, columns, and layers. Tensors of dimensions 3 and 4 are very common in scientific computation and we spent a lot of time working with such data objects. But very high dimensional tensors also arise in various areas of science such as quantum chemistry.</p>\n<p>The gist of our activity under the auspices of this research contract was to extend key computational ideas associated with matrices to tensors.</p>\n<p>\"Blocking\" is central to the design of an efficient matrix method if it is to be run on a modern computer system. When a matrix computation is blocked, nearby \"chunks\" of data are kept together during the calculation and this reduces the amount of data transfer between the computer's memory and the computer's arithmetic units. We developed a framework for blocking tensor computations. In particular, we showed how to flatten a blocked tensor into a blocked matrix preserving the \"nearby\" feature of the data.</p>\n<p>Symmetry is another feature that permeates matrix computations. A matrix is symmetric if the value in row i and column j is the same as the value in row j and column i. A tensor can have many different types of symmetry and we showed how such structure can be exploited.</p>\n<p>The generalized singular value decomposition is a way to take apart a pair of matrices in such a way that common properties are exposed. We extended this idea by developing a \"higher order\" generalized singular value decomposition that simulateously reveals common features that are latent in a collection of matrices. The connection to tensors is based on the idea that if you \"slice\" a 3-d tensor into layers, each layer is a matrix. Thus, our higher order generalized singular value decomposition is a way of decomposing a 3d tensor.</p>\n<p>On the exposition side,&nbsp; we added three tensor-related&nbsp; sections to the recently published 4th edition of Matrix Computations, the most widely cited text in the field. It is our hope that this addition will make it easier for researchers and students to navigate the notationally-challenged tensor literature.</p>\n<p>The outreach component of the project was realized by giving general audience lectures on computational science&nbsp; at a number of colleges and universities.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/31/2013<br>\n\t\t\t\t\tModified by: Charles&nbsp;Van Loan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nA \"matrix\" in mathematics is a 2-dimensional table of numbers with rows and columns. A \"tensor\" is a higher dimensional analog of this idea. For example, a 3-dimensional tensor can be thought of as a \"Rubik's cube of data''. It has rows, columns, and layers. Tensors of dimensions 3 and 4 are very common in scientific computation and we spent a lot of time working with such data objects. But very high dimensional tensors also arise in various areas of science such as quantum chemistry.\n\nThe gist of our activity under the auspices of this research contract was to extend key computational ideas associated with matrices to tensors.\n\n\"Blocking\" is central to the design of an efficient matrix method if it is to be run on a modern computer system. When a matrix computation is blocked, nearby \"chunks\" of data are kept together during the calculation and this reduces the amount of data transfer between the computer's memory and the computer's arithmetic units. We developed a framework for blocking tensor computations. In particular, we showed how to flatten a blocked tensor into a blocked matrix preserving the \"nearby\" feature of the data.\n\nSymmetry is another feature that permeates matrix computations. A matrix is symmetric if the value in row i and column j is the same as the value in row j and column i. A tensor can have many different types of symmetry and we showed how such structure can be exploited.\n\nThe generalized singular value decomposition is a way to take apart a pair of matrices in such a way that common properties are exposed. We extended this idea by developing a \"higher order\" generalized singular value decomposition that simulateously reveals common features that are latent in a collection of matrices. The connection to tensors is based on the idea that if you \"slice\" a 3-d tensor into layers, each layer is a matrix. Thus, our higher order generalized singular value decomposition is a way of decomposing a 3d tensor.\n\nOn the exposition side,  we added three tensor-related  sections to the recently published 4th edition of Matrix Computations, the most widely cited text in the field. It is our hope that this addition will make it easier for researchers and students to navigate the notationally-challenged tensor literature.\n\nThe outreach component of the project was realized by giving general audience lectures on computational science  at a number of colleges and universities.\n\n \n\n\t\t\t\t\tLast Modified: 12/31/2013\n\n\t\t\t\t\tSubmitted by: Charles Van Loan"
 }
}