{
 "awd_id": "1017297",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: A Unified Many-Core Architecture for Enabling Speculative Multithreading and Transactional Memory",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2010-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 351831.0,
 "awd_amount": 351831.0,
 "awd_min_amd_letter_date": "2010-08-02",
 "awd_max_amd_letter_date": "2010-08-02",
 "awd_abstract_narration": "As multi-core and many-core processors are consensually becoming the de facto standard for all types of computing platforms, two techniques, Speculative Multithreading (SpecMT) and Transactional Memory (TM), have been intensively investigated on such platforms to enhance single-thread performance and to simplify the parallel programming model. These two computing models, proposed separately, share many common features in their underlying implementations. In this research, we give a holistic view and investigate a unified many-core architecture to support both technologies under one implementation. We first map SpecMT onto a hardware-based TM architecture and develop enabling techniques to showcase the performance potential while maintaining the benefit of a TM programming model. Each thread spawned speculatively or non-speculatively is transactified into a transaction. Compilers are used to determine when and where to initiate SpecMT with hardware to dynamically shepherd the decisions and throttle the extent of SpecMT. These transactions, executing different code regions, can be launched out of the sequential program order but must be committed in the original order with hardware support. This research focuses on (1) architectural support for enabling both SpecMT and TM, (2) compiler's support for SpecMT and thread transactification, (3) quantifying the benefits of different types of transactions, and (4) economical and feasible mechanisms to support SpecMT on heterogeneous many-core platforms such as the incoming integrated CPU-GPU systems. The success of such a unified many-core architecture will provide a foundation for delivering high performance for single-thread applications while improving the productivity for software developers substantially.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hsien-Hsin",
   "pi_last_name": "Lee",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Hsien-Hsin S Lee",
   "pi_email_addr": "leehs@gatech.edu",
   "nsf_id": "000319030",
   "pi_start_date": "2010-08-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Corporation",
  "perf_str_addr": "926 DALNEY ST NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303186395",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 351831.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"text-decoration: underline;\"><strong>Activity overview</strong></span></p>\n<p>The research activity of the past year includes the following two msjot subjects</p>\n<ul>\n<li>Enabling efficient conflict resolution with Eager Conflict Serialization (EagerCS)</li>\n<li>3D-stacked DRAM cache analysis for bandwidth-aware management</li>\n</ul>\n<p><strong><span style=\"text-decoration: underline;\">Resarch findings</span></strong></p>\n<p><strong>Enabling efficient conflict resolution with EagerCS: </strong>To evaluate the benefits we can get from our proposed technique, we analyzed the impact of our technique on abort number and execution time. In our simulation, we used McSim simulator with our designed HTM extension and STAMP benchmark suites with standard configurations. McSim is a manycore simulation infrastructure where a timing simulator and a functional simulator are decoupled. We further extend it with HTM support with multiple configurations.</p>\n<p>We first analyzed the number of aborts with EagarCS. From the result shown, we can see that as an eager HTM system, EagerCS outperforms baseline eager HTM in all benchmarks. The reduction in abort number can be as high as 80% in vacation. Although lazy HTM is showing fewer aborts in some benchmarks, EagerCS can still approach the better one very closely. We then analyzed the execution time comparison between eager HTM, lazy HTM, and EagerCS. We also showed that EagerCS shows better performance than baseline eager HTM in most benchmarks except kmeans. In kmeans, EagerCS takes little longer execution time of 7%. It is because kmeans has high contention rate and the lazy detection for &ldquo;hard&rdquo; cycle brings extra overhead. Lazy HTM as well shows better performance in benchmark genome and intruder. However, our proposed technique can always approach the better one with small margin. We also performed analysis for impact of speculative commit. Also observed was that total number of speculative commits is relatively small and most of them failed the speculation. This is in&nbsp; accord with our intuition that conflict order usually matches commit order</p>\n<p>These results in our experiments indicated a great potential of reducing aborts and improving performance with our lightweight EagerCS technique. By decoupling detection for different types of cycles, EagerCS makes order model easier and can be implemented with limited hardware and performance overhead. As an eager HTM system, EagerCS can achieve better abort number and performance than baseline eager HTM systems. The abort number reduction can be as high as 80% while the performance improvement can reach as high as 30% depending on the conflict behavior of benchmark programs. When lazy system can achieve better performance, EagerCS also approaches the&nbsp;better method closely.&nbsp;</p>\n<p><strong>3D-stacked DRAM cache analysis for bandwidth-aware management:</strong> In our experiments, Multi2Sim, a cycle-level x86 simulator is used for performance evaluation. In order to support 3D-stacked DRAM cache, we port DRAMSim2 and integrate it with Multi2Sim. In the simulations, we model an 8 core processor with 2 levels SRAM caches and a L3 DRAM cache. The 3D-stacked DRAM cache has 4 channels with 128 bits bus and 128 MB size. In the experiments, we SPEC CPU2006 benchmark suites and sample 300M instructions using Simpoint. Due to some applications' low sensitivity to memory hierarchy optimizations, we focus on memory intensive benchmarks. In order to reduce simulation time, we work on the same application across all 8 cores.&nbsp;In our work, we first analyzed the bandwidth pressure of DRAM cache. In the experiment, we compared number of DRAM cache Read/Write and actual DRAM cache requests. As shown in Figure 6, the actual number of DRAM cache requests is significantly higher than number of DRAM cache read/write. From the result, we can see that although 3D-stacked DRAM brings higher bandwidth, it also brings higher bandwidth demand.&nbsp; To achieve better DRAM cache performance, we should also consider bandwidth issue in our DRAM cache management policy. Secondly, we analyzed the impact of unnecessary DRAM cache accesses. We performed experiment to measure the impact of reducing requests generated by cache misses. Figure 7 is the reduction rate of DRAM cache queuing delay normalized to baseline system if avoiding all accesses of cache misses. The result shows huge impact on DRAM cache queuing delay. It implies the great performance potential of doing so. To achieve this target,&nbsp;we not only need a cache hit/miss predictor, but also a solution to dirty block issue.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/22/2017<br>\n\t\t\t\t\tModified by: Hsien-Hsin&nbsp;S&nbsp;Lee</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nActivity overview\n\nThe research activity of the past year includes the following two msjot subjects\n\nEnabling efficient conflict resolution with Eager Conflict Serialization (EagerCS)\n3D-stacked DRAM cache analysis for bandwidth-aware management\n\n\nResarch findings\n\nEnabling efficient conflict resolution with EagerCS: To evaluate the benefits we can get from our proposed technique, we analyzed the impact of our technique on abort number and execution time. In our simulation, we used McSim simulator with our designed HTM extension and STAMP benchmark suites with standard configurations. McSim is a manycore simulation infrastructure where a timing simulator and a functional simulator are decoupled. We further extend it with HTM support with multiple configurations.\n\nWe first analyzed the number of aborts with EagarCS. From the result shown, we can see that as an eager HTM system, EagerCS outperforms baseline eager HTM in all benchmarks. The reduction in abort number can be as high as 80% in vacation. Although lazy HTM is showing fewer aborts in some benchmarks, EagerCS can still approach the better one very closely. We then analyzed the execution time comparison between eager HTM, lazy HTM, and EagerCS. We also showed that EagerCS shows better performance than baseline eager HTM in most benchmarks except kmeans. In kmeans, EagerCS takes little longer execution time of 7%. It is because kmeans has high contention rate and the lazy detection for \"hard\" cycle brings extra overhead. Lazy HTM as well shows better performance in benchmark genome and intruder. However, our proposed technique can always approach the better one with small margin. We also performed analysis for impact of speculative commit. Also observed was that total number of speculative commits is relatively small and most of them failed the speculation. This is in  accord with our intuition that conflict order usually matches commit order\n\nThese results in our experiments indicated a great potential of reducing aborts and improving performance with our lightweight EagerCS technique. By decoupling detection for different types of cycles, EagerCS makes order model easier and can be implemented with limited hardware and performance overhead. As an eager HTM system, EagerCS can achieve better abort number and performance than baseline eager HTM systems. The abort number reduction can be as high as 80% while the performance improvement can reach as high as 30% depending on the conflict behavior of benchmark programs. When lazy system can achieve better performance, EagerCS also approaches the better method closely. \n\n3D-stacked DRAM cache analysis for bandwidth-aware management: In our experiments, Multi2Sim, a cycle-level x86 simulator is used for performance evaluation. In order to support 3D-stacked DRAM cache, we port DRAMSim2 and integrate it with Multi2Sim. In the simulations, we model an 8 core processor with 2 levels SRAM caches and a L3 DRAM cache. The 3D-stacked DRAM cache has 4 channels with 128 bits bus and 128 MB size. In the experiments, we SPEC CPU2006 benchmark suites and sample 300M instructions using Simpoint. Due to some applications' low sensitivity to memory hierarchy optimizations, we focus on memory intensive benchmarks. In order to reduce simulation time, we work on the same application across all 8 cores. In our work, we first analyzed the bandwidth pressure of DRAM cache. In the experiment, we compared number of DRAM cache Read/Write and actual DRAM cache requests. As shown in Figure 6, the actual number of DRAM cache requests is significantly higher than number of DRAM cache read/write. From the result, we can see that although 3D-stacked DRAM brings higher bandwidth, it also brings higher bandwidth demand.  To achieve better DRAM cache performance, we should also consider bandwidth issue in our DRAM cache management policy. Secondly, we analyzed the impact of unnecessary DRAM cache accesses. We performed experiment to measure the impact of reducing requests generated by cache misses. Figure 7 is the reduction rate of DRAM cache queuing delay normalized to baseline system if avoiding all accesses of cache misses. The result shows huge impact on DRAM cache queuing delay. It implies the great performance potential of doing so. To achieve this target, we not only need a cache hit/miss predictor, but also a solution to dirty block issue.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/22/2017\n\n\t\t\t\t\tSubmitted by: Hsien-Hsin S Lee"
 }
}