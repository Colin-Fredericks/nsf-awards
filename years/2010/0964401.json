{
 "awd_id": "0964401",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Medium:  New Directions in Computational Complexity",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2010-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 599990.0,
 "awd_amount": 599990.0,
 "awd_min_amd_letter_date": "2010-04-29",
 "awd_max_amd_letter_date": "2010-04-29",
 "awd_abstract_narration": "Studies in computational complexity in three directions are proposed: \r\nholographic algorithms, Darwinian evolution, and multicore algorithms.\r\nIn the first of these areas, holographic reductions have been shown to\r\nbe a fruitful source of new efficient algorithms for certain problems,\r\nand evidence of intractability for othrs. In this research the aim is to\r\narrive at a better understanding of the possibilities and limitations of\r\nholographic algorithms, by exploring ways in which specific currently\r\nknown limitations of this class of methods can be circumvented. For\r\nevolution the goal is to understand better what classes of mechanisms\r\ncan evolve through the Darwinian processes of variation and selection\r\nwhen only feasible resources in terms of population sizes and numbers of\r\ngenerations are available. In the area of multi-core algorithms, a\r\nmethodology will be developed for expressing and analyzing parallel\r\nalgorithms that are optimal for a wide range of hardware performance\r\nparameters. Such algorithms would make possible portable software, that\r\nis aware of the parameters of the machine on which it executes, and can\r\nrun efficiently on all such machines.\r\n\r\nThe work on multi-core algorithms aims to have the practical goal of\r\nincreasing the effective exploitation of multi-core computers as these\r\nbecome more pervasive. The work on evolution will highlight the fact\r\nthat the question of how complex mechanisms could have evolved within\r\nthe resources available, is a question that is resolvable by the methods\r\nof computational complexity, and aims to provide more precise\r\nmathematical specifications of what the Darwinian process can achieve.\r\nThe work on holographic algorithms aims to make progress in our\r\nunderstanding of what are widely regarded as the most fundamental\r\nquestions regarding the power of practical computation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Leslie",
   "pi_last_name": "Valiant",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Leslie G Valiant",
   "pi_email_addr": "valiant@seas.harvard.edu",
   "nsf_id": "000207249",
   "pi_start_date": "2010-04-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "1033 MASSACHUSETTS AVE STE 3",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021385366",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "792700",
   "pgm_ele_name": "COMPLEXITY & CRYPTOGRAPHY"
  },
  {
   "pgm_ele_code": "793400",
   "pgm_ele_name": "PARAL/DISTRIBUTED ALGORITHMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 599990.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><br /><br />Computational complexity is the study of the resources needed for realizing computations. The most frequently studied resource is the number of computational steps, but other important resources are storage space and the number of operations that can be done in parallel. For any computational task one seeks to understand the resources needed. A positive result shows, for example, that for that task a certain number of steps are sufficient. The development of a new algorithm is often justified by a proof that it takes fewer steps than any previously known algorithm. In the opposite direction, negative results that show that any algorithm for a task needs infeasibly many steps can be also of great practical relevance if the task is one we do not wish anyone to be able to accomplish, such as breaking a cryptographic code.<br /><br />One thrust of the work performed is in the area of holographic algorithms. This is an approach to algorithms, suggested by this investigator, that is partially inspired by the mathematics of quantum computation, but aimed at conventional computers. In this framework algorithms have been found for certain problems that are exponentially faster than any previously known. In a different direction this framework has been shown to be useful for proving that pairs of apparently dissimilar tasks are in fact identical, or at least of equal computational difficulty. Further, these two aspects can be sometimes put together to prove dichotomy theorems, that show that for certain classes of tasks, each task is either feasibly computable, or equivalent to a single task that is suspected to be intractable. Such dichotomy theorems enable the problem of identifying the computational complexity of a new task to be resolved routinely, without requiring new research.<br /><br />A second thrust has been in biology. Biological processes at every level can be regarded as being logically complex, and as realizing complex computations. This investigator has suggested that any mechanism that might realize biological evolution can be viewed as a learning mechanism, in the same technical sense as in the field of machine learning. Thus the quantitative capabilities and limitations of any particular realization of the general Darwinian mechanism, might be explored within the better understood framework of machine learning. In neuroscience the fact that the biological processes can be regarded as computations have been appreciated for much longer, but no quantitatively satisfactory theory of how the human brain accomplishes basic learning and memory in the manner that it does has wide acceptance. In this project a new suggestion is made of how the hippocampal system might be helping the cortex in realizing these tasks.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/05/2015<br>\n\t\t\t\t\tModified by: Leslie&nbsp;G&nbsp;Valiant</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\nComputational complexity is the study of the resources needed for realizing computations. The most frequently studied resource is the number of computational steps, but other important resources are storage space and the number of operations that can be done in parallel. For any computational task one seeks to understand the resources needed. A positive result shows, for example, that for that task a certain number of steps are sufficient. The development of a new algorithm is often justified by a proof that it takes fewer steps than any previously known algorithm. In the opposite direction, negative results that show that any algorithm for a task needs infeasibly many steps can be also of great practical relevance if the task is one we do not wish anyone to be able to accomplish, such as breaking a cryptographic code.\n\nOne thrust of the work performed is in the area of holographic algorithms. This is an approach to algorithms, suggested by this investigator, that is partially inspired by the mathematics of quantum computation, but aimed at conventional computers. In this framework algorithms have been found for certain problems that are exponentially faster than any previously known. In a different direction this framework has been shown to be useful for proving that pairs of apparently dissimilar tasks are in fact identical, or at least of equal computational difficulty. Further, these two aspects can be sometimes put together to prove dichotomy theorems, that show that for certain classes of tasks, each task is either feasibly computable, or equivalent to a single task that is suspected to be intractable. Such dichotomy theorems enable the problem of identifying the computational complexity of a new task to be resolved routinely, without requiring new research.\n\nA second thrust has been in biology. Biological processes at every level can be regarded as being logically complex, and as realizing complex computations. This investigator has suggested that any mechanism that might realize biological evolution can be viewed as a learning mechanism, in the same technical sense as in the field of machine learning. Thus the quantitative capabilities and limitations of any particular realization of the general Darwinian mechanism, might be explored within the better understood framework of machine learning. In neuroscience the fact that the biological processes can be regarded as computations have been appreciated for much longer, but no quantitatively satisfactory theory of how the human brain accomplishes basic learning and memory in the manner that it does has wide acceptance. In this project a new suggestion is made of how the hippocampal system might be helping the cortex in realizing these tasks.\n\n\t\t\t\t\tLast Modified: 10/05/2015\n\n\t\t\t\t\tSubmitted by: Leslie G Valiant"
 }
}