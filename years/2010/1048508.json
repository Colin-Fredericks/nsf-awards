{
 "awd_id": "1048508",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:  Exploring Compressive Sampling for Extreme-Scale Data Visualization",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Lawrence Rosenblum",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 85000.0,
 "awd_amount": 85000.0,
 "awd_min_amd_letter_date": "2010-08-24",
 "awd_max_amd_letter_date": "2010-08-24",
 "awd_abstract_narration": "Abstract\r\n\r\nThis EAGER aims to provide practical evidence of feasibility for a larger project called instance-optimal sampling. The instance-optimal sampling is a foundational framework for optimal representation of extreme-scale (scattered, unstructured, and structured) datasets. Using the dense polytope packing algorithms, the instance-optimal sampling framework develops strategies for sampling a given dataset at the optimally minimal sampling rate. The instance-optimal representation is derived based on the multidimensional notion of Nyquist frequencies; therefore, this approach is best complemented with the compressive sampling (CS) methods that exploit the sparsity of a dataset to reduce the sampling rate significantly below the Nyquist rate with no loss of information.\r\n\r\nThe main motivation in this research is that the synergy of compressive sampling and instance-optimal sampling would potentially allow the reduction of an extreme-scale dataset to sizes that are logarithmically proportional to number of samples in that dataset and linearly proportional to its sparsity. The research addresses the computational efficiency issue of sparse reconstruction for volumetric and time-varying datasets, which can lay the basis for applying CS to computer graphics problems. The main challenge is the computational cost of the reconstruction algorithm for 3-D or time-varying data. This research examines the feasibility of adopting a tensor-product approach to compressive sampling.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alireza",
   "pi_last_name": "Entezari",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alireza Entezari",
   "pi_email_addr": "entezari@cise.ufl.edu",
   "nsf_id": "000505902",
   "pi_start_date": "2010-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Florida",
  "inst_street_address": "1523 UNION RD RM 207",
  "inst_street_address_2": "",
  "inst_city_name": "GAINESVILLE",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3523923516",
  "inst_zip_code": "326111941",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "FL03",
  "org_lgl_bus_name": "UNIVERSITY OF FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NNFQH1JAPEP3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Florida",
  "perf_str_addr": "1523 UNION RD RM 207",
  "perf_city_name": "GAINESVILLE",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "326111941",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "FL03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 85000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project was an exploratory project to examine the possibility of utilizing the emerging compressive sensing framework in the scientific data visualization pipeline. ?Our investigations demonstrated that the prospects of using&nbsp;compressive sensing framework in the context of volumetric data are quite attractive. For example, we managed to demonstrated that the a typical brain aneurysm MRI dataset that in the traditional sense requires an acquistion of 256x256x256 (&asymp;16 million) samples, can be represented nearly perfectly with only 3% of the samples (acquired in the CS way). The reconstruction using the standard CS techniques takes about 7-8 hrs and our GPU enabled reconstruction managed to bring that down to about 40-45 minutes.</p>\n<p>&nbsp;</p>\n<p>There needs to be further theoretical investigations&nbsp;about more scalable reconstruction algorithms (beyond greedy and&nbsp;convex optimization methods) that could scale well for today's large-scale volumetric data. Another theoretical line of research that is&nbsp;promising is the design of optimal representational basis that could&nbsp;provide ideal sparse representation of volumetric data.&nbsp;Although these two lines of research pose significant challenges,&nbsp;these potential outcomes are truly significant and could enable visualization of large scale data on common computing platforms. Moreover, such a technology could enable interactive visualization of regular datasets on low-end (e.g., mobile) platforms.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/20/2012<br>\n\t\t\t\t\tModified by: Alireza&nbsp;Entezari</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project was an exploratory project to examine the possibility of utilizing the emerging compressive sensing framework in the scientific data visualization pipeline. ?Our investigations demonstrated that the prospects of using compressive sensing framework in the context of volumetric data are quite attractive. For example, we managed to demonstrated that the a typical brain aneurysm MRI dataset that in the traditional sense requires an acquistion of 256x256x256 (&asymp;16 million) samples, can be represented nearly perfectly with only 3% of the samples (acquired in the CS way). The reconstruction using the standard CS techniques takes about 7-8 hrs and our GPU enabled reconstruction managed to bring that down to about 40-45 minutes.\n\n \n\nThere needs to be further theoretical investigations about more scalable reconstruction algorithms (beyond greedy and convex optimization methods) that could scale well for today's large-scale volumetric data. Another theoretical line of research that is promising is the design of optimal representational basis that could provide ideal sparse representation of volumetric data. Although these two lines of research pose significant challenges, these potential outcomes are truly significant and could enable visualization of large scale data on common computing platforms. Moreover, such a technology could enable interactive visualization of regular datasets on low-end (e.g., mobile) platforms.\n\n\t\t\t\t\tLast Modified: 08/20/2012\n\n\t\t\t\t\tSubmitted by: Alireza Entezari"
 }
}