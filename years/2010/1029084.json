{
 "awd_id": "1029084",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Neural Basis of Cross-modal Influences on Perception",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "alumit ishai",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 625694.0,
 "awd_amount": 625694.0,
 "awd_min_amd_letter_date": "2010-08-26",
 "awd_max_amd_letter_date": "2013-07-10",
 "awd_abstract_narration": "Many of the objects and events that we encounter in everyday life, such as a barking dog or a honking car, are both seen and heard.  A basic task our brains must carry out is to bring together the sensory information that is received concurrently by our eyes and ears, so that we perceive a world of unitary objects having both auditory and visual properties. With funding from the National Science Foundation, Dr. Steven A. Hillyard and colleagues, of the University of California, San Diego, are investigating when and where in the brain the visual and auditory signals are combined and integrated to produce coherent, multi-dimensional perceptions of objects in the environment. The sensory inputs from the eyes and ears are projected initially to separate regions of the brain specialized for perceiving the visual and auditory modalities, respectively. The timing and anatomical localization of neural interactions between auditory and visual inputs is being analyzed by means of scalp recordings of brain potentials that are triggered by these sensory events, together with magnetic resonance imaging of stimulus-induced brain activity patterns. A major aim is to analyze the brain interactions that cause a stimulus in one modality (auditory or visual) to alter the perception of a stimulus in the other modality.  Three types of such auditory-visual interactions are being studied: (1) the brightness enhancement of a visual event when accompanied by a sound, (2) the ventriloquist illusion, which is a shift in perceived sound location towards the position of a concurrent visual event, and (3) the double-flash illusion that is induced when a single flash is interposed between two pulsed sounds.  In each case, the precisely timed sequence of neural interactions in the brain that underlie the perceptual experience will be identified, and the influence of selective attention on these interactions will be determined. The overall aim is to understand the neural mechanisms by which stimuli in different sensory modalities are integrated in the brain to achieve unified perceptions of multi-modal events in the world.\r\n\r\nBecause much of our everyday experience involves recognizing and reacting to the sights and sounds of our surroundings, understanding the principles by which these auditory and visual inputs are synthesized in the brain is important. The ability to combine auditory and visual signals effectively is particularly important in teaching and learning situations, where spoken words must be put together with a variety of pictorial, graphic, and written information in order to understand the material.  This research program can contribute to the development of more efficient learning environments and teaching techniques, and lead to improved designs for communication media such as audio-visual teaching tools, information displays, and critical warning signals. These studies are exploring the role of selective attention in synthesizing auditory and visual signals, research that can lead to improved teaching techniques that emphasize the training of attention. By studying the brain systems that enable auditory and visual inputs to be combined into perceptual wholes, this research can also help to understand what goes wrong in the brains of patients who suffer from abnormalities of perception, including those with learning disabilities, attention deficit disorder, autism, and schizophrenia.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Hillyard",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Steven Hillyard",
   "pi_email_addr": "shillyard@ucsd.edu",
   "nsf_id": "000540330",
   "pi_start_date": "2010-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 149553.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 154104.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 158282.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 163755.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Events in the natural world are perceived through multiple sensory modalities, and multi-modal interactions in the brain are critical for generating coherent perceptual experiences and adaptive behavioral responses.&nbsp; Our NSF-sponsored research has been concerned with understanding how cross-modal interactions take place in the human brain to achieve perceptual integration. The major research theme over the past four years has been to study how the visual and auditory systems of the brain interact, and in particular to show how sounds influence our visual perceptions.&nbsp; Our principal finding has been that a salient sound automatically attracts attention to its location so that the perception of a subsequent visual stimulus that occurs at the same location is enhanced.&nbsp; Thus, when a visual stimulus follows a salient sound at the same location it is perceived more accurately, enters awareness more rapidly, appears brighter and elicits a larger evoked potential in the visual cortex than does a visual stimulus presented at a different location (e.g., in the opposite visual field).</p>\n<p>&nbsp;</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We investigated the neural basis of this cross-modal facilitation of visual perception by recording changes in the electroencephalogram (EEG) elicited by sounds.&nbsp; We discovered that presenting a salient sound to one side elicits a positive potential shift in the contralateral visual cortex, which was termed the &ldquo;Auditory Contralateral Occipital Positivity&rdquo; (ACOP).&nbsp; We hypothesized that this ACOP was the neural sign of the perceptual priming of the visual cortex by the sound.&nbsp;&nbsp; In support of this hypothesis, we found that letters that were flashed at the location of an immediately preceding sound were discriminated more accurately than letters flashed in the opposite visual field.&nbsp; Most importantly, the ACOP triggered by the sound was larger when the subsequent letter discrimination was correct as opposed to when it was incorrect.&nbsp; We concluded that the spatially non-predictive sound automatically captured visual attention to the sound&rsquo;s location, which resulted in a lateralized facilitation of visual processing that was indexed by the ACOP.</p>\n<p>&nbsp;</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Over the past year we have extended these studies of visual cortex activity elicited by sounds into the frequency domain, using wavelet analyses.&nbsp;&nbsp; We found that the sounds provoked a robust desynchronization (blocking) of the ongoing alpha rhythm of the EEG, and that the time course of this alpha-blocking closely paralleled that of the ACOP slow potential. In support of the close connection between the ACOP and alpha blocking, we reanalyzed the data from the letter discrimination experiment and found that trials with correct discriminations had stronger alpha blocking that incorrect trials, thereby closely paralleling the findings with ACOP.&nbsp; These studies demonstrate that the blocking of alpha rhythm provoked by a sound represents a fundamental neural mechanism by which cross-modal attention facilitates visual perception.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; These studies have advanced our understanding of the brain mechanisms by which information from different sensory modalities is combined in the brain to achieve perceptual coherence in a multi-sensory world.&nbsp; This research also has broader implications for the fields of communication, education, and mental health.&nbsp; Because much of our everyday perceptual experience involves multi-sensory integration, co-ordination between sensory inputs in different modalities helps us localize and re...",
  "por_txt_cntn": "\n               Events in the natural world are perceived through multiple sensory modalities, and multi-modal interactions in the brain are critical for generating coherent perceptual experiences and adaptive behavioral responses.  Our NSF-sponsored research has been concerned with understanding how cross-modal interactions take place in the human brain to achieve perceptual integration. The major research theme over the past four years has been to study how the visual and auditory systems of the brain interact, and in particular to show how sounds influence our visual perceptions.  Our principal finding has been that a salient sound automatically attracts attention to its location so that the perception of a subsequent visual stimulus that occurs at the same location is enhanced.  Thus, when a visual stimulus follows a salient sound at the same location it is perceived more accurately, enters awareness more rapidly, appears brighter and elicits a larger evoked potential in the visual cortex than does a visual stimulus presented at a different location (e.g., in the opposite visual field).\n\n \n\n            We investigated the neural basis of this cross-modal facilitation of visual perception by recording changes in the electroencephalogram (EEG) elicited by sounds.  We discovered that presenting a salient sound to one side elicits a positive potential shift in the contralateral visual cortex, which was termed the \"Auditory Contralateral Occipital Positivity\" (ACOP).  We hypothesized that this ACOP was the neural sign of the perceptual priming of the visual cortex by the sound.   In support of this hypothesis, we found that letters that were flashed at the location of an immediately preceding sound were discriminated more accurately than letters flashed in the opposite visual field.  Most importantly, the ACOP triggered by the sound was larger when the subsequent letter discrimination was correct as opposed to when it was incorrect.  We concluded that the spatially non-predictive sound automatically captured visual attention to the sound\u00c6s location, which resulted in a lateralized facilitation of visual processing that was indexed by the ACOP.\n\n \n\n                Over the past year we have extended these studies of visual cortex activity elicited by sounds into the frequency domain, using wavelet analyses.   We found that the sounds provoked a robust desynchronization (blocking) of the ongoing alpha rhythm of the EEG, and that the time course of this alpha-blocking closely paralleled that of the ACOP slow potential. In support of the close connection between the ACOP and alpha blocking, we reanalyzed the data from the letter discrimination experiment and found that trials with correct discriminations had stronger alpha blocking that incorrect trials, thereby closely paralleling the findings with ACOP.  These studies demonstrate that the blocking of alpha rhythm provoked by a sound represents a fundamental neural mechanism by which cross-modal attention facilitates visual perception.\n\n \n\n \n\n               These studies have advanced our understanding of the brain mechanisms by which information from different sensory modalities is combined in the brain to achieve perceptual coherence in a multi-sensory world.  This research also has broader implications for the fields of communication, education, and mental health.  Because much of our everyday perceptual experience involves multi-sensory integration, co-ordination between sensory inputs in different modalities helps us localize and recognize events in the environment, focus attention on relevant objects in cluttered and noisy surroundings, and learn more efficiently than when stimuli arrive over only one modality.  The findings from this project may lead to enhanced data displays and learning environments for a variety of real-world tasks that require audio-visual integration.  These findings may also contribute to better understanding of clinical syndromes that involve deficit..."
 }
}