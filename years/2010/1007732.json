{
 "awd_id": "1007732",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Multiple Problems in Multiple Testing and Simultaneous Inference",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2010-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 369997.0,
 "awd_amount": 369997.0,
 "awd_min_amd_letter_date": "2010-06-10",
 "awd_max_amd_letter_date": "2012-06-04",
 "awd_abstract_narration": "The investigator develops new methods and theory for problems in multiple testing and simultaneous inference.  The classical approach to dealing with multiplicity is to require decision rules that control the familywise error rate, the probability of rejecting at least one true null hypothesis. But when the number of tests is large, control of the familywise error rate is so stringent that alternative hypotheses have little chance of being detected. In response, the false discovery rate and other measures of error control have gained wide use.  For each measure of error control, it is desired to construct procedures that exhibit error control under the weakest possible assumptions. Resampling methods  offer viable approaches to obtaining valid distributional approximations while assuming very little about the stochastic mechanism generating the data. While many new methods have been developed, many more questions remain and are studied. Some of the technical challenges include:  asymptotics that grow with both sample size and number of tests; orders of error in approximation;  uniformity in approximation; optimality theory;  direction errors. Related problems are also studied, such as the statistical evaluation of bioequivalence across multiple measures, testing for stochastic dominance, and inference for partially identified econometric models.\r\n\r\nVirtually any scientific experiment sets out to answer  questions about the process under investigation, which  often can  be translated formally into a set of hypotheses. It is the exception that a single hypothesis is considered. For example, in clinical trials, even a  single treatment  may be evaluated using multiple outcome measures,  multiple time points, multiple doses, and multiple subgroups.  Moreover, due to effects of ``data snooping'' (or ``data mining''), additional hypotheses  arise as well. The statistician is then faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any resulting inferences or interesting conclusions can reliably be viewed as real structure rather than artifacts of random data.  In general, the philosophical approach is the development of practical methods that may be applied in increasingly complex situations as the scope of modern data analysis continues to grow. The broader impact of this work is potentially quite large because the resulting inferential tools can be applied to such diverse fields as genetics, bioengineering, image processing and neuroimaging, clinical trials, education,  astronomy, finance and econometrics.  For example, current methods in biotechnology and genomics generate DNA microarray experiments, where expression levels in cells for thousands of genes must be analyzed simultaneously. The many burgeoning fields of applications demand new statistical methods, creating challenging and exciting opportunities for young scholars under the direction of the PI.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Romano",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph P Romano",
   "pi_email_addr": "romano@stanford.edu",
   "nsf_id": "000197385",
   "pi_start_date": "2010-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 JANE STANFORD WAY",
  "perf_city_name": "STANFORD",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 119926.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 123302.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 126769.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The major goal of the award was the development of new statistical methods for the analysis of data and inference in modern day statistical applications. &nbsp;These days statisticians are faced with challenging problems when analyzing data due to complex dependencies, high dimensionality, &nbsp;and problems of error control when making simultaneous inferences. &nbsp;The basic question is how to differentiate interesting features of the data from random artifacts, so that experiments not only have statistical validity, but are replicable and lead to reliable conclusions. Such problems arise in both the hard and social sciences, and affect such diverse areas as: environmental statistics and climate change, finance, education, genomics and microarray analysis, clinical trials, econometrics, and astronomy. &nbsp;By developing methods for the analysis of empirical data based on fundamental principles in statistics, our methods can be (and have been) applied to these fields and others as well. Understanding random variation and error control in a principled and rigorous manner allows us to develop the best statistical tools to meet the challenges that empirical researchers face. &nbsp;As such, statisticians are the gatekeepers of science, and allow for honest evaluation of data. &nbsp;Many of the newly developed techniques are computer-intensive, but our algorithms are computationally feasible, easy to apply, and are backed with mathematical justification. &nbsp;As an example, we have applied many of these methods for studying global warming and climate change. One important consequence to climate researchers and the public at large is that any claims of a hiatus in warming are statistically unfounded. It is only with rigorous framework and acommanying statistical testing methods which account for error control and statistical variability that we could arrive at such conclusions.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/05/2015<br>\n\t\t\t\t\tModified by: Joseph&nbsp;P&nbsp;Romano</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe major goal of the award was the development of new statistical methods for the analysis of data and inference in modern day statistical applications.  These days statisticians are faced with challenging problems when analyzing data due to complex dependencies, high dimensionality,  and problems of error control when making simultaneous inferences.  The basic question is how to differentiate interesting features of the data from random artifacts, so that experiments not only have statistical validity, but are replicable and lead to reliable conclusions. Such problems arise in both the hard and social sciences, and affect such diverse areas as: environmental statistics and climate change, finance, education, genomics and microarray analysis, clinical trials, econometrics, and astronomy.  By developing methods for the analysis of empirical data based on fundamental principles in statistics, our methods can be (and have been) applied to these fields and others as well. Understanding random variation and error control in a principled and rigorous manner allows us to develop the best statistical tools to meet the challenges that empirical researchers face.  As such, statisticians are the gatekeepers of science, and allow for honest evaluation of data.  Many of the newly developed techniques are computer-intensive, but our algorithms are computationally feasible, easy to apply, and are backed with mathematical justification.  As an example, we have applied many of these methods for studying global warming and climate change. One important consequence to climate researchers and the public at large is that any claims of a hiatus in warming are statistically unfounded. It is only with rigorous framework and acommanying statistical testing methods which account for error control and statistical variability that we could arrive at such conclusions.\n\n\t\t\t\t\tLast Modified: 08/05/2015\n\n\t\t\t\t\tSubmitted by: Joseph P Romano"
 }
}