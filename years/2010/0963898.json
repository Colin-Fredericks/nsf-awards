{
 "awd_id": "0963898",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI:  Medium:  Collaborative Research:  Semi-Supervised Discriminative Training of Language Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2010-06-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 518250.0,
 "awd_min_amd_letter_date": "2010-06-09",
 "awd_max_amd_letter_date": "2014-06-03",
 "awd_abstract_narration": "This project is conducting fundamental research in statistical language modeling to improve human language technologies, including automatic speech recognition (ASR) and machine translation (MT). \r\n\r\nA language model (LM) is conventionally optimized, using text in the target language, to assign high probability to well-formed sentences. This method has a fundamental shortcoming: the optimization does not explicitly target the kinds of distinctions necessary to accomplish the task at hand, such as discriminating (for ASR) between different words that are acoustically confusable or (for MT) between different target-language words that express the multiple meanings of a polysemous source-language word. \r\n\r\nDiscriminative optimization of the LM, which would overcome this shortcoming, requires large quantities of paired input-output sequences: speech and its reference transcription for ASR or source-language (e.g. Chinese) sentences and their translations into the target language (say, English) for MT. Such resources are expensive, and limit the efficacy of discriminative training methods. \r\n\r\nIn a radical departure from convention, this project is investigating discriminative training using easily available, *unpaired* input and output sequences: un-transcribed speech or monolingual source-language text and unpaired target-language text. Two key ideas are being pursued: (i) unlabeled input sequences (e.g. speech or Chinese text) are processed to learn likely confusions encountered by the ASR or MT system; (ii) unpaired output sequences (English text) are leveraged to discriminate between these well-formed sentences from the (supposed) ill-formed sentences the system could potentially confuse them with. \r\n\r\nThis self-supervised discriminative training, if successful, will advance machine intelligence in fundamental ways that impact many other applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sanjeev",
   "pi_last_name": "Khudanpur",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Sanjeev P Khudanpur",
   "pi_email_addr": "khudanpur@jhu.edu",
   "nsf_id": "000236251",
   "pi_start_date": "2010-06-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Damianos",
   "pi_last_name": "Karakos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Damianos Karakos",
   "pi_email_addr": "damianos.karakos@jhu.edu",
   "nsf_id": "000074021",
   "pi_start_date": "2010-06-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Callison-Burch",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chris Callison-Burch",
   "pi_email_addr": "ccb@cis.upenn.edu",
   "nsf_id": "000084214",
   "pi_start_date": "2010-06-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N CHARLES ST",
  "perf_city_name": "BALTIMORE",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "729800",
   "pgm_ele_name": "International Research Collab"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5940",
   "pgm_ref_txt": "TURKEY"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 161834.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 166590.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 189826.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this collaborative project between Johns Hopkins University (JHU) and Oregon Health and Sciences University (OHSU) was to improve automatic speech recognition (ASR) and statistical machine translation (SMT) systems by improving the statistical language models (LM) that constitute key components of ASR and SMT systems.&nbsp; ASR and SMT, in turn, are core technologies that enable numerous applications, including consumer-facing products like the Apple&rsquo;s Siri, Amazon&rsquo;s Echo, Microsoft&rsquo;s Cortana, Google Translate and, most recently, Skype&rsquo;s Translator.&nbsp;</p>\n<p>Building good statistical models for ASR and SMT is a resource-intensive exercise: it requires obtaining large volumes of manually transcribed speech for ASR systems, and large text corpora that are translated by hand from the intended source- to the target language for SMT systems.&nbsp; Good ASR and SMT systems are therefore usually not available for minority languages and commercially unimportant domains.&nbsp; This goal of this project was to lower this resource barrier by developing new methods that do no rely on such expensive resources, and instead try to leverage easier-to-obtain resources such as un-transcribed speech and un-translated texts.&nbsp; Such methods are often described as &ldquo;unsupervised training&rdquo; and &ldquo;semi-supervised training&rdquo; methods.</p>\n<p>The key new idea investigated in this project was that of &ldquo;self-supervised training.&rdquo; &nbsp;For instance, if an ASR system with modest accuracy is presented with lots of un-transcribed speech, and notices its own recurring inability to transcribe something &ndash; e.g. a speech sound that is confusable between <em>speech</em> and <em>beach</em> &ndash; then, rather than requiring a human to tell it what the correct word is in each instance (supervised training), the ASR system turns to a text corpus to analyze the contexts in which the confusable words are used &ndash; e.g. <em>his speech pathologist</em> versus <em>the beach ball</em>. &nbsp;The ASR system then employs the outcome of such analysis to the context of each instance of the confusable sound, and guesses what the correct word may be.&nbsp; Finally, it aggregates these context-informed guesses across all the confusable instances to create a training set, and uses this set in lieu of manual transcripts for re-estimating its language model (self-supervised training).</p>\n<p>This idea extends to SMT systems straightforwardly. For instance, if the SMT system notices that it is frequently unable to decide how to translate an English word (e.g. <em>bank</em>) into Spanish (<em>orilla</em> or <em>banca</em>), it turns to a Spanish text corpus to analyze contexts in which <em>orilla</em> and <em>banca</em> are used, guesses which contexts better match each instance in which <em>bank</em> needs to be translated into Spanish, and creates a new training set to refine its model for English-to-Spanish translation.</p>\n<p>The main technical accomplishment of the project was a thorough and successful investigation of the main idea.&nbsp; It was demonstrated that the proposed self-supervised training methods lead to modest but statistically significant improvements in both ASR transcription accuracy and SMT translation quality.&nbsp; The methods were evaluated on common benchmark tests developed by the ASR and SMT research communities, making it easier for others to validate their strengths and analyze their limitations.&nbsp; More than 20 peer-reviewed scientific papers based on the research undertaken in this project have been published, adding further support to the validity of these findings.&nbsp; While the problem is not entirely solved by the proposed approach &ndash; ASR and SMT in low-resource settings remain a major technical challenge &ndash; the research done in this project has led to further insights into what makes the...",
  "por_txt_cntn": "\nThe goal of this collaborative project between Johns Hopkins University (JHU) and Oregon Health and Sciences University (OHSU) was to improve automatic speech recognition (ASR) and statistical machine translation (SMT) systems by improving the statistical language models (LM) that constitute key components of ASR and SMT systems.  ASR and SMT, in turn, are core technologies that enable numerous applications, including consumer-facing products like the Apple\u00c6s Siri, Amazon\u00c6s Echo, Microsoft\u00c6s Cortana, Google Translate and, most recently, Skype\u00c6s Translator. \n\nBuilding good statistical models for ASR and SMT is a resource-intensive exercise: it requires obtaining large volumes of manually transcribed speech for ASR systems, and large text corpora that are translated by hand from the intended source- to the target language for SMT systems.  Good ASR and SMT systems are therefore usually not available for minority languages and commercially unimportant domains.  This goal of this project was to lower this resource barrier by developing new methods that do no rely on such expensive resources, and instead try to leverage easier-to-obtain resources such as un-transcribed speech and un-translated texts.  Such methods are often described as \"unsupervised training\" and \"semi-supervised training\" methods.\n\nThe key new idea investigated in this project was that of \"self-supervised training.\"  For instance, if an ASR system with modest accuracy is presented with lots of un-transcribed speech, and notices its own recurring inability to transcribe something &ndash; e.g. a speech sound that is confusable between speech and beach &ndash; then, rather than requiring a human to tell it what the correct word is in each instance (supervised training), the ASR system turns to a text corpus to analyze the contexts in which the confusable words are used &ndash; e.g. his speech pathologist versus the beach ball.  The ASR system then employs the outcome of such analysis to the context of each instance of the confusable sound, and guesses what the correct word may be.  Finally, it aggregates these context-informed guesses across all the confusable instances to create a training set, and uses this set in lieu of manual transcripts for re-estimating its language model (self-supervised training).\n\nThis idea extends to SMT systems straightforwardly. For instance, if the SMT system notices that it is frequently unable to decide how to translate an English word (e.g. bank) into Spanish (orilla or banca), it turns to a Spanish text corpus to analyze contexts in which orilla and banca are used, guesses which contexts better match each instance in which bank needs to be translated into Spanish, and creates a new training set to refine its model for English-to-Spanish translation.\n\nThe main technical accomplishment of the project was a thorough and successful investigation of the main idea.  It was demonstrated that the proposed self-supervised training methods lead to modest but statistically significant improvements in both ASR transcription accuracy and SMT translation quality.  The methods were evaluated on common benchmark tests developed by the ASR and SMT research communities, making it easier for others to validate their strengths and analyze their limitations.  More than 20 peer-reviewed scientific papers based on the research undertaken in this project have been published, adding further support to the validity of these findings.  While the problem is not entirely solved by the proposed approach &ndash; ASR and SMT in low-resource settings remain a major technical challenge &ndash; the research done in this project has led to further insights into what makes these tasks difficult.\n\nA major societal benefit arising from this project is the close and intensive research collaborations that were developed between academia and industry, and between US and non-US researchers in addressing these problems.  Distinguished researchers from IBM, Google, Microsoft..."
 }
}