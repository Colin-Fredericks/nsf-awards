{
 "awd_id": "1049080",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:   VizWiz - Enabling Blind People to Answer Visual Questions On-the-Go with Remote Automatic and Human-Powered Services",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2011-08-31",
 "tot_intn_awd_amt": 49999.0,
 "awd_amount": 49999.0,
 "awd_min_amd_letter_date": "2010-07-19",
 "awd_max_amd_letter_date": "2010-07-19",
 "awd_abstract_narration": "The lack of access to visual information like text labels, icons, and colors frustrates blind people and severely decreases their independence.  Current access technology uses fully-automatic approaches to address some problems in this space but is error-prone, limited in scope, and expensive.  Blind people who can afford to do so must carry multiple special-purpose portable devices with different audio and tactile interfaces in order to access critical data about their environment such as product information from bar codes and location information via GPS.  These devices would likely be used more often if they had greater functionality and failed less often.  Providing a fallback by making it easy to consult a human assistant could be part of the solution.  The PI's talking VizWiz application for mobile phones is one such prototype that connects blind people to remote human workers who answer general questions about the users' visual environments.  VizWiz currently allows blind users to take a picture, speak a question, and receive answers back quickly.  Preliminary findings have demonstrated the potential advantages of including humans in the loop to help overcome visual problems that are still too difficult to be solved by automatic approaches alone, but questions remain about the efficacy, privacy, speed, and cost of these approaches.  In this project the PI will seek answers to some of these questions, by conducting a longitudinal study of VizWiz with blind people to better understand how the application might fit into their everyday lives.  He will endeavor to determine how users' existing social networks might be employed as a source of answers using applications for Facebook and Twitter.  And he will seek to define new services with appropriate interfaces that let users mediate between automatic and human-powered remote sources for answers.  A mobile accessibility solution using both automated and human Web services represents a significant advance in accessibility but presents challenging user interface questions.  Understanding issues such as those enumerated above is necessary for human-powered services to be accepted as part of assistive technology.  \r\n\r\nBroader Impacts:  This exploratory research represents a new paradigm in human-computer interaction in which humans are both clients and providers.  VizWiz has the potential to improve the independence of blind people, and may be both less expensive and more sustainable than current accessibility solutions.  This project will improve our understanding of the types of tools that would be useful for blind people regardless of what is possible today with automatic computer vision, and will help us better understand how to recruit people to answer questions while respecting the asker's values.  The research will involve blind people throughout; the resulting interfaces and functionality will be evaluated by blind people in the world going about their everyday lives.   The interfaces, applications, and framework created and improved as part of this project will be released as open source so other researchers may build on the PI's results.  Project outcomes will be broadly applicable to other problems where automated solutions may occasionally need human intervention.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Bigham",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey Bigham",
   "pi_email_addr": "jbigham@cmu.edu",
   "nsf_id": "000541549",
   "pi_start_date": "2010-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "910 GENESEE ST",
  "perf_city_name": "ROCHESTER",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146113847",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 49999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project was a scientific investigation into VizWiz, is a mobile application that we created that lets blind people take a picture, speak a question about it, and receive an answer from people out on the web. We conducted a large-scale longitudinal field study of how blind people use VizWiz. To facilitate this we released VizWiz on the Apple App Store.&nbsp; Since then, nearly 1500 blind participants have used VizWiz to ask nearly 20,000 questions. The submitted questions provide an unprecedented look into what blind people actually want to know about their visual environments, helping to set a roadmap for computer vision. This is in addition to the near-term benefit of providing a valuable service to blind people. We also used our experience developing and releasing a human-powered technology to develop and publish design guidelines for technology in this area that may help future developers and researchers better articulate the advantages and disadvantages of future technology.&nbsp;</p>\n<p>Unsurprisingly, the primary difficulty that blind people have using VizWiz is taking good pictures &ndash; taking picture is difficult if you&rsquo;re unable to see the viewfinder. We have worked with local blind photographers to motivate new automatic computer vision techniques that help blind people take better pictures in common scenarios. For instance, in one of our prototypes, blind people first take a close-up picture of an object of interest, their phone then tracks that object, and helps them frame it in a picture from farther away. Our techniques for supporting blind photography will be useful in a variety of ways. First, blind people want to take pictures for many of the same reasons that sighted people do, e.g. to share experiences, or capture useful information. Second, tools like VizWiz require good pictures to work their best.</p>\n<p>This award funded a graduate student researcher for one year, who worked on all research aspects of this project &ndash; inventing new technology, designing experiments, and writing and presenting results. In addition, three undergraduate researchers contributed to this project, including one student who is blind. The research products they developed in collaboration with the PI are some of the first examples of real-time human computation, and may be applied in a diversity of tools.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/04/2011<br>\n\t\t\t\t\tModified by: Jeffrey&nbsp;Bigham</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2011/1049080/1049080_10018731_1317736855691_ScreenShot2011-10-04at9.58.48AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2011/1049080/1049080_10018731_1317736855691_ScreenShot2011-10-04at9.58.48AM--rgov-800width.jpg\" title=\"Questions Asked by VizWiz Users\"><img src=\"/por/images/Reports/POR/2011/1049080/1049080_10018731_1317736855691_ScreenShot2011-10-04at9.58.48AM--rgov-66x44.jpg\" alt=\"Questions Asked by VizWiz Users\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Six questions asked by VizWiz users illustrating the diversity of answers they may want to know and the ability of the crowd to provide them.</div>\n<div class=\"imageCredit\">Jeffrey Bigham</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jeffrey&nbsp;Bigham</div>\n<div class=\"imageTitle\">Questions Asked by VizWiz Users</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project was a scientific investigation into VizWiz, is a mobile application that we created that lets blind people take a picture, speak a question about it, and receive an answer from people out on the web. We conducted a large-scale longitudinal field study of how blind people use VizWiz. To facilitate this we released VizWiz on the Apple App Store.  Since then, nearly 1500 blind participants have used VizWiz to ask nearly 20,000 questions. The submitted questions provide an unprecedented look into what blind people actually want to know about their visual environments, helping to set a roadmap for computer vision. This is in addition to the near-term benefit of providing a valuable service to blind people. We also used our experience developing and releasing a human-powered technology to develop and publish design guidelines for technology in this area that may help future developers and researchers better articulate the advantages and disadvantages of future technology. \n\nUnsurprisingly, the primary difficulty that blind people have using VizWiz is taking good pictures &ndash; taking picture is difficult if you\u00c6re unable to see the viewfinder. We have worked with local blind photographers to motivate new automatic computer vision techniques that help blind people take better pictures in common scenarios. For instance, in one of our prototypes, blind people first take a close-up picture of an object of interest, their phone then tracks that object, and helps them frame it in a picture from farther away. Our techniques for supporting blind photography will be useful in a variety of ways. First, blind people want to take pictures for many of the same reasons that sighted people do, e.g. to share experiences, or capture useful information. Second, tools like VizWiz require good pictures to work their best.\n\nThis award funded a graduate student researcher for one year, who worked on all research aspects of this project &ndash; inventing new technology, designing experiments, and writing and presenting results. In addition, three undergraduate researchers contributed to this project, including one student who is blind. The research products they developed in collaboration with the PI are some of the first examples of real-time human computation, and may be applied in a diversity of tools. \n\n\t\t\t\t\tLast Modified: 10/04/2011\n\n\t\t\t\t\tSubmitted by: Jeffrey Bigham"
 }
}