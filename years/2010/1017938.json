{
 "awd_id": "1017938",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Small: Body Language Animation for Virtual Worlds and Computer-Mediated Communication",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 495208.0,
 "awd_amount": 495208.0,
 "awd_min_amd_letter_date": "2010-09-02",
 "awd_max_amd_letter_date": "2010-09-02",
 "awd_abstract_narration": "This project will enable full-body nonverbal communication in virtual worlds, applying state-of-the-art machine learning and computer animation techniques to the synthesis of nonverbal communication, advancing and refining these techniques in the process. Virtual worlds are an emerging computer-mediated communication medium that situates geographically distributed participants in a shared communication space and enables embodied interaction with others in a simulated environment. Participants are represented as animated virtual humans that can convey both speech and body language. Yet no viable technology exists that can animate the virtual human's body during a live conversation without resorting to esoteric hardware or brittle algorithmic techniques.\r\n\r\nThis research will develop an approach that can convey body language through virtual humans in real time, using a natural control interface: the speech and motion of the participants. The proposal is based on, and sometimes advances, the state of the art in machine learning, computer animation, and the relevant aspects of linguistics and cognitive psychology. Body language animation based purely on visual tracking of the participant's motion is prone to significant defects due to tracking noise and failure. Thus this approach analyzes the speech of the participant together with the motion. This principled integration of live speech and motion input constitutes a fundamentally new approach to the control of nonverbal expression of human self-representations in virtual worlds.\r\n\r\nThe ability to convey rich nonverbal communication in virtual worlds will advance the capabilities of computer-mediated communication as a whole. This will provide basic infrastructure for distributed collaboration in science and engineering. Powerful forms of situated learning and social-scientific inquiry in education will be enabled, with positive impact on the self-efficacy of students who traditionally underperform in science curricula. Social science will be enriched with a new medium for the study of human interaction.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vladlen",
   "pi_last_name": "Koltun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vladlen Koltun",
   "pi_email_addr": "vladlen@stanford.edu",
   "nsf_id": "000487179",
   "pi_start_date": "2010-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 JANE STANFORD WAY",
  "perf_city_name": "STANFORD",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 495208.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The primary goal of this project was to develop new approaches for the simulation of human motion and behavior in virtual environments. We have developed two major types of approaches. First, we demonstrated that high-fidelity human motion can be produced from first principles, <span>given only a model of the human body and a compact set of objectives. For example, walking is produced by moving the center of mass forward at an appropriate velocity. Running is produced when the desired velocity is increased, without any other modifications to the model or the objective. We have shown that such&nbsp;</span><em>de novo</em><span>&nbsp;optimization can produce motion that both qualitatively&nbsp;and quantitatively&nbsp;matches human reference data. Second, we have developed a number of algorithms for learning motor skills from demonstrations. Using these approaches, virtual characters can be programmed by simply demonstrating the desired motions and behaviors. Specifically, our approaches optimize parameterized <em>policies</em> that specify the actions that the character needs to perform, depending on the character's current state. Optimizing such policies is very challenging, since their dimensionality is extremely high: in principle, a policy specifies how each joint in the character's body must move depending on the body's current state. We have developed a number of approaches that deal with this high dimensionality and successfully produce policies for such motor skills as walking, running, driving, and swimming.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/24/2013<br>\n\t\t\t\t\tModified by: Vladlen&nbsp;Koltun</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe primary goal of this project was to develop new approaches for the simulation of human motion and behavior in virtual environments. We have developed two major types of approaches. First, we demonstrated that high-fidelity human motion can be produced from first principles, given only a model of the human body and a compact set of objectives. For example, walking is produced by moving the center of mass forward at an appropriate velocity. Running is produced when the desired velocity is increased, without any other modifications to the model or the objective. We have shown that such de novo optimization can produce motion that both qualitatively and quantitatively matches human reference data. Second, we have developed a number of algorithms for learning motor skills from demonstrations. Using these approaches, virtual characters can be programmed by simply demonstrating the desired motions and behaviors. Specifically, our approaches optimize parameterized policies that specify the actions that the character needs to perform, depending on the character's current state. Optimizing such policies is very challenging, since their dimensionality is extremely high: in principle, a policy specifies how each joint in the character's body must move depending on the body's current state. We have developed a number of approaches that deal with this high dimensionality and successfully produce policies for such motor skills as walking, running, driving, and swimming.\n\n\t\t\t\t\tLast Modified: 11/24/2013\n\n\t\t\t\t\tSubmitted by: Vladlen Koltun"
 }
}