{
 "awd_id": "1018590",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Collaborative Research: Building a Large Multilingual Semantic Network for Text Processing Applications",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2010-09-15",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 224540.0,
 "awd_amount": 224540.0,
 "awd_min_amd_letter_date": "2010-09-13",
 "awd_max_amd_letter_date": "2010-09-13",
 "awd_abstract_narration": "This project is devoted to building a large multilingual semantic network\r\nthrough the application of novel techniques for semantic analysis\r\nspecifically targeted at the Wikipedia corpus. The driving hypothesis of\r\nthe project is that the structure of Wikipedia can be effectively used to\r\ncreate a highly structured graph of world knowledge in which nodes\r\ncorrespond to entities and concepts described in Wikipedia, while edges\r\ncapture ontological relations such as hypernymy and meronymy. Special\r\nemphasis is given to exploiting the multilingual information available in\r\nWikipedia in order to improve the performance of each semantic analysis\r\ntool. Significant research effort is therefore aimed at developing tools\r\nfor word sense disambiguation, reference resolution and the extraction of\r\nontological relations that use multilingual reinforcement and the\r\nconsistent structure and focused content of Wikipedia to solve these tasks\r\naccurately. An additional research challenge is the effective integration\r\nof inherently noisy evidence from multiple Wikipedia articles in order to\r\nincrease the reliability of the overall knowledge encoded in the global\r\nWikipedia graph. Computing probabilistic confidence values for every piece\r\nof structural information added to the network is an important step in\r\nthis integration, and it is also meant to provide increased utility for\r\ndownstream applications. The proposed highly structured semantic network\r\ncomplements existing semantic resources and is expected to have a broad\r\nimpact on a wide range of natural language processing applications in need\r\nof large scale world knowledge.\r\n\r\nFor further information, please see the project website:\r\nhttp://lit.csci.unt.edu/index.php/Mu.Se.Net",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Razvan",
   "pi_last_name": "Bunescu",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Razvan C Bunescu",
   "pi_email_addr": "rbunescu@uncc.edu",
   "nsf_id": "000515843",
   "pi_start_date": "2010-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio University",
  "inst_street_address": "1 OHIO UNIVERSITY",
  "inst_street_address_2": "",
  "inst_city_name": "ATHENS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "7405932857",
  "inst_zip_code": "457012979",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "OH12",
  "org_lgl_bus_name": "OHIO UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "LXHMMWRKN5N8"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio University",
  "perf_str_addr": "1 OHIO UNIVERSITY",
  "perf_city_name": "ATHENS",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "457012979",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "OH12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 224540.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>In this project, we built semantic analysis tools specifically targeted at the Wikipedia corpus, with the aim of building a large multilingual semantic network in which edges connect entities or concepts that are related to one another through ontological relations such as hypernymy (<em>a \"book\" is a \"publication\"</em>) or meronymy (<em>a \"book\" has \"chapters\"</em>). Each node is associated with lexicalizations in different languages, based on the multilingual information present in Wikipedia. </span></p>\n<p><span>The project resulted in several publications, datasets, and software systems, including:</span></p>\n<div><ol>\n<li>A taxonomic relation extraction system and a database of taxonomic relations based on Wikipedia. The system was trained on data extracted from lists and revision histories in Wikipedia, with no manual supervision. The extracted graph database contains over 2 million entity nodes and 3 million relations between pairs of entities.</li>\n<li><span>Supervised and semi-supervised learning approaches for multilingual word sense disambiguation and semi-supervised techniques for sense clustering. We explored the cumulative impact of features originating from multiple supporting languages on the task of word sense disambiguation, and built disambiguation systems for several languages. We also addressed the task of sense clustering in Wikipedia, using a rich feature space obtained from multilingual data, and built a system that can automatically determine if two word senses should be merged.</span></li>\n<li>An adaptive clustering model for coreference resolution, <span>addressing the task of clustering together nouns and pronouns that refer to the same discourse entity</span> (<em>\"it\" refers to a \"book\"</em>). The clustering model improves over the expert rules of a state-of-the-art deterministic system by using the rules as features over pairs of clusters. Statistics from a large web n-gram corpus are used to compute&nbsp;semantic compatibility features (<em>a \"book\" may \"inspire\", but a \"book\" cannot \"eat\"</em>), leading to&nbsp;improved performance for pronoun resolution.</li>\n</ol></div>\n<p><span>All the publications, datasets, and systems are publicly available at:&nbsp;</span><br /><a href=\"http://lit.eecs.umich.edu/research/projects/musenet\" target=\"_blank\">http://lit.eecs.umich.edu/research/projects/musenet</a></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/26/2014<br>\n\t\t\t\t\tModified by: Razvan&nbsp;C&nbsp;Bunescu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project, we built semantic analysis tools specifically targeted at the Wikipedia corpus, with the aim of building a large multilingual semantic network in which edges connect entities or concepts that are related to one another through ontological relations such as hypernymy (a \"book\" is a \"publication\") or meronymy (a \"book\" has \"chapters\"). Each node is associated with lexicalizations in different languages, based on the multilingual information present in Wikipedia. \n\nThe project resulted in several publications, datasets, and software systems, including:\n\nA taxonomic relation extraction system and a database of taxonomic relations based on Wikipedia. The system was trained on data extracted from lists and revision histories in Wikipedia, with no manual supervision. The extracted graph database contains over 2 million entity nodes and 3 million relations between pairs of entities.\nSupervised and semi-supervised learning approaches for multilingual word sense disambiguation and semi-supervised techniques for sense clustering. We explored the cumulative impact of features originating from multiple supporting languages on the task of word sense disambiguation, and built disambiguation systems for several languages. We also addressed the task of sense clustering in Wikipedia, using a rich feature space obtained from multilingual data, and built a system that can automatically determine if two word senses should be merged.\nAn adaptive clustering model for coreference resolution, addressing the task of clustering together nouns and pronouns that refer to the same discourse entity (\"it\" refers to a \"book\"). The clustering model improves over the expert rules of a state-of-the-art deterministic system by using the rules as features over pairs of clusters. Statistics from a large web n-gram corpus are used to compute semantic compatibility features (a \"book\" may \"inspire\", but a \"book\" cannot \"eat\"), leading to improved performance for pronoun resolution.\n\n\nAll the publications, datasets, and systems are publicly available at: \nhttp://lit.eecs.umich.edu/research/projects/musenet\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/26/2014\n\n\t\t\t\t\tSubmitted by: Razvan C Bunescu"
 }
}