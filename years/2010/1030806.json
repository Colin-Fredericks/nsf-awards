{
 "awd_id": "1030806",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Mapping Affect and Facial Dynamics during Dyadic Conversation",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Sally Dickerson",
 "awd_eff_date": "2010-10-01",
 "awd_exp_date": "2013-09-30",
 "tot_intn_awd_amt": 199997.0,
 "awd_amount": 199997.0,
 "awd_min_amd_letter_date": "2010-09-26",
 "awd_max_amd_letter_date": "2010-09-26",
 "awd_abstract_narration": "As we talk with others, facial expressions, head movements and nonverbal speech characteristics serve as crucial cues to our emotional tone.  For this process to be effective, the cues to expressed affect must change constantly and are difficult to study.  The lack of tools for measuring and modeling the multiple cues used to interpret live facial expressions has limited the scientific study of the regulation of facial expression in conversation. This proposal consists of experiments designed to manipulate the specific cues thought to determine perceivers' understanding of communicators' emotional states during live conversations. The primary goals of this project are twofold. The first goal is to develop and refine tools (e.g., computer software programs) that will allow scientists to better understand the specific facial and movement cues that allow people to express and understand communicated affect during live conversation. The second goal is to refine the software that allows such tests of affective communication, so that it serves as a new tool for a host of behavioral scientists who wish to study and manipulate the nature of live interactions.  For example, this software will allow experimenters to manipulate the perceived gender of two people who are having a live getting-acquainted conversation -- so that a man who is actually talking to another man sees a realistic, convincing facial avatar of a woman (who perfectly mimics the facial and head movements of the actual male interaction partner). This software developed and refined in this proposal will thus allow social and cognitive psychologists to manipulate, for example, the social categories of people's interaction partners in ways not imagined a decade ago. \r\n\r\nThe impact of this project is exceptionally wide ranging. One area of application is measuring and modeling facial dynamics -- that is, using this technology to further study live facial expression. The current project will provide open source software that can be distributed free for research purposes. Research labs who wish to use the software will also be provided with free supporting materials (e.g., plans, equipment lists) to facilitate the dissemination of the technology resulting from the current project. A second area of application is the study of intercultural communication. This new technology will allow sophisticated studies of social interactions and communication in small group, high stress settings in which emotional regulation is critical (e.g., intercultural interactions by police or military personnel) or in negotiation settings (e.g., diplomatic relations).  Further the technology could be used to allow live video interactions between people that maintain the tone and content of communications while still maintaining speaker confidentiality.  A third area of application is educational technology. A problem with virtual learning environments is the difficulty of detecting students' nonverbal cues -- cues that a live classroom teacher often uses to assess student understanding or confusion. The work may lead to automatic recognition of these cues from video capture. This information could then be used to improve the virtual learning environment or refine the specific teaching materials that are broadcast to students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Boker",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Steven M Boker",
   "pi_email_addr": "boker@virginia.edu",
   "nsf_id": "000418364",
   "pi_start_date": "2010-09-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gerald",
   "pi_last_name": "Clore",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Gerald L Clore",
   "pi_email_addr": "gclore@virginia.edu",
   "nsf_id": "000378989",
   "pi_start_date": "2010-09-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia Main Campus",
  "perf_str_addr": "1001 EMMET ST N",
  "perf_city_name": "CHARLOTTESVILLE",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229034833",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "133200",
   "pgm_ele_name": "Social Psychology"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 199997.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Thin Slice Video Ratings of Emotion</strong></p>\n<p>Untrained raters saw short five second videos of people they had never met and were asked to rate what the person was feeling by saying how accurate were statements such as, \"The person in the video felt happy.\" &nbsp;The videos were extracts from real conversations between unacquainted young adults and showed only the one of the two people in the conversation as in Figure 1-a. &nbsp;The raters tended to agree about what the person in the video was feeling during that five seconds. &nbsp;A statistical method called factor analyses with oblique rotation found six factors of emotion used by the raters: &nbsp;(1) Anger, (2) Joy, (3) Anxiety, (4) Sadness, (5) Shame, (6) Compassion. &nbsp;Raters in a second experiment performed the same task, except that the video clips were presented silently. &nbsp;The same six factors of emotion were found, suggesting that these dimensions of emotion are present both in the audio track and in the facial expressions alone.</p>\n<p><strong>Software</strong></p>\n<p>A new graphical user interface (FaceModelBuilder) for specifying Combined Appearance Models (CAMs) was written and is in beta testing. &nbsp;The software tracks facial expressions in real time as shown in Figure 1-b. &nbsp;The interface is written in PyQT so that it can be run on Macs, Windows PCs, and Linux machines. &nbsp;FaceModelBuilder is in beta testing at the University of Virginia and the Max Planck Institute for Human Development in Berlin. &nbsp;FaceModelBuilder is under the Apache 2.0 license so that, once it is released, it will be free and open source.</p>\n<p><strong>Tracking Emotion</strong></p>\n<p>A computer algorithm was developed that uses the CAM software to track facial expressions and output an estimate of the previously described six emotion factors. &nbsp;This effectively maps the correlated space of emotion words onto a person--specific model of facial expression as shown in Figure 1-c. &nbsp;The method is fast and can be used to generate six emotion scores for each video frame in real time. &nbsp;We are now working on validating and improving the ratings from this method. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/06/2013<br>\n\t\t\t\t\tModified by: Steven&nbsp;M&nbsp;Boker</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2013/1030806/1030806_10049485_1383762634904_4A_03_03_P1_3up--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2013/1030806/1030806_10049485_1383762634904_4A_03_03_P1_3up--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2013/1030806/1030806_10049485_1383762634904_4A_03_03_P1_3up--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">(a) A video still frame from a video clip seen by the raters.  (b) The CAM software tracks the facial expression.  (c) The emotion tracking algorithm produces estimates of six different emotion factors.</div>\n<div class=\"imageCredit\">(c) 2013 Steven M. Boker</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Steven&nbsp;M&nbsp;Boker</div>\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThin Slice Video Ratings of Emotion\n\nUntrained raters saw short five second videos of people they had never met and were asked to rate what the person was feeling by saying how accurate were statements such as, \"The person in the video felt happy.\"  The videos were extracts from real conversations between unacquainted young adults and showed only the one of the two people in the conversation as in Figure 1-a.  The raters tended to agree about what the person in the video was feeling during that five seconds.  A statistical method called factor analyses with oblique rotation found six factors of emotion used by the raters:  (1) Anger, (2) Joy, (3) Anxiety, (4) Sadness, (5) Shame, (6) Compassion.  Raters in a second experiment performed the same task, except that the video clips were presented silently.  The same six factors of emotion were found, suggesting that these dimensions of emotion are present both in the audio track and in the facial expressions alone.\n\nSoftware\n\nA new graphical user interface (FaceModelBuilder) for specifying Combined Appearance Models (CAMs) was written and is in beta testing.  The software tracks facial expressions in real time as shown in Figure 1-b.  The interface is written in PyQT so that it can be run on Macs, Windows PCs, and Linux machines.  FaceModelBuilder is in beta testing at the University of Virginia and the Max Planck Institute for Human Development in Berlin.  FaceModelBuilder is under the Apache 2.0 license so that, once it is released, it will be free and open source.\n\nTracking Emotion\n\nA computer algorithm was developed that uses the CAM software to track facial expressions and output an estimate of the previously described six emotion factors.  This effectively maps the correlated space of emotion words onto a person--specific model of facial expression as shown in Figure 1-c.  The method is fast and can be used to generate six emotion scores for each video frame in real time.  We are now working on validating and improving the ratings from this method.  \n\n \n\n\t\t\t\t\tLast Modified: 11/06/2013\n\n\t\t\t\t\tSubmitted by: Steven M Boker"
 }
}