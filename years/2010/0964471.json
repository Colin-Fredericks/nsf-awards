{
 "awd_id": "0964471",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "DC:  Medium:  Tackling and Understanding Intermediate Data in Cloud Applications as a First-Class Citizen",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2010-06-28",
 "awd_max_amd_letter_date": "2010-06-28",
 "awd_abstract_narration": "Cloud computing infrastructures involve thousands of servers, petabytes of storage, and hundreds of users running various applications that involve gigabytes to terabytes of data. This project focuses on intermediate data that is generated during the execution of parallelized dataflow programs in clouds. Such cloud intermediate data brings forth several unique characteristics: they are massive-scale, distributed, subjected to computational barriers, and prolong job run-times when subjected to server failures. Further, the size of intermediate data in a cloud application is often comparable to or larger than input or output data size, and it can thus range in terabytes. Thus, in spite of extensive existing work on traditional storage problems, there is a critical need for new algorithms and systems that target cloud intermediate data.\r\n\tThis project is the first to treat cloud intermediate data as a first-class citizen. The project will involve new algorithm design and analysis, original systems building and implementation, deployment in real world testbeds, and performance of measurement studies. Concretely, this project will build a new system that explicitly manages intermediate data in cloud dataflow programs in order to improve their fault-tolerance, and design and realize barrier relaxation strategies to improve performance of cloud programs. We will implement using open software, deploy, and experimentally evaluate our systems atop the NSF infrastructure called the Cloud Computing Testbed (CCT) that is hosted at the University of Illinois. Finally, we will perform measurement studies of workload characteristics of cloud intermediate data. \r\n\tA fuller understanding of intermediate data in clouds can spawn research in managing cloud infrastructures, improve run-time performance of cloud applications, and lead to new cloud programming paradigms. Our contributions will directly improve the performance and fault-tolerance of applications that are run on the community infrastructure CCT, and positively impact design and deployment of existing and emerging industry clouds. Our results will be published and released in open software and datasets.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Indranil",
   "pi_last_name": "Gupta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Indranil Gupta",
   "pi_email_addr": "indy@illinois.edu",
   "nsf_id": "000148881",
   "pi_start_date": "2010-06-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Roy",
   "pi_last_name": "Campbell",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Roy H Campbell",
   "pi_email_addr": "rhc@illinois.edu",
   "nsf_id": "000309848",
   "pi_start_date": "2010-06-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "779300",
   "pgm_ele_name": "DATA-INTENSIVE COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7793",
   "pgm_ref_txt": "DATA-INTENSIVE COMPUTING"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 600000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Big data needs to be processed and stored efficiently. Today, large clusters of servers (in datacenters) run distributed software systems such as Hadoop, NoSQL databases, etc. in order to achieve this. Computation systems like Hadoop take as input large amounts of data, and after processing, produce large amounts of output data. While most research focuses on input and output data, very little work has existed (until this project) on the intermediate data. This is the data that is generated during the processing, in between input data and output data. It is an important focus because better and efficient management of this intermediate data is critical to faster computation (generating output quickly), better usage of resources (using fewer servers to do the same work), and better management of these systems (making system administrators' job of managing these clusters, easier).</p>\n<p>This NSF project project has designed, implemented, deployed, and experimentally evaluated, multiple systems that deal with intermediate data in distributed software systems used in the cloud. Our work has involved design of new algorithms, their implementation into existing and popular open-source systems in industry (e.g., Hadoop, NoSQL systems, etc.), and their experimental evaluation on real clusters. Below, we describe technical details of our salient contributions below, inlined with their impact.</p>\n<p>We have built systems that manage intermediate data in Hadoop and graph processing systems more efficiently, speeding up these systems significantly (thus increasing how many jobs can be run by the same set of servers in a given time) -- these include our ISS system (which tolerates server failures) and our barrier-breaking version of Hadoop (which reduces run time for jobs). We have built systems that allow users to specifiy deadlines and priorities for Hadoop jobs and schedule jobs intelligently to meet these deadlines (Natjam) -- this is critical for industry use cases such as advertisement processing in companies, and reduces the capital expenses of running such infrastructures (via consolidation of clusters). We have measured and characterized the traits of intermediate and file system data in HDFS (the file system underneath Hadoop), thus making it easier for companies and sysadmins to understand how this data behaves and build systems better attuned to these real traits.</p>\n<p>We have built new software (LFGraph) for distributed processing of graphs (e.g., Facebook-style social graphs or Internet graphs), in a way that is orders of magnitude faster than existing graph processing systems. We have built a system (Zorro) that make graph processing systems efficient while also fault-tolerant by opportunistically scrounging data after a failure and achieving very high accuracy.</p>\n<p>Second, in the area of NoSQL storage systems we have built new algorithms that compact intermediate data more efficiently, thus improving read performance in these systems. We have also built metadata management systems for NoSQL databases, which make it easier for system administrators to manage these systems -- besides making their job easier, this also reduces operational expenses of these infrastructures.&nbsp;</p>\n<p>Our software systems are available for open download on our website (http://dprg.cs.uiuc.edu/downloads). As most of our work deals directly with, and has made changes directly to (open-source) software that are used widely in industry today, the potential long-term impact of this project's results is very high.</p>\n<p>Finally, on the education front, during Spring 2015, PI Indranil Gupta has taught 2 MOOCs (free online courses) on Coursera about \"Cloud Computing Concepts\". The courses were wildly popular, with an aggregate enrolment of 124K students. co-PI Roy Campbell is offering a Coursera MOOC on \"Cloud Computing Applications\" in Fall 2015. This is in addition to the regula...",
  "por_txt_cntn": "\nBig data needs to be processed and stored efficiently. Today, large clusters of servers (in datacenters) run distributed software systems such as Hadoop, NoSQL databases, etc. in order to achieve this. Computation systems like Hadoop take as input large amounts of data, and after processing, produce large amounts of output data. While most research focuses on input and output data, very little work has existed (until this project) on the intermediate data. This is the data that is generated during the processing, in between input data and output data. It is an important focus because better and efficient management of this intermediate data is critical to faster computation (generating output quickly), better usage of resources (using fewer servers to do the same work), and better management of these systems (making system administrators' job of managing these clusters, easier).\n\nThis NSF project project has designed, implemented, deployed, and experimentally evaluated, multiple systems that deal with intermediate data in distributed software systems used in the cloud. Our work has involved design of new algorithms, their implementation into existing and popular open-source systems in industry (e.g., Hadoop, NoSQL systems, etc.), and their experimental evaluation on real clusters. Below, we describe technical details of our salient contributions below, inlined with their impact.\n\nWe have built systems that manage intermediate data in Hadoop and graph processing systems more efficiently, speeding up these systems significantly (thus increasing how many jobs can be run by the same set of servers in a given time) -- these include our ISS system (which tolerates server failures) and our barrier-breaking version of Hadoop (which reduces run time for jobs). We have built systems that allow users to specifiy deadlines and priorities for Hadoop jobs and schedule jobs intelligently to meet these deadlines (Natjam) -- this is critical for industry use cases such as advertisement processing in companies, and reduces the capital expenses of running such infrastructures (via consolidation of clusters). We have measured and characterized the traits of intermediate and file system data in HDFS (the file system underneath Hadoop), thus making it easier for companies and sysadmins to understand how this data behaves and build systems better attuned to these real traits.\n\nWe have built new software (LFGraph) for distributed processing of graphs (e.g., Facebook-style social graphs or Internet graphs), in a way that is orders of magnitude faster than existing graph processing systems. We have built a system (Zorro) that make graph processing systems efficient while also fault-tolerant by opportunistically scrounging data after a failure and achieving very high accuracy.\n\nSecond, in the area of NoSQL storage systems we have built new algorithms that compact intermediate data more efficiently, thus improving read performance in these systems. We have also built metadata management systems for NoSQL databases, which make it easier for system administrators to manage these systems -- besides making their job easier, this also reduces operational expenses of these infrastructures. \n\nOur software systems are available for open download on our website (http://dprg.cs.uiuc.edu/downloads). As most of our work deals directly with, and has made changes directly to (open-source) software that are used widely in industry today, the potential long-term impact of this project's results is very high.\n\nFinally, on the education front, during Spring 2015, PI Indranil Gupta has taught 2 MOOCs (free online courses) on Coursera about \"Cloud Computing Concepts\". The courses were wildly popular, with an aggregate enrolment of 124K students. co-PI Roy Campbell is offering a Coursera MOOC on \"Cloud Computing Applications\" in Fall 2015. This is in addition to the regular on-campus courses that the PI and co-PI teach about distributed systems, advanced distributed systems..."
 }
}