{
 "awd_id": "1012763",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "TC:Large:Nudging Users Towards Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Fen Zhao",
 "awd_eff_date": "2010-07-01",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 2683663.0,
 "awd_amount": 2749662.0,
 "awd_min_amd_letter_date": "2010-06-16",
 "awd_max_amd_letter_date": "2013-08-12",
 "awd_abstract_narration": "Making the ``right'' privacy decision --- that is, balancing revelation and protection of personal information in ways that maximize a user's welfare --- is difficult. The complexity is such that our judgments in this area are prone to errors, leading to decisions that we may later stand to regret. These errors stem from lack of information or computational ability; but also from problems of self-control and limited self-insight. Our research focuses on designing and testing systems that anticipate and counter cognitive and behavioral biases that hamper users' privacy (as well as security) decision making. Our approach is informed by the growing body of behavioral economics research on ?soft,? or asymmetric, paternalism, as well as by research in behavioral decision research and usability. Inspired by these streams of research, we design and study systems that ``nudge'' users towards certain privacy or security behaviors ? which the users themselves have stated to prefer, or which empirical evidence has demonstrated to be beneficial. Helping users avoid mistakes, decrease regret, and achieve more rapidly the desired balance between sharing and protecting personal information has clear, and significant, societal importance. However, our research will also inform the work of privacy (and security) technologists and policy makers by advancing our understanding of what makes privacy decision making difficult, and how to counter biases that adversely affect privacy- and security-sensitive behavior.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alessandro",
   "pi_last_name": "Acquisti",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alessandro Acquisti",
   "pi_email_addr": "acquisti@andrew.cmu.edu",
   "nsf_id": "000487138",
   "pi_start_date": "2010-06-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Norman",
   "pi_last_name": "Sadeh",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Norman M Sadeh",
   "pi_email_addr": "sadeh@cs.cmu.edu",
   "nsf_id": "000468857",
   "pi_start_date": "2010-06-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lorrie",
   "pi_last_name": "Cranor",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Lorrie F Cranor",
   "pi_email_addr": "lorrie@acm.org",
   "nsf_id": "000252230",
   "pi_start_date": "2010-06-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "779500",
   "pgm_ele_name": "TRUSTWORTHY COMPUTING"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7795",
   "pgm_ref_txt": "TRUSTWORTHY COMPUTING"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 495299.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 539388.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 1133461.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 581514.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project focused on the study, design, and testing of systems that 1) anticipate cognitive and behavioral biases that may hamper users&rsquo; privacy and security decision making, and that 2) try to overcome those biases. Such systems can help and assist privacy and security decision making by &ldquo;nudging&rdquo; users towards certain behaviors that the users themselves have claimed to prefer, or which sound empirical evidence has demonstrated to be preferable, from a privacy perspective - an approach inspired by the growing body of behavioral research on &ldquo;soft&rdquo; paternalism.</p>\n<p>Across the years, our research focused on several sub-areas, including: the behavioral analysis of privacy decision making; the investigation of users&rsquo; regrets associated with disclosure behavior on social media; the design and exploration of nudging interventions for Web 2.0 and mobile applications; the analysis of privacy and security choices by smartphone app developers; the investigation of individuals&rsquo; self-commitment and goal setting behaviors in social media context; the exploration of users&rsquo; benefit accrued from data sharing. In particular, much of our early work under the project focused on understanding hurdles faced by individuals in online privacy and security decision making, and developing ways of effectively assisting individuals in their privacy and security decisions. Through our studies, we gained insights into the impact of cognitive and behavioral biases on online security and privacy decision making. Then, we started designing and testing interventions aimed at helping users make &ldquo;better&rdquo; online security and privacy decisions&mdash;that is, decisions that minimize adverse outcomes or are less likely to be regretted. In doing so, we posited that many diverse efforts in the field of human computer interaction can be largely viewed as implementations of soft paternalistic concepts, whereby interventions are intended to gently guide users toward safer practices rather than imposing particular decisions. We suggested that prior work on the design of user interface technologies for security and privacy can be examined from a nudging perspective: every design decision potentially nudges users in one direction or another. Ultimately, our research provided evidence that online interfaces can nudge individuals either toward more protective behaviors or, in fact, sometimes toward riskier ones. Our research also explored practical and ethical questions associated with nudging for security and privacy.</p>\n<p>Across the several sub-areas of the project, we designed various apps and ran numerous experiments whose results were disseminated across journals, conference proceedings, workshop presentations, and keynotes. The projects have brought together faculty members, post-doctoral students, PhD students, Master students, and undergraduate students, offering extensive mentoring opportunities. Many of the students who worked on the project have now faculty positions or industry research positions. The results of the studies caught the attention and interest of policy makers (the PIs have been involved in discussions with policy-makers on issues associated to the behavioral bias and privacy nudges investigated under this project) and industry (influencing the deployment of privacy nudges in existing online services).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2017<br>\n\t\t\t\t\tModified by: Alessandro&nbsp;Acquisti</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project focused on the study, design, and testing of systems that 1) anticipate cognitive and behavioral biases that may hamper users? privacy and security decision making, and that 2) try to overcome those biases. Such systems can help and assist privacy and security decision making by \"nudging\" users towards certain behaviors that the users themselves have claimed to prefer, or which sound empirical evidence has demonstrated to be preferable, from a privacy perspective - an approach inspired by the growing body of behavioral research on \"soft\" paternalism.\n\nAcross the years, our research focused on several sub-areas, including: the behavioral analysis of privacy decision making; the investigation of users? regrets associated with disclosure behavior on social media; the design and exploration of nudging interventions for Web 2.0 and mobile applications; the analysis of privacy and security choices by smartphone app developers; the investigation of individuals? self-commitment and goal setting behaviors in social media context; the exploration of users? benefit accrued from data sharing. In particular, much of our early work under the project focused on understanding hurdles faced by individuals in online privacy and security decision making, and developing ways of effectively assisting individuals in their privacy and security decisions. Through our studies, we gained insights into the impact of cognitive and behavioral biases on online security and privacy decision making. Then, we started designing and testing interventions aimed at helping users make \"better\" online security and privacy decisions&mdash;that is, decisions that minimize adverse outcomes or are less likely to be regretted. In doing so, we posited that many diverse efforts in the field of human computer interaction can be largely viewed as implementations of soft paternalistic concepts, whereby interventions are intended to gently guide users toward safer practices rather than imposing particular decisions. We suggested that prior work on the design of user interface technologies for security and privacy can be examined from a nudging perspective: every design decision potentially nudges users in one direction or another. Ultimately, our research provided evidence that online interfaces can nudge individuals either toward more protective behaviors or, in fact, sometimes toward riskier ones. Our research also explored practical and ethical questions associated with nudging for security and privacy.\n\nAcross the several sub-areas of the project, we designed various apps and ran numerous experiments whose results were disseminated across journals, conference proceedings, workshop presentations, and keynotes. The projects have brought together faculty members, post-doctoral students, PhD students, Master students, and undergraduate students, offering extensive mentoring opportunities. Many of the students who worked on the project have now faculty positions or industry research positions. The results of the studies caught the attention and interest of policy makers (the PIs have been involved in discussions with policy-makers on issues associated to the behavioral bias and privacy nudges investigated under this project) and industry (influencing the deployment of privacy nudges in existing online services).\n\n\t\t\t\t\tLast Modified: 09/01/2017\n\n\t\t\t\t\tSubmitted by: Alessandro Acquisti"
 }
}