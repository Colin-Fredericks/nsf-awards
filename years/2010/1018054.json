{
 "awd_id": "1018054",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC:  Small:  Navigating the Aural Web",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2010-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 424311.0,
 "awd_amount": 472311.0,
 "awd_min_amd_letter_date": "2010-07-28",
 "awd_max_amd_letter_date": "2013-05-07",
 "awd_abstract_narration": "The PI's goal in this project is to establish advanced design strategies for the aural navigation of complex Web information architectures, where users exclusively or primarily listen to, rather than look at, content and navigational prompts.   Conventional on-screen visual displays may not work well, if at all, in many situations.  The most obvious instances occur when persons who are blind or visually impaired need to use technologies designed for sighted users.  A much more common situation, however, occurs with users of mobile devices.  These users are often engaged in another activity (e.g., walking around a city or driving a car) where it is inconvenient, distracting or even dangerous to continuously look at the screen.  On the one hand, Web accessibility guidelines have focused on ensuring that websites are readable by assistive technologies (screen readers).  On the other hand, recent aural browsers enable a more intelligent visual-to-aural transformation of specific features of Web pages.  These advances, however, do not fully address the larger issue of aurally navigating complex information architectures.  Previous work by the PI has shown that an effective aural experience requires the elaboration of new navigation patterns in order to overcome the limits imposed by the linearity of the aural medium.  The PI will build on these preliminary results in the current project, in order to provide an advanced level of usability for audio-based web interactions.  He will explore conceptual design patterns for aural navigation, by iteratively creating and refining aural design strategies inspired by the structural paradigms of human dialogues for back and history navigation, and browsing in large collections.  A series of evaluation studies involving both visually impaired participants using screen readers and sighted participants using mobile devices will assess the potential and limits of the aural navigation paradigms to enhance the effectiveness of Web navigation.  \r\n.\r\nBroader Impacts:  This project will directly involve blind users in the design and evaluation of new aural design strategies, in collaboration with the Indiana School for the Blind and Visually Impaired (ISBVI).  Project outcomes will yield a better understanding of the design issues and solutions for aural navigation, which will provide a solid long-term intellectual basis for the creation of better applications for visually-impaired users and for audio-only navigation contexts.   In addition, the PI will work proactively to expand student participation in the research enterprise on his campus, by establishing user experience undergraduate labs.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Davide",
   "pi_last_name": "Bolchini",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Davide P Bolchini",
   "pi_email_addr": "dbolchin@iupui.edu",
   "nsf_id": "000511635",
   "pi_start_date": "2010-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "107 S INDIANA AVE",
  "perf_city_name": "BLOOMINGTON",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474057000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 144088.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 296223.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objective of this project is to establish and evaluate design strategies for aural navigation in complex information architectures that can provide an advanced level of usability for audio-based web interactions. The rationale for our work is that a more complete understanding of the design issues and corresponding solutions involved in aural navigation provides a solid, long-term intellectual basis for the creation of substantially better applications for visually-impaired users and for audio-only usage contexts. The project generated four major results:</p>\n<p>1. TOPIC- AND LIST-BASED BACK BROWSING SHORTCUTS FOR AURAL NAVIGATION FOR MOBILE AND SCREEN-READER USERS. Aural navigation is crucial for two contexts. First, it benefits mobile users navigating the web while unable to look at the screen, generally while engaged in a secondary task (such as walking, running or driving); second, it also enables the blind and visually impaired to browse a website more effectively byleveraging their auditory senses. Most importantly, the basic function of &ldquo;back&rdquo; navigation is very inefficient in the aural mode because it forces users to listen to part of the content of each previous page to retrieve the desired content. To address this problem, we introduced topic- and list-based &ldquo;back&rdquo;, two strategies to enhance aural web browsing while on-the-go. In an experiment with mobile phone users, study participants navigating with topic and list-based &ldquo;back&rdquo; completed tasks respectively 18% and 25% faster &ndash; and reported a better navigation experience &ndash; than those using the back button provided by the web browser. We hypothesized that the same strategies enhancing aural mobile navigation could also benefit visually-impaired users in browsing the web with screen readers (text-to-speech software that &ldquo;reads aloud&rdquo; a web page). In our study on topic- and list-based back conducted at the Indiana School for the Blind and Visually Impaired (IBSVI), blind users leveraging topic-based back reached previously visited pages 40% faster than those who relied on existing mechanisms. Participants navigating with list-based back completed tasks 79% faster than those who used the traditional back mechanisms.</p>\n<p>2. AURAL FLOWS FOR MOBILE WEB BROWSING. Because large websites exhibit a complex hypertextual and hierarchical structure, simply &ldquo;reading aloud&rdquo; a mobile website weakens the user&rsquo;s orientation in the site information architecture. To address this problem, we introduced techniques to linearize the information architecture of a website into aural flows: dynamic, user-controllable concatenations of web content to be listened to on-the-go, instead of looked at, inspired to the notion of &ldquo;playlist&rdquo; for the music experience. To test this, we contributed and evaluated ANFORA (Aural Navigation Flows on Rich Architectures), a novel framework and systemprototype that generates real-time aural flows (rendered through text-to-speech) from existing websites, thus optimizing them for the user experience on-the-go. For example, with ANFORA users can browse the latest news on the go from NPR in semi-aural mode (listening to and occasionally looking at the screen), and conveniently consume web content as audio playlists. The results of this work informed several referred papers, public showcases, and a provisional U.S.patent.</p>\n<p>3. AURAL FAST BROWSING WITH GUIDED TOURS. Navigating back and forth from a list of links (index) to its target pages is common on the web, but tethers screen-reader users to unnecessary cognitive and mechanical steps. This problem worsens when indexes lack information scent: cues that enable users to select a link with confidence during fact finding. We investigated how blind users who navigate the web with screen readers can bypass a scentless index with guided tours: a much simpler b...",
  "por_txt_cntn": "\nThe objective of this project is to establish and evaluate design strategies for aural navigation in complex information architectures that can provide an advanced level of usability for audio-based web interactions. The rationale for our work is that a more complete understanding of the design issues and corresponding solutions involved in aural navigation provides a solid, long-term intellectual basis for the creation of substantially better applications for visually-impaired users and for audio-only usage contexts. The project generated four major results:\n\n1. TOPIC- AND LIST-BASED BACK BROWSING SHORTCUTS FOR AURAL NAVIGATION FOR MOBILE AND SCREEN-READER USERS. Aural navigation is crucial for two contexts. First, it benefits mobile users navigating the web while unable to look at the screen, generally while engaged in a secondary task (such as walking, running or driving); second, it also enables the blind and visually impaired to browse a website more effectively byleveraging their auditory senses. Most importantly, the basic function of \"back\" navigation is very inefficient in the aural mode because it forces users to listen to part of the content of each previous page to retrieve the desired content. To address this problem, we introduced topic- and list-based \"back\", two strategies to enhance aural web browsing while on-the-go. In an experiment with mobile phone users, study participants navigating with topic and list-based \"back\" completed tasks respectively 18% and 25% faster &ndash; and reported a better navigation experience &ndash; than those using the back button provided by the web browser. We hypothesized that the same strategies enhancing aural mobile navigation could also benefit visually-impaired users in browsing the web with screen readers (text-to-speech software that \"reads aloud\" a web page). In our study on topic- and list-based back conducted at the Indiana School for the Blind and Visually Impaired (IBSVI), blind users leveraging topic-based back reached previously visited pages 40% faster than those who relied on existing mechanisms. Participants navigating with list-based back completed tasks 79% faster than those who used the traditional back mechanisms.\n\n2. AURAL FLOWS FOR MOBILE WEB BROWSING. Because large websites exhibit a complex hypertextual and hierarchical structure, simply \"reading aloud\" a mobile website weakens the user\u00c6s orientation in the site information architecture. To address this problem, we introduced techniques to linearize the information architecture of a website into aural flows: dynamic, user-controllable concatenations of web content to be listened to on-the-go, instead of looked at, inspired to the notion of \"playlist\" for the music experience. To test this, we contributed and evaluated ANFORA (Aural Navigation Flows on Rich Architectures), a novel framework and systemprototype that generates real-time aural flows (rendered through text-to-speech) from existing websites, thus optimizing them for the user experience on-the-go. For example, with ANFORA users can browse the latest news on the go from NPR in semi-aural mode (listening to and occasionally looking at the screen), and conveniently consume web content as audio playlists. The results of this work informed several referred papers, public showcases, and a provisional U.S.patent.\n\n3. AURAL FAST BROWSING WITH GUIDED TOURS. Navigating back and forth from a list of links (index) to its target pages is common on the web, but tethers screen-reader users to unnecessary cognitive and mechanical steps. This problem worsens when indexes lack information scent: cues that enable users to select a link with confidence during fact finding. We investigated how blind users who navigate the web with screen readers can bypass a scentless index with guided tours: a much simpler browsing pattern that linearly concatenates items of a collection. In a study at the Indiana School for the Blind and Visually Impaired (ISBVI), guided tours l..."
 }
}