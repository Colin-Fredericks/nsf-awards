{
 "awd_id": "1018863",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: A New Voice Source Model: From Glottal Areas to Better Speech Synthesis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 458000.0,
 "awd_min_amd_letter_date": "2010-07-29",
 "awd_max_amd_letter_date": "2011-06-15",
 "awd_abstract_narration": "The goal of the proposed research is to develop and evaluate a new voice\r\nsource model based on physiological observations of the vocal folds of 30 adult speakers. Shortcomings of existing source models can be in part attributed to the way in which they were developed: based on limited data from a few speakers, without direct physiological observations, and without perceptual validation. A larger dataset would help in not only developing a source model that could account for a range of voice qualities within and across speakers, but also result in an understanding of how and which model parameter(s) are speaker and/or gender specific. Model development will consider the perceptual effects of the model's parameters from the earliest stages.\r\nA better source model might also improve the performance of speech processing algorithms such as text-to-speech synthesis (TTS). Typically in the development of such algorithms, the emphasis has been on acoustic features related to the speech spectral envelope. The acoustics of the voice source, on the other hand, have received less attention. \r\nThe proposed work involves: 1) recording high-speed images of vocal fold\r\nvibrations with simultaneous audio recordings from 15 male and 15 female speakers, 2) extracting glottal area functions from the images to parameterize a new voice source model, 3) performing perception experiments to uncover which model parameters are perceptually salient, and 4) using the new voice source model in TTS. \r\nThe project's interdisciplinary team (with expertise in modeling, synthesis, recognition, phonetics, and psycholinguistics) is uniquely qualified to conduct this transformative research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Abeer",
   "pi_last_name": "Alwan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Abeer Alwan",
   "pi_email_addr": "alwan@ee.ucla.edu",
   "nsf_id": "000090848",
   "pi_start_date": "2010-07-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Patricia",
   "pi_last_name": "Keating",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Patricia A Keating",
   "pi_email_addr": "keating@humnet.ucla.edu",
   "nsf_id": "000092027",
   "pi_start_date": "2010-07-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jody",
   "pi_last_name": "Kreiman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jody Kreiman",
   "pi_email_addr": "jkreiman@ucla.edu",
   "nsf_id": "000441064",
   "pi_start_date": "2010-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "10889 WILSHIRE BLVD STE 700",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900244200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 163326.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 294674.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we investigate how the human vocal folds vibrate when producing sound. To that end, we used a high-speed camera to record vocal cord vibrations for 9 speakers (4 males and 5 females). These recordings were done simultaneously with microphone recordings of the speech sounds so that the images and voice samples can be compared.&nbsp; These data are important for understanding how voice is produced, and how changes in voice production affect the sound of a person&rsquo;s voice.</p>\n<p><strong>Outcomes:</strong></p>\n<p><strong>1) Produced a database of high-speed digital image recordings of vocal fold vibrations with simultaneous acoustic recordings. </strong>This database will be made freely available next year through the Linguistic Data Consortium.</p>\n<p><strong>2) Developed a software tool, the &lsquo;Glottaltopogram&rsquo;, which is available freely on our website for anyone to use. </strong>With sampling rates of thousands of frames per second, high-speed videoendoscopy produces a large amount of data that is difficult to analyze subjectively.<strong> </strong>The software is a method of analyzing high-speed images of vocal fold vibrations. This tool reveals the overall synchronization of the vibrational patterns of the vocal folds over the entire laryngeal area. Experimental results showed that this method is effective in visualizing pathological and normal vocal fold vibratory patterns.</p>\n<p>&nbsp;<strong>3) Analyzed Physiological and Acoustic Measurements. </strong>We examined the relationships between physiological measurements of the voice source and acoustic measurements recorded by the microphone. We also analyzed inter- and intra-speaker variability.&nbsp;Based on the analyses and motivated by data from high-speed laryngeal videoendoscopy, a new voice source model was developed to capture perceptually-important aspects. This model could be used to improve synthetic speech voices.</p>\n<p>&nbsp;<strong>4) Conducted Perceptual Validation of Model Parameters.</strong>&nbsp;Many glottal source models have been proposed, but none has been systematically validated perceptually. Our new model, along with four other source models, was fitted to 40 natural voice sources (20 male and 20 female). We generated synthetic copies of the voices using each modeled source pulse, with all other synthesis parameters held constant, and then conducted experiments in which listeners assessed the extent of perceived similarity between the target voice samples and each copy. Results showed that the proposed model provided a more accurate fit and a better perceptual match to the target than did the other models</p>\n<p>&nbsp;</p>\n<p><strong>Broader Impact:</strong> The project has provided valuable research experience for two&nbsp;PhD students in Electrical Engineering, and a PhD student in Linguistics. These students have been exposed to issues related to signal acquisition,&nbsp;high-speed imaging, image and acoustic analysis, voice source modeling,&nbsp;speech synthesis, perceptual experiments, cross-linguistic issues, and&nbsp;intra- and inter-speaker variation in speech production.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>The project also included community outreach in the form of participation in World Voice Day 2014. Public exhibits on the UCLA campus, staffed by project participants and by students, presented scientific, engineering, health, and artistic aspects of the study of the human voice. The event was very successful and was covered in the campus newspaper.</p>\n<p>&nbsp;</p>\n<p>All publications have been posted on our webpage and presented at leading conferences in Engineering, Computer Science, Speech Science and Linguistics (e.g., ICASSP, Interspeech, ASA) and journals. The high-speed video analysis tool \"GTG tool\" has been posted on the project's webpage.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/20/2015<br>\n\t\t\t\t\tModif...",
  "por_txt_cntn": "\nIn this project, we investigate how the human vocal folds vibrate when producing sound. To that end, we used a high-speed camera to record vocal cord vibrations for 9 speakers (4 males and 5 females). These recordings were done simultaneously with microphone recordings of the speech sounds so that the images and voice samples can be compared.  These data are important for understanding how voice is produced, and how changes in voice production affect the sound of a person\u00c6s voice.\n\nOutcomes:\n\n1) Produced a database of high-speed digital image recordings of vocal fold vibrations with simultaneous acoustic recordings. This database will be made freely available next year through the Linguistic Data Consortium.\n\n2) Developed a software tool, the \u00e6Glottaltopogram\u00c6, which is available freely on our website for anyone to use. With sampling rates of thousands of frames per second, high-speed videoendoscopy produces a large amount of data that is difficult to analyze subjectively. The software is a method of analyzing high-speed images of vocal fold vibrations. This tool reveals the overall synchronization of the vibrational patterns of the vocal folds over the entire laryngeal area. Experimental results showed that this method is effective in visualizing pathological and normal vocal fold vibratory patterns.\n\n 3) Analyzed Physiological and Acoustic Measurements. We examined the relationships between physiological measurements of the voice source and acoustic measurements recorded by the microphone. We also analyzed inter- and intra-speaker variability. Based on the analyses and motivated by data from high-speed laryngeal videoendoscopy, a new voice source model was developed to capture perceptually-important aspects. This model could be used to improve synthetic speech voices.\n\n 4) Conducted Perceptual Validation of Model Parameters. Many glottal source models have been proposed, but none has been systematically validated perceptually. Our new model, along with four other source models, was fitted to 40 natural voice sources (20 male and 20 female). We generated synthetic copies of the voices using each modeled source pulse, with all other synthesis parameters held constant, and then conducted experiments in which listeners assessed the extent of perceived similarity between the target voice samples and each copy. Results showed that the proposed model provided a more accurate fit and a better perceptual match to the target than did the other models\n\n \n\nBroader Impact: The project has provided valuable research experience for two PhD students in Electrical Engineering, and a PhD student in Linguistics. These students have been exposed to issues related to signal acquisition, high-speed imaging, image and acoustic analysis, voice source modeling, speech synthesis, perceptual experiments, cross-linguistic issues, and intra- and inter-speaker variation in speech production. \n\n \n\n \n\nThe project also included community outreach in the form of participation in World Voice Day 2014. Public exhibits on the UCLA campus, staffed by project participants and by students, presented scientific, engineering, health, and artistic aspects of the study of the human voice. The event was very successful and was covered in the campus newspaper.\n\n \n\nAll publications have been posted on our webpage and presented at leading conferences in Engineering, Computer Science, Speech Science and Linguistics (e.g., ICASSP, Interspeech, ASA) and journals. The high-speed video analysis tool \"GTG tool\" has been posted on the project's webpage.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/20/2015\n\n\t\t\t\t\tSubmitted by: Abeer A Alwan"
 }
}