{
 "awd_id": "0958585",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "II-NEW: Hadoop cluster acquisition, deployment and training for speech and language processing",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2010-04-01",
 "awd_exp_date": "2012-10-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 418500.0,
 "awd_min_amd_letter_date": "2010-02-16",
 "awd_max_amd_letter_date": "2011-03-08",
 "awd_abstract_narration": "This project aims to educate, train and equip graduate students in what is becoming a critical paradigm in speech and NLP: distributed algorithms, a paradigm pursued by Google with great success. This distributed processing paradigm represents more than just a computational convenience, but rather an approach for designing algorithms to optimize performance within such an environment, yielding massive improvements over standard algorithms directly deployed in parallel.\r\n\r\nThe objectives of the current institutional infrastructure proposal are to (1) acquire an extensive (384 core) cluster of processors for use as a Hadoop cluster at the Center for Spoken Language Understanding (CSLU) at OHSU; (2) integrate the cluster within the existing computing infrastructure; and (3) develop educational resources (tutorials, lab sessions, course modules and seminars) focused on both ``how-to'' information for using the Hadoop cluster and more general topics in algorithms for distributed computing. At CSLU -- part of the Division of Biomedical Computer Science at OHSU -- nearly all of the problems are within the scope of basic or applied NLP or speech processing research.\r\n\r\nApart from training graduate students via both course-work and work on research projects, the infrastructure created in this project will contribute to advances in speech processing and NLP and applications that make use of these technologies, including national defense applications in text and speech mining along with biomedical applications. It will enable at least eight funded NSF projects and at least five projects from other agencies to pursue novel directions in their data analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Izhak",
   "pi_last_name": "Shafran",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Izhak Shafran",
   "pi_email_addr": "zakshafran@gmail.com",
   "nsf_id": "000096166",
   "pi_start_date": "2010-02-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Roark",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Brian E Roark",
   "pi_email_addr": "roarkbr@gmail.com",
   "nsf_id": "000434316",
   "pi_start_date": "2010-02-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon Health & Science University",
  "inst_street_address": "3181 SW SAM JACKSON PARK RD",
  "inst_street_address_2": "",
  "inst_city_name": "PORTLAND",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5034947784",
  "inst_zip_code": "972393011",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OR01",
  "org_lgl_bus_name": "OREGON HEALTH & SCIENCE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPSNT86JKN51"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon Health & Science University",
  "perf_str_addr": "3181 SW SAM JACKSON PARK RD",
  "perf_city_name": "PORTLAND",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "972393011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OR01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 400000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 18500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objectives of this NSF award were to:</p>\n<ol>\n<li>acquire and deploy a high-performance computing cluster;</li>\n<li>develop and teach a course on distributed computing.&nbsp;</li>\n</ol>\n<p>Both the objectives outlined in the proposal were met.&nbsp;</p>\n<p><strong>High-Performance Computing Cluster</strong></p>\n<p><span style=\"text-decoration: underline;\">Cluster Acquisition and Deployment</span>: After discussions with engineers from several major manufacturers, benchmarking demo units from two of them, we settled on a configuration with 14 nodes from Dell, each with four servers, each server with two sockets, and each socket with 6 CPU cores, in all 672 CPU cores. Each server was configured with 48GB memory, in all about 768GB of memory. The nodes in the cluster are networked via conventional gigabyte ethernet as well as high-speed low-latency Inifiniband network using 40GB QDR dual-port adapters and four 36 port switches organized in two level hierarchy.</p>\n<p><span style=\"text-decoration: underline;\">Hadoop File System and Map Reduce</span>: The new cluster has been configured to support distributed computing with map-reduce framework. The local disks on the nodes, 300GB on 15Krpm disks, have been configured using Hadoop, the public domain implementation of Google file system. This filesystem consists of one namenode, a secondary node and 54 datanodes that form the bulk of the storage, in all about 12TB. The distributed computing is supported through Hadoop, the public domain implementation of Google&rsquo;s map-reduce framework. The installation has been tested using standard benchmarks and performs as expected.</p>\n<p><span style=\"text-decoration: underline;\">Condor Queue Management</span>: Certain algorithms are not amenable to map-reduce framework and to support them we configured a queue management system with the open-source Condor package.</p>\n<p><strong>Research Outcomes</strong></p>\n<p>The new cluster has been operational for more than a year now and most of the compute intensive research at CSLU is performed on this cluster. As such more than a dozen journal papers and more than two dozen conference publications have benefitted from it so far. Several ongoing projects have already availed this new resource, of which three prominent ones are:</p>\n<p><span style=\"text-decoration: underline;\">2011 Johns Hopkins Summer Workshop</span>: One of the most immediate beneficiary of the new cluster was the summer project hosted at Johns Hopkins Center for Language and Speech Processing (CLSP). The workshop research group, working on &ldquo;Confusion-based Statistical Language Modeling for Machine Translation and Speech Recognition&rdquo;, was focused on confusion-based statistical language models that require large amounts of data to develop different algorithms for estimating model parameters. The cluster was utilized to generate ASR lattices for about 2000 hours of English conversational telephone speech, 450 hours of English broadcast news, and 200 hours of Arabic broadcast conversations. These word- lattices were extensively used in the 6-week workshop to investigate, develop and evaluate discriminative learning strategies for language modeling. For more details, see the final workshop report and the associated publications.</p>\n<p><span style=\"text-decoration: underline;\">Structural Variations in Breast Cancer</span>: The cluster has enabled our colleagues in computational biology to investigate novel algorithms for more accurately detecting large changes in DNA sequence &ndash; deletions, inversions and translocations &ndash; often seen in cancer. Accurate identification of these large structural variations (SVs) are expected to hasten advances in personalized medicine where interventions can target tumors with specific genomic aberrations. The problem of identifying these large structural variations (SVs) is computationally hard. To simplify the tas...",
  "por_txt_cntn": "\nThe objectives of this NSF award were to:\n\nacquire and deploy a high-performance computing cluster;\ndevelop and teach a course on distributed computing. \n\n\nBoth the objectives outlined in the proposal were met. \n\nHigh-Performance Computing Cluster\n\nCluster Acquisition and Deployment: After discussions with engineers from several major manufacturers, benchmarking demo units from two of them, we settled on a configuration with 14 nodes from Dell, each with four servers, each server with two sockets, and each socket with 6 CPU cores, in all 672 CPU cores. Each server was configured with 48GB memory, in all about 768GB of memory. The nodes in the cluster are networked via conventional gigabyte ethernet as well as high-speed low-latency Inifiniband network using 40GB QDR dual-port adapters and four 36 port switches organized in two level hierarchy.\n\nHadoop File System and Map Reduce: The new cluster has been configured to support distributed computing with map-reduce framework. The local disks on the nodes, 300GB on 15Krpm disks, have been configured using Hadoop, the public domain implementation of Google file system. This filesystem consists of one namenode, a secondary node and 54 datanodes that form the bulk of the storage, in all about 12TB. The distributed computing is supported through Hadoop, the public domain implementation of Google\u00c6s map-reduce framework. The installation has been tested using standard benchmarks and performs as expected.\n\nCondor Queue Management: Certain algorithms are not amenable to map-reduce framework and to support them we configured a queue management system with the open-source Condor package.\n\nResearch Outcomes\n\nThe new cluster has been operational for more than a year now and most of the compute intensive research at CSLU is performed on this cluster. As such more than a dozen journal papers and more than two dozen conference publications have benefitted from it so far. Several ongoing projects have already availed this new resource, of which three prominent ones are:\n\n2011 Johns Hopkins Summer Workshop: One of the most immediate beneficiary of the new cluster was the summer project hosted at Johns Hopkins Center for Language and Speech Processing (CLSP). The workshop research group, working on \"Confusion-based Statistical Language Modeling for Machine Translation and Speech Recognition\", was focused on confusion-based statistical language models that require large amounts of data to develop different algorithms for estimating model parameters. The cluster was utilized to generate ASR lattices for about 2000 hours of English conversational telephone speech, 450 hours of English broadcast news, and 200 hours of Arabic broadcast conversations. These word- lattices were extensively used in the 6-week workshop to investigate, develop and evaluate discriminative learning strategies for language modeling. For more details, see the final workshop report and the associated publications.\n\nStructural Variations in Breast Cancer: The cluster has enabled our colleagues in computational biology to investigate novel algorithms for more accurately detecting large changes in DNA sequence &ndash; deletions, inversions and translocations &ndash; often seen in cancer. Accurate identification of these large structural variations (SVs) are expected to hasten advances in personalized medicine where interventions can target tumors with specific genomic aberrations. The problem of identifying these large structural variations (SVs) is computationally hard. To simplify the task and make it practical, current algorithms make unrealistic assumptions and their solutions are unsatisfactory. Our colleagues in computational biology are integrating all of the available evidence simultaneously in a statistical framework using distributed algorithms on the cluster. More details can be found in publications related to their open-source CloudBreak package.\n\nThe infrastructure and expertise developed with this award has attracted..."
 }
}