{
 "awd_id": "0964703",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Medium:  How Do Static Analysis Tools Affect End-User Quality",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2010-06-01",
 "awd_exp_date": "2015-05-31",
 "tot_intn_awd_amt": 700118.0,
 "awd_amount": 722118.0,
 "awd_min_amd_letter_date": "2010-05-28",
 "awd_max_amd_letter_date": "2013-05-10",
 "awd_abstract_narration": "The perceived quality of a software product depends strongly on field failures viz., defects experienced by users after the software is released to the field. Software managers work to constantly improve quality control processes, seeking to reduce the number of field failures. Static analysis is a powerful and elegant technique that finds defects without running code, by reasoning about what the program does when executed. It has been incubating in academia and is now emerging in industry. This research asks this question: How can the performance and practical use of static analysis tools be improved ? The goal of the research is to find ways to improve the performance of static analysis tools, as well as the quality-control processes that use them. This will help commercial and open-source organizations make more effective use static analysis tools, and substantially reduce field failures.\r\n\r\nUsing historical  data from several  open-source and commercial exemplars, the research will retrospectively evaluate the association of field failures with static analysis warnings. The research will evaluate the impact of factors such as experience of the developer, the complexity of the code, and the type of static analysis warning on failure properties such criticality, and defect latency (time until a defect becomes a failure). A wide variety of projects will be studied, including both commercial and open-source. The resulting data will be analyzed using statistical modeling to determine the factors that influence the success of static analysis tools in preventing field failures. Some field failures may have no associated static analysis warnings. This research will identify and characterize these failures, paving the way for new static analysis research. An integrated educational  initiative in  this proposal  is the  training of undergraduates  by  using bug  fixes  as pedagogical  material; undergraduates will also help annotate the corpus of field failures with information relevant to our analysis. An important byproduct of this research, is a large, diverse, annotated corpus of field failures of use to other educators and researchers in empirical software engineering, testing, and static analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vladimir",
   "pi_last_name": "Filkov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vladimir Filkov",
   "pi_email_addr": "filkov@cs.ucdavis.edu",
   "nsf_id": "000217276",
   "pi_start_date": "2010-05-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Premkumar",
   "pi_last_name": "Devanbu",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Premkumar T Devanbu",
   "pi_email_addr": "devanbu@cs.ucdavis.edu",
   "nsf_id": "000195677",
   "pi_start_date": "2010-05-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zhendong",
   "pi_last_name": "Su",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhendong Su",
   "pi_email_addr": "su@cs.ucdavis.edu",
   "nsf_id": "000336300",
   "pi_start_date": "2010-05-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Earl",
   "pi_last_name": "Barr",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Earl T Barr",
   "pi_email_addr": "e.barr@ucl.ac.uk",
   "nsf_id": "000541910",
   "pi_start_date": "2010-05-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "1850 RESEARCH PARK DR STE 300",
  "perf_city_name": "DAVIS",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956186153",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "794400",
   "pgm_ele_name": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 700118.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 12000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 10000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Software underlies much of modern life; software is created by software engineers, who make mistakes from time to time, like anyone else. These errors (bugs) must be found and fixed. In our research, we studied two popular ways to find defects: 1) find defects using programs (\"bug finders\") that are actually designed to find bugs in other programs. Bug finders are not perfect: they produce warnings of suspcious regions of code. &nbsp;2) since defects often follow patterns, we use statistical machine learning methods to try and learn these patterns, and thus predict where they may occur again. Both these approaches require human effort to inspect the code that is indicated as risky or suspicious. In this research, we systematically evaluated both these methods using historical data about where defects occurred, and were repaired: Could these methods find these historical defects, had they been used? <br /><br />We found that a) Both methods are effective b) statistical methods complement the automatic bug finders, but not the other way around and c) we have created a large dataset that we released for other scientists who wish to study this area.&nbsp;<br /><br />Our work as a whole, helps software engineers and software companies make best use of quality-control methods. Some of our work on defect prediction has been cited by engineers as Google in their own efforts. &nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/05/2015<br>\n\t\t\t\t\tModified by: Premkumar&nbsp;T&nbsp;Devanbu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSoftware underlies much of modern life; software is created by software engineers, who make mistakes from time to time, like anyone else. These errors (bugs) must be found and fixed. In our research, we studied two popular ways to find defects: 1) find defects using programs (\"bug finders\") that are actually designed to find bugs in other programs. Bug finders are not perfect: they produce warnings of suspcious regions of code.  2) since defects often follow patterns, we use statistical machine learning methods to try and learn these patterns, and thus predict where they may occur again. Both these approaches require human effort to inspect the code that is indicated as risky or suspicious. In this research, we systematically evaluated both these methods using historical data about where defects occurred, and were repaired: Could these methods find these historical defects, had they been used? \n\nWe found that a) Both methods are effective b) statistical methods complement the automatic bug finders, but not the other way around and c) we have created a large dataset that we released for other scientists who wish to study this area. \n\nOur work as a whole, helps software engineers and software companies make best use of quality-control methods. Some of our work on defect prediction has been cited by engineers as Google in their own efforts.  \n\n\t\t\t\t\tLast Modified: 09/05/2015\n\n\t\t\t\t\tSubmitted by: Premkumar T Devanbu"
 }
}