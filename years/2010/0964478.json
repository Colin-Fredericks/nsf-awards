{
 "awd_id": "0964478",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CSR: Medium: Collaborative Research: Scaling the Implicitly Parallel Programming Model with Lifelong Thread Extraction and Dynamic Adaptation",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2010-05-01",
 "awd_exp_date": "2015-04-30",
 "tot_intn_awd_amt": 399998.0,
 "awd_amount": 399998.0,
 "awd_min_amd_letter_date": "2010-03-08",
 "awd_max_amd_letter_date": "2011-09-07",
 "awd_abstract_narration": "The microprocessor industry has moved toward multicore designs to leverage increasing transistor counts in the face of physical and micro-architectural limitations.  Unfortunately, providing multiple cores does not translate into performance for most applications. Rather than pushing all the burden onto programmers, this project advocates the use of the implicitly parallel programming model to eliminate the laborious and error-prone process of explicit parallel programming.  Implicit parallel programming leverages sequential languages to facilitate shorter development and debug cycles, and relies on automatic tools, both static compilers and run-time systems, to identify parallelism and customize it to the target platform.  Implicit parallelism can be systematically extracted using: (1) decoupled softwarepipelining, a technique to extract the pipeline parallelism found in many sequential applications; (2) low-frequency and high-confidence speculation to overcome limitations of memory dependence analysis; (3) whole-program scope for parallelization to eliminate analysis boundaries; (4) simple extensions to the sequential programming model that give the programmer the power to refine the meaning of a program; (5) dynamic adaptation to ensure efficiency is maintained across changing environments. This project is developing the set of technologies to realize an implicitly parallel programming system with scalable, lifelong thread extraction and dynamic adaptation.  At the broader level, the implicitly parallel programming approach will free programmers to consider the problems they are trying to solve, rather than forcing them to overcome the processor industry's failure to continue to scale performance.  This approach will keep computers accessible, helping computing to have the same increasingly positive impact on other fields.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Scott",
   "pi_last_name": "Mahlke",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Scott Mahlke",
   "pi_email_addr": "mahlke@eecs.umich.edu",
   "nsf_id": "000296943",
   "pi_start_date": "2010-03-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "Regents of the University of Michigan - Ann Arbor",
  "perf_str_addr": "1109 GEDDES AVE STE 3300",
  "perf_city_name": "ANN ARBOR",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091015",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 128117.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 271881.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The American semiconductor industry now produces microprocessor designs consisting of multiple cores, or multicores. &nbsp;Unfortunately, providing multiple cores does not directly translate into performance for most applications. &nbsp;The industry has already fallen short of the decades-old performance growth trend, and the trend toward simpler cores means performance might even degrade. &nbsp;To make use of mulitcore, the burden is placed on software developers to develop parallel applications. &nbsp;But, the programming effort involved in creating correct and efficient parallel programs is far more substantial than writing the equivalent sequential version. &nbsp;Converting an existing single-threaded application is often more difficult than writing a new parallel application, as hard sequential constraints are ingrained in the code. &nbsp;In either case, extracting the parallelism necessary for efficient use of multicore systems is not only tedious, but is a recurring cost as machine-specific partitionings lack portability and forward performance compatibility. &nbsp;A more attractive approach is to rely on tools, both compilers and run-time optimizers, to automatically extract threads from sequential applications. This approach avoids the pitfalls of exposing the multicore problem up through the stack to the programmer.</p>\n<p>For this project, researchers from the University of Michigan, Princeton University, and University of Virginia created the enabling technologies to realize an implicitly parallel programming system with scalable, lifelong thread extraction, and dynamic adaptation. &nbsp;Implicitly parallel programs are sequential programs, but contain abundant parallelism that can be readily extracted and exploited by compilers. &nbsp;The framework, code named Paraprox, addresses the multicore challenge by reliably extracting parallelism from a wide range of applications without burdening the programmer with what should remain low-level implementation details. &nbsp;The Paraprox framework consists of three components: language extensions to C/C++ to support implicit parallelism, flexible automatic thread extraction, and a run-time layer for virtualization and dynamic adaptation.</p>\n<p>In the language area, this project developed extensions to C/C++ to allow programmers to specify commutative code regions (CommSets). &nbsp; The CommSet model provides programmers the flexibility to specify commutativity relations between arbitrary structured blocks of code and does not require the use of additional parallel constructs. &nbsp;Parallelism that is exposed using CommSets is independent of any particular parallelization strategy of concurrency control mechanism.</p>\n<p>In the thread extraction area, this project developed new thread extraction techniques for scripting languages. &nbsp;A prototype dynamic thread extraction system was created to show that interpreted languages such as Javascript can be paralllelized at run-time using lightweight compiler analysis and code generation. &nbsp;A broader approach that automatically exploits parallelism within fixed program inputs is achieved by coupling program specialization with automatic parallelization techniques. Program specialization marries a program with the values that remain invariant across the program execution, including fixed inputs, and creates a program that is highly optimized for the invariants. &nbsp;New techniques for speculative parallelization for graphics processing units (GPUs) were also developed that focus on collaborative execution between processors and accelerators like GPUs. &nbsp;A customized distributed dependence checking technique that can be efficiently deployed on GPUs is used to ensure speculative loop parallelization is indeed correct with low execution overhead.</p>\n<p>In the area of run-time layers, this project developed a prototype system to automatically orchestrate the e...",
  "por_txt_cntn": "\nThe American semiconductor industry now produces microprocessor designs consisting of multiple cores, or multicores.  Unfortunately, providing multiple cores does not directly translate into performance for most applications.  The industry has already fallen short of the decades-old performance growth trend, and the trend toward simpler cores means performance might even degrade.  To make use of mulitcore, the burden is placed on software developers to develop parallel applications.  But, the programming effort involved in creating correct and efficient parallel programs is far more substantial than writing the equivalent sequential version.  Converting an existing single-threaded application is often more difficult than writing a new parallel application, as hard sequential constraints are ingrained in the code.  In either case, extracting the parallelism necessary for efficient use of multicore systems is not only tedious, but is a recurring cost as machine-specific partitionings lack portability and forward performance compatibility.  A more attractive approach is to rely on tools, both compilers and run-time optimizers, to automatically extract threads from sequential applications. This approach avoids the pitfalls of exposing the multicore problem up through the stack to the programmer.\n\nFor this project, researchers from the University of Michigan, Princeton University, and University of Virginia created the enabling technologies to realize an implicitly parallel programming system with scalable, lifelong thread extraction, and dynamic adaptation.  Implicitly parallel programs are sequential programs, but contain abundant parallelism that can be readily extracted and exploited by compilers.  The framework, code named Paraprox, addresses the multicore challenge by reliably extracting parallelism from a wide range of applications without burdening the programmer with what should remain low-level implementation details.  The Paraprox framework consists of three components: language extensions to C/C++ to support implicit parallelism, flexible automatic thread extraction, and a run-time layer for virtualization and dynamic adaptation.\n\nIn the language area, this project developed extensions to C/C++ to allow programmers to specify commutative code regions (CommSets).   The CommSet model provides programmers the flexibility to specify commutativity relations between arbitrary structured blocks of code and does not require the use of additional parallel constructs.  Parallelism that is exposed using CommSets is independent of any particular parallelization strategy of concurrency control mechanism.\n\nIn the thread extraction area, this project developed new thread extraction techniques for scripting languages.  A prototype dynamic thread extraction system was created to show that interpreted languages such as Javascript can be paralllelized at run-time using lightweight compiler analysis and code generation.  A broader approach that automatically exploits parallelism within fixed program inputs is achieved by coupling program specialization with automatic parallelization techniques. Program specialization marries a program with the values that remain invariant across the program execution, including fixed inputs, and creates a program that is highly optimized for the invariants.  New techniques for speculative parallelization for graphics processing units (GPUs) were also developed that focus on collaborative execution between processors and accelerators like GPUs.  A customized distributed dependence checking technique that can be efficiently deployed on GPUs is used to ensure speculative loop parallelization is indeed correct with low execution overhead.\n\nIn the area of run-time layers, this project developed a prototype system to automatically orchestrate the execution of a single data-parallel kernel across multiple asymmetric processor cores and GPUs.  The programmer is responsible for developing a single application in t..."
 }
}