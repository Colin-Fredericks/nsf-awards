{
 "awd_id": "1007689",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Inference for Statistical Graphics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2010-09-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 129999.0,
 "awd_amount": 129999.0,
 "awd_min_amd_letter_date": "2010-09-02",
 "awd_max_amd_letter_date": "2010-09-02",
 "awd_abstract_narration": "Since the publication of the NSF landmark report \"Visualization in\r\nScientific Computing\" in 1987, computer-aided visualization has been\r\nrecognized as one of the most potent tool sets for scientific\r\ndiscovery. However, discoveries based on data displays are often\r\ncriticized because they are not secured by statistical inference. The\r\nteam of researchers from Iowa State University, Rice University and\r\nUniversity of Pennsylvania is addressing exactly this issue by\r\nbringing the rigors of statistical inference to visual data\r\nexploration. Statistical inference for plots are cast as comparison of\r\na plot of the actual data with plots of null data simulated under a\r\nnull hypothesis. If the actual plot stands out from a background of\r\n\"null plots\", it amounts to the rejection of the null hypothesis.\r\nExecuting this idea leads to rigorous protocols that can confer proper\r\nstatistical significance to visual discoveries. Tools of mathematical\r\nstatistics are employed to reduce composite null hypotheses to single\r\nreference distributions: conditioning on a minimal sufficient\r\nstatistic, bootstrap plug-ins, and posterior predictive sampling. The\r\nprotocols also have the potential to shift the perception of\r\nexploration-based findings in the scientific communities and\r\ndramatically increase the impact that these findings are allowed to\r\nhave. The testing protocols will be made accessible with\r\nimplementation in the open-source R language.\r\n\r\nData graphics are an essential part of communicating information. But\r\nhow reliable is the information that we gather from them? The\r\ninvestigators will develop a rigorous framework for visual inference\r\nmodeled after formal statistical testing. This framework allows the \r\nreader of a graphic to determine whether structure is real or spurious \r\n(is that a man in the moon, or just some rocks?). These protocols have \r\nthe potential to shift the perception of exploration-based findings in \r\nthe scientific community and dramatically increase the impact of \r\nexploratory work. Some aspects of the protocols are so intuitive that \r\nthey can be used for general audiences and integrated in the teaching \r\nof introductory statistics at from grade school to college.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Buja",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Buja",
   "pi_email_addr": "buja.at.wharton@gmail.com",
   "nsf_id": "000549324",
   "pi_start_date": "2010-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3451 WALNUT ST STE 440A",
  "perf_city_name": "PHILADELPHIA",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 129999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Statistics is known to be a numbers game: Data are collected, lessons are learned from them, and statistics validates the lessons.&nbsp; The problem with learning lessons from data is that the conclusions might amount to reading tea leafs, that is, seeing meaning where there is really just randomness.&nbsp; Statistics as a discipline has the task to keep data analysts honest and help them not to over-interpret their data.&nbsp; Statistics, however, is a numbers game, and most lessons learned from data are distilled into a few numeric summaries.&nbsp; For example, even news outlets report these days not only raw polling numbers before elections but also \"margins of error\".&nbsp; These margins of error are numbers that help us not to over-interpret the numbers of primary interest, the percentages of likely voters leaning one way or another.&nbsp;</p>\n<p>What our research addressed is the lesser known side of statistics: Many lessons learned from data are often conveyed visually in what we call a \"plot of the data\".&nbsp; Examples are bar charts, pie charts, scatterplots, boxplots,...&nbsp; The problem we addressed, and made progress in, is helping data analysts not to read tea leafs when they ponder plots of data.&nbsp; In some ways the problem with plots is more severe than with numerical summaries because so much more can be read into plots than into numbers.&nbsp; The danger of over-interpreting plots is therefore so much greater than for numbers.</p>\n<p>The solutions we proposed and investigated are simple and can be understood as a variation of the \"police line-up\" well-known from crime shows and novels.&nbsp; We create a list of \"innocent plots\" which are purely random and derived from artificial (simulated) data that have certifiably no lesson to learn from.&nbsp; Now we insert the plot of the actual data into a \"deck\" of plots of random data and show them to an unbiased person who hasn't seen the data yet.&nbsp; If that person is able to pick the actual data as \"special looking\", then we know that there is something to be learned in the actual data that is not there in random data.</p>\n<p>We show that this method provides valid statistical tests, and we provide a wealth of methods for tailored random data to pin down specific structure in data.&nbsp; One of the articles we wrote on this subject won a prize at the IEEE sponsored VisInf conference which specializes on data and information visualization methods.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/06/2014<br>\n\t\t\t\t\tModified by: Andreas&nbsp;Buja</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nStatistics is known to be a numbers game: Data are collected, lessons are learned from them, and statistics validates the lessons.  The problem with learning lessons from data is that the conclusions might amount to reading tea leafs, that is, seeing meaning where there is really just randomness.  Statistics as a discipline has the task to keep data analysts honest and help them not to over-interpret their data.  Statistics, however, is a numbers game, and most lessons learned from data are distilled into a few numeric summaries.  For example, even news outlets report these days not only raw polling numbers before elections but also \"margins of error\".  These margins of error are numbers that help us not to over-interpret the numbers of primary interest, the percentages of likely voters leaning one way or another. \n\nWhat our research addressed is the lesser known side of statistics: Many lessons learned from data are often conveyed visually in what we call a \"plot of the data\".  Examples are bar charts, pie charts, scatterplots, boxplots,...  The problem we addressed, and made progress in, is helping data analysts not to read tea leafs when they ponder plots of data.  In some ways the problem with plots is more severe than with numerical summaries because so much more can be read into plots than into numbers.  The danger of over-interpreting plots is therefore so much greater than for numbers.\n\nThe solutions we proposed and investigated are simple and can be understood as a variation of the \"police line-up\" well-known from crime shows and novels.  We create a list of \"innocent plots\" which are purely random and derived from artificial (simulated) data that have certifiably no lesson to learn from.  Now we insert the plot of the actual data into a \"deck\" of plots of random data and show them to an unbiased person who hasn't seen the data yet.  If that person is able to pick the actual data as \"special looking\", then we know that there is something to be learned in the actual data that is not there in random data.\n\nWe show that this method provides valid statistical tests, and we provide a wealth of methods for tailored random data to pin down specific structure in data.  One of the articles we wrote on this subject won a prize at the IEEE sponsored VisInf conference which specializes on data and information visualization methods.\n\n\t\t\t\t\tLast Modified: 04/06/2014\n\n\t\t\t\t\tSubmitted by: Andreas Buja"
 }
}