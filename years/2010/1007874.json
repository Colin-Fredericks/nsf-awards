{
 "awd_id": "1007874",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Bayesian Analysis and Applications",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2010-06-01",
 "awd_exp_date": "2015-05-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2010-05-18",
 "awd_max_amd_letter_date": "2013-08-13",
 "awd_abstract_narration": "Five research areas in Bayesian analysis, involving theory, methodology and application, will be pursued: objective Bayesian analysis, multiplicity adjustment, search and approximations in model selection, analysis of complex computer models, and differences between Bayes and empirical Bayes analysis. Research in objective Bayesian analysis will focus on the development of objective priors, together with their computational implementation, in semi-invariant contexts, which include spatial problems and problems arising in psychiatry. The Bayesian approach to multiplicity correction has the attraction that it does not depend on the error structure of the data; multiplicity correction is done only through the prior probabilities assigned to models or other multiplicity features. Understanding which probability assignments do, and do not, adjust for multiplicity will be an important feature of this research. A focus of the research on model selection will be the development of a generalization of BIC which is much more widely applicable than the standard version, especially overcoming the major hurdle of defining effective sample size for a parameter. Advances in these areas will have application to research involving the analysis and use of complex computer models of processes. Also, surprising differences between Bayes and empirical Bayes analysis arise in several of the above settings, and better understanding of these differences will also be a focus of the research.\r\n\r\nObjective Bayesian analysis has existed for over 250 years, but interest in the field has increased markedly in recent years. A major reason is that many of the significant scientific problems today (such as much of climate change research) involve some type of assimilation of data and physical modeling, typically done by Bayesian methods. Many of today?s most challenging problems ? including microarray and other bioinformatic analyses, syndromic surveillance, high-throughput screening, and many others ? involve consideration of multiple-testing with a huge number of possible tests, and require major multiplicity adjustments. For instance, the work on multiplicity will be done in the context of subgroup analysis in clinical trials, providing major new insights into HIV vaccine trials, and in refining detection methodology in high-energy physics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dongchu",
   "pi_last_name": "Sun",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dongchu Sun",
   "pi_email_addr": "sund@missouri.edu",
   "nsf_id": "000205571",
   "pi_start_date": "2010-05-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Missouri-Columbia",
  "inst_street_address": "121 UNIVERSITY HALL",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBIA",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "5738827560",
  "inst_zip_code": "652113020",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MO03",
  "org_lgl_bus_name": "UNIVERSITY OF MISSOURI SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "SZPJL5ZRCLF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of Missouri-Columbia",
  "perf_str_addr": "121 UNIVERSITY HALL",
  "perf_city_name": "COLUMBIA",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "652113020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MO03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 39796.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 40544.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 29721.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 39939.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Statistical methods are of &nbsp;importance in science, especially in experimental settings. At its simplest, an experiment might have two or more conditions, one in which there is no manipulation (i.e., &ldquo;control&rdquo;) and one in which the experimenter expects a manipulation to influence the outcome. In general, a novel theory is advanced if the manipulation can be shown to produce a difference in outcomes. The experimenter poses two models, one in which the manipulation has no effect (the &ldquo;null hypothesis&rdquo;) and one in which manipulation produces a noticeable effect (the &ldquo;alternative hypothesis&rdquo;). Traditionally, statistical hypothesis testing is used to decide if the evidence from the hypothesis is sufficient to conclude that the null hypothesis is false, thereby supporting the experimenters&rsquo; new theory.</p>\n<p>Classical statistical testing, in which results are generally summarized with &ldquo;<em>p</em>-values,&rdquo; has been criticized for several reasons. In many common settings, <em>p</em>-values have been shown to be overly optimistic in the sense that the evidence suggested by a small p-value is not as strong as generally believed. Also, classical analysis is oriented more towards proving the null hypothesis is false and not as well suited to gathering evidence in support of a null hypothesis.</p>\n<p>Bayesian statistical analysis is a branch of statistics that has revolutionized many forms of data analysis since the advent of cheap computing in the last several decades, which has enabled new algorithms to make Bayesian analyses practical. The Bayesian analogue to classical testing is the Bayes factor. When used with prior estimates of the probability that one model or the other is true before data are collected, the Bayes factor produces the odds of one model compared to the other in light of the data. The Bayes factor concept is completely different from <em>p</em>-values, which is biased towards demonstrating the null hypothesis is false. The goals of this project related to providing an accessible package of tools enabling researchers to implement Bayes factors in their work.</p>\n<p>The project had two broad goals. Bayesian analysis relies on prior information that must be supplied for all parameters in a model. Such prior information can be strong or relatively weak. In the extreme, priors are called &ldquo;noninformative,&rdquo; or in current parlance, &ldquo;objective&rdquo; in comparison with informative priors, often called &ldquo;subjective.&rdquo; One goal of the project was to incorporate as far as possible objective priors to guard against the possibility of a researcher&rsquo;s strong prior beliefs unduly influencing the statistical analysis. (Of course, the methods developed allow for subjective as well as objective prior information.)</p>\n<p>The second goal is to develop a class of Bayes factors for regression and Analysis of Variance experimental settings (i.e., linear models), arguably the two most commonly used techniques in social science research. In theoretical work, explicit formulas were derived to calculate Bayes factors using an important class of priors called Zellner <em>g</em>-priors. A novel feature of the developed methodology was the use of separate Zellner priors for different factors. The appropriate use for both fixed and random effects was also explored. Fundamental theoretical properties of these Bayes factors were derived, justifying their use in practice.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/28/2016<br>\n\t\t\t\t\tModified by: Dongchu&nbsp;Sun</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nStatistical methods are of  importance in science, especially in experimental settings. At its simplest, an experiment might have two or more conditions, one in which there is no manipulation (i.e., \"control\") and one in which the experimenter expects a manipulation to influence the outcome. In general, a novel theory is advanced if the manipulation can be shown to produce a difference in outcomes. The experimenter poses two models, one in which the manipulation has no effect (the \"null hypothesis\") and one in which manipulation produces a noticeable effect (the \"alternative hypothesis\"). Traditionally, statistical hypothesis testing is used to decide if the evidence from the hypothesis is sufficient to conclude that the null hypothesis is false, thereby supporting the experimenters? new theory.\n\nClassical statistical testing, in which results are generally summarized with \"p-values,\" has been criticized for several reasons. In many common settings, p-values have been shown to be overly optimistic in the sense that the evidence suggested by a small p-value is not as strong as generally believed. Also, classical analysis is oriented more towards proving the null hypothesis is false and not as well suited to gathering evidence in support of a null hypothesis.\n\nBayesian statistical analysis is a branch of statistics that has revolutionized many forms of data analysis since the advent of cheap computing in the last several decades, which has enabled new algorithms to make Bayesian analyses practical. The Bayesian analogue to classical testing is the Bayes factor. When used with prior estimates of the probability that one model or the other is true before data are collected, the Bayes factor produces the odds of one model compared to the other in light of the data. The Bayes factor concept is completely different from p-values, which is biased towards demonstrating the null hypothesis is false. The goals of this project related to providing an accessible package of tools enabling researchers to implement Bayes factors in their work.\n\nThe project had two broad goals. Bayesian analysis relies on prior information that must be supplied for all parameters in a model. Such prior information can be strong or relatively weak. In the extreme, priors are called \"noninformative,\" or in current parlance, \"objective\" in comparison with informative priors, often called \"subjective.\" One goal of the project was to incorporate as far as possible objective priors to guard against the possibility of a researcher?s strong prior beliefs unduly influencing the statistical analysis. (Of course, the methods developed allow for subjective as well as objective prior information.)\n\nThe second goal is to develop a class of Bayes factors for regression and Analysis of Variance experimental settings (i.e., linear models), arguably the two most commonly used techniques in social science research. In theoretical work, explicit formulas were derived to calculate Bayes factors using an important class of priors called Zellner g-priors. A novel feature of the developed methodology was the use of separate Zellner priors for different factors. The appropriate use for both fixed and random effects was also explored. Fundamental theoretical properties of these Bayes factors were derived, justifying their use in practice.\n\n \n\n\t\t\t\t\tLast Modified: 09/28/2016\n\n\t\t\t\t\tSubmitted by: Dongchu Sun"
 }
}