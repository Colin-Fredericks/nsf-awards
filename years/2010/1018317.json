{
 "awd_id": "1018317",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Modeling and Predicting Term Mismatch for Full-Text Retrieval",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 495547.0,
 "awd_amount": 495547.0,
 "awd_min_amd_letter_date": "2010-09-02",
 "awd_max_amd_letter_date": "2010-09-02",
 "awd_abstract_narration": "Many text search engines use probabilistic reasoning to determine how well a word represents a person?s information need.  The probability that a term appears in relevant documents ? documents that satisfy the information need ? is a fundamental quantity in the theory of probabilistic information retrieval, however prior research provided few clues about how to estimate it reliably.  This project uses exploratory data analysis to identify common reasons that user-specified query terms fail to match relevant documents, develops features correlated with each reason, and integrates them into a model that can be trained from data.  The resulting term necessity predictions can be used in state-of-the-art retrieval models to improve retrieval accuracy substantially.\r\n\r\nTerm necessity predictions are based on a two-stage approach to text retrieval.  A feature-based analysis of an initial retrieval develops evidence that can be linked to a variety of common reasons that a term might not match relevant documents, for example, centrality, synonymy, and abstractness.  This model-based approach can be trained from available data, making it easy to incorporate new features that test new hypotheses, or to train a corpus-specific predictive model.  It also has the advantage that probability predictions are query-specific, and linked to features that can guide automatic term weighting as well as interactive or automatic query refinement.  The project develops several focused interventions for interactive, automatic query expansion, and relevance feedback refinement of queries.\r\n\r\nThis project makes an impact on the scientific community by providing new approaches to a central problem that affects probabilistic retrieval models, and the diagnosis and correction of problems in query formation.  Improvements in search engine accuracy also affect a broad population of everyday users.  The proposed research improves search accuracy for ?ordinary people? using unstructured keyword queries, as well as professional searchers who often use sophisticated structured queries to search structured documents.\r\n\r\nResearch results will be disseminated in research papers and via project web site (http://www.cs.cmu.edu/~callan/Projects/IIS-1018317/).  New techniques will be implemented and disseminated in periodic releases of the Lemur Project?s Indri search engine (http://www.lemurproject.org/indri/).  Indri is used by a broad international research community, thus this form of dissemination makes it more likely that other researchers will study and extend the proposed research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jamie",
   "pi_last_name": "Callan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jamie Callan",
   "pi_email_addr": "callan@cs.cmu.edu",
   "nsf_id": "000173762",
   "pi_start_date": "2010-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 495547.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Text search engines are pervasive in modern life.&nbsp; They are used to search the web, large digital libraries, personal computers, email, and anywhere else that people store text documents.&nbsp; Many text search engines use probabilistic reasoning to determine how well a word represents a person&rsquo;s information need.&nbsp; The probability that a term appears in <em>relevant</em> documents &ndash; documents that satisfy the information need &ndash; is a fundamental quantity in the theory of probabilistic information retrieval, however prior research provided few clues about how to estimate it reliably.&nbsp;</p>\n<p>This project used exploratory data analysis to identify common reasons that user-specified query terms fail to match relevant documents; developed features correlated with each reason; and integrated them into a model that can be trained from data.&nbsp; The resulting <em>term necessity</em> predictions can be used in state-of-the-art retrieval models to improve retrieval accuracy substantially.</p>\n<p>Term necessity predictions are based on a two-stage approach to text retrieval.&nbsp; The original query is used to retrieve an initial set of documents.&nbsp; Analysis of the initial set produces evidence that is linked to common reasons that a term might not match relevant documents, for example it isn&rsquo;t central to the query, has many synonyms, or is too abstract.&nbsp; The evidence is used to estimate the probability that each query term appears in relevant documents.&nbsp; These estimates are used to improve the query.&nbsp; The improved query is used to re-rank the original set of documents or retrieve a new set of documents.</p>\n<p>The most important result of the project is the basic term necessity prediction method described above, because it provides a new way of fixing a longstanding problem in popular retrieval models.&nbsp; The project showed that this problem affects about 1/3 of query terms, and that its effects are more serious in longer queries.&nbsp; It also showed that in addition to improving retrieval accuracy, term necessity estimates enable the search engine to provide precise feedback to users about which query terms are likely to perform poorly; in our studies, people shown this type of feedback were able to improve their queries with much less effort than people without such diagnostic support.&nbsp; An extension of the basic term necessity prediction framework enables it to be applied to structured queries such as are created by expert searchers and automatic methods such as sequential dependency models; there are few other query-specific methods of determining the relative importance of the different components of such queries.&nbsp; Finally, two versions of the method were developed that had substantially lower computational costs to make it more practical in commercial environments where a rapid search engine response is essential.&nbsp;</p>\n<p>This project made an impact on the scientific community by providing new approaches to a central problem that affects probabilistic retrieval models and the diagnosis and correction of problems in query formation.&nbsp; The new methods developed by the project can be used by industry to improve search accuracy for ordinary people using unstructured keyword queries as well as professional searchers who often use sophisticated structured queries.&nbsp; These improvements in search engine accuracy have the potential to make an impact on a broad population of people who use text search for personal and professional purposes.&nbsp;</p>\n<p>Research results from the project were disseminated in published research papers, and in open-source software that anyone may use, modify, and/or redistribute free of charge.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/24/2014<br>\n\t\t\t\t\tModified by: Jamie&nbsp;Callan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nText search engines are pervasive in modern life.  They are used to search the web, large digital libraries, personal computers, email, and anywhere else that people store text documents.  Many text search engines use probabilistic reasoning to determine how well a word represents a person\u00c6s information need.  The probability that a term appears in relevant documents &ndash; documents that satisfy the information need &ndash; is a fundamental quantity in the theory of probabilistic information retrieval, however prior research provided few clues about how to estimate it reliably. \n\nThis project used exploratory data analysis to identify common reasons that user-specified query terms fail to match relevant documents; developed features correlated with each reason; and integrated them into a model that can be trained from data.  The resulting term necessity predictions can be used in state-of-the-art retrieval models to improve retrieval accuracy substantially.\n\nTerm necessity predictions are based on a two-stage approach to text retrieval.  The original query is used to retrieve an initial set of documents.  Analysis of the initial set produces evidence that is linked to common reasons that a term might not match relevant documents, for example it isn\u00c6t central to the query, has many synonyms, or is too abstract.  The evidence is used to estimate the probability that each query term appears in relevant documents.  These estimates are used to improve the query.  The improved query is used to re-rank the original set of documents or retrieve a new set of documents.\n\nThe most important result of the project is the basic term necessity prediction method described above, because it provides a new way of fixing a longstanding problem in popular retrieval models.  The project showed that this problem affects about 1/3 of query terms, and that its effects are more serious in longer queries.  It also showed that in addition to improving retrieval accuracy, term necessity estimates enable the search engine to provide precise feedback to users about which query terms are likely to perform poorly; in our studies, people shown this type of feedback were able to improve their queries with much less effort than people without such diagnostic support.  An extension of the basic term necessity prediction framework enables it to be applied to structured queries such as are created by expert searchers and automatic methods such as sequential dependency models; there are few other query-specific methods of determining the relative importance of the different components of such queries.  Finally, two versions of the method were developed that had substantially lower computational costs to make it more practical in commercial environments where a rapid search engine response is essential. \n\nThis project made an impact on the scientific community by providing new approaches to a central problem that affects probabilistic retrieval models and the diagnosis and correction of problems in query formation.  The new methods developed by the project can be used by industry to improve search accuracy for ordinary people using unstructured keyword queries as well as professional searchers who often use sophisticated structured queries.  These improvements in search engine accuracy have the potential to make an impact on a broad population of people who use text search for personal and professional purposes. \n\nResearch results from the project were disseminated in published research papers, and in open-source software that anyone may use, modify, and/or redistribute free of charge.\n\n\t\t\t\t\tLast Modified: 12/24/2014\n\n\t\t\t\t\tSubmitted by: Jamie Callan"
 }
}