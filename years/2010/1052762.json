{
 "awd_id": "1052762",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:  Modeling and Recognizing Collective Activities",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 80000.0,
 "awd_amount": 80000.0,
 "awd_min_amd_letter_date": "2010-08-06",
 "awd_max_amd_letter_date": "2010-08-06",
 "awd_abstract_narration": "This project explores a novel principled framework for learning generic models of collective activities. Examples of collective activities are: people talking; a group of zebras escaping from a lion. Such models are used, in turn, for detecting, classifying, and segmenting activities as well as indentifying activities that differ from the collective behavior from videos sequences. Research developed in this project is distinctly different from previous research on action classification wherein activities are analyzed by considering individuals in isolation. Furthermore, unlike many current contributions, the aim is to work under unrestrictive conditions such as dynamic cluttered background, moving, monocular and un-calibrated cameras.\r\n\r\nKey intellectual contributions of this project are: i) a learning scheme based on Random Forest that is able to adaptively characterize the coherent behavior of individuals, thus enabling discriminative classification of collective activities. This learning scheme is also relevant to other visual recognition tasks using context (e.g., scene and object recognition); ii) a methodology based on Relational Dependency Networks for segmenting different collective activities and discovering anomalous ones. \r\n\r\nThis project can provides critical building blocks toward addressing high level visual problems such as modeling the interaction between humans/animals and objects,  constructing an ontology of human/animal activities, modeling complex human/animal behaviors. This research has a potential to play a transformative role in strategic areas such as robotics and navigation. It also provides a crucial tool for analyzing and studying typical spatial-temporal collective behaviors in biology (insects, animals) or biomedicine (cells).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Silvio",
   "pi_last_name": "Savarese",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Silvio Savarese",
   "pi_email_addr": "ssilvio@stanford.edu",
   "nsf_id": "000489619",
   "pi_start_date": "2010-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "Regents of the University of Michigan - Ann Arbor",
  "perf_str_addr": "1109 GEDDES AVE STE 3300",
  "perf_city_name": "ANN ARBOR",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091015",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 80000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main purpose of this project was to provide a novel principled framework for learning generic models of collective activities from videos sequences. Examples of collective activities are: people talking; a group of zebras escaping from a lion. Such models were used, in turn, for detecting, classifying, and segmenting activities as well as identifying activities that differ from the collective behavior. Research that we developed in this project is distinctly different from previous research on action classification wherein activities are analyzed by considering individuals <em>in isolation</em>. Furthermore, unlike many current contributions, the aim was to work under unrestrictive conditions such as dynamic cluttered background, moving, monocular and un-calibrated cameras. A key intellectual contributions of this project was a learning scheme based on Random Forest that is able to adaptively characterize the coherent behavior of individuals, thus enabling discriminative classification of collective activities. This learning scheme is also relevant to other visual recognition tasks (e.g., scene and object recognition) that can take advantage of contextual cues. Another key contribution was the collection of a dataset of collective activities. Such a dataset comprises videos of various classes of collective activities and was used to experimentally demonstrate our theoretical findings. Both dataset and the software that implements our framework is available at authors&rsquo; website.</p>\n<p>This project provides a platform for addressing high level visual problems such as modeling the interaction between humans/animals and objects,&nbsp; constructing an ontology of human/animal activities, modeling complex human/animal behaviors. This research has the potential to play a transformative role in strategic areas such as robotics and navigation. It provides a crucial tool for analyzing and studying typical spatial-temporal collective behaviors in biology (insects, animals) or biomedicine (cells).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/23/2013<br>\n\t\t\t\t\tModified by: Silvio&nbsp;Savarese</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe main purpose of this project was to provide a novel principled framework for learning generic models of collective activities from videos sequences. Examples of collective activities are: people talking; a group of zebras escaping from a lion. Such models were used, in turn, for detecting, classifying, and segmenting activities as well as identifying activities that differ from the collective behavior. Research that we developed in this project is distinctly different from previous research on action classification wherein activities are analyzed by considering individuals in isolation. Furthermore, unlike many current contributions, the aim was to work under unrestrictive conditions such as dynamic cluttered background, moving, monocular and un-calibrated cameras. A key intellectual contributions of this project was a learning scheme based on Random Forest that is able to adaptively characterize the coherent behavior of individuals, thus enabling discriminative classification of collective activities. This learning scheme is also relevant to other visual recognition tasks (e.g., scene and object recognition) that can take advantage of contextual cues. Another key contribution was the collection of a dataset of collective activities. Such a dataset comprises videos of various classes of collective activities and was used to experimentally demonstrate our theoretical findings. Both dataset and the software that implements our framework is available at authors\u00c6 website.\n\nThis project provides a platform for addressing high level visual problems such as modeling the interaction between humans/animals and objects,  constructing an ontology of human/animal activities, modeling complex human/animal behaviors. This research has the potential to play a transformative role in strategic areas such as robotics and navigation. It provides a crucial tool for analyzing and studying typical spatial-temporal collective behaviors in biology (insects, animals) or biomedicine (cells).\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/23/2013\n\n\t\t\t\t\tSubmitted by: Silvio Savarese"
 }
}