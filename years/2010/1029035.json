{
 "awd_id": "1029035",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  Computational Behavioral Science:  Modeling, Analysis, and Visualization of Social and Communicative Behavior",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2010-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 1500000.0,
 "awd_amount": 1500000.0,
 "awd_min_amd_letter_date": "2010-08-19",
 "awd_max_amd_letter_date": "2012-09-11",
 "awd_abstract_narration": "Computational Behavioral Science: Modeling, Analysis, and Visualization of Social and \r\nCommunicative Behavior\r\nLead PI/Institution: James M. Rehg, Georgia Institute of Technology\r\nThis Expedition will develop novel computational methods for measuring and analyzing the behavior of children and adults during face-to-face social interactions. Social behavior plays a key role in the acquisition of social and communicative skills during childhood. Children with developmental disorders, such as autism, face great challenges in acquiring these skills, resulting in substantial lifetime risks. Current best practices for evaluating behavior and assessing risk are based on direct observation by highly-trained specialists, and cannot be easily scaled to the large number of individuals who need evaluation and treatment. For example, autism affects 1 in 110 children in the U.S., with a lifetime cost of care of $3.2 million per person. By developing methods to automatically collect fine-grained behavioral data, this project will enable large-scale objective screening and more effective delivery and assessment of therapy. Going beyond the treatment of disorders, this technology will make it possible to automatically measure behavior over long periods of time for large numbers of individuals in a wide range of settings. Many disciplines, such as education, advertising, and customer relations, could benefit from a quantitative, data-drive approach to behavioral analysis. \r\nHuman behavior is inherently multi-modal, and individuals use eye gaze, hand gestures, facial expressions, body posture, and tone of voice along with speech to convey engagement and regulate social interactions.  This project will develop multiple sensing technologies, including vision, speech, and wearable sensors, to obtain a comprehensive, integrated portrait of expressed behavior. Cameras and microphones provide an inexpensive, noninvasive means for measuring eye, face, and body movements along with speech and nonspeech utterances. Wearable sensors can measure physiological variables such as heart-rate and skin conductivity, which contain important cues about levels of internal stress and arousal that are linked to expressed behavior. This project is developing unique capabilities for synchronizing multiple sensor streams, correlating these streams to measure behavioral variables such as affect and attention, and modeling extended interactions between two or more individuals. In addition, novel behavior visualization methods are being developed to enable real-time decision support for interventions and the effective use of repositories of behavioral data. Methods are also under development for reflecting the capture and analysis process to users of the technology.\r\nThe long-term goal of this project is the creation of a new scientific discipline of computational behavioral science, which draws equally from computer science and psychology in order to transform the study of human behavior. A comprehensive education plan supports this goal through the creation of an interdisciplinary summer school for young researchers and the development of new courses in computational behavior. Outreach activities include significant and on-going collaborations with major autism research centers in Atlanta, Boston, Pittsburgh, Urbana-Champaign, and Los Angeles.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Forsyth",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "David A Forsyth",
   "pi_email_addr": "daf@cs.uiuc.edu",
   "nsf_id": "000391155",
   "pi_start_date": "2010-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karrie",
   "pi_last_name": "Karahalios",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karrie Karahalios",
   "pi_email_addr": "kkarahal@cs.uiuc.edu",
   "nsf_id": "000313162",
   "pi_start_date": "2010-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "7723",
   "pgm_ref_txt": "EXPERIMENTAL EXPEDITIONS"
  },
  {
   "pgm_ref_code": "7969",
   "pgm_ref_txt": "FY 2010 Funding for PTR"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 900000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 600000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Work at Illinois focused on two areas.&nbsp; First, research studied visualizing the data in recordings of interviews and of interactions.&nbsp; Second, research studied interpreting videos of these interactions.&nbsp;</span></p>\n<p><span>&nbsp;</span></p>\n<p><span>Visualizing behavior: Some children with autism spectrum disorders (ASD) display variants of behavior that are important to visualize and understand. Joint attention, for example &mdash;&nbsp; the act of alerting another person to attend to an object via pointing, vocalizing, and looking is critical in understanding and diagnosing ASD.&nbsp; Sessions of of an assessment protocol &nbsp; were marked up with behaviors by skilled annotators.&nbsp; Research at Illinois produced multiple visualizations of coordinated communicative behavior, then evaluated them to find those most favored by clinicians, doctors and researchers.&nbsp; These were Plexlines and Engaze.&nbsp; Plexlines emphasizes macro-behaviors, and Engaze emphasizes micro-behaviors.&nbsp; Each is valuable, for different reasons, and users suggested connecting the two in a faceted interface, which we are doing.&nbsp; Figure 1 illustrates this work.&nbsp; This work is being taken to the medical community and we are presenting our work to hospitals and partnering with Easterseals and medical centers to bring our work into their workflow.</span></p>\n<p><span>&nbsp;</span></p>\n<p><span>Visualizing sounds: Some children on the spectrum display variations in vocalization that are important to visualize and understand. Research at Illinois has investigated methods to characterize voices for visualization.&nbsp; This work investigated a theory of voice effort levels, and has supplied the content for next generation visualizations of voice that can capture paralingual vocal information, and extend to emotion.</span></p>\n<p><span>&nbsp;</span></p>\n<p><span>Interpreting videos:&nbsp; Some children on the spectrum move their bodies in ways that differ from neurotypical children.&nbsp; Research at Illinois investigated methods to find the location of body keypoints in images and to reconstruct the 3D configuration of the body from that information.&nbsp; One method, based on incremental search using deep networks, had state of the art accuracy for this problem for a while. &nbsp;</span></p>\n<p><span>&nbsp;</span></p>\n<p><span>Interpreting videos:&nbsp; One useful screening activity is to identify children who are not engaged during an interview.&nbsp; The dataset collected at Georgia Tech during the period of this award is annotated with engagement scores produced by trained screeners.&nbsp; Research at Illinois attempted to predict the score that a trained screener would produce by automated analysis of the video.&nbsp; Methods recovered face regions from the video, then predicted engagement scores using a deep network. Results showed that, for some phases of the interview, the score was quite predictable.&nbsp; For phases of the interview that are difficult to predict, work suggested ways to revise the interview to improve the behavior of automated algorithms for engagement prediction.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/01/2018<br>\n\t\t\t\t\tModified by: David&nbsp;A&nbsp;Forsyth</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1029035/1029035_10021035_1519918043591_outcomepicfinal--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1029035/1029035_10021035_1519918043591_outcomepicfinal--rgov-800width.jpg\" title=\"PlexLines and EnGaze visualizations\"><img src=\"/por/images/Reports/POR/2018/1029035/1029035_10021035_1519918043591_outcomepicfinal--rgov-66x44.jpg\" alt=\"PlexLines and EnGaze visualizations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">(a) PlexLines and (b) EnGaze visualizations of the same child in a RABC session.  Grey lines show different phases in a single session. PlexLines shows behavior with longer duration; EnGaze shows micro-behaviors.</div>\n<div class=\"imageCredit\">K. Karahalios</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">David&nbsp;A&nbsp;Forsyth</div>\n<div class=\"imageTitle\">PlexLines and EnGaze visualizations</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWork at Illinois focused on two areas.  First, research studied visualizing the data in recordings of interviews and of interactions.  Second, research studied interpreting videos of these interactions. \n\n \n\nVisualizing behavior: Some children with autism spectrum disorders (ASD) display variants of behavior that are important to visualize and understand. Joint attention, for example &mdash;  the act of alerting another person to attend to an object via pointing, vocalizing, and looking is critical in understanding and diagnosing ASD.  Sessions of of an assessment protocol   were marked up with behaviors by skilled annotators.  Research at Illinois produced multiple visualizations of coordinated communicative behavior, then evaluated them to find those most favored by clinicians, doctors and researchers.  These were Plexlines and Engaze.  Plexlines emphasizes macro-behaviors, and Engaze emphasizes micro-behaviors.  Each is valuable, for different reasons, and users suggested connecting the two in a faceted interface, which we are doing.  Figure 1 illustrates this work.  This work is being taken to the medical community and we are presenting our work to hospitals and partnering with Easterseals and medical centers to bring our work into their workflow.\n\n \n\nVisualizing sounds: Some children on the spectrum display variations in vocalization that are important to visualize and understand. Research at Illinois has investigated methods to characterize voices for visualization.  This work investigated a theory of voice effort levels, and has supplied the content for next generation visualizations of voice that can capture paralingual vocal information, and extend to emotion.\n\n \n\nInterpreting videos:  Some children on the spectrum move their bodies in ways that differ from neurotypical children.  Research at Illinois investigated methods to find the location of body keypoints in images and to reconstruct the 3D configuration of the body from that information.  One method, based on incremental search using deep networks, had state of the art accuracy for this problem for a while.  \n\n \n\nInterpreting videos:  One useful screening activity is to identify children who are not engaged during an interview.  The dataset collected at Georgia Tech during the period of this award is annotated with engagement scores produced by trained screeners.  Research at Illinois attempted to predict the score that a trained screener would produce by automated analysis of the video.  Methods recovered face regions from the video, then predicted engagement scores using a deep network. Results showed that, for some phases of the interview, the score was quite predictable.  For phases of the interview that are difficult to predict, work suggested ways to revise the interview to improve the behavior of automated algorithms for engagement prediction.\n\n \n\n\t\t\t\t\tLast Modified: 03/01/2018\n\n\t\t\t\t\tSubmitted by: David A Forsyth"
 }
}