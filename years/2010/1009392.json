{
 "awd_id": "1009392",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III:  Large:  Collaborative Research: Web Archive Cooperative",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2010-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 399754.0,
 "awd_amount": 399754.0,
 "awd_min_amd_letter_date": "2010-08-04",
 "awd_max_amd_letter_date": "2010-08-04",
 "awd_abstract_narration": "Web Science is an emerging discipline that studies the Web: how human activity is shaped by Web interactions, how the Web can benefit society, and how Web technologies can be improved. Central to Web Science is access to data that records the history of the Web, as well as data that records human activity (e.g., posed queries, tagged pages, Twitter updates). It is currently very difficult for academic researchers to obtain such Web data because it is hard to locate, it is fragmented across diverse sites, and is recorded using inconsistent formats and strategies. This project will build a Web Archive Cooperative (WAC) that will integrate existing archives (repositories of Web data), making it feasible to access large volumes of data in a simplified fashion. The WAC will be a virtual service, providing search facilities and access mechanisms to existing resources. These resources will not just be Web pages, but all types of available Web information, such as query logs, tag annotations, blogs, profiles and Twitter updates. Furthermore, resources will also include the software tools for building and managing Web archives.\r\n\r\nThe project will explore three goals for a resource discovery service: (1) the manual or automated discovery of entire existing Web related archives; (2) the selection among known archives of the ones that support a specific research question; and (3) the identification of individual resources from within the selected archives. Tools for characterizing discovered archives, especially for the case where the archive does not provide rich descriptive metadata, will also be developed. Characterization of an archive includes elements such as an estimate of the archive's coverage, particulars of the crawling parameters, like dates/frequencies, crawl duration, depth, per-site ceiling on the number of collected pages, content statistics, and link structure.  Mechanisms for integrating diverse archives will be developed, and the mechanisms will be applied to site reconstruction (from various archives) and archive views (a logical fusion of resources from multiple sources). Since integration issues are so challenging, an experimental testbed will be set up with small but diverse resources. The testbed will contain several crawls of the same target sites, each obtained with different crawlers and using different parameters. The testbed will also contain related resources.  Storage trading schemes will be developed, allowing members to trade local backup space for remote space. A Web archive replication tool will be developed based on existing notions for self-preserving objects. Alternatives for replica synchronization will be studied.\r\n\r\nWorkshops to bring together key Web Science researchers will be organized to discuss available resources and impediments to sharing. These workshops will drive research and identify needed tools and protocols. With small groups of participants, challenge problems will be established, e.g., combining a set of Web archives. Reports of these results at future workshops can incentivize others to participate in the WAC. In addition, an Advisory Board of industrial, government, and academic experts has been set up to guide the project.  A Summer Institute for Web Science graduate students will be held. At this Institute, students will learn to use the latest tools and will learn from each other's experiences in dealing with Web data. In addition, a one-day workshop will be developed, to be offered at Web Science conferences (WWW, SIGIR, etc.) to educate participants about WAC resources. An undergraduate Web Sciences track for computer science majors will be set up, taking advantage of WAC resources. The project will have impact in two ways.  First, it will provide tools and services that facilitate access to Web resources. Any researcher, from a computer scientist studying efficient Web search, to a social scientist studying how human beliefs are changing today, to a historian studying how the early Web evolved, to a biologist understanding how disease spreads, will benefit from the work.  Second, the project motivates students and young researchers to stay in academia. Currently top talent is flowing to industry because only they have comprehensive Web data, and it is so hard to do significant Web Science at universities.  The WAC can provide an alternative, attracting more researchers and teachers to this important area.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Nelson",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Nelson",
   "pi_email_addr": "mln@cs.odu.edu",
   "nsf_id": "000245189",
   "pi_start_date": "2010-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Old Dominion University Research Foundation",
  "inst_street_address": "4111 MONARCH WAY STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "NORFOLK",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7576834293",
  "inst_zip_code": "235082561",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "VA03",
  "org_lgl_bus_name": "OLD DOMINION UNIVERSITY RESEARCH FOUNDATION",
  "org_prnt_uei_num": "DSLXBD7UWRV6",
  "org_uei_num": "DSLXBD7UWRV6"
 },
 "perf_inst": {
  "perf_inst_name": "Old Dominion University",
  "perf_str_addr": "HAMPTON BLVD",
  "perf_city_name": "NORFOLK",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "235290001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "VA03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 399754.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our cultural discourse is increasingly carried in the web. With the initial emergence of the web many years ago, there was a period where conventional mediums (e.g., music, movies, books, scholarly publications) were primary and the web was a supplementary channel. This has now changed, where the web is often the primary channel, and other publishing mechanisms, if present at all, supplement the web. Unfortunately, the technology for publishing information on the web always outstrips our technology for preservation. My concern is less that we will lose data of known importance (e.g., scientific data, census data), but rather that we will lose data that we do not yet know is important.&nbsp; Archiving the web is integral to our recording our scientific and social progress.</p>\n<p>One of the main contributions of this project was the continued expansion and development of the Memento Framework (http://www.mementoweb.org/).&nbsp; <span class=\"style3\">Memento specifies a technical framework aimed at a better integration of the current and the past Web.          The Memento protocol adds a time dimension to the HTTP protocol.           Inspired by HTTP content negotiation, the protocol introduces the notion of datetime negotiation         that allows a client to request the version of a resource as it existed at a specified time in the past.&nbsp; Memento defines: 1) TimeGates (which redirect to the appropriate archived version of a resource), and 2) TimeMaps (a serialization of all archived copies and their archival time (\"Memento-Datetime\") in Link CoRE format).&nbsp; Memento became an RFC in late 2013 (http://www.rfc-editor.org/rfc/rfc7089.txt) and is a de facto standard for integrating archival holdings.</span></p>\n<p><span class=\"style3\">Some of the highlights of this project include:</span></p>\n<p><span class=\"style3\">* We showed the Memento Accept-Datetime header minimizes temporal drift in browsing sessions in web archives (as opposed to the current method of rewriting outbound links with the Memento-Datetime of the current page.&nbsp; http://arxiv.org/abs/1309.5503</span></p>\n<p><span class=\"style3\">* We demonstrated that \"query routing\" for URI lookup in archives based on TLDs from prior archive response can minimize the number of queries required by the Memento Aggregator.&nbsp; http://arxiv.org/abs/1309.4008</span></p>\n<p><span class=\"style3\">* We showed that a two week period is optimal for caching of TimeMaps.&nbsp; http://arxiv.org/abs/1307.5685</span></p>\n<p><span class=\"style3\">* We measured traffic in web archives and found that robots outnumber humans 10:1, and most traffic is for TimeMaps rather than mementos themselves.&nbsp; http://arxiv.org/abs/1309.4009</span></p>\n<p><span class=\"style3\">* We discoved that the majority of traffic (humans: 2/3, robots: 3/4) to the Internet Archive is for individual pages that are no longer available on the live web (as opposed to extended browsing sessions in the past).&nbsp; Wikipedia (13%) and Reddit (10%) lead the external referrers into the Internet Archive. http://arxiv.org/abs/1309.4016</span></p>\n<p><span class=\"style3\">* We developed the Temporal Intention Relevancy Model (TIRM) for measuring the relevance between a tweet and the resource that is linked to from that tweet.&nbsp; If the intent is to share the current state of the resource the live web is used, and if the intent is to share the state of the resource at the time of the tweet then web archives are used.&nbsp; http://arxiv.org/abs/1305.5959</span></p>\n<p><span class=\"style3\">* We found that ~11% of linked resources in tweets disappear within the first year, even when the subject is about important social events.&nbsp; http://arxiv.org/abs/1209.3026 http://arxiv.org/abs/1309.2648</span></p>\n<p><span class=\"style3\">In support of these research goals we developed and released several software packages, including:</span></p>\n<p><span class=\"style3\"...",
  "por_txt_cntn": "\nOur cultural discourse is increasingly carried in the web. With the initial emergence of the web many years ago, there was a period where conventional mediums (e.g., music, movies, books, scholarly publications) were primary and the web was a supplementary channel. This has now changed, where the web is often the primary channel, and other publishing mechanisms, if present at all, supplement the web. Unfortunately, the technology for publishing information on the web always outstrips our technology for preservation. My concern is less that we will lose data of known importance (e.g., scientific data, census data), but rather that we will lose data that we do not yet know is important.  Archiving the web is integral to our recording our scientific and social progress.\n\nOne of the main contributions of this project was the continued expansion and development of the Memento Framework (http://www.mementoweb.org/).  Memento specifies a technical framework aimed at a better integration of the current and the past Web.          The Memento protocol adds a time dimension to the HTTP protocol.           Inspired by HTTP content negotiation, the protocol introduces the notion of datetime negotiation         that allows a client to request the version of a resource as it existed at a specified time in the past.  Memento defines: 1) TimeGates (which redirect to the appropriate archived version of a resource), and 2) TimeMaps (a serialization of all archived copies and their archival time (\"Memento-Datetime\") in Link CoRE format).  Memento became an RFC in late 2013 (http://www.rfc-editor.org/rfc/rfc7089.txt) and is a de facto standard for integrating archival holdings.\n\nSome of the highlights of this project include:\n\n* We showed the Memento Accept-Datetime header minimizes temporal drift in browsing sessions in web archives (as opposed to the current method of rewriting outbound links with the Memento-Datetime of the current page.  http://arxiv.org/abs/1309.5503\n\n* We demonstrated that \"query routing\" for URI lookup in archives based on TLDs from prior archive response can minimize the number of queries required by the Memento Aggregator.  http://arxiv.org/abs/1309.4008\n\n* We showed that a two week period is optimal for caching of TimeMaps.  http://arxiv.org/abs/1307.5685\n\n* We measured traffic in web archives and found that robots outnumber humans 10:1, and most traffic is for TimeMaps rather than mementos themselves.  http://arxiv.org/abs/1309.4009\n\n* We discoved that the majority of traffic (humans: 2/3, robots: 3/4) to the Internet Archive is for individual pages that are no longer available on the live web (as opposed to extended browsing sessions in the past).  Wikipedia (13%) and Reddit (10%) lead the external referrers into the Internet Archive. http://arxiv.org/abs/1309.4016\n\n* We developed the Temporal Intention Relevancy Model (TIRM) for measuring the relevance between a tweet and the resource that is linked to from that tweet.  If the intent is to share the current state of the resource the live web is used, and if the intent is to share the state of the resource at the time of the tweet then web archives are used.  http://arxiv.org/abs/1305.5959\n\n* We found that ~11% of linked resources in tweets disappear within the first year, even when the subject is about important social events.  http://arxiv.org/abs/1209.3026 http://arxiv.org/abs/1309.2648\n\nIn support of these research goals we developed and released several software packages, including:\n\n* Carbon Date, which helps estimate the creation date of a URI http://ws-dl.blogspot.com/2013/04/2013-04-19-carbon-dating-web.html\n\n* ArcLink, an add on for the Wayback Machine archive software that provides an API for additional metadata extracted from archived web pages http://arxiv.org/abs/1305.5959\n\n* mcurl, a Memento-enabled version of the \"curl\" command line tool http://ws-dl.blogspot.com/2013/05/2013-05-29-mcurl-command-line-memento.html\n\n\n\n\nA list of popular media coverage ab..."
 }
}