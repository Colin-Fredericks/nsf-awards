{
 "awd_id": "1016754",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Active Learning of Language Models for Information Extraction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2010-08-15",
 "awd_exp_date": "2013-07-31",
 "tot_intn_awd_amt": 183736.0,
 "awd_amount": 183736.0,
 "awd_min_amd_letter_date": "2010-08-01",
 "awd_max_amd_letter_date": "2010-08-01",
 "awd_abstract_narration": "This project studies methods for extracting accurate knowledge bases from the \r\nWeb.  Fully-automated Web information extraction techniques are massively \r\nscalable, but have accuracy and coverage limitations.  This proposal \r\ninvestigates how to improve automated extraction techniques by introducing \r\ncarefully-selected human guidance.  The proposed system continually extracts \r\nknowledge from the Web, along the way dynamically synthesizing and issuing \r\nqueries to humans to increase the accuracy of the system's knowledge base and \r\nextractors.\r\n\r\nThe approach extends the PI's previous work utilizing statistical language \r\nmodels (SLMs) for information extraction.  Novel SLMs are investigated for \r\nunifying the extraction of relational data expressed in Web tables with \r\nextraction from free text.  New active learning techniques utilize the models \r\nto identify \"high-leverage\" queries -- requesting, for example, textual \r\nextraction patterns that when retrieved from the Web yield thousands of novel \r\nextractions.  The queries investigated are mostly amenable to non-experts, \r\nmeaning that much of the human input can be acquired at scale via online \r\nmass-collaboration.\r\n\r\nThe broader impact of this project lies in the potential for accurate Web \r\nextraction to radically improve Web search, allowing users to answer \r\ncomplicated questions by synthesizing information across multiple Web pages.  \r\nIn domains like medicine and biology, mining extracted knowledge bases could \r\nlead to important discoveries and novel therapies.\r\n\r\nFurther information may be found at the project web page:\r\nhttp://wail.eecs.northwestern.edu/projects/activelms/index.html",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Douglas",
   "pi_last_name": "Downey",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Douglas C Downey",
   "pi_email_addr": "dougd@allenai.org",
   "nsf_id": "000534948",
   "pi_start_date": "2010-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "633 CLARK ST",
  "perf_city_name": "EVANSTON",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602080001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 183736.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project studied methods for automatically extracting knowledge bases from the World Wide Web.&nbsp; The goal behind our work is to transform the Web&rsquo;s vast human-readable content into machine-understandable knowledge.&nbsp; This capability would enable transformative technologies, such as new search engines that answer complex questions by synthesizing information scattered across the Web.</p>\n<p>We focused on three primary research questions:</p>\n<ul>\n<li>How can we&nbsp;<strong>integrate</strong>&nbsp;knowledge extracted from both Web text and Web tables?</li>\n<li>How can&nbsp;<strong>statistical language models</strong>&nbsp;trained over large text corpora help improve extraction accuracy?</li>\n<li>How can an extraction system actively solicit&nbsp;<strong>well-selected human input</strong>&nbsp;to improve the extraction process?</li>\n</ul>\n<p>The project led to the invention of new knowledge extraction techniques, primarily aimed at Wikipedia&rsquo;s text and data tables.&nbsp; A fundamental knowledge extraction challenge involves automatically identifying relationships between concepts. &nbsp;We developed state-of-the-art methods for estimating the degree of semantic relatedness (SR) between two Wikipedia concepts, along with new methods for explaining the relationships to Web users in natural language.&nbsp; These methods leveraged machine learning techniques to mine Wikipedia&rsquo;s text, hyperlinks, and categories for semantic information. &nbsp;We also developed new techniques for extracting data from Wikipedia tables and automatically joining together different tables that contain related information.</p>\n<p>We also developed new methods for scaling up statistical language models (SLMs) for information extraction.&nbsp; &ldquo;Latent-variable&rdquo; SLMs have been shown to improve extraction systems, but the memory required to train the models forms a bottleneck.&nbsp; We developed a new method for overcoming the memory bottleneck, based on intelligently partitioning the corpus across a parallel computing cluster.&nbsp; Our experiments showed that the partitioning method decreases the memory footprint of model training by half for large data sets.</p>\n<p>The broader impacts of our work included student training, public prototype applications, and the release of data sets and code to the research community.&nbsp; Multiple PhD, MS, and undergraduate students participated in our research and co-authored publications.&nbsp; We also delivered a public prototype demonstrating our table extraction research, called &ldquo;WikiTables.&rdquo;&nbsp; An additional public prototype of the &ldquo;Atlasify&rdquo; system, which uses our semantic relatedness research to create interactive visualizations query concepts (e.g. &ldquo;nuclear power&rdquo;) on familiar reference systems (e.g. the World Map or periodic table), is under development.&nbsp; We disseminated our work to the research community in the form of multiple papers at major conferences and workshops, and we released other resources (including a codebase for our SLM training technique, new datasets for SR and table extraction, and a scalable public API for computing SR).&nbsp; The papers, prototypes, and other research products are publicly available.&nbsp; For further information, please consult the project Web site: <a href=\"http://websail.cs.northwestern.edu/activelms/\">http://websail.cs.northwestern.edu/activelms/</a></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/17/2013<br>\n\t\t\t\t\tModified by: Douglas&nbsp;C&nbsp;Downey</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"g...",
  "por_txt_cntn": "\nThis project studied methods for automatically extracting knowledge bases from the World Wide Web.  The goal behind our work is to transform the Web\u00c6s vast human-readable content into machine-understandable knowledge.  This capability would enable transformative technologies, such as new search engines that answer complex questions by synthesizing information scattered across the Web.\n\nWe focused on three primary research questions:\n\nHow can we integrate knowledge extracted from both Web text and Web tables?\nHow can statistical language models trained over large text corpora help improve extraction accuracy?\nHow can an extraction system actively solicit well-selected human input to improve the extraction process?\n\n\nThe project led to the invention of new knowledge extraction techniques, primarily aimed at Wikipedia\u00c6s text and data tables.  A fundamental knowledge extraction challenge involves automatically identifying relationships between concepts.  We developed state-of-the-art methods for estimating the degree of semantic relatedness (SR) between two Wikipedia concepts, along with new methods for explaining the relationships to Web users in natural language.  These methods leveraged machine learning techniques to mine Wikipedia\u00c6s text, hyperlinks, and categories for semantic information.  We also developed new techniques for extracting data from Wikipedia tables and automatically joining together different tables that contain related information.\n\nWe also developed new methods for scaling up statistical language models (SLMs) for information extraction.  \"Latent-variable\" SLMs have been shown to improve extraction systems, but the memory required to train the models forms a bottleneck.  We developed a new method for overcoming the memory bottleneck, based on intelligently partitioning the corpus across a parallel computing cluster.  Our experiments showed that the partitioning method decreases the memory footprint of model training by half for large data sets.\n\nThe broader impacts of our work included student training, public prototype applications, and the release of data sets and code to the research community.  Multiple PhD, MS, and undergraduate students participated in our research and co-authored publications.  We also delivered a public prototype demonstrating our table extraction research, called \"WikiTables.\"  An additional public prototype of the \"Atlasify\" system, which uses our semantic relatedness research to create interactive visualizations query concepts (e.g. \"nuclear power\") on familiar reference systems (e.g. the World Map or periodic table), is under development.  We disseminated our work to the research community in the form of multiple papers at major conferences and workshops, and we released other resources (including a codebase for our SLM training technique, new datasets for SR and table extraction, and a scalable public API for computing SR).  The papers, prototypes, and other research products are publicly available.  For further information, please consult the project Web site: http://websail.cs.northwestern.edu/activelms/\n\n \n\n\t\t\t\t\tLast Modified: 10/17/2013\n\n\t\t\t\t\tSubmitted by: Douglas C Downey"
 }
}