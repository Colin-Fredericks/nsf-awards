{
 "awd_id": "1035542",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Small: Methods and Tools: ROBOTS WITH VISION THAT FIND OBJECTS",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2010-09-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 550000.0,
 "awd_min_amd_letter_date": "2010-09-02",
 "awd_max_amd_letter_date": "2010-09-02",
 "awd_abstract_narration": "CPS:Small: Methods and Tools: Robots with vision that find objects\r\n\r\n       The objective of this research is the development of methods and software that will allow robots to detect and localize objects using Active Vision and develop descriptions of their visual appearance in terms of shape primitives. The approach is bio inspired and consists of three novel components. First, the robot will actively search the space of interest using an attention mechanism consisting of filters tuned to the appearance of objects. Second, an anthropomorphic segmentation mechanism will be used. The robot will fixate at a point within the attended area and segment the surface containing the fixation point, using contours and depth information from motion and stereo. Finally, a description of the segmented object, in terms of the contours of its visible surfaces and a qualitative description of their 3D shape will be developed.\r\n         The intellectual merit of the proposed approach comes from the bio-inspired design and the interaction of visual learning with advanced behavior. The availability of filters will allow the triggering of contextual models that work in a top-down fashion meeting at some point the bottom-up low-level processes. Thus, the approach defines, for the first time, the meeting point where perception happens. \r\n         The broader impacts of the proposed effort stem from the general usability of the proposed components. Adding top-down attention and segmentation capabilities to robots that can navigate and manipulate, will enable many technologies, for example household robots or assistive robots for the care of the elders, or robots in manufacturing, space exploration and education.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "JOHN",
   "pi_last_name": "ALOIMONOS",
   "pi_mid_init": "Y",
   "pi_sufx_name": "",
   "pi_full_name": "JOHN Y ALOIMONOS",
   "pi_email_addr": "yiannis@cfar.umd.edu",
   "nsf_id": "000176899",
   "pi_start_date": "2010-09-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Cornelia",
   "pi_last_name": "Fermuller",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Cornelia M Fermuller",
   "pi_email_addr": "fer@cfar.umd.edu",
   "nsf_id": "000235233",
   "pi_start_date": "2010-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland, College Park",
  "perf_str_addr": "3112 LEE BUILDING",
  "perf_city_name": "COLLEGE PARK",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 550000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>If I ask you for the scissors, you look around, you may go into another room, you may open a box or a drawer and after some some search you find the scissors. In our project we developed the theory and the implementation of the set of processes necessary for a robot to solve a similar problem, namely to find in images of a scene objects that it knows about. So, if the robot knows about a thousand different objects, our algorithms will allow the robot to locate those objects in images of scenes.</p>\n<p>To achieve this we had to develop a new image processing operator that was not known before, namely the \"torque\" operator. This operator provides places in the image that denote \"objecthood\", i.e. they are surrounded by boundaries. By modulating then the torque operator according to a prior object model (e.g. scissors), image places where the object may be light up, thus allowing us to find it after scrutinizing those locations through a process of segmentation.</p>\n<p>A robot that understands its environment well enough to safely navigate through it and also to search for and manipulate objects has obvious implications for a wide variety of applications. Two applications that<br />are immediately ripe are in Healthcare and in Warehouse operations. For example, a robot can navigate in a&nbsp; complex hospital environment with complete autonomy and&nbsp; be able to perform basic fetch functions<br />for patients that are bedridden or mobility-limited.</p>\n<p>.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/08/2013<br>\n\t\t\t\t\tModified by: John (Yiannis)&nbsp;Aloimonos</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIf I ask you for the scissors, you look around, you may go into another room, you may open a box or a drawer and after some some search you find the scissors. In our project we developed the theory and the implementation of the set of processes necessary for a robot to solve a similar problem, namely to find in images of a scene objects that it knows about. So, if the robot knows about a thousand different objects, our algorithms will allow the robot to locate those objects in images of scenes.\n\nTo achieve this we had to develop a new image processing operator that was not known before, namely the \"torque\" operator. This operator provides places in the image that denote \"objecthood\", i.e. they are surrounded by boundaries. By modulating then the torque operator according to a prior object model (e.g. scissors), image places where the object may be light up, thus allowing us to find it after scrutinizing those locations through a process of segmentation.\n\nA robot that understands its environment well enough to safely navigate through it and also to search for and manipulate objects has obvious implications for a wide variety of applications. Two applications that\nare immediately ripe are in Healthcare and in Warehouse operations. For example, a robot can navigate in a  complex hospital environment with complete autonomy and  be able to perform basic fetch functions\nfor patients that are bedridden or mobility-limited.\n\n.\n\n\t\t\t\t\tLast Modified: 12/08/2013\n\n\t\t\t\t\tSubmitted by: John (Yiannis) Aloimonos"
 }
}