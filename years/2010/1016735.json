{
 "awd_id": "1016735",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Adapting a Natural Logic Reasoning Platform to the Task of Entailment Inference",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2010-09-15",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 149999.0,
 "awd_amount": 149999.0,
 "awd_min_amd_letter_date": "2010-09-16",
 "awd_max_amd_letter_date": "2010-09-16",
 "awd_abstract_narration": "Current AI systems still lack the knowledge and reasoning \r\nabilities needed to handle the semantic subtleties of language \r\nand the thematic breadth of human discourse and thinking.\r\nThis project is developing a basic repertoire of lexical and\r\nother general knowledge for use in a powerful inference engine \r\n(EPILOG) designed expressly to support unrestricted language \r\nunderstanding and reasoning.\r\n\r\nThe methods being employed exploit the insights into language-based\r\ninference gained in recent years in the area of \"natural Logic\",\r\nwhich makes systematic use of word-level and structural entailment\r\nproperties of language. These are easily modeled in EPILOG, which\r\nuses a language-like meaning representation (Episodic Logic). Some\r\nvery general semantic properties are being manually encoded, and in \r\naddition, large numbers of knowledge items are being extracted\r\ncomputationally from lexical resources such as WordNet and VerbNet,\r\nand from word similarity or paraphrase clusters derived from large \r\ntext corpora.\r\n\r\nThe expected result is a knowledge base of fundamental lexical and\r\nother commonsense knowledge that will allow demonstration of many\r\npreviously infeasible language-based inferences, including both \r\nforward and backward reasoning and many multi-premise entailment \r\ninferences in existing test suites. This will significantly \r\nadvance the state of the art in basic language understanding and \r\nin mechanizing \"obvious inferences\", with potential applications \r\nto intelligent dialogue-based agents (for question answering, \r\ntutoring, personal assistance, etc.), and to knowledge bootstrapping \r\nthrough machine reading. The results will be disseminated both \r\nthrough papers at conferences and in journals, and through web sites \r\nmaking available EPILOG and the newly developed knowledge bases.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lenhart",
   "pi_last_name": "Schubert",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Lenhart K Schubert",
   "pi_email_addr": "schubert@cs.rochester.edu",
   "nsf_id": "000235732",
   "pi_start_date": "2010-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "910 GENESEE ST",
  "perf_city_name": "ROCHESTER",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146113847",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 149999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of Artificial Intelligence is to provide insight into thinking and intelligence, and to build useful and interesting artifacts that display these attributes, at least to some degree or in specialized areas. The grand goal that motivates much of the field is to endow AI systems with human-like conversational abilities and common sense, along with specialized expertise. &nbsp;At the present time, however, AI systems still lack the vast amounts of knowledge and the subtle reasoning abilities needed to handle the semantic complexities of language and the thematic breadth of human discourse and ordinary thinking.</p>\n<p>This project has taken some steps towards the grand goal of AI, both on the reasoning front and the knowledge accumulation front. &nbsp;The group working on the project has continued the development of an inference engine called EPILOG 2, to handle various types of inferences that humans find utterly obvious. For example, given that <em>\"[the bank] refused to reveal the fact that the building was to be condemned\"</em>, it follows that the building in question was to be condemned, and that the bank <em>did not </em>&nbsp;reveal this fact (presumably, when it should have done so). It also follows, for example, that an architectural structure was to be condemned, since after all buildings are architectural structures. Recent work in \"natural logic\" has focused on this type of inference, which rests in part on the semantics of so-called <em>implicative</em> words like <em>refuse, reveal</em>, and <em>fact,</em> and in part on so-called <em>entailment</em> relationships among words, such as that every building is an architectural structure. After addition of some new functionalities, EPILOG 2 easily and naturally handled these types of inferences, in part because the knowledge representation used by EPILOG is itself rather language-like.</p>\n<p>&nbsp;</p>\n<p>Part of the work of demonstrating natural-logic reasoning in EPILOG consisted of assembling and formalizing a sizable collection of implicative words (by scouring existing collections and augmenting these with the help of lexicons and thesauri), and a much larger collection of entailment relations between words (more than 100,000). The latter were obtained by \"mining\" a very large online lexicon, WordNet, and one accomplishment of the project was to develop ways of avoiding many of the errors that such a mining process can lead to. For example, in following a sequence of ever more general word senses in WordNet, starting at <em>font&nbsp;&nbsp;</em>(i.e., a print style), we arrive at the term <em>communication</em>, but we do not wish to conclude that a font is a communication, when in fact it is just a print style that may be <em>used</em> in a communication.</p>\n<p>&nbsp;</p>\n<p>Natural-logic inferences still fall far short of the kinds of inferences people make with ease, such as that <em>dining</em> entails eating a substantial meal, probably relatively late in the day (and probably only once per day), and that the diner probably starts out at least somewhat hungry and ends up more or less sated. Thus we need knowledge about the preconditions, process, and consequences entailed by verbs like <em>dine</em> -- and the many thousands of other verbs in any adequate lexicon. We also need general knowledge such as the typical times of day and frequency of meals, the enjoyment derived from eating, the cooking and preparations that are necessary, and so on.</p>\n<p>The project made progress on gathering and formalizing many pieces of knowledge of these types. One method that was employed was to formalize the essential commonalities and differences of verbs in families of related verbs (for example, <em>run, walk, amble, saunter, march</em>, etc. all refer to locomotion on foot, but differ in the manner and speed they imply). Another method was to mechanically extract meaningful fragments from large te...",
  "por_txt_cntn": "\nThe goal of Artificial Intelligence is to provide insight into thinking and intelligence, and to build useful and interesting artifacts that display these attributes, at least to some degree or in specialized areas. The grand goal that motivates much of the field is to endow AI systems with human-like conversational abilities and common sense, along with specialized expertise.  At the present time, however, AI systems still lack the vast amounts of knowledge and the subtle reasoning abilities needed to handle the semantic complexities of language and the thematic breadth of human discourse and ordinary thinking.\n\nThis project has taken some steps towards the grand goal of AI, both on the reasoning front and the knowledge accumulation front.  The group working on the project has continued the development of an inference engine called EPILOG 2, to handle various types of inferences that humans find utterly obvious. For example, given that \"[the bank] refused to reveal the fact that the building was to be condemned\", it follows that the building in question was to be condemned, and that the bank did not  reveal this fact (presumably, when it should have done so). It also follows, for example, that an architectural structure was to be condemned, since after all buildings are architectural structures. Recent work in \"natural logic\" has focused on this type of inference, which rests in part on the semantics of so-called implicative words like refuse, reveal, and fact, and in part on so-called entailment relationships among words, such as that every building is an architectural structure. After addition of some new functionalities, EPILOG 2 easily and naturally handled these types of inferences, in part because the knowledge representation used by EPILOG is itself rather language-like.\n\n \n\nPart of the work of demonstrating natural-logic reasoning in EPILOG consisted of assembling and formalizing a sizable collection of implicative words (by scouring existing collections and augmenting these with the help of lexicons and thesauri), and a much larger collection of entailment relations between words (more than 100,000). The latter were obtained by \"mining\" a very large online lexicon, WordNet, and one accomplishment of the project was to develop ways of avoiding many of the errors that such a mining process can lead to. For example, in following a sequence of ever more general word senses in WordNet, starting at font  (i.e., a print style), we arrive at the term communication, but we do not wish to conclude that a font is a communication, when in fact it is just a print style that may be used in a communication.\n\n \n\nNatural-logic inferences still fall far short of the kinds of inferences people make with ease, such as that dining entails eating a substantial meal, probably relatively late in the day (and probably only once per day), and that the diner probably starts out at least somewhat hungry and ends up more or less sated. Thus we need knowledge about the preconditions, process, and consequences entailed by verbs like dine -- and the many thousands of other verbs in any adequate lexicon. We also need general knowledge such as the typical times of day and frequency of meals, the enjoyment derived from eating, the cooking and preparations that are necessary, and so on.\n\nThe project made progress on gathering and formalizing many pieces of knowledge of these types. One method that was employed was to formalize the essential commonalities and differences of verbs in families of related verbs (for example, run, walk, amble, saunter, march, etc. all refer to locomotion on foot, but differ in the manner and speed they imply). Another method was to mechanically extract meaningful fragments from large text collections (news media, Wikipedia, etc.) and shape these into general knowledge. For example, from a weblog containing the snippet \"Before enjoying a delicious home-cooked meal, ...\", the extraction code derives \"Many meals are delicious..."
 }
}