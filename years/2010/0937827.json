{
 "awd_id": "0937827",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Dynamic Staging Architecture for Accelerating I/O Pipelines",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2010-04-01",
 "awd_exp_date": "2014-03-31",
 "tot_intn_awd_amt": 448290.0,
 "awd_amount": 448290.0,
 "awd_min_amd_letter_date": "2010-04-19",
 "awd_max_amd_letter_date": "2010-04-19",
 "awd_abstract_narration": "Petascale applications are producing terabytes of data at a great rate. Storage systems in large-scale machines are significantly stressed as I/O rates are not growing as fast to cope with data production. A variety of HPC activities such as writing output and checkpoint data are all stymied by the I/O bandwidth bottleneck. Further to this, the post-processing and subsequent analysis/visualization of computational results is increasingly time consuming due to the widening gap between the storage/processing capacities of supercomputers and users' local clusters. \r\n\r\nThis research focuses on building a novel in-job dynamic data staging architecture and in bringing it to bear on the looming petascale I/O crisis. To this end, the following objectives are investigated: (i) the concerted use of node-local memory and emerging hardware such as Solid State Disks (SSDs), from a dedicated set of nodes, as a means to alleviate the I/O bandwidth bottleneck, (ii) the multiplexing of traditional user post-processing pipelines and secondary computations with asynchronous I/O on the staging ground to perform scalable I/O and data analytics, (iii) bypassing memory to access the staging area, and (iv) enabling QoS both in the staging ground and in the communication channel connecting it to compute client and persistent storage.\r\n\r\nThis study will have a wide-ranging impact on future provisioning of extreme-scale machines and will provide formative guidelines to this end. The result of this research will be a set of integrated techniques that can fundamentally change the current parallel I/O model and accelerate petascale I/O pipelines. Further, this research will help analyze the utility of SSDs in day-to-day supercomputing I/O and inform the wider HPC community of its viability.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sudharshan",
   "pi_last_name": "Vazhkudai",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Sudharshan S Vazhkudai",
   "pi_email_addr": "svazhkud@utk.edu",
   "nsf_id": "000354494",
   "pi_start_date": "2010-04-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "201 ANDY HOLT TOWER",
  "perf_city_name": "KNOXVILLE",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "795200",
   "pgm_ele_name": "HECURA"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 448290.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Petascale applications are producing terabytes of data at a rate much faster than Moore&rsquo;s law scaling. Storage systems in large scale machines are significantly stressed as I/O rates are not growing as fast to cope with data production. A variety of HPC activities such as writing outputs, checkpointing and post-processing are all stymied by the I/O bandwidth bottleneck. Consequently, state-of the-art supercomputing applications often cannot generate as much data as desired by scientists (in terms of data or time resolution) due to the cost of I/O and post-processing pipelines.</p>\n<p>Our approach in this proposal has been to accelerate these I/O pipelines by building a novel in-job data staging architecture from a dedicated set of nodes using a combination of node-local memory and emerging hardware such as Solid State Disks (SSDs). This staging ground is then used to perform scalable I/O and data analytics while a much larger number of nodes are running the parallel application. We have further devised a novel middleware library that would allow the use of this staging area as a memory extension device so that applications can perform out-of-core computations on emerging SSD devices on large-scale HPC machines.</p>\n<p>Intellectual Merit: The techniques developed in this research project have the broader impact of re-vitalizing data analytics in an in-situ fashion on SSDs and in an out-of-core fashion.</p>\n<p>Impact: These techniques can benefit a variety of scientific applications that are both and I/O and memory starved on modern HPC systems.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/29/2014<br>\n\t\t\t\t\tModified by: Sudharshan&nbsp;S&nbsp;Vazhkudai</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPetascale applications are producing terabytes of data at a rate much faster than Moore\u00c6s law scaling. Storage systems in large scale machines are significantly stressed as I/O rates are not growing as fast to cope with data production. A variety of HPC activities such as writing outputs, checkpointing and post-processing are all stymied by the I/O bandwidth bottleneck. Consequently, state-of the-art supercomputing applications often cannot generate as much data as desired by scientists (in terms of data or time resolution) due to the cost of I/O and post-processing pipelines.\n\nOur approach in this proposal has been to accelerate these I/O pipelines by building a novel in-job data staging architecture from a dedicated set of nodes using a combination of node-local memory and emerging hardware such as Solid State Disks (SSDs). This staging ground is then used to perform scalable I/O and data analytics while a much larger number of nodes are running the parallel application. We have further devised a novel middleware library that would allow the use of this staging area as a memory extension device so that applications can perform out-of-core computations on emerging SSD devices on large-scale HPC machines.\n\nIntellectual Merit: The techniques developed in this research project have the broader impact of re-vitalizing data analytics in an in-situ fashion on SSDs and in an out-of-core fashion.\n\nImpact: These techniques can benefit a variety of scientific applications that are both and I/O and memory starved on modern HPC systems.\n\n\t\t\t\t\tLast Modified: 06/29/2014\n\n\t\t\t\t\tSubmitted by: Sudharshan S Vazhkudai"
 }
}