{
 "awd_id": "0329247",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Difficult Problems in Stereo and Motion Analysis",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Daniel F. DeMenthon",
 "awd_eff_date": "2003-09-01",
 "awd_exp_date": "2007-08-31",
 "tot_intn_awd_amt": 220000.0,
 "awd_amount": 220000.0,
 "awd_min_amd_letter_date": "2003-07-10",
 "awd_max_amd_letter_date": "2005-05-04",
 "awd_abstract_narration": "Robotics and Computer Vision Program\r\n\r\nABSTRACT\r\n\r\n\r\nProposal #: 0329247\r\nTitle: Difficult Problems in Stereo and Motion Analysis\r\nPI: Medioni, Gerard\r\nU of Southern California\r\n\r\nThe project proposes to address two difficult problems in stereo and motion analysis. These two areas of computer vision have been explored with numerous approaches and very good results have been achieved, although for limited types of scenes, such as a single, smooth and textured surface. Occlusion is a cause of many remaining difficulties, since most algorithms cannot operate on areas visible in one image only. Furthermore, even though the human visual system has no difficulty estimating the depth or velocity of occluded objects, computer vision systems are incapable of that. They either produce arbitrary results for the occluded regions, or, at best, mark them as occluded. The second issue we need to address is the need for processing at multiple scales. Many real scenes contain objects that are perceived at different scales, both due to variations in size and level of detail and due to variations in texture density. A multiple scale processing scheme is necessary for processing such scenes. It must be capable of detecting and preserving fine details, not allowing larger or richer in texture regions to dominate smaller or less textures ones, achieving good continuation of structures, and filling in missing data.\r\n\r\nTo overcome these unavoidable difficulties, it is proposed to address stereo and motion analysis from a perceptual organization point of view using the tensor voting framework. It is claimed that tokens, generated by matching corresponding pixels in the two images, form coherent perceptual structures in the appropriate space, while erroneous matches generate outlier tokens. The tensor voting framework allows us to efficiently detect perceptual structures without employing parametric models, which makes it possible to handle arbitrary structures. Furthermore, the tensor voting framework is non-iterative, requires no initial estimate and has only one critical parameter, the scale of voting. It is proposed to explicit handle occlusion by incorporating monocular information from the images in conjunction with the results of binocular processing. Our proposed methodology not only detects occluded regions, but also computes estimates for the disparity or velocity of these regions, as shown in the preliminary results presented in the project description. Instead of compromising the quality of the results by selecting a single scale of analysis for the entire scene, which would be sub-optimal in most regions, our approach automatically infers the smallest scale that can preserve the finer details, then proceeds with larger scales to ensure good continuation in regions with sparse or missing data.\r\n\r\nThe preliminary results compare favorably to the best algorithms reported to date, including some standard data sets. Based on these results, the proposed approach is expected to significantly advance the state of the art. The proposed research will also have an impact in the learning of both graduate and undergraduate students at USC, and will be made available to the scientific community through publications and presentations we intend to make, as well as on the World Wide Web. It is believed that addressing the difficulties described here is a critical step for the development of operational computer vision systems, since it is an attempt to bridge the gap between image acquisition and high-level vision, both of which are at a more mature state than mid-level computer vision.\r\n \r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gerard",
   "pi_last_name": "Medioni",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Gerard G Medioni",
   "pi_email_addr": "medioni@iris.usc.edu",
   "nsf_id": "000448680",
   "pi_start_date": "2003-07-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S FLOWER ST FL 3",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "90033",
  "perf_ctry_code": "US",
  "perf_cong_dist": "34",
  "perf_st_cong_dist": "CA34",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "684000",
   "pgm_ele_name": "ROBOTICS"
  },
  {
   "pgm_ele_code": "733900",
   "pgm_ele_name": "COMPUTER VISION"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9139",
   "pgm_ref_txt": "INFORMATION INFRASTRUCTURE & TECH APPL"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0103",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0103",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0104",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0104",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0105",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0105",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2003,
   "fund_oblg_amt": 70000.0
  },
  {
   "fund_oblg_fiscal_yr": 2004,
   "fund_oblg_amt": 70000.0
  },
  {
   "fund_oblg_fiscal_yr": 2005,
   "fund_oblg_amt": 80000.0
  }
 ],
 "por": null
}