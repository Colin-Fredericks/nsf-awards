{
 "awd_id": "0312551",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "ITR:  Collaborative Research:  Modeling and Display of Haptic Information for Enhanced Performance of Computer-Integrated Surgery",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Pinaki Mazumder",
 "awd_eff_date": "2003-08-15",
 "awd_exp_date": "2006-07-31",
 "tot_intn_awd_amt": 165794.0,
 "awd_amount": 177794.0,
 "awd_min_amd_letter_date": "2003-08-24",
 "awd_max_amd_letter_date": "2006-06-19",
 "awd_abstract_narration": "\r\n \r\nInformation Technology is making great progress in the operating room. In the practice of minimally invasive surgery, three-dimensional modeling techniques are enabling preoperative planning, new image acquisition and display techniques, and superior dexterity through teleoperated robotic systems. However, one sensory channel is presently ignored in these technological improvements: touch. In manual surgeries, haptic feedback is crucial for palpation, suture manipulation, and detection of puncture events. Sensation of forces is particularly important for invasive tool-tissue interaction tasks such as grasping, cutting, dissection, and percutaneous (GCDP) therapy. Lack of haptic information overloads the visual sensing required of the surgeon, requiring a demanding level attention. For information technology to truly enhance the practice of surgery, multiple sensory channels must be utilized. \r\n \r\nWe propose to develop instrumentation and algorithms for modeling the haptic aspect of tool-tissue interaction in four common surgical tasks, namely: grasping, cutting, dissection, and percutaneous therapy. This system will significantly enhance information display in three ways. First, data acquired in real time will be used to provide feedback to the surgeon during model-based teleoperated procedures, increasing the transparency. of the robot-assisted surgical system. Second, real-time modeling techniques will enable model-based teleoperation, removing the strict constraints imposed by time delays in traditional, direct teleoperation. Third, realistic surgical simulations will improve training, increasing surgeon competence and patient safety. The instrumentation and modeling algorithms will be used to determine parameter values for different tissue types, particularly liver, prostate, spleen, and kidney. Ex-vivo tissues and phantom hydrogel tissues will be used in these experiments. Once the instrumentation and modeling techniques are developed, we will proceed to extensive validation of the three applications of enhanced information display.  Performance experiments will verify improvements in accuracy and precision for direct and model-based teleoperation, and a computer vision/force sensing system will determine the realism of force and deformation models developed for surgical simulation. This proposal addresses the ITR challenge for developing an information-enhanced display with computational, simulation, and data analysis methods for modeling  common  surgical GCDP  tasks  for  which  currently there is no systematic model. \r\n \r\nIntellectual  merit: The specific goal of this project is to significantly improve the information-enhanced operating room through the sensing and acquisition of models representing haptic information during tool-tissue interactions in minimally  invasive surgery. This research will significantly impact: (a) tool-tissue interaction models that reflect the actual forces and deformation occurring during surgery, (b) haptic feedback to the surgeon during direct or modelbased teleoperation, thereby improving surgical outcomes, c) the development of realistic simulations for training current and future health care professionals, and d) the development of instrumented smart tools for both traditional minimally invasive and robot-assisted surgeries. \r\n \r\nBroader  Impacts: The PIs from both institutions have an excellent history of involving undergraduate students (both men and women) in research projects through Research Experience for Undergraduates (REU). Additionally, the Drexel Research Experience for Teachers (RET) site proposal recommended for funding by NSF, along with an established RET program at Johns Hopkins, will involve high-school math and science teachers from high schools to participate in summer research projects related to this proposal. These activities will enhance participation of underrepresented  groups from inner-city schools and lead to broader dissemination of scientific and technological education of the students and teachers. \r\nFor graduate students, the interdisciplinary nature of this research offers new opportunities in education, broadening the interaction of mechanical engineers and medical professionals. Finally, the proposed research will lead to improvements in surgical outcomes, benefiting the patient and the society at large. \r\n \r\nDue to the diverse areas of research required by this project, the proposed research will benefit from the collaboration of  PIs from Drexel and JHU.  Drexel  has  expertise in  haptics, FEM modeling, grasping/dissection  tasks,  and phantom tissues while JHU has expertise  in reality-based modeling for cutting/percutaneous therapies,  haptics, and validation experiments for human-machine systems. The unique facilities and collaborations at both  institutions, along with strong  ties to medical professionals, make this work appropriate as a collaborative research project. \r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Allison",
   "pi_last_name": "Okamura",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Allison M Okamura",
   "pi_email_addr": "aokamura@stanford.edu",
   "nsf_id": "000443791",
   "pi_start_date": "2003-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N CHARLES ST",
  "perf_city_name": "BALTIMORE",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "168600",
   "pgm_ele_name": "ITR SMALL GRANTS"
  },
  {
   "pgm_ele_code": "735300",
   "pgm_ele_name": "EMERGING MODELS & TECHNOLOGIES"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1652",
   "pgm_ref_txt": "HIGH END COMPUTATION AND INFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0103",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0103",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0104",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0104",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2003,
   "fund_oblg_amt": 165794.0
  },
  {
   "fund_oblg_fiscal_yr": 2004,
   "fund_oblg_amt": 6000.0
  },
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 6000.0
  }
 ],
 "por": null
}