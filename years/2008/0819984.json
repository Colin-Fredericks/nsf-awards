{
 "awd_id": "0819984",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HRI: Perceptually Situated Human-Robot Dialog Models",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2008-01-01",
 "awd_exp_date": "2012-07-31",
 "tot_intn_awd_amt": 814987.0,
 "awd_amount": 846987.0,
 "awd_min_amd_letter_date": "2008-01-30",
 "awd_max_amd_letter_date": "2011-07-15",
 "awd_abstract_narration": "Humans naturally use dialog and gestures to discuss complex phenomena and plans, especially when they refer to physical aspects of the environment while they communicate with each other.  Existing robot vision systems can sense people and the environment, but are limited in their ability to detect the detailed conversational cues people often rely upon (such as head pose, eye gaze, and body gestures), and to exploit those cues in multimodal conversational dialog.  Recent advances in computer vision have made it possible to track such detailed cues.  Robots can use passive measures to sense the presence of people, estimate their focus of attention and body pose, and to recognize human gestures and identify physical references.  But they have had limited means of integrating such information into models of natural language; heretofore, they have used dialog models for specific domains and/or were limited to one-on-one interaction.  Separately, recent advances in natural language processing have led to dialog models that can track relatively free-form conversation among multiple participants, and extract meaningful semantics about people's intentions and actions.   These multi-party dialog models have been used in meeting environments and other domains.  In this project, the PI and his team will fuse these two lines of research to achieve a perceptually situated, natural conversation model that robots can use to interact multimodally with people.  They will develop a reasonably generic dialog model that allows a situated agent to track the dialog around it, know when it is being addressed, and take direction from a human operator regarding where it should find or place various objects, what it should look for in the environment, and which individuals it should attend to, follow, or obey.   Project outcomes will extend existing dialog management techniques to a more general theory of interaction management, and will also extend current state-of-the-art vision research to be able to recognize the subtleties of nonverbal conversational cues, as well as methods for integrating those cues with ongoing dialog interpretation and interaction with the world.\r\n\r\nBroader Impacts:  There are clearly many positive societal impacts that will derive from this research.  Ultimately, development of effective human-robot interfaces will allow greater deployment of robots to perform dangerous tasks that humans would otherwise have to perform, and will also enable greater use of robots for service tasks in domestic environments.  As part of the project, the PI will conduct outreach efforts to engage secondary-school students in the hope that exposure to HRI research may increase their interest in science and engineering studies.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Trevor",
   "pi_last_name": "Darrell",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Trevor J Darrell",
   "pi_email_addr": "trevor@eecs.berkeley.edu",
   "nsf_id": "000175078",
   "pi_start_date": "2008-01-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "International Computer Science Institute",
  "inst_street_address": "2150 SHATTUCK AVE",
  "inst_street_address_2": "SUITE 250",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106662900",
  "inst_zip_code": "947041345",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "INTERNATIONAL COMPUTER SCIENCE INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GSRMP1QCXU74"
 },
 "perf_inst": {
  "perf_inst_name": "International Computer Science Institute",
  "perf_str_addr": "2150 SHATTUCK AVE",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947041345",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "763200",
   "pgm_ele_name": "HUMAN-ROBOT INTERACTION"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 814987.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our findings are in three main areas: multimodal pronoun reference for humanrobotinteraction, multimodal co-training for audio-visual gesture recognition,and grounded multimodal Learning for Human-Robot-Object Interaction, andare described in turn in the following sections.</p>\n<p>Automatic scene understanding, or the ability to categorize places and objectsin the immediate environment, is important for many HRI applications, includingmobile robotic assistants for the elderly and the disabled. Category-levelrecognition allows the system to recognize a class of objects, as opposed to justsingle instances, and is particularly useful. One approach to automatic sceneunderstanding is through image-based recognition, which involves training aclassifier for each scene or object category offline, using manually labeled images.However, to date, image-based category recognition has only reached afraction of human performance, especially in terms of the variety of recognizedcategories, partly due to the lack of labeled data. Accurate and efficient offthe-shelf recognizers are only available for a handful of objects, such as facesand cars. Thus, to enable an assistant robot, or a similar system, to accuratelyrecognize objects in the environment, the user currently would have to collect&nbsp;and manually annotate sample images of those objects.&nbsp;Alternatively, a robot can learn about its surroundings from interactions with the user.&nbsp;</p>\n<p>In this work, our goal is to enable human-computer interaction systems torecognize a variety of object categories in realistic environments without requiringmanual annotation of each category by the user. We propose a newapproach, combining speech and visual object category recognition. The approachconsists of two parts: disambiguation and adaptation. Disambiguationmeans that, instead of relying completely on one modality, we will use genericvisual object classifiers to help the speech recognizer obtain the correct objectlabel. &nbsp;The goal of adaptation is, given a labeled generic databaseand a small number of labeled adaptation examples, to build the optimal visualcategory classifiers for that particular environment. Speech and language modelscan also be adapted to the particular speaker. Since image databases arelimited in the number of categories, another goal of adaptation can be to learnout-of-vocabulary objects, i.e. objects whose images and referring words are notin the generic labeled databases. This can be acheived by exploiting unlabeledimages available on the web. For example, we can match the reference image ofthe unknown object to a subset of images returned by an image search for thetop most likely words for that object.</p>\n<p>We address the problem of unsupervised learning of object classifiers forvisually polysemous words. Visual polysemy means that a word has several dictionarysenses that are visually distinct. Web images are a rich and free resourcecompared to traditional human-labeled object datasets. Potential training datafor arbitrary objects can be easily obtained from image search engines like Yahooor Google. The drawback is that multiple word meanings often lead to mixed results,especially for polysemous words. For example, the query &ldquo;mouse&rdquo; returnsmultiple senses on the first page of results: &ldquo;computer&rdquo; mouse, &ldquo;animal&rdquo; mouse,and &ldquo;Mickey Mouse&rdquo;. The dataset thus obtained suffers from low precision ofany particular visual sense. &nbsp;In this effort we&nbsp;have also collected a 3-D Object dataset (B3DO), an ongoing collection effort using theKinect sensor in domestic environments.&nbsp;</p>\n<p>The English pronoun you is the second most frequent word in unrestricted conversation(after I and right before it). Despite this, with the exception of Guptaet al. (2007b; 2007a), its resolution has received very little attention in theliterature. This is perhaps not ...",
  "por_txt_cntn": "\nOur findings are in three main areas: multimodal pronoun reference for humanrobotinteraction, multimodal co-training for audio-visual gesture recognition,and grounded multimodal Learning for Human-Robot-Object Interaction, andare described in turn in the following sections.\n\nAutomatic scene understanding, or the ability to categorize places and objectsin the immediate environment, is important for many HRI applications, includingmobile robotic assistants for the elderly and the disabled. Category-levelrecognition allows the system to recognize a class of objects, as opposed to justsingle instances, and is particularly useful. One approach to automatic sceneunderstanding is through image-based recognition, which involves training aclassifier for each scene or object category offline, using manually labeled images.However, to date, image-based category recognition has only reached afraction of human performance, especially in terms of the variety of recognizedcategories, partly due to the lack of labeled data. Accurate and efficient offthe-shelf recognizers are only available for a handful of objects, such as facesand cars. Thus, to enable an assistant robot, or a similar system, to accuratelyrecognize objects in the environment, the user currently would have to collect and manually annotate sample images of those objects. Alternatively, a robot can learn about its surroundings from interactions with the user. \n\nIn this work, our goal is to enable human-computer interaction systems torecognize a variety of object categories in realistic environments without requiringmanual annotation of each category by the user. We propose a newapproach, combining speech and visual object category recognition. The approachconsists of two parts: disambiguation and adaptation. Disambiguationmeans that, instead of relying completely on one modality, we will use genericvisual object classifiers to help the speech recognizer obtain the correct objectlabel.  The goal of adaptation is, given a labeled generic databaseand a small number of labeled adaptation examples, to build the optimal visualcategory classifiers for that particular environment. Speech and language modelscan also be adapted to the particular speaker. Since image databases arelimited in the number of categories, another goal of adaptation can be to learnout-of-vocabulary objects, i.e. objects whose images and referring words are notin the generic labeled databases. This can be acheived by exploiting unlabeledimages available on the web. For example, we can match the reference image ofthe unknown object to a subset of images returned by an image search for thetop most likely words for that object.\n\nWe address the problem of unsupervised learning of object classifiers forvisually polysemous words. Visual polysemy means that a word has several dictionarysenses that are visually distinct. Web images are a rich and free resourcecompared to traditional human-labeled object datasets. Potential training datafor arbitrary objects can be easily obtained from image search engines like Yahooor Google. The drawback is that multiple word meanings often lead to mixed results,especially for polysemous words. For example, the query \"mouse\" returnsmultiple senses on the first page of results: \"computer\" mouse, \"animal\" mouse,and \"Mickey Mouse\". The dataset thus obtained suffers from low precision ofany particular visual sense.  In this effort we have also collected a 3-D Object dataset (B3DO), an ongoing collection effort using theKinect sensor in domestic environments. \n\nThe English pronoun you is the second most frequent word in unrestricted conversation(after I and right before it). Despite this, with the exception of Guptaet al. (2007b; 2007a), its resolution has received very little attention in theliterature. This is perhaps not surprising since the vast amount of work onanaphora and reference resolution has focused on text or discourse - mediumswhere second-person deixis is perhaps not as promi..."
 }
}