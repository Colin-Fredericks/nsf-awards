{
 "awd_id": "0812297",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI-Small: Learning to Generate High Quality Paraphrases with a Broad Coverage Lexicalized Grammar",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2008-09-01",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 380621.0,
 "awd_amount": 396621.0,
 "awd_min_amd_letter_date": "2008-08-16",
 "awd_max_amd_letter_date": "2011-03-21",
 "awd_abstract_narration": "Automatic paraphrasing is considered vital to applications as diverse as machine translation (MT), question answering, summarization, and dialogue systems. Paraphrasing has also been shown recently to hold promise for automatic methods of evaluating MT, when the paraphrases are of sufficiently high quality. \r\n\r\nThis project investigates novel methods for acquiring and generating such high\r\nquality paraphrases in order to automatically approximate the human translation\r\nerror rate (HTER) metric for MT evaluation, where human annotators post-edit MT\r\noutputs into acceptable paraphrases of the reference translations. The project\r\nemphasizes the use of a linguistically informed, grammar-based parser and\r\nrealizer for acquiring and generating paraphrases using disjunctive logical\r\nforms (DLFs), in sharp contrast to most recent work that relies entirely on\r\nshallow methods.  Specifically, the project investigates methods of (1)\r\nengineering a broad coverage English grammar from the CCGbank, with semantic\r\nroles integrated from Propbank; (2) scaling up OpenCCG for efficient parsing and realization with this grammar, adapting supertagging and parse ranking methods for generation; (3) adapting and extending previous methods of acquiring paraphrases to work on DLFs; (4) generating high quality n-best paraphrases of one or more reference sentences; and (5) experimentally evaluating whether the automatically generated paraphrases can be used with current MT metrics to yield improved correlations with human judgments of translation quality. \r\n\r\nBy providing a way to automatically approximate the HTER metric, the project will help drive future MT research. Additionally, by dramatically extending the realization capacity of OpenCCG, the project promises to benefit a wide range of NLP tasks where the breadth of target texts is of crucial importance.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "White",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael White",
   "pi_email_addr": "mwhite@ling.ohio-state.edu",
   "nsf_id": "000069401",
   "pi_start_date": "2008-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University Research Foundation -DO NOT USE",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "Columbus",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888734",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "QR7NH79713E5"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "1960 KENNY RD",
  "perf_city_name": "COLUMBUS",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 127861.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 260760.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we investigated methods of generating high-quality paraphrases using a broad coverage lexicalized grammar and automatically learned preferences for choosing among possible paraphrases. In recent years, word-level paraphrases have been investigated in a broad coverage setting, with applications in various text-to-text generation tasks such as summarization, simplification and machine translation, as well as in the automatic evaluation of machine translation systems. However, there has been little research on generating paraphrases of unrestricted text with a grammar. We developed novel techniques for generating paraphrases by parsing a sentence into a representation of its meaning, then generating back sentences that express the same meaning according to the grammar. In our case, the grammar was extracted from an enhanced version of the CCGbank, a corpus of sentence derivations based on the Penn Treebank following the principles of Combinatory Categorial Grammar (CCG). With a broad coverage grammar, there are often many expressions generated this way, not all of which adequately express the original sentence's meaning in a truly grammatical way.</p>\n<p>To select preferred outputs, we developed a realization ranker that improved upon existing models on all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research: inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two state-of-the-art probabilistic models of syntax that have been developed for CCG parsing. Using averaged perceptron models, a form of discriminative machine learning, we trained a model to combine these existing syntactic models with several n-gram language models, which are simple probabilistic sequence models not requiring a treebank. This model improved upon the state-of-the-art in terms of automatic evaluation scores on held-out test data, but nevertheless our error analysis revealed a surprising number of word order, function word and inflection errors, spurring us to tackle each issue in turn.&nbsp;</p>\n<p>To reduce the number of subject-verb agreement errors, we extended the model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and with expressions such as 'a lot of' where the correct choice is not determined solely by the head noun. We also improved animacy agreement with relativizers, reducing the number of errors where 'that' or 'which' was chosen to modify an animate noun rather than 'who' or 'whom' (and vice-versa), while also allowing both choices where corpus evidence was mixed.</p>\n<p>With function words, we showed that we could improve upon the model's predictions for when to employ 'that'-complementizers using featured inspired by Florian Jaeger's work on using the principle of uniform information density, which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency. In news text, complementizers are left out two times out of three, but in some cases the presence of 'that' is crucial to the interpretation. Generally, inserting a complementizer makes the onset of a complement clause more predictable, and thus less information dense, thereby avoiding a potential spike in information density that is associated with comprehension difficulty. See Figure 1 for an example.</p>\n<p>Finally, to improve word ordering decisions, we demonstrated that incorporating a feature inspired by Edward Gibson's dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors as confirme...",
  "por_txt_cntn": "\nIn this project, we investigated methods of generating high-quality paraphrases using a broad coverage lexicalized grammar and automatically learned preferences for choosing among possible paraphrases. In recent years, word-level paraphrases have been investigated in a broad coverage setting, with applications in various text-to-text generation tasks such as summarization, simplification and machine translation, as well as in the automatic evaluation of machine translation systems. However, there has been little research on generating paraphrases of unrestricted text with a grammar. We developed novel techniques for generating paraphrases by parsing a sentence into a representation of its meaning, then generating back sentences that express the same meaning according to the grammar. In our case, the grammar was extracted from an enhanced version of the CCGbank, a corpus of sentence derivations based on the Penn Treebank following the principles of Combinatory Categorial Grammar (CCG). With a broad coverage grammar, there are often many expressions generated this way, not all of which adequately express the original sentence's meaning in a truly grammatical way.\n\nTo select preferred outputs, we developed a realization ranker that improved upon existing models on all three interrelated sub-tasks traditionally considered part of the surface realization task in natural language generation research: inflecting lemmas with grammatical word forms, inserting function words and linearizing the words in a grammatical and natural order. The model takes as its starting point two state-of-the-art probabilistic models of syntax that have been developed for CCG parsing. Using averaged perceptron models, a form of discriminative machine learning, we trained a model to combine these existing syntactic models with several n-gram language models, which are simple probabilistic sequence models not requiring a treebank. This model improved upon the state-of-the-art in terms of automatic evaluation scores on held-out test data, but nevertheless our error analysis revealed a surprising number of word order, function word and inflection errors, spurring us to tackle each issue in turn. \n\nTo reduce the number of subject-verb agreement errors, we extended the model with features enabling it to make correct verb form choices in sentences involving complex coordinate constructions and with expressions such as 'a lot of' where the correct choice is not determined solely by the head noun. We also improved animacy agreement with relativizers, reducing the number of errors where 'that' or 'which' was chosen to modify an animate noun rather than 'who' or 'whom' (and vice-versa), while also allowing both choices where corpus evidence was mixed.\n\nWith function words, we showed that we could improve upon the model's predictions for when to employ 'that'-complementizers using featured inspired by Florian Jaeger's work on using the principle of uniform information density, which holds that human language use tends to keep information density relatively constant in order to optimize communicative efficiency. In news text, complementizers are left out two times out of three, but in some cases the presence of 'that' is crucial to the interpretation. Generally, inserting a complementizer makes the onset of a complement clause more predictable, and thus less information dense, thereby avoiding a potential spike in information density that is associated with comprehension difficulty. See Figure 1 for an example.\n\nFinally, to improve word ordering decisions, we demonstrated that incorporating a feature inspired by Edward Gibson's dependency locality theory can deliver statistically significant improvements in automatic evaluation scores, better match the distributional characteristics of sentence orderings, and significantly reduce the number of serious ordering errors as confirmed by a targeted human evaluation. Gibson's theory holds that the preference to minimize the..."
 }
}