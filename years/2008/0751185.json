{
 "awd_id": "0751185",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRI:   CRD:   Collaborative Research:   Large Analytics Library and Scalable Concept Ontology for Multimedia Research",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2008-03-15",
 "awd_exp_date": "2012-02-29",
 "tot_intn_awd_amt": 426000.0,
 "awd_amount": 454000.0,
 "awd_min_amd_letter_date": "2008-03-12",
 "awd_max_amd_letter_date": "2011-02-01",
 "awd_abstract_narration": "The video analysis community has long attempted to bridge the gap from low-level feature extraction to semantic understanding and retrieval. One important barrier impeding progress is the lack of infrastructure needed to construct useful semantic concept ontologies, building modules to extract features from the video, interpreting the semantics of what the video contains, and evaluating the tasks against benchmark truth data. To solve this fundamental problem, this project will create a shared community resource around large video collections, extracted features, video segmentation tools, scalable semantic concept lexicons with annotations, ontologies relating the concepts to each other, tools for annotation, learned models and complete software modules for automatically describing the video through concepts, and finally a benchmark set of user queries for video retrieval evaluation.\r\n\r\nThe resource will allow researchers to build their own image/video classifiers, test new low-level features, expand the concept ontology, and explore higher level search services, etc., without having to redevelop several person year?s worth of infrastructure.\r\nUsing this tool suite and reference implementation, researchers can quickly customize concept ontologies and classifiers for diverse subdomains.\r\n\r\nThe contribution of the proposed work lies in the development of a large number of critical research resources for digital video analysis and searching. The modular architecture of the proposed resources provides great flexibility in adding new ontologies and testing new analytics components developed by other researchers in different domains. The use of large diverse standardized video datasets and well-defined benchmark procedures ensures a rigorous process to assess scientific progress.\r\n\r\nThe results will facilitate rapid exploration of new ideas and solutions, contributing to advancements of major societal interest, such as next-generation media search and security.\r\n\r\nURL: http://www.informedia.cs.cmu.edu/analyticsLibrary",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Hauptmann",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander G Hauptmann",
   "pi_email_addr": "alex@cs.cmu.edu",
   "nsf_id": "000228336",
   "pi_start_date": "2008-03-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 426000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 12000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Project Report</p>\n<p>This project has created a suite of tools for systematic development and rapid deployment of video analysis, video annotations, and model learning in different domains. These tools will provide data, extracted features, learned concept models and automatic detectors, and an evaluation paradigm as a public resource. Together they provide many of the essential building blocks necessary for multimedia applications such as video search systems. Additional tools and datasets with annotations have been made available on the website LIBSCOM.ORG, including both data and running code.</p>\n<p>Part of the project dissemination and outreach activities has been the organization of workshops for large scale media analysis, bringing together researchers and disseminating our tools and results. The first such workshop took place at ACM Multimedia 2009. The workshop provided a forum to understand key factors related to research on very large scale multimedia dataset such as the construction of datasets, creation of ground truth, sharing and extending existing resources, features, algorithms, tools, etc. The project organized the next workshop on large scale multimedia collections at the International Conference for Pattern Recognition ICPR 2010, and most recently organized another very successful workshop on very large scale multimedia collections and evaluations at ACM Multimedia 2010. This was followed by a summer school course in 2011 at the Summer School in Vision, Learning and Pattern Recognition (VLPR2011). The summer school course described active ongoing work in semantic concept detection for video data and used our tools as examples.</p>\n<p>Key to this effort has been an ontology of visual concepts that could be automatically observed in the video. The Large Scale Concept<strong> </strong>Ontology for Multimedia contains associated annotated data over several core video datasets<strong>. </strong>LSCOM has been a collaborative effort to develop a large, standardized taxonomy for describing video. These concepts have been selected to be relevant for describing core content aspects of video, feasible for automatic detection with some level of accuracy and useful for video retrieval. LSCOM additionally connects all its concepts into a full ontology with hierarchical relations. The full LSCOM set contains over 2600 concepts, 449 of them have been fully annotated over the TRECVID 2005 collection. This is now available through the LIBSCOM.org website<strong>. </strong>An extended set of concepts for additional YouTube data are also now available there.</p>\n<p>Based on the local keypoints (known as the Scale-Invariant Feature Transform) extracted as salient image patches, an image can be described as a bag-of-visual-words (BoVW). The project conducted comprehensive studies on the representation choices of BoVW, including vocabulary size, weighting scheme, stop word removal, feature selection, spatial information, and visual bi-grams. This offered practical insights in how to optimize the performance of BoVW by choosing appropriate representation choices. Experiments showed that a soft-weighting scheme outperforms other popular weighting schemes such as TF-IDF with a large margin. Extensive experiments on TRECVID data sets also indicate that BoVW feature alone, with appropriate representation choices, already produces state-of-the-art concept detection performance. Based on these empirical findings, the method was applied to detect a large set of 374 semantic concepts. The detectors, as well as the features and detection scores on several recent benchmark data sets, were released to the multimedia community.</p>\n<p>The project also developed and shared code for a robust new approach to extract semantic concept information based on explicitly encoding static image appearance features together with motion information. For high-level semantic concept identificat...",
  "por_txt_cntn": "\nProject Report\n\nThis project has created a suite of tools for systematic development and rapid deployment of video analysis, video annotations, and model learning in different domains. These tools will provide data, extracted features, learned concept models and automatic detectors, and an evaluation paradigm as a public resource. Together they provide many of the essential building blocks necessary for multimedia applications such as video search systems. Additional tools and datasets with annotations have been made available on the website LIBSCOM.ORG, including both data and running code.\n\nPart of the project dissemination and outreach activities has been the organization of workshops for large scale media analysis, bringing together researchers and disseminating our tools and results. The first such workshop took place at ACM Multimedia 2009. The workshop provided a forum to understand key factors related to research on very large scale multimedia dataset such as the construction of datasets, creation of ground truth, sharing and extending existing resources, features, algorithms, tools, etc. The project organized the next workshop on large scale multimedia collections at the International Conference for Pattern Recognition ICPR 2010, and most recently organized another very successful workshop on very large scale multimedia collections and evaluations at ACM Multimedia 2010. This was followed by a summer school course in 2011 at the Summer School in Vision, Learning and Pattern Recognition (VLPR2011). The summer school course described active ongoing work in semantic concept detection for video data and used our tools as examples.\n\nKey to this effort has been an ontology of visual concepts that could be automatically observed in the video. The Large Scale Concept Ontology for Multimedia contains associated annotated data over several core video datasets. LSCOM has been a collaborative effort to develop a large, standardized taxonomy for describing video. These concepts have been selected to be relevant for describing core content aspects of video, feasible for automatic detection with some level of accuracy and useful for video retrieval. LSCOM additionally connects all its concepts into a full ontology with hierarchical relations. The full LSCOM set contains over 2600 concepts, 449 of them have been fully annotated over the TRECVID 2005 collection. This is now available through the LIBSCOM.org website. An extended set of concepts for additional YouTube data are also now available there.\n\nBased on the local keypoints (known as the Scale-Invariant Feature Transform) extracted as salient image patches, an image can be described as a bag-of-visual-words (BoVW). The project conducted comprehensive studies on the representation choices of BoVW, including vocabulary size, weighting scheme, stop word removal, feature selection, spatial information, and visual bi-grams. This offered practical insights in how to optimize the performance of BoVW by choosing appropriate representation choices. Experiments showed that a soft-weighting scheme outperforms other popular weighting schemes such as TF-IDF with a large margin. Extensive experiments on TRECVID data sets also indicate that BoVW feature alone, with appropriate representation choices, already produces state-of-the-art concept detection performance. Based on these empirical findings, the method was applied to detect a large set of 374 semantic concepts. The detectors, as well as the features and detection scores on several recent benchmark data sets, were released to the multimedia community.\n\nThe project also developed and shared code for a robust new approach to extract semantic concept information based on explicitly encoding static image appearance features together with motion information. For high-level semantic concept identification detection in broadcast video, multimodality classifiers were trained which combine the traditional static image features and a new motion f..."
 }
}