{
 "awd_id": "0747520",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Anywhere Augmentation: Practical Mobile Augmented Reality in Unprepared Physical Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2008-04-01",
 "awd_exp_date": "2015-03-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2008-03-18",
 "awd_max_amd_letter_date": "2012-06-29",
 "awd_abstract_narration": "The PI introduces the term Anywhere Augmentation to refer to the idea of linking location-specific computing services with the physical world, making them readily and directly available in any situation and location.  This project embodies a novel approach to Anywhere Augmentation based on efficient human input for wearable computing and augmented reality (AR), through both sketch-based interfaces on hand-held devices and direct-overlay 3D user interfaces.  Mobile augmented reality is a powerful interface for wearable computing.  If computer users are enabled to place arbitrary annotations in 3D space wherever they go, the physical world becomes the user interface.  Instead of embedding computing and display equipment in the environment as in the case of ubiquitous computing, graphical annotations are overlaid on top of the environment by means of optical see-through glasses or video overlay.  Robust registration between the physical world and the augmentations is necessary.  Current approaches rely on the availability of a 3D model of the environment or on its instrumentation with active or passive markers.  The PI's approach is novel in that he proposes to emphasize, support, and utilize the expertise of the human in the loop to make Anywhere Augmentation feasible.  Human users generally have a clear grasp of the layout of the scene in front of them, and they can easily identify the physical objects with which information should be linked.  The PI plans to enable users to transfer their intuitional scene understanding to the computer through intelligently constrained and assisted user interfaces.  Current real-time computer vision techniques and algorithms, while far from being able to facilitate automatic scene understanding, are very well suited to constrain and guide a user?s informed input for scene analysis and augmentation, delivered in the form of a few simple point selections, stroke gestures, and common classifications.  As a main source of input, the PI will employ video feeds from small head-worn or palm-top-device cameras.  The devices, algorithms and interaction techniques will be applicable to novel settings and application scenarios (e.g., visualization of occluded infrastructure, navigational guidance, and social and educational applications for high-school students).\r\n\r\nBroader Impact:  The PI will use this research as a case study and platform for projects supporting the teaching of human-computer interaction fundamentals.  As a first step towards actively furthering the inclusion of minority students in the benefits of the research, the PI has partnered with Jackson State University, MS, and will also collaborate with outreach programs supporting local underrepresented K-12 students, where he will enhance the after-school programs and field trips with carefully planned AR experimentation and mentoring.  The goal of Anywhere Augmentation will be tested by making research products (new devices, tools, and interfaces) available to students and field scientists in a wide variety of environments (e.g., as a campus navigation and inventory tool, for an emergency response scenario, at UCSB lab open houses, and at international conferences).  The PI will also introduce innovations in three courses in the curriculum of the UCSB Computer Science Department (an undergraduate elective on HCI which he established, the undergraduate senior CS design project course cycle, and a new graduate course on 3D user interfaces), so they include hands-on experiences on effective UI design and programming for mobile devices, including mobile AR interfaces.  To this end, the PI will also involve the UCSB Allosphere, a 3-story spherical surround-view 3D immersive space, as a mobile AR simulator.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tobias",
   "pi_last_name": "Hollerer",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Tobias H Hollerer",
   "pi_email_addr": "holl@cs.ucsb.edu",
   "nsf_id": "000166428",
   "pi_start_date": "2008-03-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "3227 CHEADLE HALL",
  "perf_city_name": "SANTA BARBARA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931060001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 100001.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 109999.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 90000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We have introduced the term \"Anywhere Augmentation\" for the concept of building Augmented Reality systems that work in arbitrary environments with no prior preparation. Our work lowers the barrier to broad acceptance of augmented reality by expanding beyond research prototypes that only work in prepared, controlled environments. To this end we have built a framework to unite the resources that are commonly available to an Anywhere Augmentation user &ndash; rough global position tracking, local sensors, globally available GIS data, and user input &ndash; to address the two main areas of limitation hindering widespread applicability of this technology: generality and robustness. Our contributions towards the goal of Anywhere Augmentation lie in several areas, including tracking in unprepared environments, interface and interaction design, and application development.</p>\n<p>A correlated goal that emerged a couple of years into our agenda is the support of \"Social Augmented Reality.\" In analogy of the world wide web and the 'social web' having been kick-started by massively volunteered (published, shared, agreed to provide) information, we envision Augmented Reality truly taking off only when a critical mass of users will be automatically improve the experience for other users simply by using the technology. To this end, we have researched potential uses of video streams that are gathered when using augmented reality applications - we believe that such data can be used to improve the experience for the next user in the same physical environment (or for remotely accessing the environment).</p>\n<p>The images on the right showcase several significant results from the conducted research: Wide-Area AR tracking, Real-Time World Modeling in the form of real-time panorama acquisition as well as tracking and mapping, gestural user interfaces, novel AR device prototypes, and evaluation studies using a simulator approach. The research resulted in about forty research papers at international journal and conference venues. Several of these papers received accolades and awards, including two Best Paper nominations, four Best Paper Honorable Mentions, and the IEEE ISMAR 2012 Best Paper Award.</p>\n<p>The last few years of this research agenda coincided with an unprecedented growth of augmented reality productization efforts, and the PI's research team was actively engaged in helping with this development through collaborations with industry leaders, professional preparation of researchers for corresponding careers, and the spin-off of an Augmented Reality startup company. Seven PhD students that were at some point supported by this grant completed their respective PhD dissertations over the course of this project. Two of them are now Assistant Professors at US research universities, four of them work on R&amp;D in the AR/VR industry, and one student is pursuing the aforementioned startup company.&nbsp;</p>\n<p>For educational impact, the PI established three new courses that apply and transfer know-how from the research conducted for this grant to graduate and undergraduate education: Human-Computer Interaction, Android Programming, and Mixed and Augmented Reality Systems. Together with Prof. Dieter Schmalstieg from TU Graz in Austria, the PI is completing the writing for a textbook on Augmented Reality: Augmented Reality, Principles and Practice, to be published by Pearson / Addison Wesley.&nbsp;</p>\n<p>Throughout the performance period of this grant, the research team was strongly engaged in outreach and in utilizing the fascination of Augmented Reality technologies for recruitment of underrepresented minorities into STEM disciplines.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/14/2015<br>\n\t\t\t\t\tModified by: Tobias&nbsp;H&nbsp;Hollerer</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCoun...",
  "por_txt_cntn": "\nWe have introduced the term \"Anywhere Augmentation\" for the concept of building Augmented Reality systems that work in arbitrary environments with no prior preparation. Our work lowers the barrier to broad acceptance of augmented reality by expanding beyond research prototypes that only work in prepared, controlled environments. To this end we have built a framework to unite the resources that are commonly available to an Anywhere Augmentation user &ndash; rough global position tracking, local sensors, globally available GIS data, and user input &ndash; to address the two main areas of limitation hindering widespread applicability of this technology: generality and robustness. Our contributions towards the goal of Anywhere Augmentation lie in several areas, including tracking in unprepared environments, interface and interaction design, and application development.\n\nA correlated goal that emerged a couple of years into our agenda is the support of \"Social Augmented Reality.\" In analogy of the world wide web and the 'social web' having been kick-started by massively volunteered (published, shared, agreed to provide) information, we envision Augmented Reality truly taking off only when a critical mass of users will be automatically improve the experience for other users simply by using the technology. To this end, we have researched potential uses of video streams that are gathered when using augmented reality applications - we believe that such data can be used to improve the experience for the next user in the same physical environment (or for remotely accessing the environment).\n\nThe images on the right showcase several significant results from the conducted research: Wide-Area AR tracking, Real-Time World Modeling in the form of real-time panorama acquisition as well as tracking and mapping, gestural user interfaces, novel AR device prototypes, and evaluation studies using a simulator approach. The research resulted in about forty research papers at international journal and conference venues. Several of these papers received accolades and awards, including two Best Paper nominations, four Best Paper Honorable Mentions, and the IEEE ISMAR 2012 Best Paper Award.\n\nThe last few years of this research agenda coincided with an unprecedented growth of augmented reality productization efforts, and the PI's research team was actively engaged in helping with this development through collaborations with industry leaders, professional preparation of researchers for corresponding careers, and the spin-off of an Augmented Reality startup company. Seven PhD students that were at some point supported by this grant completed their respective PhD dissertations over the course of this project. Two of them are now Assistant Professors at US research universities, four of them work on R&amp;D in the AR/VR industry, and one student is pursuing the aforementioned startup company. \n\nFor educational impact, the PI established three new courses that apply and transfer know-how from the research conducted for this grant to graduate and undergraduate education: Human-Computer Interaction, Android Programming, and Mixed and Augmented Reality Systems. Together with Prof. Dieter Schmalstieg from TU Graz in Austria, the PI is completing the writing for a textbook on Augmented Reality: Augmented Reality, Principles and Practice, to be published by Pearson / Addison Wesley. \n\nThroughout the performance period of this grant, the research team was strongly engaged in outreach and in utilizing the fascination of Augmented Reality technologies for recruitment of underrepresented minorities into STEM disciplines.\n\n \n\n\t\t\t\t\tLast Modified: 07/14/2015\n\n\t\t\t\t\tSubmitted by: Tobias H Hollerer"
 }
}