{
 "awd_id": "0803524",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III-COR-Medium: Providing Provenance through Workflows and Database Transformations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2008-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 816600.0,
 "awd_amount": 914494.0,
 "awd_min_amd_letter_date": "2008-08-04",
 "awd_max_amd_letter_date": "2013-07-15",
 "awd_abstract_narration": "\r\nData provenance is a fundamental issue in the processing of scientific \r\ninformation and beyond. Two lines of research have been pursued in \r\nrecent years with direct bearing on the issues of data provenance. In \r\none of them, provenance in workflows, the emphasis is on extracting \r\nprovenance from logs of events marking the execution of different \r\nmodules to various intial and derived datasets. In the other line of \r\nresearch, provenance in databases, the emphasis is on the propagation of \r\nprovenance through the operators that make up database views, or on \r\npropagation of provenance through copy/cut-and-paste operations within \r\nand among databases.  These two bodies of work employ different \r\ntechniques and at first glance their results appear quite different. \r\nHowever, in many scientific applications, database manipulations \r\nco-exist with the execution of workflow modules, and the provenance of \r\nthe resulting data should integrate both kinds of processing into a \r\nusable paradigm. \r\n\r\nBy analyzing the work on data provenance in workflows and in databases, \r\nthe PIs identify what they believe are the main difficulties in unifying \r\nand integrating these two different kinds of data provenance:\r\n(1) the lack of a data model that is rich enough to capture the \r\ninteraction between the structure of the data and the structure of the \r\nworkflow; and\r\n(2) the lack of a high-level specification framework in which database \r\noperators and workflow modules can be treated uniformly. \r\n\r\nIn this project, the PIs aim to overcome these difficulties and thus \r\nprovide concepts and tools that allow a truly comprehensive approach to \r\nthe provenace of scientific data. The project's approach relies on a \r\ndata model that supports nested collections and on a functional language \r\napproach to workflow specification. Based on this, the project aims to \r\ndeliver a framework and tools for defining, managing and querying data \r\nprovenance in complex\r\nscientific workflows that include database manipulations. The project is \r\nexpected to impact bioinformatics (through interdisciplinary \r\ncollaborations in the Penn Center for Bioinformatics and the Penn Genome \r\nFrontiers Institute) and phyloinformatics (through contributions to the \r\nNSF AToL program) as well as ongoing standardization work on provenance \r\nin workflows and in the business processes (eg., BPEL) community.\r\n\r\nThe results of this project are disseminated as publications, through \r\ndirect collaborations and through the project website: \r\nhttp://db.cis.upenn.edu/research/UNIPROVE.html.\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Susan",
   "pi_last_name": "Davidson",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Susan B Davidson",
   "pi_email_addr": "susan@cis.upenn.edu",
   "nsf_id": "000135773",
   "pi_start_date": "2008-08-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Val",
   "pi_last_name": "Tannen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Val Tannen",
   "pi_email_addr": "val@cis.upenn.edu",
   "nsf_id": "000186637",
   "pi_start_date": "2008-08-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sanjeev",
   "pi_last_name": "Khanna",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sanjeev Khanna",
   "pi_email_addr": "sanjeev@cis.upenn.edu",
   "nsf_id": "000308324",
   "pi_start_date": "2008-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3451 WALNUT ST STE 440A",
  "perf_city_name": "PHILADELPHIA",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 816600.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 52630.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 45264.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p, li { white-space: pre-wrap; } -->\n<p style=\"text-indent: 0px; margin: 0px;\">Data provenance, the process of tracing and recording the origins of data and how it moves between programs and databases, is a fundamental issue in the processing of scientific information and beyond.  It is important for the verifiability and repeatability of results, as well as for debugging and trouble-shooting the process by which final results were obtained.  Prior to the work of this grant,  two lines of research had been pursued with direct bearing on data provenance. In one of them, provenance in workflows, the emphasis has been on extracting provenance from logs of events marking the execution of different processing steps over various initial and derived datasets. In the other line of research, provenance in databases, the emphasis has been on the propagation of provenance through query operators, or on propagation of provenance through copy/cut-and-paste operations within and among databases.  These two bodies of work used different techniques, and at first glance their results appear quite different. However, in many scientific applications database manipulations co-exist with the execution of workflow modules, and the provenance of the resulting data should integrate both kinds of processing into a usable paradigm.  The objective of this research was to provide a framework for integrating database and workflow provenance.</p>\n<p style=\"text-indent: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"text-indent: 0px; margin: 0px;\">Results of this research have included fundamental contributions to the theory of provenance as well as practical tools.  In particular, it has extended the theory of database \"provenance semirings\" to workflows in which the processing steps may be affected by what has happened in the past (\"stateful\" execution, e.g. steps that are guided by an underlying database or steps that represent active-learning), i.e. complex applications whose control flow is guided by a finite state machine as well as by the state of an underlying database (data-dependent process  models).  Examples of such applications include e-commerce and crowd-mining.   It has shown how questions such as \"Identify the data sources that contributed some data leading to the production of publication p\"  (reachability queries) can be efficiently answered using workflow provenance, as well as more complex questions such as \"Find all publications p that resulted from starting with data of type x, then performing a repeated analysis using either technique a1 or technique a2, terminated by producing a result of type s, and eventually ending by publishing p\" (regular path queries).  It has also shown how provenance support can be used by analysts to interactively test and explore the effect of hypothetical modications to the logic of an application and/or to the underlying database.</p>\n<p style=\"text-indent: 0px; margin: 0px;\">&nbsp;</p>\n<p style=\"text-indent: 0px; margin: 0px;\">Since the size of provenance generated by complex applications can be overwhelmingly large, techniques for reducing the size of provenance shown to users in response to queries were also explored.  These techniques included providing \"views\" of provenance, i.e. personalizations of provenance according to user interest and/or authority (access control), as well as summarizations of provenance (\"approximate\" provenance).</p>\n<p style=\"text-indent: 0px; margin: 0px;\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/10/2014<br>\n\t\t\t\t\tModified by: Susan&nbsp;B&nbsp;Davidson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nData provenance, the process of tracing and recording the origins of data and how it moves between programs and databases, is a fundamental issue in the processing of scientific information and beyond.  It is important for the verifiability and repeatability of results, as well as for debugging and trouble-shooting the process by which final results were obtained.  Prior to the work of this grant,  two lines of research had been pursued with direct bearing on data provenance. In one of them, provenance in workflows, the emphasis has been on extracting provenance from logs of events marking the execution of different processing steps over various initial and derived datasets. In the other line of research, provenance in databases, the emphasis has been on the propagation of provenance through query operators, or on propagation of provenance through copy/cut-and-paste operations within and among databases.  These two bodies of work used different techniques, and at first glance their results appear quite different. However, in many scientific applications database manipulations co-exist with the execution of workflow modules, and the provenance of the resulting data should integrate both kinds of processing into a usable paradigm.  The objective of this research was to provide a framework for integrating database and workflow provenance.\n \nResults of this research have included fundamental contributions to the theory of provenance as well as practical tools.  In particular, it has extended the theory of database \"provenance semirings\" to workflows in which the processing steps may be affected by what has happened in the past (\"stateful\" execution, e.g. steps that are guided by an underlying database or steps that represent active-learning), i.e. complex applications whose control flow is guided by a finite state machine as well as by the state of an underlying database (data-dependent process  models).  Examples of such applications include e-commerce and crowd-mining.   It has shown how questions such as \"Identify the data sources that contributed some data leading to the production of publication p\"  (reachability queries) can be efficiently answered using workflow provenance, as well as more complex questions such as \"Find all publications p that resulted from starting with data of type x, then performing a repeated analysis using either technique a1 or technique a2, terminated by producing a result of type s, and eventually ending by publishing p\" (regular path queries).  It has also shown how provenance support can be used by analysts to interactively test and explore the effect of hypothetical modications to the logic of an application and/or to the underlying database.\n \nSince the size of provenance generated by complex applications can be overwhelmingly large, techniques for reducing the size of provenance shown to users in response to queries were also explored.  These techniques included providing \"views\" of provenance, i.e. personalizations of provenance according to user interest and/or authority (access control), as well as summarizations of provenance (\"approximate\" provenance).\n \n\n\t\t\t\t\tLast Modified: 09/10/2014\n\n\t\t\t\t\tSubmitted by: Susan B Davidson"
 }
}