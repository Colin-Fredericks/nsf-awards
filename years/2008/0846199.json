{
 "awd_id": "0846199",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Source Coding and Simulation",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "John Cozzens",
 "awd_eff_date": "2008-09-01",
 "awd_exp_date": "2011-02-28",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 244179.0,
 "awd_min_amd_letter_date": "2008-08-09",
 "awd_max_amd_letter_date": "2010-08-18",
 "awd_abstract_narration": "The intimate connection between source coding (data compression,\r\nquantization) and simulation (the generation of a signal with prescribed distributions from a purely random mechanism such as coin flips) has been known for over three decades.  Recent results suggest even deeper connections with potentially significant implications for the related problems of compression code design, modeling random processes for the analysis and design of signal processing and coding systems, and understanding the nature and structure of mathematical models of random processes capturing the important properties of signals arising in the real world. This research is concerned with developing precise results characterizing and applying these connections to obtain new insight, theory, and algorithms.\r\n\r\nIn 1977 the performance of an optimized source coding system was shown to be bounded by the quality of a constrained simulation of the source, with equality under certain conditions.  In 2008 this connection was strengthened by a precise formulation and proof of an information theoretic ``folk theorem'' stating that source coding systems performing near the Shannon optimum yield bit streams that are ``nearly coin flips'' in the rigorous sense of closeness in Ornstein's d-bar process distance. Together these results imply that source coding systems --- including digital speech, audio, image, and video communication and storage systems --- and simulation systems --- comprising stationary codings or filterings of iid bits --- are mathematically approximately equivalent systems, and hence the theory and design algorithms appropriate for one yield corresponding results for the other. This project exploits these connections to develop new theory and design algorithms for codes for compression, simulation, and modeling based on known distributions and on distributions learned from data.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Gray",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Robert M Gray",
   "pi_email_addr": "rmgray@stanford.edu",
   "nsf_id": "000100510",
   "pi_start_date": "2008-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 JANE STANFORD WAY",
  "perf_city_name": "STANFORD",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "472000",
   "pgm_ele_name": "SIGNAL PROCESSING SYS PROGRAM"
  },
  {
   "pgm_ele_code": "793600",
   "pgm_ele_name": "SIGNAL PROCESSING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9237",
   "pgm_ref_txt": "SMALL GRANTS-EXPLORATORY RSRCH"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 44179.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Data compression is one of the enabling technologies for delivering analog information such as voice, audio, and video through digital networks such as the Internet in a fast and efficient manner. The conversion of analog signals into bits while preserving good fidelity has long provided a rich field for the development of theory describing the fundamental performance limits and algorithms for designing codes with performance near the theoretical limits and reasonable implementation complexity and cost. Shannon's information theory provides one of the fundamental methods of characterizing the optimal achievable performance on information sources possessing a reliable probabilistic model, but it does not show how to construct good codes. Compression algorithms have usually been developed in an ad hoc manner, and first tested in the context of simple statistical models following well-understood and simple probabilistic models such as memoryless signals following a Gaussian (bell-shaped) probability law.<br />It has long been known mathematically that the theory underlying data compression is closely related to the theory of simulating a complicated random process model given a much simpler process, such as fair coin flips. For example, what is the ``best'' possible imitation of a memoryless Gaussian random process that a computer can produce based on using a single coin flip for each output sample of the Gaussian process? &nbsp;It turns out that if one can do a good job of simulation of a signal, then one can also do a good job of compressing the signal for transmission through a digital channel.<br />The goal of this grant was to find properties of good simulation codes, that is, of codes that convert coin flips into good imitations of analog signals, and to use these properties as a guideline for designing good compression systems.&nbsp;During the course of the grant, a variety of such properties were found, properties which were shown to be necessary for codes to perform near the fundamental theoretical limits. A technique was developed for the design of codes having these necessary properties, and both theoretical and experimental results demonstrating success of the algorithm were developed. When applied to &nbsp;standard benchmarks for compression systems, the resulting codes were shown to provide performance quite close to the Shannon limits, in most cases closer than any previously known codes. The work yielded new insight into the structure of good codes and successful algorithms forthe design of the best known codes for simple signals. The work was the focus of a PhD dissertation and exploratory research by other graduate students.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/24/2011<br>\n\t\t\t\t\tModified by: Robert&nbsp;M&nbsp;Gray</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nData compression is one of the enabling technologies for delivering analog information such as voice, audio, and video through digital networks such as the Internet in a fast and efficient manner. The conversion of analog signals into bits while preserving good fidelity has long provided a rich field for the development of theory describing the fundamental performance limits and algorithms for designing codes with performance near the theoretical limits and reasonable implementation complexity and cost. Shannon's information theory provides one of the fundamental methods of characterizing the optimal achievable performance on information sources possessing a reliable probabilistic model, but it does not show how to construct good codes. Compression algorithms have usually been developed in an ad hoc manner, and first tested in the context of simple statistical models following well-understood and simple probabilistic models such as memoryless signals following a Gaussian (bell-shaped) probability law.\nIt has long been known mathematically that the theory underlying data compression is closely related to the theory of simulating a complicated random process model given a much simpler process, such as fair coin flips. For example, what is the ``best'' possible imitation of a memoryless Gaussian random process that a computer can produce based on using a single coin flip for each output sample of the Gaussian process?  It turns out that if one can do a good job of simulation of a signal, then one can also do a good job of compressing the signal for transmission through a digital channel.\nThe goal of this grant was to find properties of good simulation codes, that is, of codes that convert coin flips into good imitations of analog signals, and to use these properties as a guideline for designing good compression systems. During the course of the grant, a variety of such properties were found, properties which were shown to be necessary for codes to perform near the fundamental theoretical limits. A technique was developed for the design of codes having these necessary properties, and both theoretical and experimental results demonstrating success of the algorithm were developed. When applied to  standard benchmarks for compression systems, the resulting codes were shown to provide performance quite close to the Shannon limits, in most cases closer than any previously known codes. The work yielded new insight into the structure of good codes and successful algorithms forthe design of the best known codes for simple signals. The work was the focus of a PhD dissertation and exploratory research by other graduate students.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/24/2011\n\n\t\t\t\t\tSubmitted by: Robert M Gray"
 }
}