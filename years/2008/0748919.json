{
 "awd_id": "0748919",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Information Engineering and Synthesis for Resource-poor Languages",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2008-06-15",
 "awd_exp_date": "2017-05-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 614532.0,
 "awd_min_amd_letter_date": "2008-04-11",
 "awd_max_amd_letter_date": "2016-07-12",
 "awd_abstract_narration": "For the majority of the world's languages, the amount of linguistic resources (e.g., annotated corpora and parallel data) is very limited. Consequently, supervised methods and many unsupervised methods cannot be applied directly, leaving these languages largely untouched and unnoticed. Another crucial issue, which has received little attention from the natural language processing (NLP) community, is that to date there have been very few studies that examine a large number of languages and incorporate cross-lingual information into NLP systems. As a result, languages are researched and processed in isolation rather than being looked at as part of a big language family.\r\n\r\nThis proposed research has two intertwined goals. The first goal is to create a framework that allows the rapid development of resources for resource-poor languages. This goal will be accomplished by bootstrapping NLP tools with initial seeds created by projecting syntactic information from resource-rich languages to resource-poor ones. The second goal is to use the automatically created resources to perform cross-lingual study on a large number of languages to discover linguistic knowledge. The knowledge will not only deepen our understanding on languages, but also provide additional information that can be incorporated into the bootstrapping module to produce better NLP tools. The research explores two key ideas: The first idea is to take advantage of resource-rich languages by using them to create seeds for bootstrapping NLP tools. The second idea is to identify the relation between languages and use this information to help machine learning. Both ideas point to the same direction; that is, languages are related to one another and should be treated as such. \r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fei",
   "pi_last_name": "Xia",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fei Xia",
   "pi_email_addr": "fxia@u.washington.edu",
   "nsf_id": "000232098",
   "pi_start_date": "2008-04-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 BROOKLYN AVE NE",
  "perf_city_name": "SEATTLE",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981951016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 90211.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 114632.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 107016.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 103322.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 100744.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 98607.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp; &nbsp; &nbsp; There are approximately seven thousand languages in the world today, and most of them have very limited linguistic resources (e.g., grammars, annotated corpora and parallel data). Consequently, existing supervised methods and many unsupervised methods for natural language processing (NLP) cannot be applied directly, leaving these languages largely untouched by the NLP community. The first goal of the RiPLes project is to create a framework that allows the rapid development of resources for resource-poor languages (RPLs). We accomplish this goal by bootstrapping NLP systems with initial seeds created by projecting syntactic information from resource-rich languages to RPLs. The second goal is to use the automatically created resources to perform cross-lingual study on a large number of languages to discover linguistic knowledge. The acquired knowledge not only deepens our understanding on languages, but also provides additional information that can be incorporated into the bootstrapping module to produce better NLP tools.&nbsp;&nbsp;<br />&nbsp; &nbsp; &nbsp; &nbsp;Previous research on unsupervised learning requires resources such as a large amount of monolingual or parallel data, a dictionary, or a grammar, which are unavailable to many RPLs. In our research, we take advantage of interlinear glossed text (IGT), a common data type used by linguists when describing languages. The canonical form of an IGT instance consists of three parts: a language line (e.g., a phrase or a sentence in one language), a gloss line with word-to-word or morpheme-to-morpheme gloss, and a translation line which is often in English. The gloss line serves as a bridge for projecting syntactic information from the translation line (in English) to the language line (often in resource-poor languages). The gloss line may also include rich morphological information such as grammatical markers for number, person, tense, and so on.&nbsp;&nbsp;<br />&nbsp; &nbsp; &nbsp; The framework we have designed has four main components: (1) Data collection: we built NLP systems to crawl the Web for linguistic documents, identify IGT instances in the documents, and determine the language names and language codes of the language line in the IGT. (2) Syntactic projection: we project syntactic information from the translation line to the language line with the help of the gloss line in IGT. We address the problem of translation divergence by automatically learning common divergence patterns. (3) Bootstrapping NLP tools, in which we train Part-of-speech taggers and parsers using the output of syntactic projection. (4) Cross-lingual study: with the data created by the projection algorithm and bootstrapped NLP tools, we have built a knowledge discovery module that answers various typological questions (e.g., the word order of the language). The accuracy of the answers is high when there is a sufficient number of IGT instances for that language. In addition to the implementation of the whole framework, the outcome of the project also includes the ODIN database which contains more than 200 thousand IGT instances in more than thirteen hundred languages, a new data model for representing enriched IGT, and several GUI tools for editing IGT and running our framework.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/30/2019<br>\n\t\t\t\t\tModified by: Fei&nbsp;Xia</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n      There are approximately seven thousand languages in the world today, and most of them have very limited linguistic resources (e.g., grammars, annotated corpora and parallel data). Consequently, existing supervised methods and many unsupervised methods for natural language processing (NLP) cannot be applied directly, leaving these languages largely untouched by the NLP community. The first goal of the RiPLes project is to create a framework that allows the rapid development of resources for resource-poor languages (RPLs). We accomplish this goal by bootstrapping NLP systems with initial seeds created by projecting syntactic information from resource-rich languages to RPLs. The second goal is to use the automatically created resources to perform cross-lingual study on a large number of languages to discover linguistic knowledge. The acquired knowledge not only deepens our understanding on languages, but also provides additional information that can be incorporated into the bootstrapping module to produce better NLP tools.  \n       Previous research on unsupervised learning requires resources such as a large amount of monolingual or parallel data, a dictionary, or a grammar, which are unavailable to many RPLs. In our research, we take advantage of interlinear glossed text (IGT), a common data type used by linguists when describing languages. The canonical form of an IGT instance consists of three parts: a language line (e.g., a phrase or a sentence in one language), a gloss line with word-to-word or morpheme-to-morpheme gloss, and a translation line which is often in English. The gloss line serves as a bridge for projecting syntactic information from the translation line (in English) to the language line (often in resource-poor languages). The gloss line may also include rich morphological information such as grammatical markers for number, person, tense, and so on.  \n      The framework we have designed has four main components: (1) Data collection: we built NLP systems to crawl the Web for linguistic documents, identify IGT instances in the documents, and determine the language names and language codes of the language line in the IGT. (2) Syntactic projection: we project syntactic information from the translation line to the language line with the help of the gloss line in IGT. We address the problem of translation divergence by automatically learning common divergence patterns. (3) Bootstrapping NLP tools, in which we train Part-of-speech taggers and parsers using the output of syntactic projection. (4) Cross-lingual study: with the data created by the projection algorithm and bootstrapped NLP tools, we have built a knowledge discovery module that answers various typological questions (e.g., the word order of the language). The accuracy of the answers is high when there is a sufficient number of IGT instances for that language. In addition to the implementation of the whole framework, the outcome of the project also includes the ODIN database which contains more than 200 thousand IGT instances in more than thirteen hundred languages, a new data model for representing enriched IGT, and several GUI tools for editing IGT and running our framework.\n\n\t\t\t\t\tLast Modified: 08/30/2019\n\n\t\t\t\t\tSubmitted by: Fei Xia"
 }
}