{
 "awd_id": "0821474",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI:   Development of a Vision-based Real-Time Body Motion Tracking Instrument for Advanced  Radiation Treatment",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2008-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 256000.0,
 "awd_min_amd_letter_date": "2008-09-02",
 "awd_max_amd_letter_date": "2012-09-24",
 "awd_abstract_narration": "This cross disciplinary project for instrument development combines the fields of computer vision (and computer science overall) with radiation therapy and behavioral sciences.  This project is a joint venture among the Medical School, the Psychology Department, and the Institute of Technology.  The goal is to develop a minimally invasive, full-body, patient tracking system to be used with the helical tomotherapy system currently in use at the institution. The purpose of this effort is to detect when a patient becomes misaligned during radiation treatment.  Adding visual feedback to the helical tomotherapy treatment should improve the safety of the patient, reduce the considerable treatment time, and improve the cure rate.  It should be noted that the proposed instrument has a wide variety of implications for other forms of radiation therapy and medical systems that require a specific patient pose with respect to the medical device.  In order for these devices to revolutionize the treatment field, the interaction of the patient with the device should be studied from a behavioral sciences point of view. In this project, behavioral sciences will play an important role in capturing the behavioral patterns exhibited when the patient is in treatment, and in devising new patient placement protocols with profound impacts on cancer treatment.\r\n\r\nDuring the early stages of this development, the proposed instrument will not be used in human trials, focusing instead on the following tasks:\r\n\r\n-\tDevelopment of efficient computer vision algorithms (structured light) for detecting the patient's movements in a non-intrusive way that simultaneously offers the possibility for creating a closed-loop helical tomography instrument,\r\n-\tSelection of features, so cumbersome reflective markers are no longer needed,\r\n-\tCreation of the probabilistic framework necessary to utilize the features found from multiple calibrated cameras to determine the probability distribution of likely body positions,\r\n-\tIncorporation of behavioral science research that utilizes cyber-enabled principles to create patient-friendly medical devices and treatments, and\r\n-\tExperimental validation of the proposed instrument and improvement of the instrument with cyber-enabled behavioral science input.\r\n\r\nBroader Impacts:\r\n\r\nThis project introduces computer vision methodologies/hardware and cyber-enabled behavioral science research to radiation therapy with the objective of having safer and more effective radiation treatment. It may cause fundamental changes in the way that humans and medical devices interact. Educational programs will enable training to the instrument/methodologies. Some of these methodologies will be included in the curriculum. A website will be developed for dissemination of findings. Users from around the world will be given access. Outreach programs involving demonstrations to pertinent groups will be created.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nikolaos",
   "pi_last_name": "Papanikolopoulos",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Nikolaos P Papanikolopoulos",
   "pi_email_addr": "npapas@cs.umn.edu",
   "nsf_id": "000205563",
   "pi_start_date": "2008-09-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mats",
   "pi_last_name": "Heimdahl",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Mats P Heimdahl",
   "pi_email_addr": "heimdahl@cs.umn.edu",
   "nsf_id": "000445846",
   "pi_start_date": "2008-09-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Van Wyk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Van Wyk",
   "pi_email_addr": "evw@umn.edu",
   "nsf_id": "000389599",
   "pi_start_date": "2008-09-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Susanta",
   "pi_last_name": "Hui",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Susanta K Hui",
   "pi_email_addr": "huixx019@umn.edu",
   "nsf_id": "000497421",
   "pi_start_date": "2008-09-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "2221 UNIVERSITY AVE SE STE 100",
  "perf_city_name": "MINNEAPOLIS",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554143074",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 24000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project sudied many problems in the area of using computer vision to detect the movement of patients in radiation therapy machines. The first&nbsp;problem that was studied is the one of measuring the quality of camera and projector placement for structured light systems. Placement of sensors is important for all detection tasks as clever algorithms can be defeated by poor sensor placement, but good placement can lead to acceptable&nbsp;results even from subpar algorithms. Camera placement for observation tasks in multiple camera systems is an area that has been explored.</p>\n<p>&nbsp;</p>\n<p>However, the interaction between projectors and cameras in structured light systems is significantly different than the interactions in multiple&nbsp;camera systems, leading to different placement criteria. The quality of differing camera and projector placements was determined in this project by examining the physics of the problem. The spread of light from the projector into the scene and the uncertainty in the camera's detection of this&nbsp;light was modeled and accounted for. Taking this into consideration, judging the placement of cameras and projectors in such systems could be performed mathematically. One of the main areas where improved placement is applicable is tracking a patient's body position during radiation therapy, where small body movements direct radiation where it is not intended. To detect such movements, the cameras and projectors in a structured light system must be placed such that the body can be reconstructed with the desired precision.<br /><br />Another problem that was studied is the problem of calibrating the projector-camera system. Structured light systems operate on similar principles to stereoscopic camera systems, except that at least one camera is replaced by a projector. The 3D scene reconstructions generated can be considered as good as that constructed by a stereoscopic camera system. One central difficulty in using a stereoscopic camera set to perform&nbsp;scene reconstruction is that features in one camera's image have to be precisely matched to features in the other cameras' images. This can be&nbsp;difficult, and it is known as the correspondence problem. Structured light systems are often used instead of stereoscopic camera systems because by illuminating known patterns with a projector, the correspondence problem is greatly simplified.</p>\n<p>This project&nbsp; also developed a Euclidean calibration method for camera and projector systems in general scenes as the ones found in radiation therapy machines. This method is optimal in terms of the reprojection error of scene points. In addition, this method does not require manual human intervention to move specific occlusions so they intersect calibration patterns just right, or painstaking hand measurement of illuminated points. A set of simulations was run which characterizes the reprojection error of the method with respect to the level of corruption in the camera images. It was shown that the error in the camera and projector method is, on average, a little higher than the error for calibration-pattern based stereo camera calibration, but that its variance increases with increased image noise. Poor placement of the cameras and projector was shown to be an aggravating factor in these situations. The real-world efficacy of the method was demonstrated by calibrating a camera and projector system on complex and cluttered scenes commonly found in the helical tomotherapy environment.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/10/2013<br>\n\t\t\t\t\tModified by: Nikolaos&nbsp;Papanikolopoulos</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project sudied many problems in the area of using computer vision to detect the movement of patients in radiation therapy machines. The first problem that was studied is the one of measuring the quality of camera and projector placement for structured light systems. Placement of sensors is important for all detection tasks as clever algorithms can be defeated by poor sensor placement, but good placement can lead to acceptable results even from subpar algorithms. Camera placement for observation tasks in multiple camera systems is an area that has been explored.\n\n \n\nHowever, the interaction between projectors and cameras in structured light systems is significantly different than the interactions in multiple camera systems, leading to different placement criteria. The quality of differing camera and projector placements was determined in this project by examining the physics of the problem. The spread of light from the projector into the scene and the uncertainty in the camera's detection of this light was modeled and accounted for. Taking this into consideration, judging the placement of cameras and projectors in such systems could be performed mathematically. One of the main areas where improved placement is applicable is tracking a patient's body position during radiation therapy, where small body movements direct radiation where it is not intended. To detect such movements, the cameras and projectors in a structured light system must be placed such that the body can be reconstructed with the desired precision.\n\nAnother problem that was studied is the problem of calibrating the projector-camera system. Structured light systems operate on similar principles to stereoscopic camera systems, except that at least one camera is replaced by a projector. The 3D scene reconstructions generated can be considered as good as that constructed by a stereoscopic camera system. One central difficulty in using a stereoscopic camera set to perform scene reconstruction is that features in one camera's image have to be precisely matched to features in the other cameras' images. This can be difficult, and it is known as the correspondence problem. Structured light systems are often used instead of stereoscopic camera systems because by illuminating known patterns with a projector, the correspondence problem is greatly simplified.\n\nThis project  also developed a Euclidean calibration method for camera and projector systems in general scenes as the ones found in radiation therapy machines. This method is optimal in terms of the reprojection error of scene points. In addition, this method does not require manual human intervention to move specific occlusions so they intersect calibration patterns just right, or painstaking hand measurement of illuminated points. A set of simulations was run which characterizes the reprojection error of the method with respect to the level of corruption in the camera images. It was shown that the error in the camera and projector method is, on average, a little higher than the error for calibration-pattern based stereo camera calibration, but that its variance increases with increased image noise. Poor placement of the cameras and projector was shown to be an aggravating factor in these situations. The real-world efficacy of the method was demonstrated by calibrating a camera and projector system on complex and cluttered scenes commonly found in the helical tomotherapy environment.\n\n\t\t\t\t\tLast Modified: 09/10/2013\n\n\t\t\t\t\tSubmitted by: Nikolaos Papanikolopoulos"
 }
}