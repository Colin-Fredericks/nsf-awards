{
 "awd_id": "0830764",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:   Minimum Sobolov Norm Methods",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Dmitri Maslov",
 "awd_eff_date": "2008-09-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 299600.0,
 "awd_amount": 307600.0,
 "awd_min_amd_letter_date": "2008-08-29",
 "awd_max_amd_letter_date": "2011-08-11",
 "awd_abstract_narration": "Collaborative Research: Minimum Sobolev Norm Methods\r\n\r\nThe aim of this research project is to design fast and accurate\r\nnumerical algorithms for the solution of large classes of mathematical\r\nequations that arise in engineering and science. In particular, the\r\nmain concerns are the solution of integro-differential equations on\r\ncomplex domains and of signal and image processing problems. The\r\napproach is based on formulating the estimate of the solution of the\r\nequation at a point as the value of the smoothest solution (on\r\naverage) at that point based on the given data. The resulting discrete\r\nequations can be shown to have specially structured matrices, which\r\ncan be exploited to create fast solvers for these equations. The\r\nresulting methods have two main computational advantages. First, they\r\ncan be designed to avoid gridding or triangulation of the complex\r\ndomain. Second, these methods exhibit local convergence; that is, the\r\nrate at which the approximant converges to the solution at a point\r\ndepends only on the local smoothness of the solution. These advantages\r\nenable the method to tackle equations with complicated singularity\r\nstructures with relative ease.\r\n\r\nLet Hs denote a Sobolev Hilbert space whose elements have s > 1\r\nfractional derivatives. Suppose an unknown function f in Hs satisfies\r\nthe equation L(F) = g, where L is a linear operator and g is a known\r\nfunction. Let Ln denote n linear functionals on Hr. Let q denote a\r\nlinear functional on Hs. Then the best minmax estimate for q(f) can be\r\ncomputed from the minimum Sobolev norm function p in Hs that satisfies\r\nthe constraints Ln(L(p)) = Ln(g). This p can be computed very rapidly\r\nsince the optimal p is given by a nice set of equations that has Fast\r\nMultipole Method (FMM) structure when written in the proper\r\nrepresentation. Also, it is possible to work with Lp Sobolev spaces\r\nwith p = 1. In these cases the optimization problem is more\r\ncomplicated and can be reduced to linear programming problems, for\r\nwhich fast solvers are being developed that exploit the underlying FMM\r\nstructure of the constraint matrix. The theoretical work consists of\r\nstudying the convergence of the solution as n gets bigger, and also in\r\nproving the FMM structure of the resulting discrete equations. The\r\nalgorithmic work consists of designing fast algorithms for\r\nconstructing the FMM representation and then designing fast algorithms\r\nfor the direct (non-iterative) solution of these equations. The\r\napplication work consists of applying these ideas to image\r\nsegmentation and multi-rate signal processing. Also, mesh free,\r\nlocally convergent schemes are being developed for the solution of\r\nintegral equations and elliptic partial differential equations on\r\ncomplex domains in two dimensions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ming",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ming Gu",
   "pi_email_addr": "mgu@math.berkeley.edu",
   "nsf_id": "000205573",
   "pi_start_date": "2008-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "1608 4TH ST STE 201",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947101749",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "286500",
   "pgm_ele_name": "NUMERIC, SYMBOLIC & GEO COMPUT"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  },
  {
   "pgm_ele_code": "793300",
   "pgm_ele_name": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7752",
   "pgm_ref_txt": "CDI NON SOLICITED RESEARCH"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 119462.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 107252.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 80886.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><ol>\n<li><strong>&nbsp;&nbsp;</strong><strong>Fast and Stable Structured Matrix Computations. </strong>Partly through our NSF-supported work, structured matrix computations have become a very active area of research. Many matrices arising from scientific computing applications are naturally &ldquo;structured&rdquo; as they can be characterized by far fewer parameters than the number of entries in the matrix. There are diverse forms of &ldquo;structures,&rdquo; such as sparse (matrices with very large number of zero entries) and Toeplitz (matrices that are constant along each diagonal.) Often these structures are closely related and can be exploited for very significant savings in computational and storage costs. We have developed new structured algorithms based a particular matrix structure: semi-separability. Loosely speaking, a semi-separable matrix is one whose off-diagonal submatrices all have relatively low rank in a given precision. Surprisingly large classes of matrices can be represented as semi-separable matrices and therefore allow fast matrix computations. Partly through our work, fast structured algorithms are now an accepted approach to solving many very large sparse linear systems of equations. &nbsp;</li>\n<li><strong>Communication-Avoiding Matrix Factorizations</strong>. On modern architectures, data communication has overtaken numerical computation as the dominant cost in large-scale matrix computations.&nbsp; Largely due to the work of Demmel and his colleagues, communication avoidance has become the central theme in current numerical linear algebra research. In this work, we consider the LU factorization and rank-revealing QR factorization algorithms. These algorithms are the workhorse in matrix computations and yet their practical performance can be far from optimal due to their excessive communication costs. In this work, we develop communication-avoiding versions of these algorithms that require orders of magnitude less communication. In both cases, we have to design the algorithms carefully so as to not create numerical instability while introducing new flows of numerical computation.&nbsp;</li>\n<li><strong>Randomized Algorithms </strong>A classical problem in matrix computations is the efficient and reliable approximation of a given matrix by a matrix of&nbsp; much lower rank, with applications throughout wide areas of computational sciences and engineering.&nbsp; This problem becomes especially important today, as huge data sets are routinely processed with low-rank approximation techniques. Among the different approaches in the literature for computing low- rank approximations, randomized algorithms have attracted much of researchers&rsquo; recent attention due to their surprising reliability and computational efficiency in different application areas. Typically, such algorithms are shown to compute, with very high probability, low-rank approximations that are within a constant factor from optimal, and are known to perform even better in many practical situations. In this work [S1], we point out a close connection between randomized algorithms and the classical subspace iteration method in numerical linear algebra. Based on this connection, we provide strong new insight into both randomized algorithms and the subspace iteration method. This insight further allows us to develop a new class of condition number estimators that are far more reliable than any in existence.</li>\n</ol>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/22/2013<br>\n\t\t\t\t\tModified by: Ming&nbsp;Gu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n  Fast and Stable Structured Matrix Computations. Partly through our NSF-supported work, structured matrix computations have become a very active area of research. Many matrices arising from scientific computing applications are naturally \"structured\" as they can be characterized by far fewer parameters than the number of entries in the matrix. There are diverse forms of \"structures,\" such as sparse (matrices with very large number of zero entries) and Toeplitz (matrices that are constant along each diagonal.) Often these structures are closely related and can be exploited for very significant savings in computational and storage costs. We have developed new structured algorithms based a particular matrix structure: semi-separability. Loosely speaking, a semi-separable matrix is one whose off-diagonal submatrices all have relatively low rank in a given precision. Surprisingly large classes of matrices can be represented as semi-separable matrices and therefore allow fast matrix computations. Partly through our work, fast structured algorithms are now an accepted approach to solving many very large sparse linear systems of equations.  \nCommunication-Avoiding Matrix Factorizations. On modern architectures, data communication has overtaken numerical computation as the dominant cost in large-scale matrix computations.  Largely due to the work of Demmel and his colleagues, communication avoidance has become the central theme in current numerical linear algebra research. In this work, we consider the LU factorization and rank-revealing QR factorization algorithms. These algorithms are the workhorse in matrix computations and yet their practical performance can be far from optimal due to their excessive communication costs. In this work, we develop communication-avoiding versions of these algorithms that require orders of magnitude less communication. In both cases, we have to design the algorithms carefully so as to not create numerical instability while introducing new flows of numerical computation. \nRandomized Algorithms A classical problem in matrix computations is the efficient and reliable approximation of a given matrix by a matrix of  much lower rank, with applications throughout wide areas of computational sciences and engineering.  This problem becomes especially important today, as huge data sets are routinely processed with low-rank approximation techniques. Among the different approaches in the literature for computing low- rank approximations, randomized algorithms have attracted much of researchers\u00c6 recent attention due to their surprising reliability and computational efficiency in different application areas. Typically, such algorithms are shown to compute, with very high probability, low-rank approximations that are within a constant factor from optimal, and are known to perform even better in many practical situations. In this work [S1], we point out a close connection between randomized algorithms and the classical subspace iteration method in numerical linear algebra. Based on this connection, we provide strong new insight into both randomized algorithms and the subspace iteration method. This insight further allows us to develop a new class of condition number estimators that are far more reliable than any in existence.\n\n\n \n\n\t\t\t\t\tLast Modified: 08/22/2013\n\n\t\t\t\t\tSubmitted by: Ming Gu"
 }
}