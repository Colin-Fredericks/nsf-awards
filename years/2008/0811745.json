{
 "awd_id": "0811745",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI-Small: Efficient hidden structure annotation via structural multiple-sequence alignments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2008-08-01",
 "awd_exp_date": "2012-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 437750.0,
 "awd_min_amd_letter_date": "2008-07-23",
 "awd_max_amd_letter_date": "2011-02-25",
 "awd_abstract_narration": "The focus of this project is to develop finite-state syntactic processing models for natural language that use features encoding global structural constraints derived through multiple sequence alignment (MSA) techniques, to significantly improve accuracy without expensive context-free inference.  MSAs are widely used in computational biology for building finite-state models that capture long-distance dependencies in sequences (e.g., in RNA secondary structure).  Given a large set of functionally aligned sequences in MSA format, finite-state models can be constructed that allow for the efficient alignment of new sequences with the given MSA.  In natural language processing (NLP), only very rarely have MSA techniques been used, and then to characterize phonetic or semantic similarity.  This project is exploring the definition of a purely syntactic functional alignment between semantically unrelated strings from the same language, to define a structural MSA for constructing finite-state syntactic models.  The project has two specific aims. The first aim is to develop natural language sequence processing algorithms and models that can: a) define sequence alignments with respect to syntactic function; b) build structural MSAs based on defined functional alignments; c) derive finite-state models to efficiently align new sequences with the built MSA; and d) extract features from an alignment with the MSA for improved sequence modeling.  The second aim is to empirically validate this approach within a number of large-scale text processing applications in multiple domains and languages.  The resulting algorithms are expected to provide improved finite-state natural language models that will contribute to the state-of-the-art in critical text processing applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Roark",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Brian E Roark",
   "pi_email_addr": "roarkbr@gmail.com",
   "nsf_id": "000434316",
   "pi_start_date": "2008-07-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon Health & Science University",
  "inst_street_address": "3181 SW SAM JACKSON PARK RD",
  "inst_street_address_2": "",
  "inst_city_name": "PORTLAND",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5034947784",
  "inst_zip_code": "972393011",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OR01",
  "org_lgl_bus_name": "OREGON HEALTH & SCIENCE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPSNT86JKN51"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon Health & Science University",
  "perf_str_addr": "3181 SW SAM JACKSON PARK RD",
  "perf_city_name": "PORTLAND",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "972393011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OR01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 400000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 13750.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The focus of this project was to develop finite-state syntactic processing models for natural language that use features encoding global structural constraints derived through multiple sequence alignment (MSA) techniques, to significantly improve accuracy without expensive context-free inference. &nbsp;MSAs are widely used in computational biology for building finite-state models that capture long-distance dependencies in sequences (e.g., in RNA secondary structure). &nbsp;Given a large set of functionally aligned sequences in MSA format, finite-state models can be constructed that allow for the efficient alignment of new sequences with the given MSA. In natural language processing (NLP), only very rarely have MSA techniques been used, and then to characterize phonetic or semantic similarity. &nbsp;This project investigated methods for making use of new kinds of finite-state sequence models - such as multiple sequence alignments - for tasks that often rely on higher complexity models, such as syntactic processing. &nbsp;Syntactic parsing of natural language is an important step in many widely-used applications, such as information extraction and machine translation; but it can be too slow to support large scale use within such applications. &nbsp;In particular, the cubic complexity of syntactic parsing with context-free grammars makes linear complexity finite-state preprocessing and approximation an important component to scalable parsing systems, particularly those with very large grammars. &nbsp;For this reason, much of the syntactic processing work in this project was explored within a pre-processing scenario. &nbsp;In addition, the finite-state frameworks that we developed were applied to diverse problems in bioinformatics and natural language processing, including unsupervised morphological model induction, natural language generation, and weighted finite-state transducer representations of sequence models such as taggers.</p>\n<p>Over the three years of the project, we have published more than a dozen papers in leading journals and academic conferences on diverse topics related to the application of multiple sequence alignments in natural language processing and efficient inference with weighted finite-state transducers. &nbsp;Among the accomplishments of the project, we showed that:</p>\n<p>- Finite state tagging of syntactic constraints can lead to both worst-case and typical case complexity reductions, and very large efficiency gains relative to unconstrained context-free parsing.</p>\n<p>- Multiple sequence alignments can be used to build competitive systems for the learning of morphological structure from text. &nbsp;Further, supervised finite-state tagging approaches can be used to learn systems that simulate black-box or non-stochastic morphological models, leading to better generalization to new data and more efficient annotation.</p>\n<p>- Multiple sequence alignments can also be used for prenominal modifier ordering, an important task in natural language generation. &nbsp;Semi-supervised methods for learning such models on arbitrary amounts of text were shown to yield very high accuracy on this task.</p>\n<p>- Novel semirings -- multi-dimensional lexicographic semirings and new string semirings -- can be used to efficiently apply finite-state tagging models to the output of speech recognizers, yielding the Viterbi-best tag sequences for every path in the word lattice.</p>\n<p>In the course of the project, five graduate students received critical training in the discipline while working on this project. &nbsp;One visiting PhD student is finishing her PhD at the University of Aberdeen in Scotland, and will be starting a post-doctoral fellowship at Johns Hopkins University. &nbsp;Two others are in the final year of their PhD program, and are working on related topics in their PhD dissertation topics. &nbsp;A fourth has more than one year to go, but has begun...",
  "por_txt_cntn": "\nThe focus of this project was to develop finite-state syntactic processing models for natural language that use features encoding global structural constraints derived through multiple sequence alignment (MSA) techniques, to significantly improve accuracy without expensive context-free inference.  MSAs are widely used in computational biology for building finite-state models that capture long-distance dependencies in sequences (e.g., in RNA secondary structure).  Given a large set of functionally aligned sequences in MSA format, finite-state models can be constructed that allow for the efficient alignment of new sequences with the given MSA. In natural language processing (NLP), only very rarely have MSA techniques been used, and then to characterize phonetic or semantic similarity.  This project investigated methods for making use of new kinds of finite-state sequence models - such as multiple sequence alignments - for tasks that often rely on higher complexity models, such as syntactic processing.  Syntactic parsing of natural language is an important step in many widely-used applications, such as information extraction and machine translation; but it can be too slow to support large scale use within such applications.  In particular, the cubic complexity of syntactic parsing with context-free grammars makes linear complexity finite-state preprocessing and approximation an important component to scalable parsing systems, particularly those with very large grammars.  For this reason, much of the syntactic processing work in this project was explored within a pre-processing scenario.  In addition, the finite-state frameworks that we developed were applied to diverse problems in bioinformatics and natural language processing, including unsupervised morphological model induction, natural language generation, and weighted finite-state transducer representations of sequence models such as taggers.\n\nOver the three years of the project, we have published more than a dozen papers in leading journals and academic conferences on diverse topics related to the application of multiple sequence alignments in natural language processing and efficient inference with weighted finite-state transducers.  Among the accomplishments of the project, we showed that:\n\n- Finite state tagging of syntactic constraints can lead to both worst-case and typical case complexity reductions, and very large efficiency gains relative to unconstrained context-free parsing.\n\n- Multiple sequence alignments can be used to build competitive systems for the learning of morphological structure from text.  Further, supervised finite-state tagging approaches can be used to learn systems that simulate black-box or non-stochastic morphological models, leading to better generalization to new data and more efficient annotation.\n\n- Multiple sequence alignments can also be used for prenominal modifier ordering, an important task in natural language generation.  Semi-supervised methods for learning such models on arbitrary amounts of text were shown to yield very high accuracy on this task.\n\n- Novel semirings -- multi-dimensional lexicographic semirings and new string semirings -- can be used to efficiently apply finite-state tagging models to the output of speech recognizers, yielding the Viterbi-best tag sequences for every path in the word lattice.\n\nIn the course of the project, five graduate students received critical training in the discipline while working on this project.  One visiting PhD student is finishing her PhD at the University of Aberdeen in Scotland, and will be starting a post-doctoral fellowship at Johns Hopkins University.  Two others are in the final year of their PhD program, and are working on related topics in their PhD dissertation topics.  A fourth has more than one year to go, but has begun making serious progress towards defining her topic.  Three undergraduate students worked on this project as summer interns funded under NSF's Research Experience ..."
 }
}