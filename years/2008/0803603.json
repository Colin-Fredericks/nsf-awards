{
 "awd_id": "0803603",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "INT2-Medium:    Understanding the meaning of images",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2008-08-15",
 "awd_exp_date": "2012-07-31",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 566000.0,
 "awd_min_amd_letter_date": "2008-08-01",
 "awd_max_amd_letter_date": "2009-06-19",
 "awd_abstract_narration": "The ability to recognize objects in images is a core problem in  computer vision.  The last decade has seen astonishing advances in our methods to build object detectors.   However, images convey richer information about the objects depicted in them: objects may  form a scene (\"A view of mountains and meadows\"); objects are in relations with one another (\"The cat sits on the  mat\"); different instances may look different (\"The tabby cat sits on the blue mat\"); objects may acting on others  (\"The cat is chasing the mouse\"). This task of identifying the entities depicted in images, their  attributes and relations is image understanding.  This poses a number of new research questions: What objects should one remark on? What attributes of and relations  between the objects depicted the image are important?  That is, what is the visually salient information conveyed in an image?\r\n\r\nMany images (e.g. a large fraction of those on the web) are accompanied by text which describes or gives  additional information about the entities depicted in them.  The entities referred to in this text are typically visually  salient ones. This correspondence between the information conveyed in the text and the image can be used in the  creation of image understanding systems. Much current work treats image annotations that consist of individual  words. The richer representations of meaning required to train image understanding systems can be obtained if annotating text is treated as sentences (rather than just bags of words).  Sentences provide cues to: what is salient in an image; what salient objects likely look like (e.g.  color, texture and form); and what relations might appear between them. Exposing this information will provide a rich body of training data for the next generation of computer vision systems.\r\n\r\nResearch in natural language processing has created statistical wide- coverage parsers that can recover the semantic interpretation of sentences.  These parsers  differ from purely syntactic parsers in that they are based on linguistically expressive grammars that allow such interpretations to be built directly from the syntactic analysis.  However, linking sentences with accompanying images requires a level of representation that goes beyond lists of the entities, states and events mentioned in a sentence. The writer of an image caption will typically assume that the reader sees the image, and can therefore refer to the entities depicted in it as known to the reader. There is a need parsers that are able to uncover the information structure of sentences -- what information is assumed to be shared knowledge between speaker and hearer, and what is new information asserted by the sentence. How information structure is encoded in natural language is well understood, and can be modeled with the same kinds of grammars that are used by those parsers that return semantic interpretations.   Although there are currently no large corpora annotated with information structure, we will exploit the correspondence between images and their captions to develop novel, partially supervised, training regimes for parsers. These training regimes could also enable the bootstrapping of parsers for languages with no or little annotated training data.\r\n\r\nThis project will build a novel parser that recovers richer linguistic representations, including information structure. It will build a  novel image understanding system that recovers the salient entities depicted in an image together with their attributes and relations. The project will train these systems both separately on datasets  consisting of sentences marked up with correct parses and images marked up with labels attached to objects, and jointly on a dataset of captioned images.\r\n\r\nIntellectual merits: The project goals are ambitious, but within reach, because both object recognition and parsing technology has advanced significantly.  The project presents the vision and parsing communities with new goals, which are practically important and technically demanding.  The aim of integrating natural language processing and computer vision creates a novel impetus to develop parsers that return richer linguistic representations, which will in turn have a deep impact on research within the natural language processing community itself.  It will open up key directions in computer vision and natural language processing by demanding and enabling the recovery of richer representations of linguistic and visual information, and by studying how linguistic descriptions are grounded in the visual world.\r\n\r\nBroader impact: The project has significant practical implications in a number of  areas such as image search, natural language interfaces for robotics,  and will ultimately pave the way for new applications such as  automatic captioning systems. The resulting advances in object  recognition offer possibilities for the creation of safer autonomous  vehicles, safer homes for better home care, and efficient management  of surveillance data.\r\n\r\nURL:  http://luthuli.cs.uiuc.edu/~daf/meaningofimages.html",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Forsyth",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "David A Forsyth",
   "pi_email_addr": "daf@cs.uiuc.edu",
   "nsf_id": "000391155",
   "pi_start_date": "2008-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Julia",
   "pi_last_name": "Hockenmaier",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Julia C Hockenmaier",
   "pi_email_addr": "juliahmr@illinois.edu",
   "nsf_id": "000237005",
   "pi_start_date": "2008-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7717",
   "pgm_ref_txt": "INTEGRATIVE INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 550000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": null
}