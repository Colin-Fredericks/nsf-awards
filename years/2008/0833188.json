{
 "awd_id": "0833188",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Simplifying Parallel Programming for CSE Applications using a Multi-Paradigm Approach",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2008-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 800000.0,
 "awd_amount": 828000.0,
 "awd_min_amd_letter_date": "2008-08-01",
 "awd_max_amd_letter_date": "2013-05-13",
 "awd_abstract_narration": "Scientific applications can model interactions of medicines with proteins, predict the behavior of nano-materials, model the climate, and lead to better understanding of physical phenomenon. These applications demand ever greater computational resources, which can only be supplied by new parallel computers with ever increasing capability and complexity.  Parallel computing can bring about new breakthroughs only if the complexity of efficient parallel programming can be overcome. Yet developing parallel applications remains significantly more difficult than serial development. Petascale machines with hundreds of thousands(and possibly millions) of processors add to the complexity, as do new sophisticated algorithms and multi-physics applications. \r\n\r\n\r\nThis project is developing a new approach to parallel programming which builds upon the automatic resource management and composibility of the Charm++ framework. This approach includes development of multiple, individually incomplete, programming models. Each model simplifies parallel programming while still covering significant categories of applications.  This collection of interoperable models, supported by complete models including Adaptive MPI and Charm++, provides a powerful environment for developing future petascale applications. A compiler framework is being developed which provides a common representation and facilitates compatibility between models. In addition, the vision includes abstractions supported by libraries for commonly needed data types and functionalities. These abstractions will support and interoperate with domain specific frameworks. The results of this project will enable the large community of computational scientists and engineers to harness petascale machines with relative ease in order to generate breakthroughs in scientific discovery and engineering design.\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Laxmikant",
   "pi_last_name": "Kale",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Laxmikant V Kale",
   "pi_email_addr": "kale@uiuc.edu",
   "nsf_id": "000123469",
   "pi_start_date": "2008-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Padua",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "David A Padua",
   "pi_email_addr": "padua@uiuc.edu",
   "nsf_id": "000317715",
   "pi_start_date": "2008-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vikram",
   "pi_last_name": "Adve",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Vikram S Adve",
   "pi_email_addr": "vadve@cs.uiuc.edu",
   "nsf_id": "000334755",
   "pi_start_date": "2008-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "758300",
   "pgm_ele_name": "ITR-HECURA"
  },
  {
   "pgm_ele_code": "794200",
   "pgm_ele_name": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ele_code": "795200",
   "pgm_ele_name": "HECURA"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "7952",
   "pgm_ref_txt": "HECURA"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 800000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 12000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>For several decades, computers became smaller, faster and cheaper every year. The speed increase stopped around 2003, because of physical limitations. Further increases in speed of computation can come mainly, if not only, by using many processors in parallel to solve a problem faster. At the high end, it has been possible to put together machines consisting of over a million processors. Advances in scientific and engineering modeling have brought the possibility of breakthroughs of societal importance on the horizon, if these large computers can be used for such modeling.</p>\n<p>Yet, programming parallel computers has remained a challenging problem. In the work conducted within this project over a period of six years, several advances were made towards solving this problem.</p>\n<p>One of the insights in this project is the idea that we need a varied toolbox, rather than a single magical tool: i.e. instead of looking for a single parallel programming language, we should look at a collection of specialized languages, each good for some specific subset of problems, and all able to work together in an interoperable framework.</p>\n<p>To explore these ideas, this project developed several incremental parallel languages, and extended some existing parallel languages. One of the early insights was that the languages can be specialized by observing the kinds of interactions among the set of independent entities. It is often the case that a class of parallel algorithms tend to exhibit a somewhat stylized pattern of interaction that can be captured and expressed by a specialized programming language. Based on this observation, languages such as Charisma and Multiphase Shared Arrays were defined, and other languages such as Hierarchical Tiled Arrays and Deterministic Parallel Java were extended. A new language DivCon, for expressing divide-and-conquer algorithms that dealing with distributed data arrays, was developed. A library that acts as a powerful programming methodology for algorithms involving a tree as a basic data structure was developed.</p>\n<p>We demonstrated that simple support by a compiler for a carefully designed language, with well-known static analysis techniques, could improve the productivity of a parallel programming methodology significantly.&nbsp; These ideas were embodied in a parallel language called &ldquo;Charj&rdquo;. As measured by lines-of-code as well as by other subjective productivity metrics, Charj programs are elegant and are likely to improve programmer productivity.</p>\n<p>Although new programming languages are attractive, from the point of view of their innate properties and benefits, the inertia of existing code base and entrenched expertise in the older methodologies constitute major hurdles to their adoption. To this end, this project developed a framework that supports interoperability between the most popular parallel programming system, namely MPI, and the novel languages in the family of languages developed under this project. With this, software developers can incorporate the novel and radical programming languages in their ongoing project by possibly writing a small module in the new language. This allows for incremental adoption of the new methodologies. This idea was demonstrated in large scientific applications in production use within the Department of Energy.</p>\n<p>Several extensions and improvements were made to the Charm++ framework. Charm++ acts as a base language to which many of the new languages were translated. Its message driven execution model is key to the interoperability among different languages mentioned above. Charm++&rsquo;s adaptive runtime system provides a common substrate for optimizing resource management across modules written in different languages. &nbsp;</p>\n<p>The utility and impact of this project was demonstrated by submissions to the international High Performance Computing Challenge Competition ...",
  "por_txt_cntn": "\nFor several decades, computers became smaller, faster and cheaper every year. The speed increase stopped around 2003, because of physical limitations. Further increases in speed of computation can come mainly, if not only, by using many processors in parallel to solve a problem faster. At the high end, it has been possible to put together machines consisting of over a million processors. Advances in scientific and engineering modeling have brought the possibility of breakthroughs of societal importance on the horizon, if these large computers can be used for such modeling.\n\nYet, programming parallel computers has remained a challenging problem. In the work conducted within this project over a period of six years, several advances were made towards solving this problem.\n\nOne of the insights in this project is the idea that we need a varied toolbox, rather than a single magical tool: i.e. instead of looking for a single parallel programming language, we should look at a collection of specialized languages, each good for some specific subset of problems, and all able to work together in an interoperable framework.\n\nTo explore these ideas, this project developed several incremental parallel languages, and extended some existing parallel languages. One of the early insights was that the languages can be specialized by observing the kinds of interactions among the set of independent entities. It is often the case that a class of parallel algorithms tend to exhibit a somewhat stylized pattern of interaction that can be captured and expressed by a specialized programming language. Based on this observation, languages such as Charisma and Multiphase Shared Arrays were defined, and other languages such as Hierarchical Tiled Arrays and Deterministic Parallel Java were extended. A new language DivCon, for expressing divide-and-conquer algorithms that dealing with distributed data arrays, was developed. A library that acts as a powerful programming methodology for algorithms involving a tree as a basic data structure was developed.\n\nWe demonstrated that simple support by a compiler for a carefully designed language, with well-known static analysis techniques, could improve the productivity of a parallel programming methodology significantly.  These ideas were embodied in a parallel language called \"Charj\". As measured by lines-of-code as well as by other subjective productivity metrics, Charj programs are elegant and are likely to improve programmer productivity.\n\nAlthough new programming languages are attractive, from the point of view of their innate properties and benefits, the inertia of existing code base and entrenched expertise in the older methodologies constitute major hurdles to their adoption. To this end, this project developed a framework that supports interoperability between the most popular parallel programming system, namely MPI, and the novel languages in the family of languages developed under this project. With this, software developers can incorporate the novel and radical programming languages in their ongoing project by possibly writing a small module in the new language. This allows for incremental adoption of the new methodologies. This idea was demonstrated in large scientific applications in production use within the Department of Energy.\n\nSeveral extensions and improvements were made to the Charm++ framework. Charm++ acts as a base language to which many of the new languages were translated. Its message driven execution model is key to the interoperability among different languages mentioned above. Charm++\u00c6s adaptive runtime system provides a common substrate for optimizing resource management across modules written in different languages.  \n\nThe utility and impact of this project was demonstrated by submissions to the international High Performance Computing Challenge Competition at the Supercomputing conference. This competition judges the productivity and performance of different programming models. In two c..."
 }
}