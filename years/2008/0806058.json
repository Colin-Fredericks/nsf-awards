{
 "awd_id": "0806058",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  A Paradigm for Dimension Reduction with Respect to a General Functional",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2008-07-01",
 "awd_exp_date": "2011-06-30",
 "tot_intn_awd_amt": 47039.0,
 "awd_amount": 47039.0,
 "awd_min_amd_letter_date": "2008-06-20",
 "awd_max_amd_letter_date": "2010-03-31",
 "awd_abstract_narration": "The proposed research aims to developing a general formulation and the related methods for sufficient dimension reduction (SDR) where a specific functional (or parameter) of the conditional distribution is of interest. The past two decades have seen vigorous development of the SDR methods and have accrued a striking record of their successful applications. However, to a large extent these methods treat the conditional distribution as the object of interest, without discriminating between parameter of interest and nuisance parameter. While there are methods that target statistical functionals, they are specific to the parameter in consideration and as such are difficult to apply to other parameters. The investigators propose a new paradigm for SDR that focuses on a functional of the conditional distribution, which can be any one in a very wide class that covers most of applications. In addition, the investigators propose to develop a coherent collection of associated techniques for estimation, computation, and asymptotic inference. \r\n\r\nHigh throughput technologies that produce massive amount of complex and high-dimensional data are increasingly prevalent in such diverse areas as business, government administration, environmental studies, machine learning, and bioinformatics. These provide considerable momentum in the Statistics community to develop new theories and methodologies, and to reformulate the existing ones, that are capable of discovering critical evidence from high-dimensional and massive data. SDR is a recent area of statistical research that arose amidst, and has been propelled by, these new demands. The investigators propose to reformulate the theories and methodologies of SDR so that they can be specifically tailored to target to be estimated. This new paradigm not only synthesizes, broadens, and deepens the recent advances in SDR, but brings the understanding of SDR on a par with classical statistical inference theory, by following the tradition of sufficiency, efficiency, information, parameter of interests, and nuisance parameters, which are the key ideas that has helped to propel classical inference to its maturity",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bing",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bing Li",
   "pi_email_addr": "bing@stat.psu.edu",
   "nsf_id": "000312183",
   "pi_start_date": "2008-06-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "201 OLD MAIN",
  "perf_city_name": "UNIVERSITY PARK",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 1000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 1000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 45039.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Recent developments in scientific research and computing technology,<br />particularly those related to machine learning, bioinformatics, <br />pattern recognition, and market analysis, often create large quantities <br />of high dimensional data. They raise new questions and provide fresh <br />momentum for contemporary statistical research. One of the new features <br />of these data is that they are often collected without specific <br />designs - or with designs not sufficiently rigorous to be accommodated <br />by classical statistical theories and methodologies. In other words, <br />with increased volumes and dimensions also come the increased redundancy&nbsp; and irrelevancy. As a result, how to deal with redundancy and irrelevancy in vast amount of data by appropriately reducing the data, and thereby single out useful information and connections, has become one of the focal points of contemporary statistical research. Dimension reduction and sparse variable selection are but two fast growing areas that reflect these new challenges. The main goals of proposed research are to investigate and develop nonparametric methods of dimension reduction, and in particular a systematic method that can target specific aspects of the underlying distribution, such as means, medians, quantiles, and variances.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We have made the following&nbsp;developments towards, or related to, the proposed research:</p>\n<p>1. We have investigated the local nature of dimension reduction as well as<br />the rules it follows when we aggregate the local results (with Xiangrong Yin).</p>\n<p>2. We have introduced a class of ensemble estimators that incorporate<br />the dimension reduction estimators for conditional mean functions<br />to retrieve the information about the whole conditional distribution (with Xiangrong Yin).</p>\n<p>3. We have introduced a groupwise dimension reduction technique to<br />incorporate the domain information in the predictor (with Lexin Li and Lixing Zhu).</p>\n<p>4. We have investigated and quantified the predictive potential of<br />kernal principal components (with Andreas Artemiou).</p>\n<p>5. We have also introduced dimension reduction methods for<br />the situations where the predictors are not vectors, but matrices or multi-dimensional arrays. Such type of predictors are increasingly common --- for example, the EEG data, an image, or a video clip are all of this type<br />(with Minkyung Kim and Naomi Altman).</p>\n<p>6. We have developed a dimension reduction method that does not make strong assumptions on the distribution of the predictors (with Yuexiao Dong).</p>\n<p>7. We have modified the powerful method of support vector machine<br />perform sufficient dimension reduction in a regression setting<br />(with Andreas Artemiou and Lexin Li).</p>\n<p>8. We have introduced a reproducing kernel Hilbert space method<br />for sparse estimation of conditional graphical model, and applied<br />the method to gene network analysis (with Hyonho Chun and Hongyu Zhao).</p>\n<p>Most of these works have appeared in, or have been in the reviewing process of, leading statistical journals. In particular, Project 1 is in second revision for the Annals of Statistics; Project 3 and 8 have appeared in or&nbsp;is accepted by the Journal of American Statistical Association; Project 5 and 7 have appeared in or&nbsp;is accepted by the Annals of Statistics; Project 6 has appeared in Biometrika. Project 1 is near completion.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/26/2011<br>\n\t\t\t\t\tModified by: Bing&nbsp;Li</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n         Recent developments in scientific research and computing technology,\nparticularly those related to machine learning, bioinformatics, \npattern recognition, and market analysis, often create large quantities \nof high dimensional data. They raise new questions and provide fresh \nmomentum for contemporary statistical research. One of the new features \nof these data is that they are often collected without specific \ndesigns - or with designs not sufficiently rigorous to be accommodated \nby classical statistical theories and methodologies. In other words, \nwith increased volumes and dimensions also come the increased redundancy  and irrelevancy. As a result, how to deal with redundancy and irrelevancy in vast amount of data by appropriately reducing the data, and thereby single out useful information and connections, has become one of the focal points of contemporary statistical research. Dimension reduction and sparse variable selection are but two fast growing areas that reflect these new challenges. The main goals of proposed research are to investigate and develop nonparametric methods of dimension reduction, and in particular a systematic method that can target specific aspects of the underlying distribution, such as means, medians, quantiles, and variances.\n\n         We have made the following developments towards, or related to, the proposed research:\n\n1. We have investigated the local nature of dimension reduction as well as\nthe rules it follows when we aggregate the local results (with Xiangrong Yin).\n\n2. We have introduced a class of ensemble estimators that incorporate\nthe dimension reduction estimators for conditional mean functions\nto retrieve the information about the whole conditional distribution (with Xiangrong Yin).\n\n3. We have introduced a groupwise dimension reduction technique to\nincorporate the domain information in the predictor (with Lexin Li and Lixing Zhu).\n\n4. We have investigated and quantified the predictive potential of\nkernal principal components (with Andreas Artemiou).\n\n5. We have also introduced dimension reduction methods for\nthe situations where the predictors are not vectors, but matrices or multi-dimensional arrays. Such type of predictors are increasingly common --- for example, the EEG data, an image, or a video clip are all of this type\n(with Minkyung Kim and Naomi Altman).\n\n6. We have developed a dimension reduction method that does not make strong assumptions on the distribution of the predictors (with Yuexiao Dong).\n\n7. We have modified the powerful method of support vector machine\nperform sufficient dimension reduction in a regression setting\n(with Andreas Artemiou and Lexin Li).\n\n8. We have introduced a reproducing kernel Hilbert space method\nfor sparse estimation of conditional graphical model, and applied\nthe method to gene network analysis (with Hyonho Chun and Hongyu Zhao).\n\nMost of these works have appeared in, or have been in the reviewing process of, leading statistical journals. In particular, Project 1 is in second revision for the Annals of Statistics; Project 3 and 8 have appeared in or is accepted by the Journal of American Statistical Association; Project 5 and 7 have appeared in or is accepted by the Annals of Statistics; Project 6 has appeared in Biometrika. Project 1 is near completion.\n\n\t\t\t\t\tLast Modified: 09/26/2011\n\n\t\t\t\t\tSubmitted by: Bing Li"
 }
}