{
 "awd_id": "0751153",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CRI:   IAD:    Accelerator-Based High-Performance Computing",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2008-04-01",
 "awd_exp_date": "2013-03-31",
 "tot_intn_awd_amt": 569866.0,
 "awd_amount": 569866.0,
 "awd_min_amd_letter_date": "2008-03-26",
 "awd_max_amd_letter_date": "2010-03-31",
 "awd_abstract_narration": "Commodity processors are highly programmable, but their need to support general purpose computation limits both peak and sustained performance. Such observations have motivated the use of \"accelerator\" boards, which are co-processing elements that interface with the host server through a standard hardware bus such as PCI-Express but have their own computational engine and typically their own memory as well. Unlike the main processor, the accelerator does not support general applications; instead, its hardware and software is tuned for only specific types of computations.  Accelerators can offload the most demanding parts of an application from the host processor, speeding up the desired computation using their specialized resources. This improved performance enables various forms of high-performance computing (HPC), but comes at a high cost in programmability.\r\n\r\nThis research targets high-performance computing research using PC-based clusters for cost and scalability combined with accelerators for high performance. The Purdue Everest project encompasses several related efforts in achieving high performance, low power consumption, and high programmability for highly heterogeneous systems.  Acquiring a 30-node Gigabit Ethernet-based cluster of multicore PC-based workstations equipped with various accelerator boards (e.g., GPU, Cell, FPGA, Crypto) will enable research into effective and highly-programmable use of accelerator-based clusters. Supporting multiple accelerators per node allows applications to use different accelerator boards in different phases.  This cluster also allows fair apples-to-apples comparisons of different accelerators by keeping the other system factors constant. This research also investigates the use of multiple concurrency domains, with parallelism across the cluster, across the cores in a single node, among the host processors and accelerators in a single node, and across the processing elements of a given accelerator.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vijay",
   "pi_last_name": "Pai",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Vijay S Pai",
   "pi_email_addr": "vpai@purdue.edu",
   "nsf_id": "000184842",
   "pi_start_date": "2008-03-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rudolf",
   "pi_last_name": "Eigenmann",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rudolf Eigenmann",
   "pi_email_addr": "eigenman@udel.edu",
   "nsf_id": "000315744",
   "pi_start_date": "2008-03-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Charlie",
   "pi_last_name": "Hu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Charlie Hu",
   "pi_email_addr": "ychu@purdue.edu",
   "nsf_id": "000118830",
   "pi_start_date": "2008-03-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mithuna",
   "pi_last_name": "Thottethodi",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Mithuna S Thottethodi",
   "pi_email_addr": "mithuna@purdue.edu",
   "nsf_id": "000253970",
   "pi_start_date": "2008-03-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vijay",
   "pi_last_name": "Raghunathan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vijay Raghunathan",
   "pi_email_addr": "vr@purdue.edu",
   "nsf_id": "000203148",
   "pi_start_date": "2008-03-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "2550 NORTHWESTERN AVE # 1100",
  "perf_city_name": "WEST LAFAYETTE",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479061332",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 492268.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 38012.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 39586.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><br /> Automatic compiler techniques have been developed that fit large-data  computations into limited memory sizes on GPUs/accelerator devices. The  techniques use pipelining methods to overlap computation on the device  with the communication needed to move data to and from the devices. In  doing so, computations could be run on GPUs that previously had failed  due to insufficient memory and significant speedups were achieved as a  result of the new pipelining methods.</p>\n<pre>A key challenge faced by users of public clouds today is how to\nrequest for the right amount of resources in the production datacenter\nthat satisfies a target performance for a given cloud application. An\nobvious approach is to develop a performance model for a class of\napplications such as MapReduce.  However, several recent studies have\nshown that even for the class of well-studied MapReduce jobs, their\nrunning times can be seriously affected by numerous external factors\nranging from dozen or so configuration parameters, to the physical\nmachine characteristics (CPU, memory, disk, and network bandwidth), to\nimplementation deficiencies such as Java, garbage collection.  These\nfactors make direct performance modeling extremely difficult. In this\nstudy, we proposed a more practical systematic methodology to solve\nthis problem.\n\nIn particular, we developed a projection model that can prescribe the\nright amount of resources for MapReduce jobs to meet a given job\ncompletion time. The model is based on insights into performance\nbottlenecks of MapReduce jobs and their scaling properties, and\nparameterized with component running times based on profiling on small\nclusters with sampled inputs.\n\nUsing the CAP testbed, we developed the projection model. We then\nevaluated its effectiveness using a wide variety of MapReduce\nbenchmarks running on CAP.  Our evaluation results show our projection\nmodel can predict job running times with 2.7\\% of accuracy when\nscaling to 32 nodes on the CAP testbed.  CAP turns out to be valuable\nfor this project as our experience has confirmed that evalution on\nAmazon EC2 faced performanec unpredictability as different jobs would\ncompete for the shared network resources; current Amazone EC2 does not\nprovide network isolation.</pre><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/11/2013<br>\n\t\t\t\t\tModified by: Vijay&nbsp;S&nbsp;Pai</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n Automatic compiler techniques have been developed that fit large-data  computations into limited memory sizes on GPUs/accelerator devices. The  techniques use pipelining methods to overlap computation on the device  with the communication needed to move data to and from the devices. In  doing so, computations could be run on GPUs that previously had failed  due to insufficient memory and significant speedups were achieved as a  result of the new pipelining methods.\nA key challenge faced by users of public clouds today is how to\nrequest for the right amount of resources in the production datacenter\nthat satisfies a target performance for a given cloud application. An\nobvious approach is to develop a performance model for a class of\napplications such as MapReduce.  However, several recent studies have\nshown that even for the class of well-studied MapReduce jobs, their\nrunning times can be seriously affected by numerous external factors\nranging from dozen or so configuration parameters, to the physical\nmachine characteristics (CPU, memory, disk, and network bandwidth), to\nimplementation deficiencies such as Java, garbage collection.  These\nfactors make direct performance modeling extremely difficult. In this\nstudy, we proposed a more practical systematic methodology to solve\nthis problem.\n\nIn particular, we developed a projection model that can prescribe the\nright amount of resources for MapReduce jobs to meet a given job\ncompletion time. The model is based on insights into performance\nbottlenecks of MapReduce jobs and their scaling properties, and\nparameterized with component running times based on profiling on small\nclusters with sampled inputs.\n\nUsing the CAP testbed, we developed the projection model. We then\nevaluated its effectiveness using a wide variety of MapReduce\nbenchmarks running on CAP.  Our evaluation results show our projection\nmodel can predict job running times with 2.7\\% of accuracy when\nscaling to 32 nodes on the CAP testbed.  CAP turns out to be valuable\nfor this project as our experience has confirmed that evalution on\nAmazon EC2 faced performanec unpredictability as different jobs would\ncompete for the shared network resources; current Amazone EC2 does not\nprovide network isolation.\n\n\t\t\t\t\tLast Modified: 06/11/2013\n\n\t\t\t\t\tSubmitted by: Vijay S Pai"
 }
}