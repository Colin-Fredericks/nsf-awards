{
 "awd_id": "0757370",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "PILOT: Unlocking Body Memories for Creativity: Controlling Virtual Characters with Tangible Interfaces to Augment Expression and Cognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2008-08-01",
 "awd_exp_date": "2011-07-31",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 261817.0,
 "awd_min_amd_letter_date": "2008-03-28",
 "awd_max_amd_letter_date": "2010-07-06",
 "awd_abstract_narration": "This project extends three different research domains: ideomotor theory, virtual environments and tangible interfaces, and seeks to augment human creativity in a fundamental manner. A central component of artistic creativity is the ability to imagine and perform new actions. The ideomotor theory in cognitive science suggests that imagination and action share a common coding in the brain. This project extends body memories using virtual environments and tangible interfaces. The premise is that augmenting creativity involves expanding the number of solutions one can generate, i.e. broadening the creative space. The project develops an environment that includes video game characters that encode our body movements through tangible user interfaces as a way to broaden the body memories of the player. The project is multidisciplinary, and has a range of possible broader impacts, scientific, technological and social. On the scientific front, the project would contribute to a better understanding of the cognitive mechanisms underlying imagination, motor learning and action perception. This has potential implications in areas such as conflict resolution, medical rehabilitation and educational technologies. A variety of applications are possible on the technology front, including template combos of movements that generate specific character impressions in viewers, and algorithms that isolate and accentuate movement features that humans perceive as biological. On the social front, once movement patterns become ?swappable?, it would be possible to share entire personalities online by sharing whole body movement patterns. This could lead to more refined understanding and empathy for others.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexandra",
   "pi_last_name": "Mazalek",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexandra Mazalek",
   "pi_email_addr": "mazalek@ryerson.ca",
   "nsf_id": "000104898",
   "pi_start_date": "2008-03-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Nitsche",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Nitsche",
   "pi_email_addr": "michael.nitsche@gatech.edu",
   "nsf_id": "000079445",
   "pi_start_date": "2008-03-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 NORTH AVE NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "778800",
   "pgm_ele_name": "CreativeIT"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7655",
   "pgm_ref_txt": "ITR-CreativeIT"
  },
  {
   "pgm_ref_code": "7788",
   "pgm_ref_txt": "CreativeIT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 224843.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 36974.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Field:</strong></p>\n<p>We combined approaches from tangible interfaces, virtual worlds, and cognitive science to investigate whether we can expand a person&rsquo;s creative expression space and body memory through new human computer interfaces. The approach builds on common coding theory, which argues that our action codes are tightly coupled to the perceptual codes that are associated with the effects those actions have on the world. As a result of this tight coupling, the perception and imagination of actions involves the implicit activation of our motor system. Simplified, the model argues that we understand somebody else&rsquo;s movements and imagine our own actions through our own body memory.</p>\n<p><strong>Approach:</strong></p>\n<p>Through a range of experiments, we tested whether the common coding effect applies to the way we read and connect to a virtual avatar under our control. First, we successfully showed that participants are able to identify their own recorded movement patterns even if they are heavily abstracted or mediated through an embodied control interface, like a puppet. For the second step, we built a puppet-like interface that allows interactors to control a 3D character in a virtual environment. Using this interface and a basic game-like setting in the 3D world, we demonstrated that interactors are able to recognize their own performance in the abstracted movements of their avatar that acts like a virtual puppet. Once this connection was clear, we tested whether we can use this own body movement recognition in the movements of the avatar to improve an interactor&rsquo;s cognition in a specific task. We ran a comparative test that showed that our puppet interface outperformed other game-like interfaces available to date in stimulating participants for a mental rotation task. Our last experiment manipulated the control scheme of the interactor to the avatar and found that such a manipulation affected the manner in which they performed a&nbsp; creative sketching task.</p>\n<p><strong>Relevance:</strong></p>\n<p>The findings of this work are relevant for improvements of human-computer interaction and also provide insight into the role of the motor and cognitive system in our interactions with digital media. We also conducted initial pilot tests of our interface with stroke patients, which showed that our interface could be useful as a tool for them to connect to a virtual avatar and its performance. Because the system supports extension of one&rsquo;s body memory, this indicates that it has promise as a possible tool for rehabilitation of patients who need to (re)learn specific movement patterns, such as stroke patients or patients with certain brain injuries. The system is based on affordable hardware technology and widely known game-like environments, which makes it a possible option for a home-based rehabilitation system.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2011<br>\n\t\t\t\t\tModified by: Alexandra&nbsp;Mazalek</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2011/0757370/0757370_10024714_1319914080860_POR_PuppetPlayer--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2011/0757370/0757370_10024714_1319914080860_POR_PuppetPlayer--rgov-800width.jpg\" title=\"Puppet Interface\"><img src=\"/por/images/Reports/POR/2011/0757370/0757370_10024714_1319914080860_POR_PuppetPlayer--rgov-66x44.jpg\" alt=\"Puppet Interface\"></a>\n<div class=\"imageCaptionContainer\">\n...",
  "por_txt_cntn": "\nField:\n\nWe combined approaches from tangible interfaces, virtual worlds, and cognitive science to investigate whether we can expand a person\u00c6s creative expression space and body memory through new human computer interfaces. The approach builds on common coding theory, which argues that our action codes are tightly coupled to the perceptual codes that are associated with the effects those actions have on the world. As a result of this tight coupling, the perception and imagination of actions involves the implicit activation of our motor system. Simplified, the model argues that we understand somebody else\u00c6s movements and imagine our own actions through our own body memory.\n\nApproach:\n\nThrough a range of experiments, we tested whether the common coding effect applies to the way we read and connect to a virtual avatar under our control. First, we successfully showed that participants are able to identify their own recorded movement patterns even if they are heavily abstracted or mediated through an embodied control interface, like a puppet. For the second step, we built a puppet-like interface that allows interactors to control a 3D character in a virtual environment. Using this interface and a basic game-like setting in the 3D world, we demonstrated that interactors are able to recognize their own performance in the abstracted movements of their avatar that acts like a virtual puppet. Once this connection was clear, we tested whether we can use this own body movement recognition in the movements of the avatar to improve an interactor\u00c6s cognition in a specific task. We ran a comparative test that showed that our puppet interface outperformed other game-like interfaces available to date in stimulating participants for a mental rotation task. Our last experiment manipulated the control scheme of the interactor to the avatar and found that such a manipulation affected the manner in which they performed a  creative sketching task.\n\nRelevance:\n\nThe findings of this work are relevant for improvements of human-computer interaction and also provide insight into the role of the motor and cognitive system in our interactions with digital media. We also conducted initial pilot tests of our interface with stroke patients, which showed that our interface could be useful as a tool for them to connect to a virtual avatar and its performance. Because the system supports extension of one\u00c6s body memory, this indicates that it has promise as a possible tool for rehabilitation of patients who need to (re)learn specific movement patterns, such as stroke patients or patients with certain brain injuries. The system is based on affordable hardware technology and widely known game-like environments, which makes it a possible option for a home-based rehabilitation system.\n\n\t\t\t\t\tLast Modified: 10/29/2011\n\n\t\t\t\t\tSubmitted by: Alexandra Mazalek"
 }
}