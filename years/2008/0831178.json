{
 "awd_id": "0831178",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:   CT-M:   Privacy, compliance and information risk in complex organizational processes",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jeremy Epstein",
 "awd_eff_date": "2008-09-01",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2008-08-15",
 "awd_max_amd_letter_date": "2011-07-14",
 "awd_abstract_narration": "Modern organizations, such as businesses, non-profits, government  \r\nagencies, and universities, collect and use personal information from  \r\na range of sources, shared with specific expectations about how it  \r\nwill be managed and used. Accordingly, they must find ways to comply  \r\nwith expectations, which may be complex and varied, as well as with  \r\nrelevant privacy laws and regulations, while they minimize  \r\noperational risk and carry out core functions of the organization  \r\nefficiently and effectively. Designing organizational processes to  \r\nmanage personal information is one of the greatest challenges facing  \r\norganizations (see, e.g. a recent survey by Deloitte and the Ponemon  \r\nInstitute [TI07]), with far-reaching implications for every  \r\nindividual whose personal information is available to modern  \r\norganizations, i.e. all of us.\r\n\r\nThis project responds to these challenges by developing methods,  \r\nalgorithms and prototype tools for integrating privacy, compliance,  \r\nand risk evaluation into complex organizational processes. It  \r\nexplores, articulates and characterizes formally the scope and nature  \r\nof privacy-expectations of stakeholders as well as those of key  \r\nregulations, such as HIPAA, GLBA, COPPA, BASEL 2, and Sarbanes-Oxley  \r\n(SOX). It incorporates the diverse perspectives and areas of  \r\nexpertise of its multidisciplinary research team, which includes  \r\nthree computer scientists, one philosopher, and collaborating  \r\nresearchers from IBM. This industry connection facilitates  \r\ninteraction with product teams that have served complex organizations  \r\nconcerned with business process integrity, information security,  \r\nprivacy, and information risk management. The research builds on  \r\n\"contextual integrity\" (a philosophical account of privacy) as well  \r\nas language and risk-based methods for privacy policy specification  \r\nand enforcement. Extensive training and educational opportunities are  \r\nprovided to undergraduate and graduate students and research results  \r\nintegrated into courses at CMU, NYU, Stanford, and UPenn.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Anupam",
   "pi_last_name": "Datta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anupam Datta",
   "pi_email_addr": "danupam@andrew.cmu.edu",
   "nsf_id": "000501887",
   "pi_start_date": "2008-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "737100",
   "pgm_ele_name": "CYBER TRUST"
  },
  {
   "pgm_ele_code": "779500",
   "pgm_ele_name": "TRUSTWORTHY COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 62500.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 125000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 62500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Privacy has become a significant concern in modern society as personal<br />information about individuals is increasingly collected, used, and shared,<br />often using digital technologies, by a wide range of organizations. To mitigate privacy concerns, organizations are required to respect privacy laws in regulated sectors (e.g., HIPAA in healthcare, GLBA in financial sector) and to adhere to self-declared privacy policies in self-regulated sectors (e.g., privacy policies of companies such as Google and Facebook in Web services). Enforcing these kinds of privacy policies in organizations is difficult because privacy laws and enterprise policies typically identify a complex set of conditions governing the disclosure of personal information. For example, the HIPAA Privacy Rule includes over 80 clauses that permit, deny, and even require the disclosure of personal health information, making it difficult to manually ensure that all disclosures are compliant with the law. <br /><br />The research team at Carnegie Mellon University created a formal language for specifying a rich class of privacy policies. They then used this language to produce the first complete formal specification of disclosure clauses in two important US privacy laws --- the Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule and the Gramm-Leach-Bliley Act (GLBA). Recognizing that certain portions of complex privacy policies such as HIPAA are subjective and might require input from human auditors for compliance determination, the specification clearly separates out the subjective and the objective portions of a given policy. The team then developed an algorithm that checks audit logs for compliance with privacy policies expressed in their language.&nbsp; The algorithm has two distinct characteristics. First, it automatically checks the objective portion of the privacy policy for compliance and outputs the subjective portion for inspection by human auditors. Second, recognizing that audit logs are often incomplete in practice (i.e., they may not contain sufficient information to determine whether a policy is violated or not), the algorithm proceeds iteratively: in each iteration it provably checks as much of the policy it possibly can over the current log and outputs a residual policy that can only be checked when the log is extended with additional information.&nbsp; Initial experiments with a prototype implementation checking compliance of simulated audit logs with the HIPAA Privacy Rule indicates that the algorithm is fast enough to be used in practice. <br /><br />Related collaborative efforts by research teams at Stanford University and the University of Pennsylvania resulted in other algorithms for checking compliance of actions with privacy policies with a focus on the healthcare domain. In addition, a joint team from Carnegie Mellon University and New York University conducted a multidisciplinary study of the privacy implications of moving court records online from technology and policy standpoints and presented concrete recommendations on how to mitigate privacy threats arising from this move.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/17/2012<br>\n\t\t\t\t\tModified by: Anupam&nbsp;Datta</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPrivacy has become a significant concern in modern society as personal\ninformation about individuals is increasingly collected, used, and shared,\noften using digital technologies, by a wide range of organizations. To mitigate privacy concerns, organizations are required to respect privacy laws in regulated sectors (e.g., HIPAA in healthcare, GLBA in financial sector) and to adhere to self-declared privacy policies in self-regulated sectors (e.g., privacy policies of companies such as Google and Facebook in Web services). Enforcing these kinds of privacy policies in organizations is difficult because privacy laws and enterprise policies typically identify a complex set of conditions governing the disclosure of personal information. For example, the HIPAA Privacy Rule includes over 80 clauses that permit, deny, and even require the disclosure of personal health information, making it difficult to manually ensure that all disclosures are compliant with the law. \n\nThe research team at Carnegie Mellon University created a formal language for specifying a rich class of privacy policies. They then used this language to produce the first complete formal specification of disclosure clauses in two important US privacy laws --- the Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule and the Gramm-Leach-Bliley Act (GLBA). Recognizing that certain portions of complex privacy policies such as HIPAA are subjective and might require input from human auditors for compliance determination, the specification clearly separates out the subjective and the objective portions of a given policy. The team then developed an algorithm that checks audit logs for compliance with privacy policies expressed in their language.  The algorithm has two distinct characteristics. First, it automatically checks the objective portion of the privacy policy for compliance and outputs the subjective portion for inspection by human auditors. Second, recognizing that audit logs are often incomplete in practice (i.e., they may not contain sufficient information to determine whether a policy is violated or not), the algorithm proceeds iteratively: in each iteration it provably checks as much of the policy it possibly can over the current log and outputs a residual policy that can only be checked when the log is extended with additional information.  Initial experiments with a prototype implementation checking compliance of simulated audit logs with the HIPAA Privacy Rule indicates that the algorithm is fast enough to be used in practice. \n\nRelated collaborative efforts by research teams at Stanford University and the University of Pennsylvania resulted in other algorithms for checking compliance of actions with privacy policies with a focus on the healthcare domain. In addition, a joint team from Carnegie Mellon University and New York University conducted a multidisciplinary study of the privacy implications of moving court records online from technology and policy standpoints and presented concrete recommendations on how to mitigate privacy threats arising from this move.\n\n\t\t\t\t\tLast Modified: 11/17/2012\n\n\t\t\t\t\tSubmitted by: Anupam Datta"
 }
}