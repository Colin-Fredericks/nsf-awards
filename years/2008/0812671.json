{
 "awd_id": "0812671",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI-Small: Statistical Relational Models for Semantic Robot Mapping",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Richard Voyles",
 "awd_eff_date": "2008-08-01",
 "awd_exp_date": "2012-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2008-07-31",
 "awd_max_amd_letter_date": "2010-08-12",
 "awd_abstract_narration": "How can we build robots that are able to distinguish and handle the many objects located in our everyday environments?  And how can we endow these robots with the ability to reason about spatial concepts such as rooms, hallways, streets, and intersections?  Even though the robotics community has made tremendous progress in the development of efficient techniques for representing and dealing with noisy sensor information, current techniques do not have the expressive power to address these questions. In this project, we will develop statistical relational machine learning techniques that are able to extract high-level concepts from robotic sensor data.  By transferring knowledge learned in other environments, our techniques will enable robots to recognize objects and places in previously unseen environments.  Ultimately, this research will bring us closer to the dream of truly autonomous robots; robots that can interact with people and operate successfully in the complex environments we live in.\r\n\r\nThis project also includes teaching efforts and the involvement of undergraduate students in research.  Furthermore, it contains collaboration with an existing NSF project to expose young African-American students to the educational and career opportunities available in computer science, robotics and artificial intelligence.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dieter",
   "pi_last_name": "Fox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dieter Fox",
   "pi_email_addr": "fox@cs.washington.edu",
   "nsf_id": "000210667",
   "pi_start_date": "2008-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 BROOKLYN AVE NE",
  "perf_city_name": "SEATTLE",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981951016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 123749.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 135520.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 140731.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Mobile robots need to build maps, or models, of their environments in order to navigate to specific locations or to interact with their environment. For instance, executing the simple command \"<em>bring me my coffee mug from the conference room</em>\" requires a robot to recognize the coffee mug, know where the conference room is, and navigate to that room. &nbsp;Most previous work on robot mapping has focused on building maps that are spatially consistent and suitable for path planning, but not on generating maps that contain semantic information such as the different types of places and objects in an environment.</p>\n<p>The goal of this project was to develop the fundamental techniques needed to generate and utilize such semantic maps. &nbsp;The research was organized in three thrusts. The first thrust focused on building much richer, <em>3D representations</em> of indoor environments. &nbsp;This work took advantage of the new depth cameras developed mainly for the computer gaming industry, such as Microsoft's Kinect system. By combining color and depth information provided by such cameras, we were able to generate 3D models of indoor environments that went far beyond the quality of maps being built before. &nbsp;We demonstrated that such maps could be built by a person walking through a building or even by an autonomous quadcopter flying through the environment. &nbsp;The second research thrust focused on recognizing objects in an environment. &nbsp;Here, we developed novel approaches that take advantage of both color and 3D shape information to better recognize everyday objects, such as cereal boxes, coffee mugs, cars, trees, and computer keyboards. In one paper, we showed how the large sets of objects stored in Google's 3D Warehouse object database can be used to further improve object recognition. &nbsp;The third research thrust investigated how a robot could follow directions given in natural language. &nbsp;By phrasing this problem as a statistical machine translation problem, our system was able to learn to parse human directions into a semantic map of an environment. &nbsp;For instance, we showed that a robot can build a map containing information about hallways, rooms and intersections, and follow human commands such as \"<em>turn left into the next hallway and then enter the second room on the right</em>\".</p>\n<p><br />Overall, this project developed novel statistical reasoning and machine learning techniques that enable robots to reason about their environments in high level terms, such as objects and places. &nbsp;Building on such representations, these robots can learn to understand natural language commands, thereby enabling them to interact with people in a much more intuitive way.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/20/2012<br>\n\t\t\t\t\tModified by: Dieter&nbsp;Fox</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2012/0812671/0812671_10025708_1353391022707_hri--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2012/0812671/0812671_10025708_1353391022707_hri--rgov-800width.jpg\" title=\"Direction following\"><img src=\"/por/images/Reports/POR/2012/0812671/0812671_10025708_1353391022707_hri--rgov-66x44.jpg\" alt=\"Direction following\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Example of a map of an indoor environment along with natural language directions that can be followed by a robot using our statistical machine translation approach.</div>\n<...",
  "por_txt_cntn": "\nMobile robots need to build maps, or models, of their environments in order to navigate to specific locations or to interact with their environment. For instance, executing the simple command \"bring me my coffee mug from the conference room\" requires a robot to recognize the coffee mug, know where the conference room is, and navigate to that room.  Most previous work on robot mapping has focused on building maps that are spatially consistent and suitable for path planning, but not on generating maps that contain semantic information such as the different types of places and objects in an environment.\n\nThe goal of this project was to develop the fundamental techniques needed to generate and utilize such semantic maps.  The research was organized in three thrusts. The first thrust focused on building much richer, 3D representations of indoor environments.  This work took advantage of the new depth cameras developed mainly for the computer gaming industry, such as Microsoft's Kinect system. By combining color and depth information provided by such cameras, we were able to generate 3D models of indoor environments that went far beyond the quality of maps being built before.  We demonstrated that such maps could be built by a person walking through a building or even by an autonomous quadcopter flying through the environment.  The second research thrust focused on recognizing objects in an environment.  Here, we developed novel approaches that take advantage of both color and 3D shape information to better recognize everyday objects, such as cereal boxes, coffee mugs, cars, trees, and computer keyboards. In one paper, we showed how the large sets of objects stored in Google's 3D Warehouse object database can be used to further improve object recognition.  The third research thrust investigated how a robot could follow directions given in natural language.  By phrasing this problem as a statistical machine translation problem, our system was able to learn to parse human directions into a semantic map of an environment.  For instance, we showed that a robot can build a map containing information about hallways, rooms and intersections, and follow human commands such as \"turn left into the next hallway and then enter the second room on the right\".\n\n\nOverall, this project developed novel statistical reasoning and machine learning techniques that enable robots to reason about their environments in high level terms, such as objects and places.  Building on such representations, these robots can learn to understand natural language commands, thereby enabling them to interact with people in a much more intuitive way.\n\n\t\t\t\t\tLast Modified: 11/20/2012\n\n\t\t\t\t\tSubmitted by: Dieter Fox"
 }
}