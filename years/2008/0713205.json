{
 "awd_id": "0713205",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Collaborative Research: Foreign accent conversion through articulatory inversion of the vocal-tract frontal cavity",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2008-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 229883.0,
 "awd_amount": 229883.0,
 "awd_min_amd_letter_date": "2008-07-31",
 "awd_max_amd_letter_date": "2013-05-22",
 "awd_abstract_narration": "The ability to transform a ?foreign? accented voice into its ?native? counterpart could be an invaluable tool in pronunciation training for second-language learners. This requires separating those aspects of the speech signal that are determined by the anatomy of the vocal tract from those that result from the idiosyncratic way in which the speaker controls it. While these two sources interact in complex ways in the acoustic domain, a few studies indicate that they may be decoupled in the articulatory space, specifically in the vocal tract frontal cavity.\r\n\r\nThe objective of this research is to determine the extent to which foreign-accent conversion can be performed through articulatory inversion of the frontal cavity. For this purpose, two complementary problems are being investigated. First, existing articulatory datasets are being used to develop a foreign-accent conversion model that operates in the frontal cavity domain. Second, articulatory inversion models are being developed to estimate the frontal cavity configuration from speech acoustics. Results from these models are being systematically validated through perceptual tests of foreign-accentedness, speaker identity and acoustic quality.\r\n\r\nEnglish is a second language for a significant percentage of the workforce in the United States. Reduction of foreign accent becomes increasingly difficult beyond the ?critical period? of language learning, but substantial improvements in pronunciation do occur for adult second-language learners. This work will stimulate the development of new technology to facilitate such improvements. Its results may also find application for film dubbing/looping, as well as in speech technology at large (e.g., feature extraction, data compression).\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ricardo",
   "pi_last_name": "Gutierrez-Osuna",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ricardo Gutierrez-Osuna",
   "pi_email_addr": "rgutier@cs.tamu.edu",
   "nsf_id": "000255119",
   "pi_start_date": "2008-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M Engineering Experiment Station",
  "perf_str_addr": "3124 TAMU",
  "perf_city_name": "COLLEGE STATION",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433124",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 77560.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 75150.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 77173.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Despite years or decades of immersion in a new culture, older learners of a second language (L2) typically speak with a so-called &ldquo;foreign accent.&rdquo;&nbsp; Although a non-native accent does not necessarily limit intelligibility, L2 speakers can be subjected to discriminatory attitudes and negative stereotypes.&nbsp; Thus, by improving their pronunciation, adult L2 learners stand more to gain than mere intelligibility.&nbsp; Prior research has shown that L2 learners can benefit from imitating a native (L1) speaker with a similar voice as their own. However, finding such a &ldquo;golden speaker&rdquo; for each learner is impractical. The goal of this grant was to develop signal processing techniques to generate the ideal &ldquo;golden speaker&rdquo; for each L2 learner: their own voice, but with a native accent.</p>\n<p>Our approach is illustrated in Figure 1.&nbsp; In a first step (A), we build an articulatory synthesizer for the L2 learner: an algorithm that transforms the L2 speaker&rsquo;s articulatory gestures, such as tongue and lip movements, into audio.&nbsp; In a second step (B), we drive the synthesizer with articulatory gestures from a L1 speaker.&nbsp; The resulting speech audio has the voice quality of the L2 speaker but the linguistic content (and thus the native accent) of the L1 speaker.</p>\n<p>A critical step in this process is the articulatory synthesizer. Throughout the project we developed and evaluated three types of synthesizer: concatenative, statistical, and neural.&nbsp;&nbsp; In concatenative synthesis, we collect a large database of short speech segments for the L2 speaker, each segment containing an articulatory gesture and the corresponding speech audio.&nbsp; Given an L1 utterance, we divide it into short segments, and for each segment we look up the database to find an L2 segment with similar gestures. In a final step, we concatenate the individual L2 segments to produce speech.&nbsp; Unfortunately, unless the database is large (hours of speech), the concatenated speech has noticeable discontinuities at the boundaries between segments. The statistical synthesizer avoids this problem by building a continuous function from the L2 database using machine-learning algorithms.&nbsp; To generate audio, one simply provides the statistical synthesizer with sequences of gestures: if we use L2 gestures, the result is L2 speech with a non-native accent; if we use L1 gestures, the result is L2 speech with a native-accent. The neural synthesizer operates in a similar fashion as the statistical synthesizer, except it uses different machine-learning algorithms to build the continuous function.</p>\n<p>The result of this accent-conversion process is speech that has never been produced, so it cannot be compared against any reference speech signal &ndash;the L2 learner can only produce foreign-accented speech. Instead, the quality of the accent conversion has to be assessed by asking human listeners to rate it.&nbsp; Thus, a second critical component of this project was to develop suitable listening tests.&nbsp; Throughout the project we developed listening tests for four subjective measures: accentedness, acoustic quality, speaker identity, and intelligibility.&nbsp; To measure accentedness, we ask participants to listen to two utterances (one containing the accent conversion, the second containing either L1 or L2 speech), then select the one that is more native-accented.&nbsp; To measure acoustic quality, we ask participants to listen to one utterance then rate its quality on a 5-point scale (1: bad quality; 5: excellent quality). To measure speaker identity, we ask participants to listen to two utterances, then decide whether they are from the same speaker or from two different speakers; to avoid interferences between accent and identity perceptions, utterances in this test are played backwards in time.&nbsp; Finally, to measure intelligibility...",
  "por_txt_cntn": "\nDespite years or decades of immersion in a new culture, older learners of a second language (L2) typically speak with a so-called \"foreign accent.\"  Although a non-native accent does not necessarily limit intelligibility, L2 speakers can be subjected to discriminatory attitudes and negative stereotypes.  Thus, by improving their pronunciation, adult L2 learners stand more to gain than mere intelligibility.  Prior research has shown that L2 learners can benefit from imitating a native (L1) speaker with a similar voice as their own. However, finding such a \"golden speaker\" for each learner is impractical. The goal of this grant was to develop signal processing techniques to generate the ideal \"golden speaker\" for each L2 learner: their own voice, but with a native accent.\n\nOur approach is illustrated in Figure 1.  In a first step (A), we build an articulatory synthesizer for the L2 learner: an algorithm that transforms the L2 speaker\u00c6s articulatory gestures, such as tongue and lip movements, into audio.  In a second step (B), we drive the synthesizer with articulatory gestures from a L1 speaker.  The resulting speech audio has the voice quality of the L2 speaker but the linguistic content (and thus the native accent) of the L1 speaker.\n\nA critical step in this process is the articulatory synthesizer. Throughout the project we developed and evaluated three types of synthesizer: concatenative, statistical, and neural.   In concatenative synthesis, we collect a large database of short speech segments for the L2 speaker, each segment containing an articulatory gesture and the corresponding speech audio.  Given an L1 utterance, we divide it into short segments, and for each segment we look up the database to find an L2 segment with similar gestures. In a final step, we concatenate the individual L2 segments to produce speech.  Unfortunately, unless the database is large (hours of speech), the concatenated speech has noticeable discontinuities at the boundaries between segments. The statistical synthesizer avoids this problem by building a continuous function from the L2 database using machine-learning algorithms.  To generate audio, one simply provides the statistical synthesizer with sequences of gestures: if we use L2 gestures, the result is L2 speech with a non-native accent; if we use L1 gestures, the result is L2 speech with a native-accent. The neural synthesizer operates in a similar fashion as the statistical synthesizer, except it uses different machine-learning algorithms to build the continuous function.\n\nThe result of this accent-conversion process is speech that has never been produced, so it cannot be compared against any reference speech signal &ndash;the L2 learner can only produce foreign-accented speech. Instead, the quality of the accent conversion has to be assessed by asking human listeners to rate it.  Thus, a second critical component of this project was to develop suitable listening tests.  Throughout the project we developed listening tests for four subjective measures: accentedness, acoustic quality, speaker identity, and intelligibility.  To measure accentedness, we ask participants to listen to two utterances (one containing the accent conversion, the second containing either L1 or L2 speech), then select the one that is more native-accented.  To measure acoustic quality, we ask participants to listen to one utterance then rate its quality on a 5-point scale (1: bad quality; 5: excellent quality). To measure speaker identity, we ask participants to listen to two utterances, then decide whether they are from the same speaker or from two different speakers; to avoid interferences between accent and identity perceptions, utterances in this test are played backwards in time.  Finally, to measure intelligibility, we ask participants to listen to an utterance, then transcribe it; intelligibility is then measured as the proportion of words in the utterance that were properly transcribed.\n\nUsing this battery of t..."
 }
}