{
 "awd_id": "0747120",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Integrated system for object and scene recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2008-04-01",
 "awd_exp_date": "2013-03-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2008-03-27",
 "awd_max_amd_letter_date": "2012-05-10",
 "awd_abstract_narration": "Abstract\r\n\r\nTitle: Integrated system for object and scene recognition\r\n\r\nPI: Antonio Torralba\r\n\r\nInstitution: MIT\r\n\r\nIn traditional computer vision, scene and object recognition are two related visual tasks generally studied separately. By devising systems that solve these tasks in an integrated fashion it is possible to build more efficient and robust recognition systems. At the lowest level, significant computational savings can be achieved if different categories share a common set of features. More importantly, jointly trained recognition systems can use similarities between object categories to their advantage by learning features which lead to better generalization. In complex natural scenes, object recognition systems can be further improved by using contextual knowledge both about the objects likely to be found in a given scene, and also the spatial relationships between those objects. Object detection and recognition is generally posed as a matching problem between the object representation and the image features while rejecting the background features using an outlier process. The PI will formulate object detection as a problem of aligning elements of the entire scene. The background, instead of being treated as a set of outliers will be used to guide the detection process.\r\n\r\nIn developing integrated systems that try to recognize many objects, the lack of large annotated datasets becomes a major problem. The PI created and will extend two datasets; LabelMe and the 80 million tiny images datasets. LabelMe is an online annotation tool that allows sharing and labeling images for computer vision research. Both datasets offers an invaluable resource for research and teaching on computer vision and computer graphics. The datasets are also intended to foster creativity, as they allows students at all levels to explore well established algorithms as well as devise new applications in computer vision and computer graphics. The PI will also develop new image and video datasets by exploiting the millions of images available on the internet.\r\n\r\nThe creation of robust systems for scene understanding will have a major impact on many fields by allowing the creation of smart devices able to interact and understand their environment, from aids to the visually-impaired, to autonomous vehicles, robotic assistants, or online tools for searching visual information.\r\n\r\nThe PI will extend his teaching and research activities beyond the boundaries of the classroom and the laboratory by developing a substantial amount of online material.\r\n\r\nURL: http://people.csail.mit.edu/torralba/integratedSceneRecognition/",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Antonio",
   "pi_last_name": "Torralba",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Antonio Torralba",
   "pi_email_addr": "torralba@csail.mit.edu",
   "nsf_id": "000096240",
   "pi_start_date": "2008-03-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 108429.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 94493.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 95887.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 99255.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 101936.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Among the many problems that need to be addressed to build an artificial vision system, object and scene recognition are one of the central themes of today&rsquo;s research. One of the recent successes of the field is face detection. For instance, the ability to localize faces inside images automatically, accurately and fast is now a common feature in most digital cameras. But general object detection and scene understanding remains a challenging task. For instance, detecting a plate appears to be a more challenging task than detecting a face. In part because its appearance is only defined by a few shape cues and the variability in shapes, textures and colors is very large compared to faces. In many situations, what is a plate might only be constrained by the context it is part of. The current challenge for computer vision scientists is to create systems that can search and recognize objects based on both location and context, such as understanding that a plate is likely to be on top of a dining room table or in a picture of a dining room.</p>\n<p>&nbsp;</p>\n<p>We know that contextual regularities play a fundamental role in human recognition of objects in natural images. The strength of context in visual perception is illustrated in Fig. 1. The role of context becomes essential when the features of the objects are degraded or even not available (e.g., object too small or largely occluded). In this picture subjects describe the scenes as containing (a) a car in the street, and (b) a pedestrian in the street. However, the pedestrian is in fact the same shape as the car except for a 90 degrees rotation. The non-typicality of this orientation for a car within the context defined by the street makes the car appear as a pedestrian.</p>\n<p>&nbsp;</p>\n<p>The goal of this award was to explore new representations for scene understanding and to develop the datasets needed to train such generic systems. Part of the goal within this research aim was to develop large datasets of annotated images. Indeed, datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. One important part of the research carried under this award has been the continuous development of a free web based image annotation tool (fig. 2). The goal of the tool is to build a large collection of annotated images for training systems for scene understanding. The tool is made available so that other researchers can build their own datasets. Using LabelMe, we have been building the SUN database. The SUN database spans a very large number of scene categories (more than 400 scene categories) and contains more than 300,000 segmented objects.</p>\n<p>&nbsp;</p>\n<p>The dataset has allowed us exploring new representations for context reasoning and object detection. A context model can rule out some unlikely combinations or locations of objects and guide detectors to produce a semantically coherent interpretation of a scene. However, the performance benefit of context models has been limited because most of the previous methods were tested on data sets with only a few object categories, in which most images contain one or two object categories. Our model incorporates global image features, dependencies between object categories, and outputs of local detectors into one probabilistic framework. Our context model improves object recognition performance and provides a more coherent interpretation of a scene than what is achieved with detectors alone. In addition, our model can be applied to scene understanding tasks that local detectors alone cannot solve, such as detecting objects out of context (fig. 3) or querying for the most typical and the least typical scenes in a data set.</p>\n<p>&nbsp;</p>\n<p>One...",
  "por_txt_cntn": "\nAmong the many problems that need to be addressed to build an artificial vision system, object and scene recognition are one of the central themes of today\u00c6s research. One of the recent successes of the field is face detection. For instance, the ability to localize faces inside images automatically, accurately and fast is now a common feature in most digital cameras. But general object detection and scene understanding remains a challenging task. For instance, detecting a plate appears to be a more challenging task than detecting a face. In part because its appearance is only defined by a few shape cues and the variability in shapes, textures and colors is very large compared to faces. In many situations, what is a plate might only be constrained by the context it is part of. The current challenge for computer vision scientists is to create systems that can search and recognize objects based on both location and context, such as understanding that a plate is likely to be on top of a dining room table or in a picture of a dining room.\n\n \n\nWe know that contextual regularities play a fundamental role in human recognition of objects in natural images. The strength of context in visual perception is illustrated in Fig. 1. The role of context becomes essential when the features of the objects are degraded or even not available (e.g., object too small or largely occluded). In this picture subjects describe the scenes as containing (a) a car in the street, and (b) a pedestrian in the street. However, the pedestrian is in fact the same shape as the car except for a 90 degrees rotation. The non-typicality of this orientation for a car within the context defined by the street makes the car appear as a pedestrian.\n\n \n\nThe goal of this award was to explore new representations for scene understanding and to develop the datasets needed to train such generic systems. Part of the goal within this research aim was to develop large datasets of annotated images. Indeed, datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. One important part of the research carried under this award has been the continuous development of a free web based image annotation tool (fig. 2). The goal of the tool is to build a large collection of annotated images for training systems for scene understanding. The tool is made available so that other researchers can build their own datasets. Using LabelMe, we have been building the SUN database. The SUN database spans a very large number of scene categories (more than 400 scene categories) and contains more than 300,000 segmented objects.\n\n \n\nThe dataset has allowed us exploring new representations for context reasoning and object detection. A context model can rule out some unlikely combinations or locations of objects and guide detectors to produce a semantically coherent interpretation of a scene. However, the performance benefit of context models has been limited because most of the previous methods were tested on data sets with only a few object categories, in which most images contain one or two object categories. Our model incorporates global image features, dependencies between object categories, and outputs of local detectors into one probabilistic framework. Our context model improves object recognition performance and provides a more coherent interpretation of a scene than what is achieved with detectors alone. In addition, our model can be applied to scene understanding tasks that local detectors alone cannot solve, such as detecting objects out of context (fig. 3) or querying for the most typical and the least typical scenes in a data set.\n\n \n\nOne of the important points about large databases is that even relatively simple algorithms can show improved performance when trained with..."
 }
}