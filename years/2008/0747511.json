{
 "awd_id": "0747511",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Learning models for object structure",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2008-04-01",
 "awd_exp_date": "2014-03-31",
 "tot_intn_awd_amt": 449607.0,
 "awd_amount": 481607.0,
 "awd_min_amd_letter_date": "2008-03-27",
 "awd_max_amd_letter_date": "2012-03-14",
 "awd_abstract_narration": "Abstract:\r\n\r\nTitle: CAREER: Learning Models for Object Structure\r\n\r\nPI: Kobus Barnard\r\n\r\nInstitution: University of Arizona.\r\n\r\nThis project will develop approaches for learning stochastic geometric models for object categories from image data. Good representations of object form that encode the variation in typical categories (e.g. cars) are needed for important problems in computer vision and intelligent systems. One key problem is object recognition at the category level. What makes an object a member of one category (e.g. tables) instead of another (e.g. chairs) strongly relates to its structure, and automatically choosing among them to robustly recognize a new object requires appropriate representations of form.  \r\n\r\nA second problem is reasoning about object configuration and structure. For example, a standard chair should be recognizable as being similar to a table in certain ways, different in other ways, perhaps seen as blocking a particular path in a room, and considered useful as a step for reaching something. To achieve this level of understanding, representations for geometric structure that can link to physics and semantics are needed. But where should they come from?  \r\n\r\nTo address this question, this project will explore learning effective representations from image data. More specifically, this project will study the novel approach of putting representation at the core, and learn from data which objects can be modeled in this manner. The work will begin with simple effective representations that are appropriate for some objects, and then expand the pool of models, largely by exploiting the fact that many complex objects are composed of simpler, natural, substructures, and that these are shared across multiple object categories. One result of this process will be statistical models for objects based on image data that will be disseminated to the research community.\r\n\r\nThis research will have positive impact on many applications that rely on robust recognition and scene understanding from image data, particularly in cases where the configuration, orientation, and form of objects are relevant. \r\n\r\nThese include applications where robots must function in natural environments and systems for augmenting human operators in numerous industrial, military, and everyday situations.  \r\n\r\nThe learned object category representations will have additional uses in image and video retrieval and for model palettes in computer graphics applications. This research will also impact biomedical research by improving automated extraction of biological structure from image data to recognize phenotypes and to quantify the relation of form and function in high throughput experiments.\r\n\r\nThis project integrates two important educational initiatives: 1) curriculum development to increase opportunities for classroom study in computer vision, machine learning, and scientific applications at the University of Arizona; and 2) an educational outreach program targeted at Tucson high-school students from low socioeconomic groups that will promote an understanding of the integration of science and computation.\r\n\r\nProject URL: http://vision.cs.arizona.edu/kobus/CAREER\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kobus",
   "pi_last_name": "Barnard",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kobus Barnard",
   "pi_email_addr": "kobus@cs.arizona.edu",
   "nsf_id": "000488209",
   "pi_start_date": "2008-03-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Arizona",
  "inst_street_address": "845 N PARK AVE RM 538",
  "inst_street_address_2": "",
  "inst_city_name": "TUCSON",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "5206266000",
  "inst_zip_code": "85721",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "AZ07",
  "org_lgl_bus_name": "UNIVERSITY OF ARIZONA",
  "org_prnt_uei_num": "",
  "org_uei_num": "ED44Y3W6P7B9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Arizona",
  "perf_str_addr": "845 N PARK AVE RM 538",
  "perf_city_name": "TUCSON",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "85721",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "AZ07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 282018.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 167589.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project advanced our understanding of computer vision systems that integrate and exploit 3D representations of the world. While 3D has a long history in computer vision, most recent work uses representations based on 2D images. However, for many applications, understanding the 3D geometry of the world is important. Further, 3D representations of the world are often simpler than those based on images. This is because information that is lost during projection (e.g., due to occlusion) is difficult to model, and the camera parameters (position, angle, and focal length), which lead to different views, are typically unknown. In this project we developed the hypothesis that given models for what is in the world, we can often infer the camera parameters. This is because evidence for what is the world (e.g., chair versus table) is different than, and not confusable with, what the camera parameters are (e.g., longer or shorter focal length).</p>\n<p>&nbsp;</p>\n<p>Our methodology throughout this project is to construct principled models for the expected evidence in images given a hypothesis for both what is in the world and the camera parameters. We then search for a hypothesis that gives high agreement with the evidence in images and our prior understanding of the world. We applied this methodology to three different related domains as follows.</p>\n<p>&nbsp;</p>\n<p>The first domain was learning models of object structure. Here we showed that a computer program can learn the topology of simple furniture objects (e.g., the parts and how they are connected) by assuming that objects are contiguous in 3D, are constructed from simple parts (e.g., blocks), and objects within a class have the same topology. We found that eight images of an object class was often sufficient to learn the topology of classes such as tables, chairs, sofas, and cabinets. This work lead to a paper published in a top venue (NIPS&rsquo;09).</p>\n<p>&nbsp;</p>\n<p>The second domain was understanding indoor scenes from single images. Here, understanding includes the room layout, camera parameters, and the identity and geometry of objects within the room, including frames (windows, doors, and pictures). We contributed a principled fully Bayesian method for doing so. We first showed that the method can provide good estimates for the room layout with objects represented by their 3D bounding boxes. Next, we showed that we can use external information (e.g., numbers from furniture catalogues) for the dimensions of objects to identify objects within the scene, which also improved the estimated room layout by reducing confusion from non-existent objects with unrealistic dimensions. Finally, we demonstrated that more fine grained modeling of non-convex objects (e.g., tables and chairs) further improved scene understanding because the system was now able to tuck chairs partly underneath tables to better explain image evidence. In addition, we demonstrated that using context (e.g., that chairs are often near tables) to find objects improved scene understanding even further. These three thrusts lead to three papers in top venues (CVPR&rsquo;11, CVPR&rsquo;12, and CVPR&rsquo;13).</p>\n<p>&nbsp;</p>\n<p>The third domain was temporal scene understanding in the context of people moving about on a horizontal ground plane as captured by a stationary video camera. Here we were able to track people&rsquo;s 3D location on the ground plane, estimate their relative heights, width, and girth, and infer the camera parameters. Intuitively, people provide probes for the camera parameters as they move towards and away from the camera without changing size. Working entirely in 3D leads to richer information extraction, and has several advantages for tracking as well. First, in 3D, given a camera hypothesis, our system is not confused by one person occluding another. In fact, one person disappearing behind another is a good source of ev...",
  "por_txt_cntn": "\nThis project advanced our understanding of computer vision systems that integrate and exploit 3D representations of the world. While 3D has a long history in computer vision, most recent work uses representations based on 2D images. However, for many applications, understanding the 3D geometry of the world is important. Further, 3D representations of the world are often simpler than those based on images. This is because information that is lost during projection (e.g., due to occlusion) is difficult to model, and the camera parameters (position, angle, and focal length), which lead to different views, are typically unknown. In this project we developed the hypothesis that given models for what is in the world, we can often infer the camera parameters. This is because evidence for what is the world (e.g., chair versus table) is different than, and not confusable with, what the camera parameters are (e.g., longer or shorter focal length).\n\n \n\nOur methodology throughout this project is to construct principled models for the expected evidence in images given a hypothesis for both what is in the world and the camera parameters. We then search for a hypothesis that gives high agreement with the evidence in images and our prior understanding of the world. We applied this methodology to three different related domains as follows.\n\n \n\nThe first domain was learning models of object structure. Here we showed that a computer program can learn the topology of simple furniture objects (e.g., the parts and how they are connected) by assuming that objects are contiguous in 3D, are constructed from simple parts (e.g., blocks), and objects within a class have the same topology. We found that eight images of an object class was often sufficient to learn the topology of classes such as tables, chairs, sofas, and cabinets. This work lead to a paper published in a top venue (NIPS\u00c609).\n\n \n\nThe second domain was understanding indoor scenes from single images. Here, understanding includes the room layout, camera parameters, and the identity and geometry of objects within the room, including frames (windows, doors, and pictures). We contributed a principled fully Bayesian method for doing so. We first showed that the method can provide good estimates for the room layout with objects represented by their 3D bounding boxes. Next, we showed that we can use external information (e.g., numbers from furniture catalogues) for the dimensions of objects to identify objects within the scene, which also improved the estimated room layout by reducing confusion from non-existent objects with unrealistic dimensions. Finally, we demonstrated that more fine grained modeling of non-convex objects (e.g., tables and chairs) further improved scene understanding because the system was now able to tuck chairs partly underneath tables to better explain image evidence. In addition, we demonstrated that using context (e.g., that chairs are often near tables) to find objects improved scene understanding even further. These three thrusts lead to three papers in top venues (CVPR\u00c611, CVPR\u00c612, and CVPR\u00c613).\n\n \n\nThe third domain was temporal scene understanding in the context of people moving about on a horizontal ground plane as captured by a stationary video camera. Here we were able to track people\u00c6s 3D location on the ground plane, estimate their relative heights, width, and girth, and infer the camera parameters. Intuitively, people provide probes for the camera parameters as they move towards and away from the camera without changing size. Working entirely in 3D leads to richer information extraction, and has several advantages for tracking as well. First, in 3D, given a camera hypothesis, our system is not confused by one person occluding another. In fact, one person disappearing behind another is a good source of evidence. Second, we can use knowledge about the typical speed of people walking, which is much more useful than doing so with image data where apparent speed dep..."
 }
}