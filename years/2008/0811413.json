{
 "awd_id": "0811413",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CPA-ACR: Fast Recovery Using Optimal and Near-Optimal Parallelism in Data-Intensive Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2008-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 273992.0,
 "awd_amount": 289992.0,
 "awd_min_amd_letter_date": "2008-08-25",
 "awd_max_amd_letter_date": "2011-04-11",
 "awd_abstract_narration": "Recent years have seen the emergence of large-scale data clusters consisting of hundreds of thousands hard drives in data-intensive computing. Because failure of a disk could cause loss of valuable data or information, the direct and indirect costs of ever often disk failures are becoming increasingly critical issues in the deployment and operation of these computational platforms. Recent studies conclude that the trend in data-intensive computing is towards much higher disk replacement rates in the field than the Mean-Time-To-Failure estimates of the manufacturer datasheet would suggest, even in the first years of operation. Hence, the recovery becomes state of storage. \r\n\r\nExisting storage recovery solutions successfully developed optimal and near-optimal parallelism layouts such as declustered parity organizations at small-scale storage architectures. There are very few studies on multi-way replication based storage architectures that are equally important  but significantly different from erasure code based storage architectures. Moreover, it is difficult to scale up to a large size because current placement-ideal solutions have a limited number of configurations. Lastly, fast recovery demands efficient reverse data lookup, which is not well studied in current scalable data distribution schemes. The investigators develop methods and tools for achieving fast recovery by exploiting optimal and near-optimal parallelism techniques, and distributed hash table and reverse hashing techniques to improve the scalability of reverse data lookup in high-performance storage systems. The proposed research, if successful, will have broad impact in both fault-tolerance computing and high-performance computing community by providing a scalable and fast storage recovery solution.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Wang",
   "pi_email_addr": "Jun.Wang@ucf.edu",
   "nsf_id": "000277527",
   "pi_start_date": "2008-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "The University of Central Florida Board of Trustees",
  "inst_street_address": "4000 CENTRAL FLORIDA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "ORLANDO",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "4078230387",
  "inst_zip_code": "328168005",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "FL10",
  "org_lgl_bus_name": "THE UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RD7MXJV7DKT9"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Central Florida Board of Trustees",
  "perf_str_addr": "4000 CENTRAL FLORIDA BLVD",
  "perf_city_name": "ORLANDO",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "328168005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "FL10",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "408000",
   "pgm_ele_name": "ADVANCED COMP RESEARCH PROGRAM"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 95179.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 88933.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 89880.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Recent years have seen the emergence of warehouse-scale computers consisting of hundreds of thousands hard drives in data-intensive computing. Because failure of a disk could cause loss of valuable data or information, the direct and indirect costs of frequent disk failures are becoming critical issues in deploying and operating these computational platforms. Recent studies conclude that the trend in data-intensive computing is towards much higher disk replacement rates in the field than the Mean-Time-To-Failure estimates of the manufacturer datasheet would suggest, even in the first years of operation. Hence, the recovery becomes state of storage.</p>\n<p>Existing storage recovery solutions successfully developed optimal and near optimal parallelism layouts such as declustered parity organizations at small-scale storage architectures. There are very few studies on multi-way replication based storage architectures that are equally important but significantly different from erasure code based storage architectures. Moreover, it is difficult to scale up to a large size because current placement-ideal solutions have a limited number of configurations. Lastly, fast recovery demands efficient reverse data lookup,which is not well studied in current scalable data distribution schemes.</p>\n<p>The investigators develop methods and tools for achieving fast recovery by exploiting optimal and near-optimal parallelism techniques, and distributed hash table and reverse hashing techniques to improve the scalability of reverse data lookup in high-performance storage systems. The proposed research makes broad impact in both fault-tolerance computing and high-performance computing community by providing a scalable and fast storage recovery solution. First, our theoretical proofs and comprehensive simulation results show that the proposed shifted declustering data layout scheme is superiour in performance and load-balancing to traditional replication layout schemes. Second, the proposed shifted declustering data layout scheme outperforms current layouts in multi-way replication computer storage system in terms of mean time to data loss&nbsp;(MTTDL). Third, our mathematical proofs and real-life experiments show that Group based shifted declustering data layout scheme realizes a scalable reverse lookup that is up to one order of magnitude faster than existing schemes such as Google's random placement solution.</p>\n<p>Moreover, this research makes indirect key outcomes for industrial impact and cost savings in super data clusters and data-intensive computing centers, and in the environmental benefits of fault-tolerant and green computing. The outcomes include: (i) new storage system organizations and architectures for high RAS(Reliability, Availability, Serviceability), high-performance and high energy efficiency;(ii) a broad-based fundamental understanding of the relationship and tradeoffs among fault-tolerance and performance control techniques; and (iii) models, algorithms, methods and tools to analyze, and control reliability andperformance of large-scale data-intensive computational systems. In addition,several graduate students and undergraduate students including minority representatives such as female students and/or Hispanic students have been trained.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/17/2013<br>\n\t\t\t\t\tModified by: Jun&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2013/0811413/0811413_10025656_1382060650644_problem--rgov-214x142.jp...",
  "por_txt_cntn": "\nRecent years have seen the emergence of warehouse-scale computers consisting of hundreds of thousands hard drives in data-intensive computing. Because failure of a disk could cause loss of valuable data or information, the direct and indirect costs of frequent disk failures are becoming critical issues in deploying and operating these computational platforms. Recent studies conclude that the trend in data-intensive computing is towards much higher disk replacement rates in the field than the Mean-Time-To-Failure estimates of the manufacturer datasheet would suggest, even in the first years of operation. Hence, the recovery becomes state of storage.\n\nExisting storage recovery solutions successfully developed optimal and near optimal parallelism layouts such as declustered parity organizations at small-scale storage architectures. There are very few studies on multi-way replication based storage architectures that are equally important but significantly different from erasure code based storage architectures. Moreover, it is difficult to scale up to a large size because current placement-ideal solutions have a limited number of configurations. Lastly, fast recovery demands efficient reverse data lookup,which is not well studied in current scalable data distribution schemes.\n\nThe investigators develop methods and tools for achieving fast recovery by exploiting optimal and near-optimal parallelism techniques, and distributed hash table and reverse hashing techniques to improve the scalability of reverse data lookup in high-performance storage systems. The proposed research makes broad impact in both fault-tolerance computing and high-performance computing community by providing a scalable and fast storage recovery solution. First, our theoretical proofs and comprehensive simulation results show that the proposed shifted declustering data layout scheme is superiour in performance and load-balancing to traditional replication layout schemes. Second, the proposed shifted declustering data layout scheme outperforms current layouts in multi-way replication computer storage system in terms of mean time to data loss (MTTDL). Third, our mathematical proofs and real-life experiments show that Group based shifted declustering data layout scheme realizes a scalable reverse lookup that is up to one order of magnitude faster than existing schemes such as Google's random placement solution.\n\nMoreover, this research makes indirect key outcomes for industrial impact and cost savings in super data clusters and data-intensive computing centers, and in the environmental benefits of fault-tolerant and green computing. The outcomes include: (i) new storage system organizations and architectures for high RAS(Reliability, Availability, Serviceability), high-performance and high energy efficiency;(ii) a broad-based fundamental understanding of the relationship and tradeoffs among fault-tolerance and performance control techniques; and (iii) models, algorithms, methods and tools to analyze, and control reliability andperformance of large-scale data-intensive computational systems. In addition,several graduate students and undergraduate students including minority representatives such as female students and/or Hispanic students have been trained.\n\n\t\t\t\t\tLast Modified: 10/17/2013\n\n\t\t\t\t\tSubmitted by: Jun Wang"
 }
}