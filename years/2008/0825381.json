{
 "awd_id": "0825381",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Algorithms and Complexity for Global Optimization",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Sheldon Jacobson",
 "awd_eff_date": "2008-08-01",
 "awd_exp_date": "2012-12-31",
 "tot_intn_awd_amt": 260000.0,
 "awd_amount": 265849.0,
 "awd_min_amd_letter_date": "2008-06-23",
 "awd_max_amd_letter_date": "2011-03-18",
 "awd_abstract_narration": "This grant provides funding for the development of numerical algorithms for approximating the global optimum of performance measures that can have many local optima. The approach is based on modeling the unknown performance measure as a random function, and then developing deterministic algorithms that minimize the average approximation error. The research will comprise two main components. The first component will be to construct algorithms for different types of available information and different classes of objective functions, including continuous functions with various orders of differentiability. The types of information will include: exact function evaluations, derivative evaluations, and function evaluations corrupted by random noise. The second part of the research will be to determine complexity bounds. For a given problem setting, for example continuous functions with exact function evaluations, the investigator will establish bounds on the smallest average error that can be attained with any algorithm.\r\n\r\nIf successful, the algorithms developed in this project will be useful for two main types of problems. The first application is to the optimization of complex systems with continuous parameters where the system performance can be evaluated exactly and assumptions such as unimodality or convexity of the performance measure are not warranted. Such problems include optimizing groundwater contamination treatment plans and molecular geometry optimization. The second class of problems is the optimization of systems where the performance can only be observed corrupted by random noise. Such situations arise, for example, when the performance of the system being studied can only be estimated using a stochastic discrete-event simulation. In both cases the complexity bounds will indicate if a problem is tractable and provide guidance on how near to optimal a given algorithm is.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Calvin",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "James M Calvin",
   "pi_email_addr": "calvin@njit.edu",
   "nsf_id": "000425803",
   "pi_start_date": "2008-06-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New Jersey Institute of Technology",
  "inst_street_address": "323 DR MARTIN LUTHER KING JR BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "NEWARK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "9735965275",
  "inst_zip_code": "071021824",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NJ10",
  "org_lgl_bus_name": "NEW JERSEY INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "SGBMHQ7VXNH5"
 },
 "perf_inst": {
  "perf_inst_name": "New Jersey Institute of Technology",
  "perf_str_addr": "323 DR MARTIN LUTHER KING JR BLVD",
  "perf_city_name": "NEWARK",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "071021824",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NJ10",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  },
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "9146",
   "pgm_ref_txt": "MANUFACTURING BASE RESEARCH"
  },
  {
   "pgm_ref_code": "9147",
   "pgm_ref_txt": "GENERIC TECHNOL FOR MANUFACTURING CELLS"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "MANU",
   "pgm_ref_txt": "MANUFACTURING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 260000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 5849.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The problem of finding parameter values that minimize a cost function occurs frequently in science and engineering. Often the cost function is not available in a convenient closed form that can be readily optimized, but can only be evaluated by running a simulation or solving a complex system of equations. We give two examples to illustrate the type of problem that motivates this research.</p>\n<p><br />The first example comes from the problem of registering data from multiple sensors; the problem is to combine data from two or more sensors in a consistent way. For example, consider the problem of combining the information from two satellite images taken at different times or from different locations of the same or overlapping regions. It is necessary to determine the translation and rotation that most closely aligns one image with the other. An optimization method optimizes the alignment over all allowed transformations. The &nbsp;figure plots a typical cost function for an image registration problem with two translation parameters. Note the large number of local minima. Similar global optimization problems arise in medical image processing and computer vision.</p>\n<p><br />For the second example we consider optimization when the cost function value can only be estimated with some random error. For example, if one wants to minimize the long-run average cost of operating a complicated stochastic system then a common approach is to construct a computer model of the system and simulate it. Simulation can provide performance estimates that have random error that can often be characterized. Using such noise-corrupted function values to optimize the system is particularly challenging. Any special structure of the average cost function is at best difficult to exploit when exact function values are not available.</p>\n<p>There were two main outcomes of this project.</p>\n<p>First, a number of global optimization algorithms were developed that can approximate the minimum of continuous functions using only function values at sequentially chosen points. Different algorithms were developed depending on whether the exact function value is available or only a noise-corrupted approximation of the function value. For all algorithms, the convergence rate was established, so that the user can determine how much work is required to obtain an approximation with a prescribed accuracy.</p>\n<p>The second class of outcomes were complexity bounds for optimization. Upper bounds on complexity are provided by the convergence rates of the algorithms that were developed. These show how fast we can approximate the optimum with known methods. Lower complexity bounds show that it is impossible to construct an algorithm that approximates the optimum faster than the lower bound. This bound applies to any algorithm, existing algorithms or ones that might be developed in the future.<br /><br /><br /><br /><br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/01/2013<br>\n\t\t\t\t\tModified by: James&nbsp;M&nbsp;Calvin</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2013/0825381/0825381_10079505_1364819180542_mountainCost--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2013/0825381/0825381_10079505_1364819180542_mountainCost--rgov-800width.jpg\" title=\"Multimodal cost function\"><img src=\"/por/images/Reports/POR/2013/0825381/0825381_10079505_1364819180542_mountainCost--rgov-66x44.jpg\" alt=\"Multimodal cost function\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption...",
  "por_txt_cntn": "\nThe problem of finding parameter values that minimize a cost function occurs frequently in science and engineering. Often the cost function is not available in a convenient closed form that can be readily optimized, but can only be evaluated by running a simulation or solving a complex system of equations. We give two examples to illustrate the type of problem that motivates this research.\n\n\nThe first example comes from the problem of registering data from multiple sensors; the problem is to combine data from two or more sensors in a consistent way. For example, consider the problem of combining the information from two satellite images taken at different times or from different locations of the same or overlapping regions. It is necessary to determine the translation and rotation that most closely aligns one image with the other. An optimization method optimizes the alignment over all allowed transformations. The  figure plots a typical cost function for an image registration problem with two translation parameters. Note the large number of local minima. Similar global optimization problems arise in medical image processing and computer vision.\n\n\nFor the second example we consider optimization when the cost function value can only be estimated with some random error. For example, if one wants to minimize the long-run average cost of operating a complicated stochastic system then a common approach is to construct a computer model of the system and simulate it. Simulation can provide performance estimates that have random error that can often be characterized. Using such noise-corrupted function values to optimize the system is particularly challenging. Any special structure of the average cost function is at best difficult to exploit when exact function values are not available.\n\nThere were two main outcomes of this project.\n\nFirst, a number of global optimization algorithms were developed that can approximate the minimum of continuous functions using only function values at sequentially chosen points. Different algorithms were developed depending on whether the exact function value is available or only a noise-corrupted approximation of the function value. For all algorithms, the convergence rate was established, so that the user can determine how much work is required to obtain an approximation with a prescribed accuracy.\n\nThe second class of outcomes were complexity bounds for optimization. Upper bounds on complexity are provided by the convergence rates of the algorithms that were developed. These show how fast we can approximate the optimum with known methods. Lower complexity bounds show that it is impossible to construct an algorithm that approximates the optimum faster than the lower bound. This bound applies to any algorithm, existing algorithms or ones that might be developed in the future.\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 04/01/2013\n\n\t\t\t\t\tSubmitted by: James M Calvin"
 }
}