{
 "awd_id": "0748413",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:  Neural Basis of the Perception of Motion through Depth",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "alumit ishai",
 "awd_eff_date": "2008-05-15",
 "awd_exp_date": "2014-04-30",
 "tot_intn_awd_amt": 550000.0,
 "awd_amount": 550000.0,
 "awd_min_amd_letter_date": "2008-05-07",
 "awd_max_amd_letter_date": "2012-02-23",
 "awd_abstract_narration": "Objects move through three dimensions, and the accurate perception of motion through depth is a key element underlying many human behaviors. From everyday activities like driving a car or shaking hands, to specialized skills like performing surgery or hitting a baseball, seeing the 3D trajectory of a moving object is a critical and central perceptual capacity. Although much research has focused on how the brain processes motion on flat (2D) surfaces, there is surprisingly little knowledge regarding how cues to depth are combined with motion signals to represent 3D motions. The goals of this project are therefore to identify and characterize the neural mechanisms involved in representing 3D direction of motion. Because a human's eyes are horizontally offset within the head, the two eyes view the visual world with some slight difference. The visual system must exploit the dynamic pattern of differences between the two eye's views to extract the direction of 3D motion. With support from a National Science Foundation CAREER award, Dr. Alex Huk and colleagues at the University of Texas at Austin will perform a series of behavioral experiments to identify which pieces of binocular information are used to represent 3D motion. A series of functional magnetic resonance imaging experiments will then use the same experimental displays to identify the resulting signals in relevant parts of the human brain, including primary visual cortex, the middle temporal area (an area known to process 2D motion), and subregions within the posterior parietal lobe. To more directly link perceptual experiences and brain activity, measurements of perceptual sensitivity to particular forms of 3D motion will then be quantitatively compared to measurements of neural sensitivity to these same motions.\r\n\r\nThese studies will provide a thorough characterization of how the brain processes visual motion in realistic environments, extending the careful behavioral and neural studies of 2D motion and static depth processing to a dynamic 3D world. The results will not only facilitate the integration and extension of current understanding of model subsystems within the visual cortex, but will more generally characterize some of the ways by which the nervous system represents information that is fundamentally complex and multidimensional. Likewise, this work may enable the development of 3D visual display technologies that are better suited to human visual capabilities. The CAREER award will support the training of undergraduate, graduate, and postdoctoral researchers, both in the classroom and the laboratory. It will also facilitate the development of compelling visual demonstrations at the heart of educational outreach efforts in high schools in both urban and rural areas around Austin.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Huk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander Huk",
   "pi_email_addr": "alexhuk@ucla.edu",
   "nsf_id": "000194473",
   "pi_start_date": "2008-05-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "110 INNER CAMPUS DR",
  "perf_city_name": "AUSTIN",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121139",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 120000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 80000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 115000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 115000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 120000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">Our research activities have focused on intertwined behavioral (psychophysics) and neuroimaging (fMRI) experiments in humans as they view dynamic 3D visual displays. We discovered that people can discriminate the direction that objects move through depth (i.e., towards or away from the observer) even under conditions for which they cannot discriminate the position in depth of the very same object. This dissociation between (impaired) position-in-depth perception and (unimpaired) movement-through-depth perception suggests that 3D motion perception does not depend exclusively on estimating the depth of an object and then figuring out how that depth changes over time: instead, the visual system appears to do a more direct comparison between the velocities as seen by the two eyes. Because the two eyes are horizontally offset, they see slightly different versions of the visual scene. This can be witnessed by taking your thumb and moving it directly towards and away from your nose: as it approaches the face, your right eye &ldquo;sees&rdquo; leftward motion and your left eye &ldquo;sees&rdquo; rightward motion (and vice versa for receding motion). We studied this eye/velocity motion information and how it flows through the brain, and found that it is processed by a brain region in the visual system (named &ldquo;Area MT&rdquo;) which was long believed to process only frontoparallel motion (up/down/left/right) and position-in-depth.&nbsp;</span></p>\n<p class=\"p2\"><span class=\"s1\">&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">This work has also guided the development of visual display demos for teaching and outreach. We have developed state-of-the-art animations for our public presentations, and host this content on our own YouTube channel.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/21/2014<br>\n\t\t\t\t\tModified by: Alexander&nbsp;Huk</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Our research activities have focused on intertwined behavioral (psychophysics) and neuroimaging (fMRI) experiments in humans as they view dynamic 3D visual displays. We discovered that people can discriminate the direction that objects move through depth (i.e., towards or away from the observer) even under conditions for which they cannot discriminate the position in depth of the very same object. This dissociation between (impaired) position-in-depth perception and (unimpaired) movement-through-depth perception suggests that 3D motion perception does not depend exclusively on estimating the depth of an object and then figuring out how that depth changes over time: instead, the visual system appears to do a more direct comparison between the velocities as seen by the two eyes. Because the two eyes are horizontally offset, they see slightly different versions of the visual scene. This can be witnessed by taking your thumb and moving it directly towards and away from your nose: as it approaches the face, your right eye \"sees\" leftward motion and your left eye \"sees\" rightward motion (and vice versa for receding motion). We studied this eye/velocity motion information and how it flows through the brain, and found that it is processed by a brain region in the visual system (named \"Area MT\") which was long believed to process only frontoparallel motion (up/down/left/right) and position-in-depth. \n \nThis work has also guided the development of visual display demos for teaching and outreach. We have developed state-of-the-art animations for our public presentations, and host this content on our own YouTube channel. \n\n \n\n\t\t\t\t\tLast Modified: 05/21/2014\n\n\t\t\t\t\tSubmitted by: Alexander Huk"
 }
}