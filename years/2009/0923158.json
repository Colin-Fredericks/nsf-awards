{
 "awd_id": "0923158",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Development of a Gesture Based Virtual Reality System for Research in Virtual Worlds",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2009-07-15",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 782039.0,
 "awd_amount": 932000.0,
 "awd_min_amd_letter_date": "2009-08-31",
 "awd_max_amd_letter_date": "2014-07-17",
 "awd_abstract_narration": "Proposal #:\tCNS 09-23158\r\nPI(s):\t\tIlies, Horea T.; \r\n\t\tAnderson, Amy; Kazerounian, Kazem; Marsh, Kerry L.; Nowak, Kristine\r\nInstitution:\tUniversity of Connecticut\r\nTitle:  \t\tMRI/Dev.: Dev. of a Gesture Based Virtual Reality System for Research in Virtual Worlds\r\nProject Proposed:\r\nThis project, developing an integrated virtual environment system capable of allowing not only 3D visualization of data, but also interaction with data through natural hand and finger gestured based on a dual interface, exploits a multi-touch interaction interface and a vision based hand-gesture interface. Virtual reality environments rely on a collection of technologies that allow the user to go through a coherent and unified perceptual experience involving multiple senses, such as vision, touch, and sound, while interacting with 3-dimensional data. These immersive, highly visual, 3D environments currently offer a fairly high level of performance for spatially visualizing data. However, the corresponding machinery providing user interaction with these systems has not kept up the same pace of development with the visualization tools. At present some advanced commercial environments offer some user interaction capabilities achieved through wired wearable hardware (such as wired gloves and head mounted displays). This promotes, in turn, an unnatural and cumbersome interaction between the user and the virtual reality environments, curbing the acceptance of the technologies. The syntax and semantics of the hand and finger gestures developed interacts with geometric data, while the implementation relies on the Multi-Touch Surface computing platform as well as on a newly developed gesture tracking and recognition system. The environment should lead to a potent open platform for interacting with virtual geometric data in an intuitive way, without the need for wearable hardware galvanizing the state of the art at the institution in Nano and Design Engineering, Psychology, Computer Science, Structural biology, as well as support research in the Center for Health, Intervention, and Prevention (CHIP). This project addresses the problem.\r\nBroader Impacts: \r\nThis instrumentation will be open source and widely available with well documented set up procedures. The VR system will be networked with other VR-sites, including VRAC at Iowa State, to maximize the impact and stimulate technology transfer. Moreover, the instrument contributes to\r\n-\tStimulate critical avenues of interdisciplinary research involving engineering, biology, computer science,  \r\npsychology, and Human Computer Interaction (HCI),\r\n-\tStrengthen the potential for educational, student recruiting, and outreach activities, and\r\n-\tPerform targeted outreach to K-12 students, teachers, and school districts serving groups that have\r\n traditionally been underrepresented in the engineering disciplines. \r\nThe project also advances the state of the art in the teaching and practice of engineering design, as well as other fields in which geometry plays and important role. Contributing to the development of a new generation of professionals that use capabilities of virtual reality tools to augment traditional disciplines for improved engineering design, this work should have a long-lasting impact on the ability of scientists and engineers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Horea",
   "pi_last_name": "Ilies",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Horea T Ilies",
   "pi_email_addr": "horea.ilies@uconn.edu",
   "nsf_id": "000330248",
   "pi_start_date": "2009-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kazem",
   "pi_last_name": "Kazerounian",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kazem Kazerounian",
   "pi_email_addr": "kazem@engr.uconn.edu",
   "nsf_id": "000295822",
   "pi_start_date": "2009-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Kerry",
   "pi_last_name": "Marsh",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Kerry L Marsh",
   "pi_email_addr": "Kerry.L.Marsh@uconn.edu",
   "nsf_id": "000352941",
   "pi_start_date": "2009-08-31",
   "pi_end_date": "2014-07-17"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kristine",
   "pi_last_name": "Nowak",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Kristine L Nowak",
   "pi_email_addr": "kristine.nowak@uconn.edu",
   "nsf_id": "000438536",
   "pi_start_date": "2009-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Amy",
   "pi_last_name": "Anderson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Amy Anderson",
   "pi_email_addr": "amy.anderson@uconn.edu",
   "nsf_id": "000093410",
   "pi_start_date": "2009-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Connecticut",
  "inst_street_address": "438 WHITNEY RD EXTENSION UNIT 1133",
  "inst_street_address_2": "",
  "inst_city_name": "STORRS",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "8604863622",
  "inst_zip_code": "062699018",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CT02",
  "org_lgl_bus_name": "UNIVERSITY OF CONNECTICUT",
  "org_prnt_uei_num": "",
  "org_uei_num": "WNTPS995QBM7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Connecticut",
  "perf_str_addr": "438 WHITNEY RD EXTENSION UNIT 1133",
  "perf_city_name": "STORRS",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "062699018",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CT02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "768400",
   "pgm_ele_name": "CESER-Cyberinfrastructure for"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 782039.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 149961.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><pre><pre><!--StartFragment-->Human interaction with spatial data in virtual environments is a ubiquitous task&nbsp;</pre>\n<pre>in many fields of science and engineering and is becoming a popular paradigm&nbsp;</pre>\n<pre>in consumer industries. New interaction paradigms that allow users to employ</pre>\n<pre>the full manipulative capabilities of their natural hand gestures hold the promise&nbsp;</pre>\n<pre>to change the way we interact with everything from computers to robots to&nbsp;</pre>\n<pre>consumer products.</pre>\n<pre><br /></pre>\n<pre>This research produced the theoretical foundations and computational platform for&nbsp;</pre>\n<pre>inferring user's intentions for 3D data manipulation directly from natural hand&nbsp;</pre>\n<pre>gestures. The key idea is the use characteristic behavioral cues from&nbsp;</pre>\n<pre>neuropsychology to infer the manipulative gestures in real time. It was&nbsp;</pre>\n<pre>demonstrated through user studies that, by coupling characteristic behavioral&nbsp;</pre>\n<pre>cues with 3D imaging-based body tracking techniques, one can robustly infer&nbsp;</pre>\n<pre>the manipulative intentions of the user from natural hand gestures in a&nbsp;</pre>\n<pre>workspace larger than the span of the human arms. Furthermore, it was&nbsp;</pre>\n<pre>demonstrated that the resulting methods are robust against variability in</pre>\n<pre>gestures and tracking uncertainties.&nbsp;</pre>\n<pre><br /></pre>\n<pre><br /></pre>\n<pre>The results of this research lead to a deeper understanding of the fundamental&nbsp;</pre>\n<pre>problem as well as its challenges and opportunities, and are poised to extend&nbsp;</pre>\n<pre>the practicality and usability of natural hand gestures for human interaction&nbsp;</pre>\n<pre>with spatial data in virtual environments.</pre>\n<br /></pre><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/13/2015<br>\n\t\t\t\t\tModified by: Horea&nbsp;T&nbsp;Ilies</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Human interaction with spatial data in virtual environments is a ubiquitous task \nin many fields of science and engineering and is becoming a popular paradigm \nin consumer industries. New interaction paradigms that allow users to employ\nthe full manipulative capabilities of their natural hand gestures hold the promise \nto change the way we interact with everything from computers to robots to \nconsumer products.\n\n\nThis research produced the theoretical foundations and computational platform for \ninferring user's intentions for 3D data manipulation directly from natural hand \ngestures. The key idea is the use characteristic behavioral cues from \nneuropsychology to infer the manipulative gestures in real time. It was \ndemonstrated through user studies that, by coupling characteristic behavioral \ncues with 3D imaging-based body tracking techniques, one can robustly infer \nthe manipulative intentions of the user from natural hand gestures in a \nworkspace larger than the span of the human arms. Furthermore, it was \ndemonstrated that the resulting methods are robust against variability in\ngestures and tracking uncertainties. \n\n\n\n\nThe results of this research lead to a deeper understanding of the fundamental \nproblem as well as its challenges and opportunities, and are poised to extend \nthe practicality and usability of natural hand gestures for human interaction \nwith spatial data in virtual environments.\n\n\n\n\t\t\t\t\tLast Modified: 10/13/2015\n\n\t\t\t\t\tSubmitted by: Horea T Ilies"
 }
}