{
 "awd_id": "0931805",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CPS:Medium:Hybrid Systems for Modeling and Teaching the Language of Surgery",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928950",
 "po_email": "rwachter@nsf.gov",
 "po_sign_block_name": "Ralph Wachter",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2013-12-31",
 "tot_intn_awd_amt": 1499828.0,
 "awd_amount": 1515578.0,
 "awd_min_amd_letter_date": "2009-09-23",
 "awd_max_amd_letter_date": "2011-08-24",
 "awd_abstract_narration": "The objective of this research is to develop new principles for\r\ncreating and comparing models of skilled human activities, and to\r\napply those models to systems for teaching, training and assistance of\r\nhumans performing these activities. The models investigated will\r\ninclude both hybrid systems and language-based models. The research\r\nwill focus on modeling surgical manipulations during robotic minimally\r\ninvasive surgery. Models for expert performance of surgical tasks will\r\nbe derived from recorded motion and video data. Student data will be\r\ncompared with these expert models, and both physical guidance and\r\ninformation display methods will be developed to provide feedback to\r\nthe student based on the expert model.\r\n\r\nThe intellectual merit of this work lies in the development of a new\r\nset of mathematical tools for modeling human skilled activity. These\r\ntools will provide new insights into the relationship between skill,\r\nstyle, and content in human motion. Additional intellectual merit lies\r\nin the connection of hybrid systems modeling to language models, the\r\ncreation of techniques for automated training, and in the assessment\r\nof new training methods.\r\n\r\nThe broader impact of this research will be the creation of automated\r\nmethods for modeling and teaching skilled human motion. These methods\r\nwill have enormous implications for the training and re-training of\r\nthe US workforce.  This project will also impact many diversity and\r\noutreach activities, including REU programs and summer camps for K-12\r\noutreach. The senior personnel of this project also participate in the\r\nRobotic Systems Challenge and the Women in Science and Engineering\r\nprogram.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Hager",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Hager",
   "pi_email_addr": "hager@cs.jhu.edu",
   "nsf_id": "000385453",
   "pi_start_date": "2009-09-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sanjeev",
   "pi_last_name": "Khudanpur",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Sanjeev P Khudanpur",
   "pi_email_addr": "khudanpur@jhu.edu",
   "nsf_id": "000236251",
   "pi_start_date": "2009-09-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rene",
   "pi_last_name": "Vidal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rene Vidal",
   "pi_email_addr": "vidalr@upenn.edu",
   "nsf_id": "000486258",
   "pi_start_date": "2009-09-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rajesh",
   "pi_last_name": "Kumar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rajesh Kumar",
   "pi_email_addr": "rajesh@jhu.edu",
   "nsf_id": "000083560",
   "pi_start_date": "2009-09-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N CHARLES ST",
  "perf_city_name": "BALTIMORE",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 499841.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 515750.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 499987.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigated methods for modeling the performance of skilled, complex manipulation tasks using hand movement and video data. For example, modeling surgical tasks by decomposing them into sequences of surgical gestures (which we call surgemes) while assessing the skill level of a surgeon at performing those tasks. The key idea is to first build language-like models that describe the hand movements necessary to perform a skilled task, followed by analysis of the hand movements to determine if there are structures that are indicative of skilled movement. The same ideas can be applied to many different types of movement ranging from dexterous manipulation to whole body motion.</p>\n<p>We have focused specifically on data that we have acquired from a <em>da Vinci</em> Surgical System (Intuitive Surgical, Inc., Sunnyvale, CA) while surgeons with differing levels of training and experience performed a series of standardized training tasks. With this data, we have developed and evaluated a number of models and implemented a number of applications.</p>\n<p>With regards to modeling, we began with traditional hidden Markov models (HMMs), which are widely used in speech and language processing by machines. We adapted and improved these models for our domain of interest in several ways. Our most successful methods were based on adapting the notion of dictionary learning for these models. A dictionary is a small set of typical mini-movements that are highly descriptive of all of the observed hand movements. We represent the actual data in terms of the dictionary. We then learn the HMMs from these dictionary-based representations. We also extended these modeling methods to perform joint learning of the dictionary and the HMM simultaneously.&nbsp; We showed that these methods, which we call Sparse HMMs (S-HMMs), outperformed all prior methods in terms of recognition accuracy across a variety of surgical tasks including suturing, needle passing, and knot tying.</p>\n<p>We then applied the models that we developed to skill classification tasks. We did so by asking an expert to provide a &ldquo;ground truth&rdquo; label for the execution of a given task on a numeric scale, with 1 being poor, and 5 being excellent. We then disaggregated the data into three groups which we called novice, intermediate, and expert, and we learned a model, as described above, for each group. We then classified held-out data by determining which of the three models best described the held out data. We found that when the system had a chance to observe the surgeon in question during training, it was able to classify his or her skill with 94-97% accuracy, but when it did not, the accuracy dropped to 40-50%. This suggests that the classifier associates high skill with the surgical styles of the particular surgeons it has seen before, and mislabels a skilled movement performed in a different style as unskilled.&nbsp; This problem is most easily cured by acquiring larger data sets that allow the classifier to see a wider range of individual variability.</p>\n<p>We have also used time-series data models to develop new modes of training and assistance. for performing surgical tasks.&nbsp; In one study, we used the data to learn a HMM for the task, as described above. We then annotated the states of the HMM to indicate what portions of the task an operator would perform manually, versus portions of the task that the robot would perform automatically. For the automatic portions, the training data was averaged to produce a nominal trajectory for the robot. During execution, the robot observed the actions of the operator and when it recognized that it was transitioning to an automated action, took control and carried out the action. As a result, the operator was able to perform the task more efficiently, but with the same fidelity, through collaboration with the robot.</p>\n<p>In the course of this project, w...",
  "por_txt_cntn": "\nThis project investigated methods for modeling the performance of skilled, complex manipulation tasks using hand movement and video data. For example, modeling surgical tasks by decomposing them into sequences of surgical gestures (which we call surgemes) while assessing the skill level of a surgeon at performing those tasks. The key idea is to first build language-like models that describe the hand movements necessary to perform a skilled task, followed by analysis of the hand movements to determine if there are structures that are indicative of skilled movement. The same ideas can be applied to many different types of movement ranging from dexterous manipulation to whole body motion.\n\nWe have focused specifically on data that we have acquired from a da Vinci Surgical System (Intuitive Surgical, Inc., Sunnyvale, CA) while surgeons with differing levels of training and experience performed a series of standardized training tasks. With this data, we have developed and evaluated a number of models and implemented a number of applications.\n\nWith regards to modeling, we began with traditional hidden Markov models (HMMs), which are widely used in speech and language processing by machines. We adapted and improved these models for our domain of interest in several ways. Our most successful methods were based on adapting the notion of dictionary learning for these models. A dictionary is a small set of typical mini-movements that are highly descriptive of all of the observed hand movements. We represent the actual data in terms of the dictionary. We then learn the HMMs from these dictionary-based representations. We also extended these modeling methods to perform joint learning of the dictionary and the HMM simultaneously.  We showed that these methods, which we call Sparse HMMs (S-HMMs), outperformed all prior methods in terms of recognition accuracy across a variety of surgical tasks including suturing, needle passing, and knot tying.\n\nWe then applied the models that we developed to skill classification tasks. We did so by asking an expert to provide a \"ground truth\" label for the execution of a given task on a numeric scale, with 1 being poor, and 5 being excellent. We then disaggregated the data into three groups which we called novice, intermediate, and expert, and we learned a model, as described above, for each group. We then classified held-out data by determining which of the three models best described the held out data. We found that when the system had a chance to observe the surgeon in question during training, it was able to classify his or her skill with 94-97% accuracy, but when it did not, the accuracy dropped to 40-50%. This suggests that the classifier associates high skill with the surgical styles of the particular surgeons it has seen before, and mislabels a skilled movement performed in a different style as unskilled.  This problem is most easily cured by acquiring larger data sets that allow the classifier to see a wider range of individual variability.\n\nWe have also used time-series data models to develop new modes of training and assistance. for performing surgical tasks.  In one study, we used the data to learn a HMM for the task, as described above. We then annotated the states of the HMM to indicate what portions of the task an operator would perform manually, versus portions of the task that the robot would perform automatically. For the automatic portions, the training data was averaged to produce a nominal trajectory for the robot. During execution, the robot observed the actions of the operator and when it recognized that it was transitioning to an automated action, took control and carried out the action. As a result, the operator was able to perform the task more efficiently, but with the same fidelity, through collaboration with the robot.\n\nIn the course of this project, we have trained four graduate students and two postdoctoral scholars. We have also engaged several undergraduates in our research..."
 }
}