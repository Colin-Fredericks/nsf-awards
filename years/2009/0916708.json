{
 "awd_id": "0916708",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Theoretical Foundations of Evolving Knowledge Bases",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balasubramanian Kalyanasundaram",
 "awd_eff_date": "2009-09-15",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 498589.0,
 "awd_amount": 514589.0,
 "awd_min_amd_letter_date": "2009-09-10",
 "awd_max_amd_letter_date": "2010-07-07",
 "awd_abstract_narration": "This project studies evolving knowledge bases, using tools from theoretical computer science.  As opposed to a database containing facts that can be queried, a knowledge base contains general statements that can be used to derive further implications. Developing a knowledge base, in particular, a knowledge base containing commonsense knowledge that can be used for commonsense reasoning, is a fundamental task of artificial intelligence (AI). This task is taking on a somewhat different focus nowadays, as even partial solutions would have important applications in intelligent agent technology.  One common feature of current approaches to the development of commonsense knowledge bases is the interactive acquisition of web-based user input of knowledge.\r\n\r\nAlgorithms for developing commonsense knowledge bases have to perform several different tasks, such as reasoning, revising (i.e., updating the current knowledge in the presence of new, potentially conflicting information), and learning (i.e., improving the quality of the knowledge base over the long run). This research addresses all three of these areas.\r\n\r\nThe bulk of the research uses Horn formulas as the formalism for knowledge representation. Horn formulas are an important class of logical expressions that have been studied for decades in complexity theory, logic programming, databases, and AI, with efficient algorithms for basic reasoning tasks.  Evolving knowledge bases present many new problems for Horn knowledge bases. Particular problems addressed by this research, based on the challenges referred to above, include: Horn-to-Horn belief revision, learning Horn formulas in the model of learning from entailment, approximate minimization of Horn formulas, probabilistic analysis of the set of consequences of random Horn formulas and related combinatorial problems, Horn approximation of knowledge bases and complexity problems for non-classical generalizations of Horn formulas. The latter point towards extending the knowledge representation formalism to handle different aspects of commonsense reasoning.\r\n\r\nIn more general terms, the objective of the research is to develop algorithms for building knowledge bases that can evolve over time. This task is relevant for both short and long term applications. The research contains a comprehensive approach to several important areas which so far have been mostly been studied separately. In the long term, research in this area will contribute to the development of tools for building better intelligent agents possessing commonsense knowledge and capable of commonsense reasoning. Such agents will expand the scope of and improve the quality of automated services.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gyorgy",
   "pi_last_name": "Turan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gyorgy Turan",
   "pi_email_addr": "gyt@uic.edu",
   "nsf_id": "000296459",
   "pi_start_date": "2009-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Sloan",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Robert H Sloan",
   "pi_email_addr": "sloan@uic.edu",
   "nsf_id": "000167339",
   "pi_start_date": "2009-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Chicago",
  "inst_street_address": "809 S MARSHFIELD AVE M/C 551",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3129962862",
  "inst_zip_code": "606124305",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "W8XEAJDKMXH3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Chicago",
  "perf_str_addr": "809 S MARSHFIELD AVE M/C 551",
  "perf_city_name": "CHICAGO",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606124305",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "792600",
   "pgm_ele_name": "ALGORITHMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 498589.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of the overall project was to contribute to the mathematical foundations of the subject of <em>knowledge bases, </em>which are collections of facts and/or rules that are used as part of many intelligent automated systems. Knowledge bases range in size from medium-large, such as the collection of facts that might underlie an expert system concerned with one specific medical problem, to truly huge, such as the collection of facts that IBM gathered for the <em>Jeopardy</em>-playing Watson system.</p>\n<p>Two particular issues the project was concerned with were:</p>\n<ol>\n<li>&nbsp;Explore the current capabilities of knowledge bases for so-called commonsense knowledge and reasoning, generally defined as things that relatively young children know and can do, but that computers often have trouble with. This exploration is necessary for gauging progress achieved so far and for identifying problems where theoretical contributions are needed.</li>\n<li>Study how knowledge bases should best be updated as available information changes over time. This is one of the basic tasks of knowledge base development, which has been studied in related contexts for a long time, and raises many interesting problems for theory and applications.</li>\n</ol>\n<p>&nbsp;</p>\n<p>In terms of sheer numbers, results of this project have so far been published in 4 journal articles, 15 conference papers, 3 Ph.D. dissertations, 1 Masters thesis, and 3 reports in arXiv. At the time of writing this Outcomes Report, some additional publications are still under review and in preparation.</p>\n<p>The most interesting and important results on commonsense knowledge bases were obtained by studying an available large open-source knowledge base called ConceptNet from MIT. We measured its usefulness for commonsense reasoning by administering the verbal half of the standard IQ test for young children (the WPPSI-III) to this knowledge base. ConceptNet&rsquo;s overall score was about average for a four-year-old child, but its subscores were extremely uneven. The unevenness in subscores suggests the areas of knowledge bases and artificial intelligence where the most work is needed.</p>\n<p>&nbsp;We also used the tools of large network analysis to examine this knowledge base considered as a very large network or graph.&nbsp; This mathematical method has become popular in the past ten to fifteen years for studying networks such as the Web or various biological systems and it also stimulated important work in theoretical computer science. Our initial findings suggest that tools from network analysis may inform commonsense knowledge bases and reasoning.</p>\n<p>The most interesting and important results on updating knowledge bases fall under the heading of &ldquo;belief revision,&rdquo; the updating of knowledge bases after contradictory new information is received, following a set of rules for rational updates, such as making sure that new information is indeed accounted for in an update.&nbsp; This subject has been studied since the 1980s, but with a focus on philosophical logic. Previous computational complexity results on belief revision were mostly negative, often showing that the corresponding computational tasks are even harder than satisfiability problems.</p>\n<p>In this project, we developed results on the subject of belief revision restricted to Horn logic, which is a fragment of full propositional logic that is used in practice as a knowledge base framework, because deriving conclusions from general full logic is computationally intractable, whereas deriving conclusions from Horn logic is computationally efficient. Thus efficient algorithms for Horn belief change could be useful tools for knowledge base development. Understanding the possibilities for the development of such algorithms is an ongoing topic of current research.</p>\n<p>Besides being a basic framework for representing knowledge, Horn logic has ...",
  "por_txt_cntn": "\nThe goal of the overall project was to contribute to the mathematical foundations of the subject of knowledge bases, which are collections of facts and/or rules that are used as part of many intelligent automated systems. Knowledge bases range in size from medium-large, such as the collection of facts that might underlie an expert system concerned with one specific medical problem, to truly huge, such as the collection of facts that IBM gathered for the Jeopardy-playing Watson system.\n\nTwo particular issues the project was concerned with were:\n\n Explore the current capabilities of knowledge bases for so-called commonsense knowledge and reasoning, generally defined as things that relatively young children know and can do, but that computers often have trouble with. This exploration is necessary for gauging progress achieved so far and for identifying problems where theoretical contributions are needed.\nStudy how knowledge bases should best be updated as available information changes over time. This is one of the basic tasks of knowledge base development, which has been studied in related contexts for a long time, and raises many interesting problems for theory and applications.\n\n\n \n\nIn terms of sheer numbers, results of this project have so far been published in 4 journal articles, 15 conference papers, 3 Ph.D. dissertations, 1 Masters thesis, and 3 reports in arXiv. At the time of writing this Outcomes Report, some additional publications are still under review and in preparation.\n\nThe most interesting and important results on commonsense knowledge bases were obtained by studying an available large open-source knowledge base called ConceptNet from MIT. We measured its usefulness for commonsense reasoning by administering the verbal half of the standard IQ test for young children (the WPPSI-III) to this knowledge base. ConceptNet\u00c6s overall score was about average for a four-year-old child, but its subscores were extremely uneven. The unevenness in subscores suggests the areas of knowledge bases and artificial intelligence where the most work is needed.\n\n We also used the tools of large network analysis to examine this knowledge base considered as a very large network or graph.  This mathematical method has become popular in the past ten to fifteen years for studying networks such as the Web or various biological systems and it also stimulated important work in theoretical computer science. Our initial findings suggest that tools from network analysis may inform commonsense knowledge bases and reasoning.\n\nThe most interesting and important results on updating knowledge bases fall under the heading of \"belief revision,\" the updating of knowledge bases after contradictory new information is received, following a set of rules for rational updates, such as making sure that new information is indeed accounted for in an update.  This subject has been studied since the 1980s, but with a focus on philosophical logic. Previous computational complexity results on belief revision were mostly negative, often showing that the corresponding computational tasks are even harder than satisfiability problems.\n\nIn this project, we developed results on the subject of belief revision restricted to Horn logic, which is a fragment of full propositional logic that is used in practice as a knowledge base framework, because deriving conclusions from general full logic is computationally intractable, whereas deriving conclusions from Horn logic is computationally efficient. Thus efficient algorithms for Horn belief change could be useful tools for knowledge base development. Understanding the possibilities for the development of such algorithms is an ongoing topic of current research.\n\nBesides being a basic framework for representing knowledge, Horn logic has close connections to several other important research areas such as combinatorics and algebra. We continued our study of Horn logic by working on several aspects related to the optimization and evol..."
 }
}