{
 "awd_id": "0923494",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Development of a Next-Generation Multimodal Data Management Human-Sensing Instrument for Trustworthy Research Collaboration and Quality of Life Improvement",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2009-10-01",
 "awd_exp_date": "2013-09-30",
 "tot_intn_awd_amt": 722622.0,
 "awd_amount": 770622.0,
 "awd_min_amd_letter_date": "2009-09-29",
 "awd_max_amd_letter_date": "2013-04-15",
 "awd_abstract_narration": "Proposal #:\tCNS 09-23494\t\t\t\t\r\nPI(s):\t\tMakedon, Fillia S.; Athitsos, Vassilis; Huang, Heng; Le, Zhengyi; Popa, Dan O.\r\nInstitution:\tUniversity of Texas - Arlington\r\nTitle: MRI/Dev.: Next Generation Multimodal Data Management Human-Sensing Instrument for \r\n                       Trustworthy Research Collaboration and Quality of Life Improvement\r\nProject Proposed:\r\nThis project, developing an instrument that serves as an interactive personal care and human activity monitoring center, aims to keep a person with high quality life and safe at home as long as possible. The instrument enables privacy-preserving and secure data sharing through wireless connection with remote users in an assistive living environment. Providing mental and emotional support, the zooscopion (zScope) can connect devices, humans, objects, and the environment. It can connect to other assistive living projects, making them interoperable and can deliver a Digital Library of sanitized research data and cases with high educational and training value. zScope combines and correlates many types of data and extracts events of interest that indicate changes, risks, etc. It can analyze facial expressions to detect pain, environmental data, house data (such as door opening, telephone sounds, vacuum cleaner, etc.), human performance metrics (e.g., hand strength), both in continuous and discrete format. Data are modeled and assembled in meaningful ways to predict and prevent physical and digital problems (e.g., respectively, falls and intrusions). Privacy and security are being made part of the data modeling at the design phase. The instrument will take sensor data, human body measurements, camera data when requested, known pattern of behavior from other cases, brain scans, and clinical information, aiming to provide high resolution displays of longitudinal as well as episodic events. It outputs a visual interactive display of patterns and significant human behavioral 'events' valuable in assistive environments, setting where to use non-invasive monitoring technologies, helping recognize 'behavioral biomarkers' that will be connected to other types of health indicators that may come from brain imaging, genetic analysis, clinical results, or psychological evaluations. It will work with the next generation of data that include behavioral, clinical, body motion, etc., and have low latency tracking. \r\nBroader Impacts: \r\nThis work enables human-centric type of experiments and provides novel new ways of interaction, visualization, and secure collaboration. Developing 'smarter' living environments for the aged opens new ways to education with immersive compelling projects that provide a better understanding of the role of science and engineering when combining health data (genomic information) to behavior, predict trends, and provide indicators of how medication and clinical assessments connect to longitudinal behavior. Long term goals include behavioral markers for assessing the confluence of environment, drugs, and human psychology. The instrument is also expected to respond to queries regarding emerging needs for new analysis of collected information. It includes training and educational modules with search and browsing tools and a recommender facility to support decision making and use stored strategies. Moreover, utilizing existing outreach programs, the project will support local minority students and high school students A new generation of scientists that can work together across domain silos towards human centered goals might be in the making!",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fillia",
   "pi_last_name": "Makedon",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Fillia S Makedon",
   "pi_email_addr": "makedon@cse.uta.edu",
   "nsf_id": "000191699",
   "pi_start_date": "2009-09-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dan",
   "pi_last_name": "Popa",
   "pi_mid_init": "O",
   "pi_sufx_name": "",
   "pi_full_name": "Dan O Popa",
   "pi_email_addr": "dan.popa@louisville.edu",
   "nsf_id": "000085594",
   "pi_start_date": "2009-09-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Heng",
   "pi_last_name": "Huang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Heng Huang",
   "pi_email_addr": "heng@umd.edu",
   "nsf_id": "000086248",
   "pi_start_date": "2009-09-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vassilis",
   "pi_last_name": "Athitsos",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vassilis Athitsos",
   "pi_email_addr": "athitsos@uta.edu",
   "nsf_id": "000308836",
   "pi_start_date": "2009-09-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zhengyi",
   "pi_last_name": "Le",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhengyi Le",
   "pi_email_addr": "zyle@uta.edu",
   "nsf_id": "000502178",
   "pi_start_date": "2009-09-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Arlington",
  "inst_street_address": "701 S NEDDERMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "ARLINGTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "8172722105",
  "inst_zip_code": "760199800",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT ARLINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "LMLUKUPJJ9N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Arlington",
  "perf_str_addr": "701 S NEDDERMAN DR",
  "perf_city_name": "ARLINGTON",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "760199800",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 722622.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>zScope is a one stop human monitoring instrument that collects and analyzes multisensing human activity data to support independent living of the elderly in assistive pervasive environments. The zScope instrument provides multiple views into the life of an elderly person that include eating, sleeping, walking, and other activities. An important focus is to understand and track the needs of a human, provide guidance to his/her caregivers, ensure the safety of the elderly and provide smart services that improve the quality of their lives. The research carried out has produced a big body of publications in many different related scientific areas and has brought great visibility to the challenges involved in applying computer technology to provide in place aging. The project developed a number of novel methods and software and hardware for efficient data collection, analysis, interpretation, implementation and data management.</p>\n<p>As a fist step of the project, methods were developed and experiments were done to optimally place sensors in order to ensure that key areas of daily life activities were covered and monitored. The experiments included the use of both static and mobile sensors and took into consideration reliability concerns, such as sensor failures, in order to improve robustness.</p>\n<p>A wide range of methods and tools were developed that range from supporting indoor localization, fall detection, medication tracking and sleep monitoring, to limb and finger motion tracking to enable at home rehabilitation and gesture recognition. As part of zScope development, tools were designed and implemented to detect events of interest in the monitored environment and to provide alerts and recommendations to the interested parties, such as caregivers, doctors or family members.</p>\n<p>To facilitate interaction with computerized systems, especially for people with disabilities, intelligent interfaces were developed. One type of interface supports robust audio-visual speech recognition and dialog management, while another interface supports as eye-tracking and point-of-gaze detection in 3D space. Both types of interfaces are important assistive technologies for zScope users.</p>\n<p>In addition to passive data collection and analysis, there was development of active, assistive robotic actuators to solve certain needs.&nbsp; For example, tools were developed to support human interaction with human-like robotic companions, which were found to have therapeutic effects when interacting with special populations. Besides the human-like robots, the project also developed robotic platforms, such as smart wheelchairs, that can be used by people with severe disabilities, utilizing alternative inputs such as eye tracking and voice.</p>\n<p>The vast amount of data generated and collected though zScope, require advanced storage, management and querying techniques. Methods were thus developed using state-of-the-art data warehousing, retrieval and analysis. The work published as a result of this project includes methods for multi-modal data fusion, feature selection and dimensionality reduction, searching and matching on time-series data, and robust classification. Subsets of the data collected during the life of the project have also been made available for public use.</p>\n<p>The results and findings of this project constitute a valuable asset on the way towards the commercialization and wider adoption of pervasive technologies related to assisted living. Further work needs to be done in the area of experimentation to identify best ways for adopting the outcomes to real life, using actual human subjects in realistic settings.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/04/2014<br>\n\t\t\t\t\tModified by: Fillia&nbsp;S&nbsp;Makedon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nzScope is a one stop human monitoring instrument that collects and analyzes multisensing human activity data to support independent living of the elderly in assistive pervasive environments. The zScope instrument provides multiple views into the life of an elderly person that include eating, sleeping, walking, and other activities. An important focus is to understand and track the needs of a human, provide guidance to his/her caregivers, ensure the safety of the elderly and provide smart services that improve the quality of their lives. The research carried out has produced a big body of publications in many different related scientific areas and has brought great visibility to the challenges involved in applying computer technology to provide in place aging. The project developed a number of novel methods and software and hardware for efficient data collection, analysis, interpretation, implementation and data management.\n\nAs a fist step of the project, methods were developed and experiments were done to optimally place sensors in order to ensure that key areas of daily life activities were covered and monitored. The experiments included the use of both static and mobile sensors and took into consideration reliability concerns, such as sensor failures, in order to improve robustness.\n\nA wide range of methods and tools were developed that range from supporting indoor localization, fall detection, medication tracking and sleep monitoring, to limb and finger motion tracking to enable at home rehabilitation and gesture recognition. As part of zScope development, tools were designed and implemented to detect events of interest in the monitored environment and to provide alerts and recommendations to the interested parties, such as caregivers, doctors or family members.\n\nTo facilitate interaction with computerized systems, especially for people with disabilities, intelligent interfaces were developed. One type of interface supports robust audio-visual speech recognition and dialog management, while another interface supports as eye-tracking and point-of-gaze detection in 3D space. Both types of interfaces are important assistive technologies for zScope users.\n\nIn addition to passive data collection and analysis, there was development of active, assistive robotic actuators to solve certain needs.  For example, tools were developed to support human interaction with human-like robotic companions, which were found to have therapeutic effects when interacting with special populations. Besides the human-like robots, the project also developed robotic platforms, such as smart wheelchairs, that can be used by people with severe disabilities, utilizing alternative inputs such as eye tracking and voice.\n\nThe vast amount of data generated and collected though zScope, require advanced storage, management and querying techniques. Methods were thus developed using state-of-the-art data warehousing, retrieval and analysis. The work published as a result of this project includes methods for multi-modal data fusion, feature selection and dimensionality reduction, searching and matching on time-series data, and robust classification. Subsets of the data collected during the life of the project have also been made available for public use.\n\nThe results and findings of this project constitute a valuable asset on the way towards the commercialization and wider adoption of pervasive technologies related to assisted living. Further work needs to be done in the area of experimentation to identify best ways for adopting the outcomes to real life, using actual human subjects in realistic settings.\n\n \n\n\t\t\t\t\tLast Modified: 01/04/2014\n\n\t\t\t\t\tSubmitted by: Fillia S Makedon"
 }
}