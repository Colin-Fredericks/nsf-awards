{
 "awd_id": "0916599",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: General Knowledge Bootstrapping from Text",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2009-07-15",
 "awd_exp_date": "2013-06-30",
 "tot_intn_awd_amt": 443535.0,
 "awd_amount": 459435.0,
 "awd_min_amd_letter_date": "2009-07-14",
 "awd_max_amd_letter_date": "2010-08-28",
 "awd_abstract_narration": "The goal of this project is to extend methods of extracting general knowledge from texts, so as to obtain not only simple \"factoids\"\r\nsuch as \"A door can be open\" or \"A person may respond to a question\"\r\n(exemplifying the millions of outputs of the U. Rochester KNEXT system), but also general, conditional knowledge such as that \"If a car crashes into a tree, the driver may be hurt or killed\".\r\nSuch conditional knowledge is crucial for intelligent agents that can understand language and make commonsense inferences. The approach employed in the project involves bootstrapping of two principal sorts: (1) abstraction from simple factoids, both individually and collectively; (2) use of already-derived factoids to boost the performance of a natural language parser/interpreter, enabling\r\n(a) extraction of more complex conditional facts from miscellaneous texts, and (b) direct interpretation of general conditional facts stated in English in sources such as Common Sense Open Mind or WordNet glosses. The evaluation methodology for the derived knowledge involves both direct human judgement and judgement of inferences automatically generated with the aid of the extracted knowledge, using the EPILOG inference engine at U. Rochester.\r\n\r\nThe general knowledge obtained in this work will be made available to the broader AI community, and will advance the state of the art both in natural language understanding and in knowledge-dependent commonsense reasoning (for example, in question answering). It will also provide evidence relevant to the hypothesis that language understanding is a process dependent not only on a few thousand syntactic rules, but also on millions of pattern-like items of general knowledge that bias the parsing and interpretation process.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lenhart",
   "pi_last_name": "Schubert",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Lenhart K Schubert",
   "pi_email_addr": "schubert@cs.rochester.edu",
   "nsf_id": "000235732",
   "pi_start_date": "2009-07-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Carlson",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory N Carlson",
   "pi_email_addr": "carlson@ling.rochester.edu",
   "nsf_id": "000389251",
   "pi_start_date": "2009-07-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Rochester",
  "inst_street_address": "910 GENESEE ST",
  "inst_street_address_2": "STE 200",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5852754031",
  "inst_zip_code": "146113847",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "UNIVERSITY OF ROCHESTER",
  "org_prnt_uei_num": "",
  "org_uei_num": "F27KDXZMF9Y8"
 },
 "perf_inst": {
  "perf_inst_name": "University of Rochester",
  "perf_str_addr": "910 GENESEE ST",
  "perf_city_name": "ROCHESTER",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146113847",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 291155.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 168280.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>It has been recognized in AI since the 1970s that human-level language understanding, commonsense reasoning, and goal-oriented planning in the real world depended on the possession and use of very large amounts of knowledge -- perhaps tens or hundreds of millions of individual (but richly interrelated) items. This raised the question of how this knowledge could be acquired by a machine, in a form suitable for reasoning, and this long-standing challenge has been called the \"knowledge acquisition bottleneck\".<br /><br />The present project extended previous work aimed at alleviating this bottleneck through knowledge acquisition from large text repositories, such as the 100-million word British National Corpus, the more than one billion word \"Gigaword\" newswire corpus, Wikipedia, and personal blogs. That previous work led to the KNEXT knowledge extraction system at the University of Rochester, and a repository of many millions of general \"factoids\" -- simple general claims, expressed in a formal, language-like logic called Episodic Logic; examples are that <em>a person can have a brain</em>, <em>a dog may bark</em>, <em>people may want to be rid of a dictator</em>, and so on. Unfortunately these factoids, though indicative of what the world is like, are too weakly formulated to be usable for inference; for instance, we cannot infer that a given person does have a brain, only that this is a possibility. Thus the present project has been aimed at \"bootstrapping\" KNEXT-like knowledge -- extending it and boosting the logical strength of the factoids to enable inferences.<br /><br />The research accomplished knowledge extensions and strengthening in three primary ways:</p>\n<ol>\n<li>Logical \"sharpening\" of many millions of the original KNEXT factoids, making them suitable for reasoning; this relied on various lexical and software resources, allowing nouns and verbs to be semantically classified in multiple ways, with the help of a new pattern matching and transformation system, TTT, created in the course of this project;</li>\n<li>Derivation of axiomatic knowledge about relationships among nominal concepts (<em>person, dog, creature, tree, plant, computer, artifact,</em> ...) as characterized in the online WordNet lexicon; again this relied on multiple lexical resources and TTT; and</li>\n<li>Direct creation of axioms expressing what is implied by verb concepts such as<em> walking</em>, <em>giving</em> (something to someone), <em>refusing</em> (to so something), <em>believing</em> (something), etc., using as a guide the online VerbNet resource, which contains a systematic classification of verbs.&nbsp;&nbsp; </li>\n</ol>\n<p>These new knowledge items support \"obvious\" inferences such as that <em>John may well occasionally go to a dentist, very likely has teeth as a part</em>, and <em>possibly some of those teeth are occasionally fixed </em>and <em>some may be lost</em> (given only that John is a person); or that <em>IBM may well have products, probably experiences sales, may have headquarters, may have websites, may grow, etc</em>. (given only that IBM is a company). Such inferences are obtainable with the EPILOG inference engine, whose capabilities have also been strengthened in the course of the project.<br /><br />Within the field of artificial intelligence, these results constitute significant progress towards overcoming the knowledge acquisition bottleneck. The work also has implications for linguistics. For example, the effort to axiomatize relations among nominal concepts in WordNet showed the importance of the mass-count distinction in making formal sense of the \"hyponym\" relations in WordNet. This is illustrated by the fact that the hyponym relation between \"gold dust\" and \"gold\" can be formalized by saying that all gold dust is gold, whereas the hyponym relation between \"gold\" and \"noble metal\" cannot be formalized by saying that all gold is a noble metal (rather, gol...",
  "por_txt_cntn": "\nIt has been recognized in AI since the 1970s that human-level language understanding, commonsense reasoning, and goal-oriented planning in the real world depended on the possession and use of very large amounts of knowledge -- perhaps tens or hundreds of millions of individual (but richly interrelated) items. This raised the question of how this knowledge could be acquired by a machine, in a form suitable for reasoning, and this long-standing challenge has been called the \"knowledge acquisition bottleneck\".\n\nThe present project extended previous work aimed at alleviating this bottleneck through knowledge acquisition from large text repositories, such as the 100-million word British National Corpus, the more than one billion word \"Gigaword\" newswire corpus, Wikipedia, and personal blogs. That previous work led to the KNEXT knowledge extraction system at the University of Rochester, and a repository of many millions of general \"factoids\" -- simple general claims, expressed in a formal, language-like logic called Episodic Logic; examples are that a person can have a brain, a dog may bark, people may want to be rid of a dictator, and so on. Unfortunately these factoids, though indicative of what the world is like, are too weakly formulated to be usable for inference; for instance, we cannot infer that a given person does have a brain, only that this is a possibility. Thus the present project has been aimed at \"bootstrapping\" KNEXT-like knowledge -- extending it and boosting the logical strength of the factoids to enable inferences.\n\nThe research accomplished knowledge extensions and strengthening in three primary ways:\n\nLogical \"sharpening\" of many millions of the original KNEXT factoids, making them suitable for reasoning; this relied on various lexical and software resources, allowing nouns and verbs to be semantically classified in multiple ways, with the help of a new pattern matching and transformation system, TTT, created in the course of this project;\nDerivation of axiomatic knowledge about relationships among nominal concepts (person, dog, creature, tree, plant, computer, artifact, ...) as characterized in the online WordNet lexicon; again this relied on multiple lexical resources and TTT; and\nDirect creation of axioms expressing what is implied by verb concepts such as walking, giving (something to someone), refusing (to so something), believing (something), etc., using as a guide the online VerbNet resource, which contains a systematic classification of verbs.   \n\n\nThese new knowledge items support \"obvious\" inferences such as that John may well occasionally go to a dentist, very likely has teeth as a part, and possibly some of those teeth are occasionally fixed and some may be lost (given only that John is a person); or that IBM may well have products, probably experiences sales, may have headquarters, may have websites, may grow, etc. (given only that IBM is a company). Such inferences are obtainable with the EPILOG inference engine, whose capabilities have also been strengthened in the course of the project.\n\nWithin the field of artificial intelligence, these results constitute significant progress towards overcoming the knowledge acquisition bottleneck. The work also has implications for linguistics. For example, the effort to axiomatize relations among nominal concepts in WordNet showed the importance of the mass-count distinction in making formal sense of the \"hyponym\" relations in WordNet. This is illustrated by the fact that the hyponym relation between \"gold dust\" and \"gold\" can be formalized by saying that all gold dust is gold, whereas the hyponym relation between \"gold\" and \"noble metal\" cannot be formalized by saying that all gold is a noble metal (rather, gold, considered as an elementary substance, is a noble metal). The problem can be traced to the fact that \"gold dust\" and \"gold\" are mass nouns, while \"noble metal\" is a count noun.\n\nMore broadly, by providing large amounts of new, inference-enabling kno..."
 }
}