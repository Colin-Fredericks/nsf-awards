{
 "awd_id": "0937865",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Optimization Algorithms for Large-scale, Thermal-aware Storage Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2009-09-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 907449.0,
 "awd_amount": 931449.0,
 "awd_min_amd_letter_date": "2009-09-24",
 "awd_max_amd_letter_date": "2011-03-23",
 "awd_abstract_narration": "This project investigates optimization problems that arise while performing thermal management in very large data storage centers. To satisfy the growing data management needs, such storage centers contain possibly hundreds of thousands of hard disks and other components, and typically are consistently active. These generate a lot of heat, and hence the storage system must be cooled to maintain reliability, resulting in significant cooling costs. The cooling mechanism and the workload assignments in a storage center are intricately tied together.\r\nThis project is developing a general science of thermal management for large scale storage systems, by focusing on thermal modeling and management at different levels of the system hierarchy. Thermal aware techniques for allocating data access tasks to specific disks on which data is located, for controlling the schedules and speeds of thousands of tasks and disks to optimize quality of service, and for reorganizing data layouts on disks are being developed. This project will enable better thermal management in data storage centers, which can potentially result in significant reductions in the carbon footprint caused by those. The project will train several Ph.D. students in conducting research both at the University, and through internships at Industrial Research Labs.\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Samir",
   "pi_last_name": "Khuller",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Samir Khuller",
   "pi_email_addr": "samir.khuller@northwestern.edu",
   "nsf_id": "000177377",
   "pi_start_date": "2009-09-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ankur",
   "pi_last_name": "Srivastava",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ankur Srivastava",
   "pi_email_addr": "ankurs@eng.umd.edu",
   "nsf_id": "000313791",
   "pi_start_date": "2009-09-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Amol",
   "pi_last_name": "Deshpande",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Amol V Deshpande",
   "pi_email_addr": "amol@cs.umd.edu",
   "nsf_id": "000486255",
   "pi_start_date": "2009-09-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland, College Park",
  "perf_str_addr": "3112 LEE BUILDING",
  "perf_city_name": "COLLEGE PARK",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "795200",
   "pgm_ele_name": "HECURA"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7952",
   "pgm_ref_txt": "HECURA"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 597900.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 309549.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 24000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this NSF project was to develop tools and techniques for solving optimization problems that arise while performing thermal management in very large data storage centers. Future storage centers are envisioned to contain possibly hundreds of thousands of hard drives and other components, and typically are expected to be active 24/7. Those components generate a lot of heat, and hence the storage system must be cooled to maintain reliability. The primary motivation for our work comes from the observation that the cooling mechanism and the workload assignments in a storage center are intricately tied together. The key outcomes of the project are as follows.<br /><br />First, we developed techniques for thermal scheduling, where the goal is to schedule the workload in a thermally aware manner, assigning jobs to machines not just based on local load of the machines, but based on the overall thermal profile of the data center.&nbsp; The heat generated by jobs running on a machine raises its own as well as the temperatures of nearby machines due to hot air recirculation effects. In addition, the data center geometry plays a significant role in determining these cross-effect parameters, which are often asymmetric. We developed several thermally-aware scheduling algorithms for jobs such that either the maximum temperature in the data center is minimized, or the total profit of jobs assigned is maximized while keeping the maximum temperature below a certain limit, under several different cross-effect models. We also developed solutions to address the problem architecture level where CPU frequencies were scaled in such a way that Silicon and disk-level temperatures were explicitly controlled to be within acceptable levels rather than maintaining a specific air temperature; air temperature in itself may be an inaccurate predictor of device-level temperatures that primarily influence reliability issues. We developed various heuristics that exploited the mathematical structure of the problem.<br /><br />Second, we developed a suite of techniques for minimizing the total resource consumption, and thereby the total energy consumption, when analyzing or querying very large volumes of data in distributed fashion. For read-only analytical tasks, we developed a data replication and placement strategy that minimizes the number of machines that need to be involved in execution of a task. Similarly, for transactional workloads, we designed a scalable, workload-aware approach for minimizing the number of distributed transactions. We also designed and implemented a runtime platform for executing big data analysis tasks on a powerful, multi-core server. Moreover, we also studied the problem of reducing the 'on' time of servers. It has been observed that a majority of servers run at or below 20% utilization most of the time, however, they draw nearly the same amount of power irrespective of their utilization. Hence, effective batching of jobs, respecting their requirements and machine capacities, can reduce the number of servers running at any given time; we significantly improve the existing results and give new, faster algorithms, with provable performance bounds.<br /><br />Third, we focused on reducing communication costs in data centers as well as distributed computing environments that use these data centers (e.g., Youtube running on cellphones), which also has implications for energy minimization. A widely used billing rule for network bandwidth is the peak bandwidth rule, where the billing cycle is divided into slots, and the billing is on the peak bandwidth consumed in any slot of the billing cycle. Since the data traffic is not known a priori, we considered the online problem of minimizing the maximum bandwidth. Interestingly, the problem of peak bandwidth usage minimization has connections to the energy minimization problem. The power consumed by processors is directly p...",
  "por_txt_cntn": "\nThe goal of this NSF project was to develop tools and techniques for solving optimization problems that arise while performing thermal management in very large data storage centers. Future storage centers are envisioned to contain possibly hundreds of thousands of hard drives and other components, and typically are expected to be active 24/7. Those components generate a lot of heat, and hence the storage system must be cooled to maintain reliability. The primary motivation for our work comes from the observation that the cooling mechanism and the workload assignments in a storage center are intricately tied together. The key outcomes of the project are as follows.\n\nFirst, we developed techniques for thermal scheduling, where the goal is to schedule the workload in a thermally aware manner, assigning jobs to machines not just based on local load of the machines, but based on the overall thermal profile of the data center.  The heat generated by jobs running on a machine raises its own as well as the temperatures of nearby machines due to hot air recirculation effects. In addition, the data center geometry plays a significant role in determining these cross-effect parameters, which are often asymmetric. We developed several thermally-aware scheduling algorithms for jobs such that either the maximum temperature in the data center is minimized, or the total profit of jobs assigned is maximized while keeping the maximum temperature below a certain limit, under several different cross-effect models. We also developed solutions to address the problem architecture level where CPU frequencies were scaled in such a way that Silicon and disk-level temperatures were explicitly controlled to be within acceptable levels rather than maintaining a specific air temperature; air temperature in itself may be an inaccurate predictor of device-level temperatures that primarily influence reliability issues. We developed various heuristics that exploited the mathematical structure of the problem.\n\nSecond, we developed a suite of techniques for minimizing the total resource consumption, and thereby the total energy consumption, when analyzing or querying very large volumes of data in distributed fashion. For read-only analytical tasks, we developed a data replication and placement strategy that minimizes the number of machines that need to be involved in execution of a task. Similarly, for transactional workloads, we designed a scalable, workload-aware approach for minimizing the number of distributed transactions. We also designed and implemented a runtime platform for executing big data analysis tasks on a powerful, multi-core server. Moreover, we also studied the problem of reducing the 'on' time of servers. It has been observed that a majority of servers run at or below 20% utilization most of the time, however, they draw nearly the same amount of power irrespective of their utilization. Hence, effective batching of jobs, respecting their requirements and machine capacities, can reduce the number of servers running at any given time; we significantly improve the existing results and give new, faster algorithms, with provable performance bounds.\n\nThird, we focused on reducing communication costs in data centers as well as distributed computing environments that use these data centers (e.g., Youtube running on cellphones), which also has implications for energy minimization. A widely used billing rule for network bandwidth is the peak bandwidth rule, where the billing cycle is divided into slots, and the billing is on the peak bandwidth consumed in any slot of the billing cycle. Since the data traffic is not known a priori, we considered the online problem of minimizing the maximum bandwidth. Interestingly, the problem of peak bandwidth usage minimization has connections to the energy minimization problem. The power consumed by processors is directly proportional to the speed at which they are running, and the speed is dictated by release times, d..."
 }
}