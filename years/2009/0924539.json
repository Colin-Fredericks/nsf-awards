{
 "awd_id": "0924539",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Spatiotemporal Dynamics of Word Processing in the Bilingual Brain",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Akaysha Tang",
 "awd_eff_date": "2009-10-01",
 "awd_exp_date": "2012-09-30",
 "tot_intn_awd_amt": 506559.0,
 "awd_amount": 506559.0,
 "awd_min_amd_letter_date": "2009-09-22",
 "awd_max_amd_letter_date": "2011-07-11",
 "awd_abstract_narration": "Almost as remarkable as the ability to know one language is the ability to learn a second. However, current understanding of how the human brain acquires, organizes, and processes multiple languages is highly limited. A fundamental issue is whether multiple languages are represented in common brain areas. When monolinguals read, words enter the cortex at its posterior tip, and work their way forward. Within two-tenths of a second, the words are encoded visually, and then in the next three-tenths of a second they are encoded for meaning in specialized areas of the left temporal and frontal lobes. With support from the National Science Foundation, Dr. Eric Halgren and Dr. Jeffrey Elman of the University of California at San Diego, with their colleagues, will study word encoding in young adults reading words in their native Spanish, or in their second language, English, which most have been using since they started school. Cortical neurons process information using electrical currents, which in turn produce minute magnetic fields. These will be detected using arrays of superconducting quantum interference devices as the magnetoencephalogram, and then mapped to particular cortical areas using magnetic resonance imaging. Experiments will determine if the English and Spanish words are encoded in the same areas. Specifically, experiments will test a model that hypothesizes that the second language does engage the same areas as the first, but in addition accesses the corresponding areas in the right hemisphere. Experiments will also attempt to confirm the suggestion that the second language uses brain areas which are otherwise engaged in high level vision, and explore if the second language is characterized by perceptual representations of words. Experiments will test how early in the processing stream English and Spanish words diverge, and whether this divergence is due to top-down strategic control or quick categorization. Subjects will vary in how well they know English and when they began learning it, so that the effects of age of acquisition, order of acquisition, and proficiency can be determined.\r\n\r\nThis research brings together neural, cognitive, language and imaging sciences, providing interdisciplinary training, especially for bilingual Spanish-English students, and introducing neuroimaging to a societal problem previously approached mainly behaviorally. Over two-thirds of the global population is multi-lingual. Bilingualism binds together the multiple American subcultures, being essential for socioeconomic integration of recent immigrants, and for the access of American goods and services to a global marketplace. Yet, second language education is often ineffective. A clearer picture of how the brain organizes multiple languages will form part of the scientific basis for developing effective second language teaching methodologies.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Halgren",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Halgren",
   "pi_email_addr": "ehalgren@ucsd.edu",
   "nsf_id": "000501518",
   "pi_start_date": "2009-09-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Elman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey L Elman",
   "pi_email_addr": "jelman@ucsd.edu",
   "nsf_id": "000444346",
   "pi_start_date": "2009-09-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 169853.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 172024.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 164682.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This grant allowed us to make fundamental discoveries concerning how language is implemented in the human brain. We identified the stages that words pass through as they are transformed from sensory signals to meaning- where those stages are located and when they occur. We determined how two languages can share the same brain, especially when one language is learned earlier or better than the other. We determined how the brain is reorganized when it is deprived of a sensory modality from birth, or when it is deprived of effective language input.</p>\n<p>We measured the minute magnetic fields generated by cortical neurons when they process words, and localized them using MRI. This technique allowed us to track excitation through the brain with millisecond accuracy. Studying Spanish-English bilinguals, we found that their less proficient language recruits many areas in addition to the classical left hemisphere regions that are typically associated with language processing.&nbsp; These areas may be helpful in many tasks where information-processing is difficult, and may therefore be neural markers of inexperience, and by extension, of the learning process.<em></em></p>\n<p>Along the way we discovered a new stage used by the brain to understand spoken words, that translates from the sensory representation to the word representation. We found that this stage is specific for words but that it does not use prior knowledge of what the word means to help its processing, contrary to many models of spoken word understanding.</p>\n<p>We also studied people who have been deaf from birth and learned American Sign Language (ASL). We found that the location and timing of brain processing they use to understand signed words is the same that hearing people use to understand spoken or written words. These &lsquo;classical language areas&rsquo; are near the auditory cortex, but we showed that visual input does not abnormally activate their auditory cortex despite its long-standing deprivation of its normal input.</p>\n<p>Our results indicate a striking similarity of the areas that are activated for associating words with their meanings, across different languages and sensory modalities. In a group of college students learning ASL, we found that spoken, written and signed words all evoke very similar spatiotemporal patterns. However, this pattern was not seen in two teens who, deaf from birth, had only recently begun to learn language (ASL). Overall, these results suggest that there is a special region of cortex, and special neurophysiological processes, that are devoted to understanding the meaning of words, regardless of the path they may take to enter the brain. This special area is supplemented by other characteristic brain regions when the word is relatively unfamiliar. However, if this region is not utilized for language during a critical period, it may not be activated when effective language input is finally provided.</p>\n<p>Our results have contributed to the scientific basis of bilingualism, and may help in the design of more effective strategies for learning a second language. Our results also help understand the neural basis of language in the congenitally deaf, with implications for language training in these individuals, and in particular supports the importance of early language experience.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/10/2013<br>\n\t\t\t\t\tModified by: Eric&nbsp;Halgren</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis grant allowed us to make fundamental discoveries concerning how language is implemented in the human brain. We identified the stages that words pass through as they are transformed from sensory signals to meaning- where those stages are located and when they occur. We determined how two languages can share the same brain, especially when one language is learned earlier or better than the other. We determined how the brain is reorganized when it is deprived of a sensory modality from birth, or when it is deprived of effective language input.\n\nWe measured the minute magnetic fields generated by cortical neurons when they process words, and localized them using MRI. This technique allowed us to track excitation through the brain with millisecond accuracy. Studying Spanish-English bilinguals, we found that their less proficient language recruits many areas in addition to the classical left hemisphere regions that are typically associated with language processing.  These areas may be helpful in many tasks where information-processing is difficult, and may therefore be neural markers of inexperience, and by extension, of the learning process.\n\nAlong the way we discovered a new stage used by the brain to understand spoken words, that translates from the sensory representation to the word representation. We found that this stage is specific for words but that it does not use prior knowledge of what the word means to help its processing, contrary to many models of spoken word understanding.\n\nWe also studied people who have been deaf from birth and learned American Sign Language (ASL). We found that the location and timing of brain processing they use to understand signed words is the same that hearing people use to understand spoken or written words. These \u00e6classical language areas\u00c6 are near the auditory cortex, but we showed that visual input does not abnormally activate their auditory cortex despite its long-standing deprivation of its normal input.\n\nOur results indicate a striking similarity of the areas that are activated for associating words with their meanings, across different languages and sensory modalities. In a group of college students learning ASL, we found that spoken, written and signed words all evoke very similar spatiotemporal patterns. However, this pattern was not seen in two teens who, deaf from birth, had only recently begun to learn language (ASL). Overall, these results suggest that there is a special region of cortex, and special neurophysiological processes, that are devoted to understanding the meaning of words, regardless of the path they may take to enter the brain. This special area is supplemented by other characteristic brain regions when the word is relatively unfamiliar. However, if this region is not utilized for language during a critical period, it may not be activated when effective language input is finally provided.\n\nOur results have contributed to the scientific basis of bilingualism, and may help in the design of more effective strategies for learning a second language. Our results also help understand the neural basis of language in the congenitally deaf, with implications for language training in these individuals, and in particular supports the importance of early language experience.\n\n\t\t\t\t\tLast Modified: 01/10/2013\n\n\t\t\t\t\tSubmitted by: Eric Halgren"
 }
}