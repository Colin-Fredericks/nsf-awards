{
 "awd_id": "0916027",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III:   Small:  RIOT:   Statistical Computing with Efficient, Transparent I/O",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 516000.0,
 "awd_min_amd_letter_date": "2009-09-04",
 "awd_max_amd_letter_date": "2010-05-11",
 "awd_abstract_narration": "\r\nRecent technological advances enable collection of massive amounts of\r\ndata in science, commerce, and society.  These datasets bring us\r\ncloser than ever before to solving important problems such as decoding\r\nhuman genomes and coping with climate changes.  Meanwhile, the\r\nexponential growth in data volume creates an urgent challenge.  Many\r\nexisting analysis tools assume datasets fit in memory; when applied to\r\nmassive datasets, they become unacceptably slow because of excessive\r\ndisk input/output (I/O) operations.\r\n\r\nAcross application domains, much of advanced data analysis is done\r\nwith custom programming by statisticians.  Progress has been hindered\r\nby the lack of easy-to-use statistical computing environments that\r\nsupport I/O-efficient processing of large datasets.  There have been\r\nmany approaches toward I/O-efficiency, but none has gained traction\r\nwith statisticians because of issues ranging from efficiency to\r\nusability.  Disk-based storage engines and I/O-efficient function\r\nlibraries are only a partial solution, because many sources of\r\nI/O-inefficiency in programs remain at a higher, inter-operation\r\nlevel.  Database systems seem to be a natural solution, with efficient\r\nI/O and a declarative language (SQL) enabling high-level\r\noptimizations.  However, much work in integrating databases and\r\nstatistical computing remains database-centric, forcing statisticians\r\nto learn unfamiliar languages and deal with their impedance mismatch\r\nwith host languages.\r\n\r\nTo make a practical impact on statistical computing, this project\r\npostulates that a better approach is to make it transparent to users\r\nhow I/O-efficiency is achieved.  Transparency means no SQL, or any new\r\nlanguage to learn.  Transparency means that existing code should run\r\nwithout modification, and automatically gain I/O-efficiency.  The\r\nproject, nicknamed RIOT, aims at extending R---a widely popular\r\nopen-source statistical computing environment---to transparently\r\nprovide efficient I/O.  Achieving transparency is challenging; RIOT\r\ndoes so with an end-to-end solution addressing issues on all fronts:\r\nI/O-efficient algorithms, pipelined execution, deferred evaluation,\r\nI/O-cost-driven expression optimization, smart storage and\r\nmaterialization, and seamless integration with an interpreted host\r\nlanguage.\r\n\r\nRIOT integrates research and education, and continues the tradition of\r\ninvolving undergraduates through REU and independent studies.  As a\r\ndatabase researcher, the PI is committed to learning and drawing from\r\nwork from programming languages and high-performance computing.\r\nFindings from RIOT help create synergy and seed further collaboration\r\nwith these communities.  To ensure practical impact on statistical\r\ncomputing, RIOT has enlisted collaboration from statisticians and the\r\nR core development team on developing, evaluating, and disseminating\r\nRIOT.\r\n\r\nFurther information can be found at: \r\nhttp://www.cs.duke.edu/dbgroup/Main/RIOT",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Yang",
   "pi_email_addr": "junyang@cs.duke.edu",
   "nsf_id": "000486379",
   "pi_start_date": "2009-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W MAIN ST",
  "perf_city_name": "DURHAM",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054640",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Recent technological advances have enabled collection of massive amounts of data in science, commerce, and society. These large, high-resolution datasets have brought us closer than ever before to solving important problems such as decoding human genomes and coping with climate changes. Meanwhile, the exponential growth in the amount of data has created an urgent and difficult technical challenge. Many existing data analysis tools still assume that datasets fit in main memory of a single machine; they are unable to cope with massive datasets.</p>\n<p>Across application domains, much of advanced data analysis is done with programs custom-developed by statisticians. Unfortunately, progress has been hindered by the lack of easy-to-use statistical computing environments that support efficient and scalable execution of programs over large datasets. High-performance libraries provide only a partial solution, because many optimization opportunities in a program remain at a higher, inter-operation level. &nbsp;The goal of this project is to provide a more usable platform for big data analytics.</p>\n<p><strong>Intellectual Merit</strong><br />To make a practical impact on the statistical computing community, this project postulates that a better approach is to make it transparent to users how efficiency and scalability is achieved. Transparency means new language to learn; the system automatically optimizes the programs written in a high-level language familiar to statisticians. &nbsp;To achieve I/O-efficiency, this project has developed an end-to-end solution that addresses issues on all fronts in an innovative way: efficient and flexible storage and indexing, pipelined execution to avoid intermediate results, I/O-cost-driven optimization through aggressively deferred evaluation, and seamless integration of these features with an interpreted host language R. &nbsp;The project has also investigated various methods for leveraging emerging hardware and platform for scalable statistical analysis, including the use of a computing cloud, solid-state drives (SSDs), and graphics processing units (GPUs). &nbsp;Users can benefit from these technologies without having to rewrite programs specifically for them.</p>\n<p>This project has generated many publications in database research venues, including CIDR 2009, ICDE 2010, PVLDB 2011, PVLDB 2012, CIKM 2012, SIGMOD 2013, PVLDB 2013, and IEEE Data Engineering Bulletin 2014. &nbsp;The software artifacts developed by the project include a proof-of-concept implementation on top of a database system (available from the project website), a prototype system for I/O-efficient linear algebra built from ground up to overcome the limitations and inefficiency of database systems (demonstrated at ICDE 2010), and a prototype system that jointly optimizes parallel execution and deployment strategies for linear algebra workloads on a cloud.</p>\n<p><strong>Broader Impact</strong><br />The PI has been part of other interdisciplinary projects funded by NSF&shy;&shy;&shy;one studied how to collect and analyze ecological data from a sensor network, and another one investigating how to simplify the development and deployment of statistical data analysis in a cloud. &nbsp;Much of the work in this project is motivated by the ecological data analysis problems faced in the first project on sensors, while many of the results from this project are now being applied in the second project to problems in statistics and political science.</p>\n<p>The project has provided training for a number of PhD students: Yi Zhang, Risi Thonangi, and Botong Huang. &nbsp;Yi Zhang, the lead student on this project, graduated with a PhD in 2012. &nbsp;The PI has also supervised undergraduate researchers, Weiping Zhang and Jiaqi Yan, to work on this project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/10/2014<br>\n\t\t\t\t\tModified by: Jun&nbsp;Yang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</d...",
  "por_txt_cntn": "\nRecent technological advances have enabled collection of massive amounts of data in science, commerce, and society. These large, high-resolution datasets have brought us closer than ever before to solving important problems such as decoding human genomes and coping with climate changes. Meanwhile, the exponential growth in the amount of data has created an urgent and difficult technical challenge. Many existing data analysis tools still assume that datasets fit in main memory of a single machine; they are unable to cope with massive datasets.\n\nAcross application domains, much of advanced data analysis is done with programs custom-developed by statisticians. Unfortunately, progress has been hindered by the lack of easy-to-use statistical computing environments that support efficient and scalable execution of programs over large datasets. High-performance libraries provide only a partial solution, because many optimization opportunities in a program remain at a higher, inter-operation level.  The goal of this project is to provide a more usable platform for big data analytics.\n\nIntellectual Merit\nTo make a practical impact on the statistical computing community, this project postulates that a better approach is to make it transparent to users how efficiency and scalability is achieved. Transparency means new language to learn; the system automatically optimizes the programs written in a high-level language familiar to statisticians.  To achieve I/O-efficiency, this project has developed an end-to-end solution that addresses issues on all fronts in an innovative way: efficient and flexible storage and indexing, pipelined execution to avoid intermediate results, I/O-cost-driven optimization through aggressively deferred evaluation, and seamless integration of these features with an interpreted host language R.  The project has also investigated various methods for leveraging emerging hardware and platform for scalable statistical analysis, including the use of a computing cloud, solid-state drives (SSDs), and graphics processing units (GPUs).  Users can benefit from these technologies without having to rewrite programs specifically for them.\n\nThis project has generated many publications in database research venues, including CIDR 2009, ICDE 2010, PVLDB 2011, PVLDB 2012, CIKM 2012, SIGMOD 2013, PVLDB 2013, and IEEE Data Engineering Bulletin 2014.  The software artifacts developed by the project include a proof-of-concept implementation on top of a database system (available from the project website), a prototype system for I/O-efficient linear algebra built from ground up to overcome the limitations and inefficiency of database systems (demonstrated at ICDE 2010), and a prototype system that jointly optimizes parallel execution and deployment strategies for linear algebra workloads on a cloud.\n\nBroader Impact\nThe PI has been part of other interdisciplinary projects funded by NSF&shy;&shy;&shy;one studied how to collect and analyze ecological data from a sensor network, and another one investigating how to simplify the development and deployment of statistical data analysis in a cloud.  Much of the work in this project is motivated by the ecological data analysis problems faced in the first project on sensors, while many of the results from this project are now being applied in the second project to problems in statistics and political science.\n\nThe project has provided training for a number of PhD students: Yi Zhang, Risi Thonangi, and Botong Huang.  Yi Zhang, the lead student on this project, graduated with a PhD in 2012.  The PI has also supervised undergraduate researchers, Weiping Zhang and Jiaqi Yan, to work on this project.\n\n\t\t\t\t\tLast Modified: 11/10/2014\n\n\t\t\t\t\tSubmitted by: Jun Yang"
 }
}