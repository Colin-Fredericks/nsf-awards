{
 "awd_id": "0916323",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR:  Small:  Collaborative Research: Combining Static Analysis and Dynamic Run-time Optimization for Parallel Discrete Event Simulation in Many-Core Environments",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 333031.0,
 "awd_amount": 358031.0,
 "awd_min_amd_letter_date": "2009-08-08",
 "awd_max_amd_letter_date": "2011-05-12",
 "awd_abstract_narration": "This project investigates how a new processor paradigm (multi-core architectures) changes the way Parallel Discrete Event Simulation (PDES) is done. This topic is important given the wide use of simulation and the emergence of multi-core architectures. PDES is likely to play an increasingly important role in discrete event simulation as Moore?s Law is sharply curtailed and explicit parallelism becomes the major avenue for improving performance of sequential applications. Improving PDES performance translates to improved.\r\n\r\nDiscrete Event Simulation (DES) is widely used for performance evaluation in many application domains. The fine grained nature of PDES causes its performance and scalability to be limited by communication latency. The emergence of multi-core architectures and their expected evolution into manycore systems offers potential relief to PDES and other fine grained parallel applications because the cost of communication within a chip is dramatically lower than conventional networked communication. Absent this dominant effect, PDES performance will be determined by issues such as load balancing, synchronization and optimism control, and the choice and configuration of various other algorithms and data structures of the simulator. Operation in a manycore environment introduces new system tradeoffs that must be effectively balanced by the system software. Primarily, the pressure on the memory system and resilience to load fluctuations will emerge as critical issues that we address in the proposed research. Finally, the more predictable nature of communication cost in this environment (due in part to the more frequent synchronization possible between nearby cores) can be exploited, especially by static analysis, for effective simulation. \r\n\r\nAs multi-cores become the default microprocessor architecture, applications that are performance constrained must evolve to use parallelism to take advantage of the resources available on the cores. This project?s new PDES can have a significant impact on a number of applications that rely on discrete event simulations. The PIs plan to incorporate the research results into a graduate level course on parallel simulation techniques and to involve undergraduate students in the project.\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nael",
   "pi_last_name": "Abu-Ghazaleh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nael Abu-Ghazaleh",
   "pi_email_addr": "nael@cs.binghamton.edu",
   "nsf_id": "000275373",
   "pi_start_date": "2009-08-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dmitry",
   "pi_last_name": "Ponomarev",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Dmitry V Ponomarev",
   "pi_email_addr": "dponomar@binghamton.edu",
   "nsf_id": "000126433",
   "pi_start_date": "2009-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Binghamton",
  "inst_street_address": "4400 VESTAL PKWY E",
  "inst_street_address_2": "",
  "inst_city_name": "BINGHAMTON",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6077776136",
  "inst_zip_code": "13902",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "L9ZDVULCHCV3",
  "org_uei_num": "NQMVAAQUFU53"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Binghamton",
  "perf_str_addr": "4400 VESTAL PKWY E",
  "perf_city_name": "BINGHAMTON",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "13902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 333031.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 12500.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 12500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Simulation is a critical capability used in the design and evaluation of systems across a wide-range of domains. &nbsp;Parallel Simulation can improve the performance and capacity of simulation, allowing us to study larger models in more details and for more scenarios. &nbsp;In this project, we explored how to improve the performance of parallel discrete event simulation on emerging multi-core and many-core computing systems. &nbsp;We identified and characterized the bottlenecks and developed new algorithms and optimizations that improve the performance of parallel simulation significatnly on these platforms. &nbsp;We looked at how communication support can be improved to take advantage of the memory hierarchy available on such systems. &nbsp;We explored these issue son three different multi-core architectures with significantly different designs. &nbsp;We developed approaches to manage the high cost of communication across a network of multi-core systems. &nbsp;We also looked at how we can analyze the model being simulated and take advantage of its properties to improve the simulation performance. &nbsp;We explored how to make simulation more effectively manage interference from other co-located applications. &nbsp;These algorithms, optimizations and the experiences gathered while developing and evaluating them represent the intellectual merit of the project.</p>\n<p>The developed techniques were integrated within the ROSS simulation engine and made available to other researchers. &nbsp;This includes both core simulation and communication algorithms, synchronization optimizations, and memory optimizations. &nbsp;We also make available our partitioning algorithms that are based on the simulation model analysis. &nbsp;Finally, we also make available our work to thread remapping approaches for managing intereference from other applications. &nbsp;Experiences with all these investigates have been published and will inform the work of other researchers in parallel simulation, parallel computing in general, as well as manycore architecture design. &nbsp;The work results have been dessiminated in 11 scientific publications in top journals and conferences in the area of parallel simulation. &nbsp;The project lead to the training of one PhD student and one MS student in this important area. &nbsp;It also supported through REU supplements, three undergraduate students who also got trained in this area. &nbsp;Educational material was developed and used in the graduate computer architecture class based on the results of the project. &nbsp;These represent the broader impacts of the project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/17/2014<br>\n\t\t\t\t\tModified by: Nael&nbsp;Abu-Ghazaleh</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSimulation is a critical capability used in the design and evaluation of systems across a wide-range of domains.  Parallel Simulation can improve the performance and capacity of simulation, allowing us to study larger models in more details and for more scenarios.  In this project, we explored how to improve the performance of parallel discrete event simulation on emerging multi-core and many-core computing systems.  We identified and characterized the bottlenecks and developed new algorithms and optimizations that improve the performance of parallel simulation significatnly on these platforms.  We looked at how communication support can be improved to take advantage of the memory hierarchy available on such systems.  We explored these issue son three different multi-core architectures with significantly different designs.  We developed approaches to manage the high cost of communication across a network of multi-core systems.  We also looked at how we can analyze the model being simulated and take advantage of its properties to improve the simulation performance.  We explored how to make simulation more effectively manage interference from other co-located applications.  These algorithms, optimizations and the experiences gathered while developing and evaluating them represent the intellectual merit of the project.\n\nThe developed techniques were integrated within the ROSS simulation engine and made available to other researchers.  This includes both core simulation and communication algorithms, synchronization optimizations, and memory optimizations.  We also make available our partitioning algorithms that are based on the simulation model analysis.  Finally, we also make available our work to thread remapping approaches for managing intereference from other applications.  Experiences with all these investigates have been published and will inform the work of other researchers in parallel simulation, parallel computing in general, as well as manycore architecture design.  The work results have been dessiminated in 11 scientific publications in top journals and conferences in the area of parallel simulation.  The project lead to the training of one PhD student and one MS student in this important area.  It also supported through REU supplements, three undergraduate students who also got trained in this area.  Educational material was developed and used in the graduate computer architecture class based on the results of the project.  These represent the broader impacts of the project.\n\n\t\t\t\t\tLast Modified: 01/17/2014\n\n\t\t\t\t\tSubmitted by: Nael Abu-Ghazaleh"
 }
}