{
 "awd_id": "0917321",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC:Small:Computational Studies of Social Nonverbal Communication",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 495920.0,
 "awd_amount": 495920.0,
 "awd_min_amd_letter_date": "2009-07-06",
 "awd_max_amd_letter_date": "2010-08-20",
 "awd_abstract_narration": "This research will create a new generation of computational tools, called contextual prediction models, for analyzing and modeling social nonverbal communication in human-centered computing. This computational study of nonverbal communication not only encompass the recent advances in machine learning, pattern analysis and computer vision, but goes further by developing and evaluating new algorithms and probabilistic models specifically designed for the domain of social and nonverbal communication. The ability to collect, analyze and ultimately predict human nonverbal cues will provide new insights into human social processes and new human-centric applications that can understand and respond to this natural human communicative channel. \r\n\r\nThis new endeavor will advance through the development of prediction models and their accompanying selection algorithms and feature representations for predicting human nonverbal behavior given a social context (such as the immediately preceding verbal and nonverbal behaviors of a conversational partner). The investigator's previous work has demonstrated the feasibility of using machine learning approaches to model nonverbal communication. Probabilistic sequential models were shown to improve performance of nonverbal behavior recognition during human-robot interactions and make possible the natural animation of virtual humans. This project addresses three fundamental challenges directly: feature representation (optimal mathematical representation of social context), feature selection (subset of social context relevant to prediction of nonverbal behaviors) and probabilistic modeling (efficiently learning the predictive relationship between social context and nonverbal behaviors). This research will evaluate and test the generalization of the computation tools using a large corpus of natural interactions in different settings (human-human, human-robot and human-computer) and domains (e.g., storytelling, interview, and meetings). \r\n\r\nThese prediction models will have broad applicability, including the improvement of nonverbal behavior recognition, the synthesis of natural animations for robots and virtual humans, the training of cultural-specific nonverbal behaviors, and the diagnoses of social disorders (e.g., autism spectrum disorder). The code resulting from this work will be made available to the research community through an open-source Matlab toolbox. The outcome of this research effort will produce state-of-the-art computational models more accessible to researchers who aim to analyze social nonverbal communication and develop natural and productive human-centered computing technologies.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Louis-Philippe",
   "pi_last_name": "Morency",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Louis-Philippe Morency",
   "pi_email_addr": "morency@cs.cmu.edu",
   "nsf_id": "000519300",
   "pi_start_date": "2009-07-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S FLOWER ST FL 3",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "90033",
  "perf_ctry_code": "US",
  "perf_cong_dist": "34",
  "perf_st_cong_dist": "CA34",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 210620.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 285300.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"font-family: Times New Roman; font-size: small;\"> </span></p>\n<p style=\"margin: 0in 0in 10pt;\"><span style=\"font-family: Calibri; font-size: small;\">The overarching goal of this project was to advance the science around computational models of social nonverbal communication by developing prediction models of human nonverbal behavior during social interactions. The ability to collect, analyze and ultimately predict human nonverbal cues will provide new insights into human social processes and new human-centric applications that can understand and respond to this natural human communicative channel. This project addressed three fundamental challenges: (1) <strong style=\"mso-bidi-font-weight: normal;\">Context Representation: </strong>mathematical representation of the contextual information during social interactions, (2) <strong style=\"mso-bidi-font-weight: normal;\"><span lang=\"EN\" style=\"mso-ansi-language: EN;\">Audio-Visual Feature Analysis:</span></strong><span lang=\"EN\" style=\"mso-ansi-language: EN;\"> </span>automatic selection of the most relevant contextual features to a specific nonverbal behavior, and (3) J<strong style=\"mso-bidi-font-weight: normal;\">oint Computational Model: </strong><span style=\"mso-spacerun: yes;\">&nbsp;</span>a probabilistic sequential model which efficiently learns the predictive relationship between context and nonverbal behavior.</span></p>\n<p><span style=\"font-family: Times New Roman; font-size: small;\"> </span></p>\n<p style=\"margin: 0in 0in 10pt;\"><span lang=\"EN\" style=\"mso-ansi-language: EN;\"><span style=\"font-family: Calibri; font-size: small;\">First, this project pushed forward our understanding of how to represent contextual features by taking into consideration the differences individuals. Not all speakers interact the same way. Some will be extremely expressive and others will only limit set of behaviors to trigger listener feedback. We proposed a new speaker-adaptive context representation which automatically matches the current speaker with the most similar speakers from our database. Our experiments on a challenging storytelling dataset show that our speaker adaptive approach outperforms the conventional non-adaptive approach, with significant improvement. </span></span></p>\n<p><span style=\"font-family: Times New Roman; font-size: small;\"> </span></p>\n<p style=\"margin: 0in 0in 10pt;\"><span lang=\"EN\" style=\"mso-ansi-language: EN;\"><span style=\"font-family: Calibri;\"><span style=\"font-size: small;\">Second, we proposed a generative approach called Co-HMM to learn the multimodal features most predictive for continuous emotion recognition. This approach takes advantage of the complementarity in multimodal dataset and learns separate Hidden Markov Models before concatenating them in one integrated Co-HMM model. This simple but efficient approach to model and analyze audio-visual data was successfully applied to the problem of continuous emotion recognition during the 2</span><sup><span style=\"font-size: x-small;\">nd</span></sup><span style=\"font-size: small;\"> Audio-Visual Emotion Challenge (AVEC 2012) where our paper won the 2</span><sup><span style=\"font-size: x-small;\">nd</span></sup><span style=\"font-size: small;\"> place for the Word-level emotion recognition. </span></span></span></p>\n<p><span style=\"font-family: Times New Roman; font-size: small;\"> </span></p>\n<p style=\"margin: 0in 0in 10pt;\"><span lang=\"EN\" style=\"mso-ansi-language: EN;\"><span style=\"font-family: Calibri; font-size: small;\">Third, we proposed a new computational model to learn the joint influence between speaker and listener during dyadic interaction. The new model called Mutual-LMDE learns separate predictive experts for speaker and listener visual behaviors and integrates them using a latent sequential model which identifies commonality and synchrony between listener and speaker behaviors. Our new Mutual-LMDE model outperforms the conventiona...",
  "por_txt_cntn": "\n \nThe overarching goal of this project was to advance the science around computational models of social nonverbal communication by developing prediction models of human nonverbal behavior during social interactions. The ability to collect, analyze and ultimately predict human nonverbal cues will provide new insights into human social processes and new human-centric applications that can understand and respond to this natural human communicative channel. This project addressed three fundamental challenges: (1) Context Representation: mathematical representation of the contextual information during social interactions, (2) Audio-Visual Feature Analysis: automatic selection of the most relevant contextual features to a specific nonverbal behavior, and (3) Joint Computational Model:  a probabilistic sequential model which efficiently learns the predictive relationship between context and nonverbal behavior.\n\n \nFirst, this project pushed forward our understanding of how to represent contextual features by taking into consideration the differences individuals. Not all speakers interact the same way. Some will be extremely expressive and others will only limit set of behaviors to trigger listener feedback. We proposed a new speaker-adaptive context representation which automatically matches the current speaker with the most similar speakers from our database. Our experiments on a challenging storytelling dataset show that our speaker adaptive approach outperforms the conventional non-adaptive approach, with significant improvement. \n\n \nSecond, we proposed a generative approach called Co-HMM to learn the multimodal features most predictive for continuous emotion recognition. This approach takes advantage of the complementarity in multimodal dataset and learns separate Hidden Markov Models before concatenating them in one integrated Co-HMM model. This simple but efficient approach to model and analyze audio-visual data was successfully applied to the problem of continuous emotion recognition during the 2nd Audio-Visual Emotion Challenge (AVEC 2012) where our paper won the 2nd place for the Word-level emotion recognition. \n\n \nThird, we proposed a new computational model to learn the joint influence between speaker and listener during dyadic interaction. The new model called Mutual-LMDE learns separate predictive experts for speaker and listener visual behaviors and integrates them using a latent sequential model which identifies commonality and synchrony between listener and speaker behaviors. Our new Mutual-LMDE model outperforms the conventional approach which ignores this mutual information. \n\n \nThe research performed as part of this grant was disseminated through 18 peer-reviewed publications, including two journal articles and publications in top conferences such as the Annual Meeting of the Association for Computational Linguistics (ACL) and International Conference on Autonomous Agents and Multi-agent Systems (AAMAS), where it won the best paper award in the virtual human track.\n\n \n\n\t\t\t\t\tLast Modified: 11/26/2014\n\n\t\t\t\t\tSubmitted by: Louis-Philippe Morency"
 }
}