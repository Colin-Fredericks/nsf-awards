{
 "awd_id": "0915754",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Modeling Coarticulation for Automatic Speech Recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 450001.0,
 "awd_amount": 466001.0,
 "awd_min_amd_letter_date": "2009-07-13",
 "awd_max_amd_letter_date": "2013-05-17",
 "awd_abstract_narration": "This project focuses on applying a model used in text-to-speech synthesis (TTS) to the task of automatic speech recognition (ASR).  The standard method in ASR for addressing variability due to phonemic context, or ?coarticulation,? requires a large amount of training data and is sensitive to differences between training and testing conditions.  Despite the effective use of stochastic models, current ASR systems are often unable to sufficiently account for the large degree of variability observed in speech.  In many cases, this variability is not due to random factors, but is due to predictable changes in the speech signal.  These factors are currently modeled in order to generate speech via TTS, but they are not yet modeled in order to recognize speech, largely because of non-local dependencies.  We apply the Asynchronous Interpolation Model (AIM) used in TTS to the task of speech recognition, by decomposing the speech signal into target vectors and weight trajectories, and then searching weight-trajectory and stochastic target-vector models for the highest-probability match to the input signal. \r\n\r\nThe goal of this research is improve the robustness of ASR to variability that is due to phonemic and lexical context.  This improvement will increase the use of ASR technology in automated information access by telephone, educational software, and universal access for individuals with visual, auditory, or speech-production challenges.  More effective models of coarticulation may increase our understanding of both human speech perception and speech production.  Results from this project are disseminated through technical papers and the CSLU Toolkit software package.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "John-Paul",
   "pi_last_name": "Hosom",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "John-Paul Hosom",
   "pi_email_addr": "jphosom.cslu@gmail.com",
   "nsf_id": "000091422",
   "pi_start_date": "2009-07-13",
   "pi_end_date": "2011-08-10"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Kain",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander Kain",
   "pi_email_addr": "kaina@ohsu.edu",
   "nsf_id": "000296505",
   "pi_start_date": "2011-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon Health & Science University",
  "inst_street_address": "3181 SW SAM JACKSON PARK RD",
  "inst_street_address_2": "",
  "inst_city_name": "PORTLAND",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5034947784",
  "inst_zip_code": "972393011",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OR01",
  "org_lgl_bus_name": "OREGON HEALTH & SCIENCE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPSNT86JKN51"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon Health & Science University",
  "perf_str_addr": "3181 SW SAM JACKSON PARK RD",
  "perf_city_name": "PORTLAND",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "972393011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OR01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 145019.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 312982.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project models the degree and manner of the coarticulation of speech. Coarticulation occurs when a conceptually isolated speech sound is influenced by, or becomes more similar to, a preceding or following speech sound. For example the vowel in the word &ldquo;fear&rdquo; is different from the vowel in the word &ldquo;feet&rdquo;. Such research has applications in furthering the state of the art in fundamental speech production research, speech disorder diagnosis, text-to-speech synthesis, and potentially increasing the intelligibility of conversational speech.<br />Specifically, we researched a methodology that models formant (spectral peaks) trajectories as a sum of phoneme targets weighted by coarticulation functions. Using a genetic algorithm search approach, we were able to determine the model parameters fully automatically, even for hard to estimate phonemes such as unvoiced bursts. To validate our findings, we carried out a perceptual listening test, and it was found that tokens reproduced by the model retained 95% of their original intelligibility; thus confirming a good model fit. As an application of the model, we focused on the difference in coarticulation between conversationally spoken speech and clearly spoken speech, and found evidence that, to some degree, conversational speech is a more coarticulated version of clear speech.<br />The project supported the academic education of graduate and undergraduate students, as well as the creation presentation of publications at international conferences.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2013<br>\n\t\t\t\t\tModified by: Alexander&nbsp;Kain</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2013/0915754/0915754_10028454_1378070898165_clear%22will%22--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2013/0915754/0915754_10028454_1378070898165_clear%22will%22--rgov-800width.jpg\" title=\"clear &quot;will&quot;\"><img src=\"/por/images/Reports/POR/2013/0915754/0915754_10028454_1378070898165_clear%22will%22--rgov-66x44.jpg\" alt=\"clear &quot;will&quot;\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The word \"will\", spoken in clear style</div>\n<div class=\"imageCredit\">Brian Bush and Alexander Kain</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;Kain</div>\n<div class=\"imageTitle\">clear \"will\"</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2013/0915754/0915754_10028454_1378070945689_conversational%22will%22--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2013/0915754/0915754_10028454_1378070945689_conversational%22will%22--rgov-800width.jpg\" title=\"conversational &quot;will&quot;\"><img src=\"/por/images/Reports/POR/2013/0915754/0915754_10028454_1378070945689_conversational%22will%22--rgov-66x44.jpg\" alt=\"conversational &quot;will&quot;\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The word \"will\", spoken in conversational style</div>\n<div class=\"imageCredit\">Brian Bush and Alexander Kain</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Alexander&nbsp;Kain</div>\n<div class=\"imageTitle\">conversational \"will\"</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2013/0915754/0915754_10028454_1378071013149_targets--rgov-214x142.jpg\" original=\"/por/images/...",
  "por_txt_cntn": "\nThis project models the degree and manner of the coarticulation of speech. Coarticulation occurs when a conceptually isolated speech sound is influenced by, or becomes more similar to, a preceding or following speech sound. For example the vowel in the word \"fear\" is different from the vowel in the word \"feet\". Such research has applications in furthering the state of the art in fundamental speech production research, speech disorder diagnosis, text-to-speech synthesis, and potentially increasing the intelligibility of conversational speech.\nSpecifically, we researched a methodology that models formant (spectral peaks) trajectories as a sum of phoneme targets weighted by coarticulation functions. Using a genetic algorithm search approach, we were able to determine the model parameters fully automatically, even for hard to estimate phonemes such as unvoiced bursts. To validate our findings, we carried out a perceptual listening test, and it was found that tokens reproduced by the model retained 95% of their original intelligibility; thus confirming a good model fit. As an application of the model, we focused on the difference in coarticulation between conversationally spoken speech and clearly spoken speech, and found evidence that, to some degree, conversational speech is a more coarticulated version of clear speech.\nThe project supported the academic education of graduate and undergraduate students, as well as the creation presentation of publications at international conferences.\n\n\t\t\t\t\tLast Modified: 09/01/2013\n\n\t\t\t\t\tSubmitted by: Alexander Kain"
 }
}