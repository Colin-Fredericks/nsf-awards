{
 "awd_id": "0937928",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Visual Characterization of I/O System Behavior for High-End Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frank Olken",
 "awd_eff_date": "2009-09-15",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 434000.0,
 "awd_amount": 434000.0,
 "awd_min_amd_letter_date": "2009-09-23",
 "awd_max_amd_letter_date": "2010-06-21",
 "awd_abstract_narration": "Abstract \r\nModern supercomputers are complex, hierarchical systems \r\nconsisting of huge numbers of cores, systems for disk storage, \r\nand nodes for I/O forwarding. These numbers continue to grow and \r\nthe need for tools to understand the behavior of the system \r\nsoftware becomes paramount: without these tools it is impossible \r\nto effectively tune that software, and high degrees of efficiency \r\nis unattainable by applications. This project addresses the \r\nchallenge of understanding the behavior of complex system \r\nsoftware on very large-scale compute platforms, like the current \r\npetascale computers. In particular, this project is developing \r\nsoftware infrastructure to provide end-to-end analysis and \r\nvisualization of I/O system software. Specifically, the \r\nobjectives are to develop, improve, and deploy (1) end-to-end, \r\nscalable tracing integrated into the I/O system (MPI-IO, I/O \r\nforwarding, and file system); (2) information visualization tools \r\nfor inspecting traces and extracting knowledge; (3) testing \r\ncomponents that drive this system to generate example patterns, \r\nincluding a component to generate anomalies; and (4) tutorials \r\nand tools for helping other system software developers \r\nincorporate this analysis and visualization system into their \r\nproduction software. The software and techniques developed in \r\nthis project will be directly applicable to and useful in other \r\nsystem software libraries which perform complex interactions on \r\nlarge systems. \r\n\r\nFor further information see the project web site at the URL: \r\nhttp://vis.cs.ucdavis.edu/NSF/Jupiter \r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kamil",
   "pi_last_name": "Iskra",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Kamil A Iskra",
   "pi_email_addr": "iskra@uchicago.edu",
   "nsf_id": "000528703",
   "pi_start_date": "2009-09-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Beckman",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Peter H Beckman",
   "pi_email_addr": "peter.beckman@northwestern.edu",
   "nsf_id": "000298331",
   "pi_start_date": "2009-09-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "5801 S ELLIS AVE",
  "perf_city_name": "CHICAGO",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606375418",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "747600",
   "pgm_ele_name": "XD-Extreme Digital"
  },
  {
   "pgm_ele_code": "795200",
   "pgm_ele_name": "HECURA"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 144840.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 289160.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Supercomputers have been described as machines that turn compute-bound problems into input/output-bound ones. Their massive&mdash;and rapidly increasing&mdash;computing subsystems demand equally capable input/output (I/O) infrastructure to supply them the input data and extract the results without undue delays. Unfortunately research into the performance analysis and optimization of I/O systems has lagged behind its computational counterparts. Yet, with the rapidly increasing complexity of new generations of high-end computing systems, we need improved methods to analyze the I/O performance, in order to identify bottlenecks and implement improvements that minimize the idle time caused by computing applications waiting for data transfers to complete. Our task in this project was to develop techniques for collecting the \"raw\" I/O performance data, which could then be studied using novel visual analysis techniques developed by our project partners from UC Davis.<br /><br />The challenge of instrumenting in detail (or, as we call it, tracing) a large system is that it consists of many thousands (possibly soon millions) of individual components, arranged in interdependent, hierarchical layers.&nbsp; In order to obtain a full view of the system behavior, all the relevant layers need to be instrumented and the activities at each layer be recorded with enough context that the progress of individual operations originating with the computing application can be followed through the layers all the way to the storage devices. Paradoxically, tracing the I/O subsystem can itself become a challenging I/O problem because of the large volumes of the trace data being generated. Yet, tracing must not put any significant additional load on the I/O system while the application is running, as it could then disturb the natural behavior of the system (the so-called observer effect). Meeting these conflicting goals requires careful planning and coordination.</p>\n<p>We were successful in obtaining I/0 event traces up to 16,000 compute processes on the Argonne Leadership Computing Facility system Intrepid, spanning application processes, file system clients, and file system servers. This was the first time that such correlated, multilayer data has been collected at this scale. The data continues to be analyzed and visualized by our project partners. Our work was also instrumental in improving the scalability of tracing workloads on the Oak Ridge Leadership Computing Facility system Jaguar up to 200,000 compute processes, in collaboration with Oak Ridge National Laboratory and TU Dresden, Germany. This achievement involved the introduction of an additional layer of staging nodes between the application and the file system, which could temporarily store the raw trace data and rearrange it in a more optimal fashion before writing it out to storage, while letting the application run undisturbed. Our improvements increased the scalability by an order of magnitude, setting a new benchmark on application tracing on high-end computing systems.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2012<br>\n\t\t\t\t\tModified by: Kamil&nbsp;A&nbsp;Iskra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSupercomputers have been described as machines that turn compute-bound problems into input/output-bound ones. Their massive&mdash;and rapidly increasing&mdash;computing subsystems demand equally capable input/output (I/O) infrastructure to supply them the input data and extract the results without undue delays. Unfortunately research into the performance analysis and optimization of I/O systems has lagged behind its computational counterparts. Yet, with the rapidly increasing complexity of new generations of high-end computing systems, we need improved methods to analyze the I/O performance, in order to identify bottlenecks and implement improvements that minimize the idle time caused by computing applications waiting for data transfers to complete. Our task in this project was to develop techniques for collecting the \"raw\" I/O performance data, which could then be studied using novel visual analysis techniques developed by our project partners from UC Davis.\n\nThe challenge of instrumenting in detail (or, as we call it, tracing) a large system is that it consists of many thousands (possibly soon millions) of individual components, arranged in interdependent, hierarchical layers.  In order to obtain a full view of the system behavior, all the relevant layers need to be instrumented and the activities at each layer be recorded with enough context that the progress of individual operations originating with the computing application can be followed through the layers all the way to the storage devices. Paradoxically, tracing the I/O subsystem can itself become a challenging I/O problem because of the large volumes of the trace data being generated. Yet, tracing must not put any significant additional load on the I/O system while the application is running, as it could then disturb the natural behavior of the system (the so-called observer effect). Meeting these conflicting goals requires careful planning and coordination.\n\nWe were successful in obtaining I/0 event traces up to 16,000 compute processes on the Argonne Leadership Computing Facility system Intrepid, spanning application processes, file system clients, and file system servers. This was the first time that such correlated, multilayer data has been collected at this scale. The data continues to be analyzed and visualized by our project partners. Our work was also instrumental in improving the scalability of tracing workloads on the Oak Ridge Leadership Computing Facility system Jaguar up to 200,000 compute processes, in collaboration with Oak Ridge National Laboratory and TU Dresden, Germany. This achievement involved the introduction of an additional layer of staging nodes between the application and the file system, which could temporarily store the raw trace data and rearrange it in a more optimal fashion before writing it out to storage, while letting the application run undisturbed. Our improvements increased the scalability by an order of magnitude, setting a new benchmark on application tracing on high-end computing systems.\n\n\t\t\t\t\tLast Modified: 10/29/2012\n\n\t\t\t\t\tSubmitted by: Kamil A Iskra"
 }
}