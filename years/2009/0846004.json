{
 "awd_id": "0846004",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Deterministic Shared Memory Multiprocessing: Vision, Architecture, and Impact on Programmability",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2009-03-01",
 "awd_exp_date": "2016-02-29",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 558938.0,
 "awd_min_amd_letter_date": "2009-02-13",
 "awd_max_amd_letter_date": "2013-06-05",
 "awd_abstract_narration": "Software innovation typically relies on performance improvements of\r\nthe underlying hardware. However, technology limitations hinder\r\nfurther significant progress in single-thread performance. Therefore,\r\nthe software industry has the immense problem of rethinking its\r\nsoftware development process and techniques to adopt multicore\r\nsystems. Popularizing parallel programming is a Grand Research\r\nChallenge for the systems community [CRA]. Being able to leverage the\r\nfull potential of multicores would put us back into exponential growth\r\nof usable performance as well as lead to significant power savings.\r\n\r\nOne of the main reasons why parallel programming is hard is that\r\nparallel code in current multicore systems can execute\r\nnondeterministically. Each time a multicore executes a parallel\r\napplication, it can produce a different output even if supplied with\r\nthe same input. This frustrates debugging efforts and limits the\r\nability to properly test parallel code, becoming a major obstacle to\r\nwidespread adoption of parallel programming. \r\n\r\nThis project poses broad intellectual questions with far-reaching\r\nimplications in modern computer systems: Can nondeterminism be removed\r\nfrom shared-memory multiprocessor systems without degrading\r\nperformance? What are the trade-offs in designing deterministic\r\nmultiprocessor systems?  What are the implications and uses of\r\ndeterministic behavior in programmability? The PI plans to answer these\r\nquestions by devising efficient, general purpose, fully deterministic\r\nshared memory multiprocessor systems and demonstrating that they can\r\nenable significant changes in how parallel programs are written,\r\ntested and deployed. An integral part of the concurrency challenge is\r\neducation, so this project also aims to develop a graduate and\r\nundergraduate curriculum that will teach students about concurrency\r\nprinciples and practical parallel programming.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Luis",
   "pi_last_name": "Ceze",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Luis Ceze",
   "pi_email_addr": "luisceze@cs.washington.edu",
   "nsf_id": "000083036",
   "pi_start_date": "2009-02-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 BROOKLYN AVE NE",
  "perf_city_name": "SEATTLE",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981951016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "732900",
   "pgm_ele_name": "COMPILERS"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "794100",
   "pgm_ele_name": "COMPUTER ARCHITECTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 179502.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 101282.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 88887.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 92658.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 96609.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">We developed techniques for deterministic multiprocessing. The key idea is to make arbitrary parallel programs execute deterministically. This dramatically simplifies debugging, testing, and replication of multithreaded programs. The non-determinism problem is at the heart of the programmability issues in current multicore systems.&nbsp; The bulk of the research was on computer architecture and compilers. We also worked on on language level techniques, operating system support, distributed systems, testing methodologies, and new static analysis techniques that further improve the utility of deterministic execution. This past year we have explored distributed systems and compilation ideas to exploit data locality, and symbolic execution techniques. We demonstrated that it is possible to explore storage class memories to execute a collection of processes (container) with continuous checkpointing of state at a low performance cost (~10%). Over the course of the entire project, we published over a dozen papers in major venues such as ASPLOS, MICRO, OOPSLA and Usenix ATC.</p>\n<p class=\"Default\">&nbsp;</p>\n<p>We have shown, contrary to popular belief in the field of parallel computer architecture, that it is possible to provide deterministic execution of arbitrary programs with little cost. We have also shown that it is possible to conveniently deal with fundamental external non-determinism. This settled some deep issues with how deterministic multiprocessing interacts with the external world. We have also shown that it is possible to deeply integrate determinism across the system stack, from language to compilers to testing systems to OS, hardware and even distributed systems. We then used the techniques developed to make distributed systems scale and to make concurrent programs safer. The project initiated collaborations with faculty in Programming Languages as well as Operating Systems and distributed systems. Some of the technology described was licensed by a startup company.&nbsp; Other groups have been using our released software to do follow-on research. This research has an impact in industry. We have heard interest from startup companies and established companies. Related work led to a startup company that created over a dozen jobs and&nbsp; also brought the benefits of this technology to the IT industry.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/20/2016<br>\n\t\t\t\t\tModified by: Luis&nbsp;Ceze</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "We developed techniques for deterministic multiprocessing. The key idea is to make arbitrary parallel programs execute deterministically. This dramatically simplifies debugging, testing, and replication of multithreaded programs. The non-determinism problem is at the heart of the programmability issues in current multicore systems.  The bulk of the research was on computer architecture and compilers. We also worked on on language level techniques, operating system support, distributed systems, testing methodologies, and new static analysis techniques that further improve the utility of deterministic execution. This past year we have explored distributed systems and compilation ideas to exploit data locality, and symbolic execution techniques. We demonstrated that it is possible to explore storage class memories to execute a collection of processes (container) with continuous checkpointing of state at a low performance cost (~10%). Over the course of the entire project, we published over a dozen papers in major venues such as ASPLOS, MICRO, OOPSLA and Usenix ATC.\n \n\nWe have shown, contrary to popular belief in the field of parallel computer architecture, that it is possible to provide deterministic execution of arbitrary programs with little cost. We have also shown that it is possible to conveniently deal with fundamental external non-determinism. This settled some deep issues with how deterministic multiprocessing interacts with the external world. We have also shown that it is possible to deeply integrate determinism across the system stack, from language to compilers to testing systems to OS, hardware and even distributed systems. We then used the techniques developed to make distributed systems scale and to make concurrent programs safer. The project initiated collaborations with faculty in Programming Languages as well as Operating Systems and distributed systems. Some of the technology described was licensed by a startup company.  Other groups have been using our released software to do follow-on research. This research has an impact in industry. We have heard interest from startup companies and established companies. Related work led to a startup company that created over a dozen jobs and  also brought the benefits of this technology to the IT industry.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/20/2016\n\n\t\t\t\t\tSubmitted by: Luis Ceze"
 }
}