{
 "awd_id": "0845469",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Enabling Independent Access to Digital Graphical Content for People with Visual Impairment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2009-03-01",
 "awd_exp_date": "2015-02-28",
 "tot_intn_awd_amt": 403987.0,
 "awd_amount": 451768.0,
 "awd_min_amd_letter_date": "2009-01-06",
 "awd_max_amd_letter_date": "2013-04-08",
 "awd_abstract_narration": "While sighted people readily enjoy the added value of the graphical content (digital images, maps, diagrams, etc.) that has become prevalent in the information age, visually impaired users can with the help of screen-reader software such as JAWS and Window-Eyes independently access only digital textual data, because such software cannot handle graphical information.  Typical procedures for manually producing tactile graphics by sighted professionals are generally time consuming and labor intensive, hence coverage is extremely limited; furthermore, there is no online and independent availability if the production has to be done by third-party professionals.  The PI's ultimate goal is to afford users with visual impairments independent real-time access to diverse types of graphical information, while taking into account often-overlooked issues such as privacy.  In this project, however, he will address one specific and challenging subproblem of practical significance: how to make digital graphical content more independently accessible to students in the science, technology, engineering, and mathematics (STEM) fields.  In addition to line-drawing graphics of a primarily binary nature, a specific type of continuous-tone image, human portraits, will also be considered, both due to the special value that facial images have in our social and emotional life, and also to demonstrate the potential of the PI's approach for handling more complex visual content.  The research activities planned to these ends include a focused study of the peculiarities of haptic perception in tactile exploration of graphics, development of visual processing techniques for automated visual-to-tactile conversion, a usability study to determine intuitive and efficient presentation and interface schemes for the target end-users, prototype development and evaluation.  Encouraging preliminary results have been obtained by the PI, demonstrating the feasibility and potential of the planned research and of the proposed methodologies.  This research will advance our knowledge in the domains of visual-tactile cognition, image understanding, human-computer interaction in general and assistive technologies for people with visual impairments in particular.  The PI?s institution provides an excellent and unique environment for performing this work.\r\n\r\nBroader Impacts:  This research will contribute to the research fields of visual-tactile cognition, automatic understanding of visual data for visual-to-tactile conversion, and development of assistive devices for the visually impaired.  It will have significant impact on society, by ultimately enabling independent access to a wide range of visual content by people who are blind.  More immediately, the project will help overcome the current barriers to the STEM fields that confront individuals with visual impairments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Baoxin",
   "pi_last_name": "Li",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Baoxin Li",
   "pi_email_addr": "Baoxin.Li@asu.edu",
   "nsf_id": "000474888",
   "pi_start_date": "2009-01-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "660 S MILL AVENUE STE 204",
  "perf_city_name": "TEMPE",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852813670",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 93827.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 92941.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 97174.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 82553.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 85273.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><pre>The significant results of the project are summarized below.</pre>\n<pre>&nbsp;</pre>\n<pre>First, our efforts demonstrated that it is possible to develop automatic or semi-automatic approaches to making graphics accessible. Such approaches may utilize tactile graphics, verbal descriptions, or in general multi-modal presentation schemes for conveying visual information in the original graphical contents.</pre>\n<pre>&nbsp;</pre>\n<pre>Second, in the course of this project, we developed several key enabling computational algorithms that form the foundation for further exploration of the problem or future development of practical systems. These include primarily (but not limited to):</pre>\n<pre> <br />\n(1) A framework for building a screen-reader-like system, a &ldquo;graphic-reader&rdquo;, which seamlessly runs in the background of a user&rsquo;s computer until any graphical information is detected in the user&rsquo;s current active window. At that moment, the system performs certain analysis tasks and then presents the results to the user via audio. One key idea first disclosed in this work is the semantic-aware feature extraction technique, by which feature extraction was modulated by assumed object identities.</pre>\n<p>&nbsp;</p>\n<p>(2) A novel approach to supporting a blind user&rsquo;s interactive exploration of a map via a touchpad made by ViewPlus. This is a multi-modal approach as textual/Braille, audio, and tactile modalities are all considered and utilized in developing the approach.</p>\n<p>&nbsp;</p>\n<p>(3) A novel Bayesian face alignment approach, which employs facial characteristics derived from anthropology and uses such data for constraining and improving computational face alignment algorithms.</p>\n<p>&nbsp;</p>\n<p>(4) A novel face representation scheme, where all face images of the same subject are modeled by a common component, a low-rank component, and sparse residuals. This effort relates to our Tactile Face sub-task, in which we evaluated the capability of a blind individual in distinguishing subjects via their face images and in associating different tactile face images of the same person.</p>\n<p>&nbsp;</p>\n<p>(5) The introduction of bias of tactile perception in tactile rendering. While being preliminary in nature, this serves to illustrate the importance of understanding the creation of tactile graphics not only as an image simplification task.</p>\n<p>&nbsp;</p>\n<p>(6) An indoor floor map interpretation system, by which a blind individual would obtain a holistic understanding of an indoor place listening to a verbal description of the major landmarks and their geospatial relations in the map.</p>\n<p>&nbsp;</p>\n<p>(7) An approach for assisting sighted transcribers in creating tactile graphics in STEM textbooks. The approach does not seek to automate the entire process, which is unlikely to be done. Instead, it aims at creating an initial drawing for the transcriber to work on.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Third, we spent significant effort on building prototype systems that were either built to facilitate the exploration of the problems (e.g., the multi-model tactile map) or designed to demonstrate how a practical system could be deployed by assembling our technological components (e.g., the Tactile Face demo system with live camera feed).</p>\n<p>&nbsp;</p>\n<p>Fourth, we acquired significant amount of data, assembled into several data sets, which are released publically for others to build on. In particular, we have a floor map dataset, a STEM graphics dataset, a collection of STEM books with manually created tactile graphics, and a test set of maps.</p>\n<p>&nbsp;</p>\n<p>Fifth, through this project, we established a set of design principles and guidelines for building automatic and semi-automatic approaches to improving accessibility of graphical contents. A specific example of this sort is the utilization of call-out buttons for declutter...",
  "por_txt_cntn": "The significant results of the project are summarized below.\n \nFirst, our efforts demonstrated that it is possible to develop automatic or semi-automatic approaches to making graphics accessible. Such approaches may utilize tactile graphics, verbal descriptions, or in general multi-modal presentation schemes for conveying visual information in the original graphical contents.\n \nSecond, in the course of this project, we developed several key enabling computational algorithms that form the foundation for further exploration of the problem or future development of practical systems. These include primarily (but not limited to):\n \n\n(1) A framework for building a screen-reader-like system, a \"graphic-reader\", which seamlessly runs in the background of a user\u00c6s computer until any graphical information is detected in the user\u00c6s current active window. At that moment, the system performs certain analysis tasks and then presents the results to the user via audio. One key idea first disclosed in this work is the semantic-aware feature extraction technique, by which feature extraction was modulated by assumed object identities.\n\n \n\n(2) A novel approach to supporting a blind user\u00c6s interactive exploration of a map via a touchpad made by ViewPlus. This is a multi-modal approach as textual/Braille, audio, and tactile modalities are all considered and utilized in developing the approach.\n\n \n\n(3) A novel Bayesian face alignment approach, which employs facial characteristics derived from anthropology and uses such data for constraining and improving computational face alignment algorithms.\n\n \n\n(4) A novel face representation scheme, where all face images of the same subject are modeled by a common component, a low-rank component, and sparse residuals. This effort relates to our Tactile Face sub-task, in which we evaluated the capability of a blind individual in distinguishing subjects via their face images and in associating different tactile face images of the same person.\n\n \n\n(5) The introduction of bias of tactile perception in tactile rendering. While being preliminary in nature, this serves to illustrate the importance of understanding the creation of tactile graphics not only as an image simplification task.\n\n \n\n(6) An indoor floor map interpretation system, by which a blind individual would obtain a holistic understanding of an indoor place listening to a verbal description of the major landmarks and their geospatial relations in the map.\n\n \n\n(7) An approach for assisting sighted transcribers in creating tactile graphics in STEM textbooks. The approach does not seek to automate the entire process, which is unlikely to be done. Instead, it aims at creating an initial drawing for the transcriber to work on.\n\n \n\n \n\nThird, we spent significant effort on building prototype systems that were either built to facilitate the exploration of the problems (e.g., the multi-model tactile map) or designed to demonstrate how a practical system could be deployed by assembling our technological components (e.g., the Tactile Face demo system with live camera feed).\n\n \n\nFourth, we acquired significant amount of data, assembled into several data sets, which are released publically for others to build on. In particular, we have a floor map dataset, a STEM graphics dataset, a collection of STEM books with manually created tactile graphics, and a test set of maps.\n\n \n\nFifth, through this project, we established a set of design principles and guidelines for building automatic and semi-automatic approaches to improving accessibility of graphical contents. A specific example of this sort is the utilization of call-out buttons for decluttering a busy map layout.\n\n \n\n\t\t\t\t\tLast Modified: 05/29/2015\n\n\t\t\t\t\tSubmitted by: Baoxin Li"
 }
}