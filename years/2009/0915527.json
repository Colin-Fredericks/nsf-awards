{
 "awd_id": "0915527",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC-Small: Displaying Prosodic Text for Reading Aloud with Expression",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2009-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 498407.0,
 "awd_amount": 498407.0,
 "awd_min_amd_letter_date": "2009-07-13",
 "awd_max_amd_letter_date": "2014-07-16",
 "awd_abstract_narration": "Reading aloud is a complex motor, perceptual, cognitive and linguistic feat that takes years to learn and master.  Text is problematic for developing readers because punctuation does not reliably mark phrase units or appropriate pause structure; commas do not always necessitate a pause, and question marks do not always necessitate rising intonation.  Young readers who are learning these conventions are left to decode the author's intended prosody by trial and error; even those who have accurate decoding skills often experience difficulty chunking text into meaningful units.  As a result, they read in a word-by-word manner with insufficient prosodic variation, which adversely impacts their ability to comprehend what they have read aloud.  Traditional reading instruction and software programs emphasize rapid, accurate decoding and word recognition; little or no emphasis is placed on facilitating expressive, prosodic oral reading.  Yet prosodic cues such as fundamental frequency F0 (perceived as pitch/intonation), intensity (perceived as loudness), and duration (perceived as length), convey a wide range of linguistic and affective functions that link the speech code to underlying semantic and syntactic content, which is crucial for language comprehension.  In this project, the PI will explore a number of innovations to enable developing readers to read aloud with expression.  She will design an interactive reading interface that displays prosodically varying text to help children read aloud fluently with appropriate expression.  Prosodic targets (F0 contour, intensity envelop, and word and pause duration) will be derived from recordings made by a fluent adult reader and translated into textual manipulations using novel semi-automated acoustic-to-graphic mappings.  The software will provide auditory and visual cues corresponding to the model adult production; near-real time visual and auditory feedback of the child's own production will enable self-monitoring to further support learning.  The resulting electronic media will resemble a children's book, displaying a story image along with the corresponding prosodic text, and will include additional listening and recording functions.  The software will be assessed using a repeated measures design, in which 32 children aged 6-8 years will read age and grade-level appropriate stories with and without the prosodic text.  The PI's hypothesis is that providing explicit visual cues pertaining to the underlying prosodic targets will improve oral reading fluency, including accuracy, rate, and expressiveness.  The additional cues may also provide the scaffolding to support comprehension of spoken text.  Efforts to scale the prosodic text rendering techniques to a larger set of spoken content will be undertaken.  Project outcomes will contribute to the fields of, digital signal processing, speech acoustics, speech and language development, reading acquisition, visual typography, and human-computer interaction.\r\n\r\nBroader Impacts:  The ultimate goal of this project is to inspire young readers to make the words on the page \"come alive\" through their expressive realization of the text.  The PI expects the tools and methodologies developed in this work will also be applicable to improving spoken prosody for non-native speakers, for individuals with speech impairments, and for those with learning disabilities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rupal",
   "pi_last_name": "Patel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rupal Patel",
   "pi_email_addr": "rupal@vocaliD.ai",
   "nsf_id": "000671566",
   "pi_start_date": "2009-07-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 HUNTINGTON AVE",
  "perf_city_name": "BOSTON",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 167194.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 331213.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Reading aloud is a motor, cognitive and linguistic feat that takes years to learn and master. Early readers must not only decode the grapheme (letter) to phoneme (sound unit) sequence, they must also integrate this decoded message with syntactic and semantic information and then coordinate their speech musculature to produce the correct sounds in sequence. Given the complexity of this task, it is not surprising that even once children have mastered decoding skills, they may continue to have difficulty reading aloud in a fluent, expressive manner. Approximately 40% of 4<sup>th</sup> graders in the US do not read fluently or with <em>proper expression</em>.<em> </em>Prosodic modifications such as variations in fundamental frequency (F0) (perceived as pitch), intensity (perceived as loudness), and duration (perceived as length) are associated with proper expression. The use of appropriate prosody is also essential for children to fully comprehend what they have read aloud. Traditional reading instruction and software programs emphasize accurate, rapid decoding and word recognition; however, little or no emphasis is placed on facilitating expressive, prosodic oral reading. This project addressed the need to improve oral reading fluency and comprehension through an interactive reading software program that provided readers with explicit visual prosodic cues.</p>\n<p>We designed and implemented an interactive software platform (ReadN&rsquo;Karaoke) to help young children read aloud with expression. We augmented written text with visual cues to speech prosody (variations in pitch, loudness and duration) to help readers segment and chunk words and phrases into meaningful units.</p>\n<p>Over the course of the project we designed various visual schemes to present prosodic cues (Patel &amp; Furr, 2011; Patel &amp; McNab, 2011; Patel,<strong> </strong>Kember &amp; Natale, 2014). Our findings showed that simple cues that could be presented in isolation and combined without distortion to the written text were most beneficial. Visual prosodic cues were generated semi-automatically using a fluent readrer&rsquo;s audio recordings. These recordings were manually annotated to demarcate the beginning and end of words and meaningful prosodic units. The software then generated the &nbsp;visual renderings as superimposed cues aligned to the written text. In this way, readers had cues on what <em>and</em> how to read. We also extended the approach to determine whether visual prosodic cues would improve reading and speaking fluency in English second language learners (Patel &amp; Kember, in review). In both groups we found that readers were able to easily learn the visual prosodic mappings and apply them in their reading with relative ease. Readers were not only more expressive as judged by acoustic analyses, but their productions were more easily understood by unfamiliar listeners suggesting that reading with prosodic cues helped readers to chunk phrases at meaningful junctures. Pitch and duration cues were the most helpful of the visual cues displayed both in isolation and in combination. In summary, visual prosodic cues enhanced reading fluency and listener comprehension.</p>\n<p>Currently, the Readn&rsquo;Karaoke software relies on manual annotation to seed visual cue rendering which is time intensive. To offer a scalable technology solution for new text requires increased automation of word boundary detection in fluent speech that can then be used to generate visual prosodic cues. Such a process would expand the scope of the application to any novel text that has accompanying fluent recordings. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/06/2015<br>\n\t\t\t\t\tModified by: Rupal&nbsp;Patel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nReading aloud is a motor, cognitive and linguistic feat that takes years to learn and master. Early readers must not only decode the grapheme (letter) to phoneme (sound unit) sequence, they must also integrate this decoded message with syntactic and semantic information and then coordinate their speech musculature to produce the correct sounds in sequence. Given the complexity of this task, it is not surprising that even once children have mastered decoding skills, they may continue to have difficulty reading aloud in a fluent, expressive manner. Approximately 40% of 4th graders in the US do not read fluently or with proper expression. Prosodic modifications such as variations in fundamental frequency (F0) (perceived as pitch), intensity (perceived as loudness), and duration (perceived as length) are associated with proper expression. The use of appropriate prosody is also essential for children to fully comprehend what they have read aloud. Traditional reading instruction and software programs emphasize accurate, rapid decoding and word recognition; however, little or no emphasis is placed on facilitating expressive, prosodic oral reading. This project addressed the need to improve oral reading fluency and comprehension through an interactive reading software program that provided readers with explicit visual prosodic cues.\n\nWe designed and implemented an interactive software platform (ReadN\u00c6Karaoke) to help young children read aloud with expression. We augmented written text with visual cues to speech prosody (variations in pitch, loudness and duration) to help readers segment and chunk words and phrases into meaningful units.\n\nOver the course of the project we designed various visual schemes to present prosodic cues (Patel &amp; Furr, 2011; Patel &amp; McNab, 2011; Patel, Kember &amp; Natale, 2014). Our findings showed that simple cues that could be presented in isolation and combined without distortion to the written text were most beneficial. Visual prosodic cues were generated semi-automatically using a fluent readrer\u00c6s audio recordings. These recordings were manually annotated to demarcate the beginning and end of words and meaningful prosodic units. The software then generated the  visual renderings as superimposed cues aligned to the written text. In this way, readers had cues on what and how to read. We also extended the approach to determine whether visual prosodic cues would improve reading and speaking fluency in English second language learners (Patel &amp; Kember, in review). In both groups we found that readers were able to easily learn the visual prosodic mappings and apply them in their reading with relative ease. Readers were not only more expressive as judged by acoustic analyses, but their productions were more easily understood by unfamiliar listeners suggesting that reading with prosodic cues helped readers to chunk phrases at meaningful junctures. Pitch and duration cues were the most helpful of the visual cues displayed both in isolation and in combination. In summary, visual prosodic cues enhanced reading fluency and listener comprehension.\n\nCurrently, the Readn\u00c6Karaoke software relies on manual annotation to seed visual cue rendering which is time intensive. To offer a scalable technology solution for new text requires increased automation of word boundary detection in fluent speech that can then be used to generate visual prosodic cues. Such a process would expand the scope of the application to any novel text that has accompanying fluent recordings.  \n\n \n\n\t\t\t\t\tLast Modified: 08/06/2015\n\n\t\t\t\t\tSubmitted by: Rupal Patel"
 }
}