{
 "awd_id": "0845643",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Architecting A Database Management System for Semantic Web Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frank Olken",
 "awd_eff_date": "2009-02-15",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2009-02-18",
 "awd_max_amd_letter_date": "2013-01-30",
 "awd_abstract_narration": "\r\nThe goal of the Semantic Web is to free Web data from the applications that control them, so that data can be easily described and exchanged. \r\nThis is accomplished by supplementing natural language and other data found on the Web with machine readable metadata in statement form (e.g., X is-a person, X has-name Joe, X has-age 35) and enabling descriptions of data ontologies so that data from different applications can be integrated through ontology mapping. One element of this vision is to turn the Web into a giant database, against which one can issue structured queries and receive structured answers in response.\r\n\r\nThe SW-Store project is undertaking the clean-slate design of a DBMS specifically architected for this type of Web metadata and the prevalent Semantic Web data model, the Resource Description Framework, or RDF. The management of Semantic Web data presents many difficult challenges. The size of the data is growing rapidly, and in theory could reach the scale of the Web. The types of queries vary greatly in complexity, ranging from keyword search to complicated parameterized subgraph matching. Data integration, inference, and reasoning must be primitive operations that can operate at scale without human intervention. A data management system must not only be a place where data is stored and from which data is accessed; it must use the machine-readable semantics of the data to develop higher level models and help guide a user through the mass of information. In sum, a data management system for the Semantic Web will look very different from a standard, transactional, relational database system.\r\n\r\nThe SW-store project researches the architecture of such a system. This research is inherently interdisciplinary, bringing in ideas from the data management, Semantic Web, and artificial intelligence communities. The project involves experimenting with partitioning schemes, where data is allocated to different nodes on a shared-nothing cluster so that queries can be run in parallel across multiple machines. It also involves exploring how ontology reasoning can be integrated inside the database system so that it can benefit from the near limitless scalability a shared-nothing cluster can offer. SW-Store further investigates providing iterative query interfaces and integrating complex queries with text search. Finally, the project involves studying the design of the storage layer for a Semantic Web data management system, looking at how data should be laid out, updates should be performed, and what materialized views to create.\r\n\r\nFurther information about the project can be found at the project Webpage: \r\nhttp://db.cs.yale.edu/swstore/\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Abadi",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel J Abadi",
   "pi_email_addr": "abadi@umd.edu",
   "nsf_id": "000508003",
   "pi_start_date": "2009-02-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "150 MUNSON ST",
  "perf_city_name": "NEW HAVEN",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065113572",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 75587.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 68026.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 102891.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 111774.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 41722.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High level summary of the project outcomes: This award lead to development of a database system designed specifically for graph data. The original use-case for our graph-store was Semantic-Web data, which is naturally represented using the Resource Description Framework as a graph connecting resources and data on the Web. However, we found that our system is also generally applicable to any type of graph, and successfully applied it to other graph applications including social websites such as Twitter and LinkedIn.</p>\n<p>The primary outcomes of the project in terms of <strong>intellectual merit</strong> include the following:</p>\n<p>(1) We found that a shared-nothing architecture that leverages state-of-the-art RDF-stores and the popular Apache Hadoop framework is capable of scaling RDF data management to march larger deployments than have possible in the past, and can dramatically improve performance. In particular, it is very helpful to use a graph-oriented data partitioning scheme to exploit the spatial locality inherent in graph pattern matching, and combine this with a careful replication of RDF triples whose location is near a partitioning border. By pushing most or even all of query processing into fast single-node RDF stores which operate in parallel, a Semantic Web database built in this way is able to perform up to <strong>three orders of magnitude faster</strong> than other attempts at horizontally scalable RDF data management.</p>\n<p>(2) We developed several algorithms for performing distributed query optimization of SPARQL and subgraph matching queries. We found that greedy algorithms for SPARQL/subgraph pattern matching operations are often sufficient when the graph data set can be held in memory on a single machine. However, as graph data sets increasingly expand and require external storage and partitioning across a cluster of machines, more sophisticated query optimization techniques become critical to avoid explosions in query latency. Of the algorithms we developed for distributed query optimization of SPARQL/subgraph matching, each algorithm performed well on specific (and different) input graphs and query requests. However, our experimental results showed that these algorithms can be combined in the same system, and the best algorithm selected for a given query (and input data) graph. When combined, these algorithms can lead to an order of magnitude improvement in query performance of SPARQL and other graph matching queries.</p>\n<p>(3) We invented an invisible loading technique that enables data to be loaded into our system without the user experiencing any overhead. &nbsp;This technique alleviates the high \"time-to-first-analysis\" problem this is suffered by most analytical database systems. &nbsp;The \"time-to-first-analysis\" problem describes the phenomenon that before data can be processed, it must be modeled and schematized (a human effort), transferred into the database's storage layer, and optionally clustered and indexed (a computational effort). This upfront effort is so significant that in many cases, the user never bothers to load the data into the system, and instead keeps the data on a file system and analyzes it using user-created scripts and mini-programs. The invisible loading technique that was developed as part of this project achieves the immediate gratification of running processing jobs directly over a file system, while still making progress towards the long-term performance benefits of loading data into an analytical system such as the one that was built as part of this project. The basic idea is to piggyback on top of user-defined scripts or Hadoop jobs, and leverage their parsing and tuple extraction operations to incrementally load and organize tuples into our system, while simultaneously processing the file system data. We found that this allows partitions of the data to be loaded into our system at almost no marginal...",
  "por_txt_cntn": "\nHigh level summary of the project outcomes: This award lead to development of a database system designed specifically for graph data. The original use-case for our graph-store was Semantic-Web data, which is naturally represented using the Resource Description Framework as a graph connecting resources and data on the Web. However, we found that our system is also generally applicable to any type of graph, and successfully applied it to other graph applications including social websites such as Twitter and LinkedIn.\n\nThe primary outcomes of the project in terms of intellectual merit include the following:\n\n(1) We found that a shared-nothing architecture that leverages state-of-the-art RDF-stores and the popular Apache Hadoop framework is capable of scaling RDF data management to march larger deployments than have possible in the past, and can dramatically improve performance. In particular, it is very helpful to use a graph-oriented data partitioning scheme to exploit the spatial locality inherent in graph pattern matching, and combine this with a careful replication of RDF triples whose location is near a partitioning border. By pushing most or even all of query processing into fast single-node RDF stores which operate in parallel, a Semantic Web database built in this way is able to perform up to three orders of magnitude faster than other attempts at horizontally scalable RDF data management.\n\n(2) We developed several algorithms for performing distributed query optimization of SPARQL and subgraph matching queries. We found that greedy algorithms for SPARQL/subgraph pattern matching operations are often sufficient when the graph data set can be held in memory on a single machine. However, as graph data sets increasingly expand and require external storage and partitioning across a cluster of machines, more sophisticated query optimization techniques become critical to avoid explosions in query latency. Of the algorithms we developed for distributed query optimization of SPARQL/subgraph matching, each algorithm performed well on specific (and different) input graphs and query requests. However, our experimental results showed that these algorithms can be combined in the same system, and the best algorithm selected for a given query (and input data) graph. When combined, these algorithms can lead to an order of magnitude improvement in query performance of SPARQL and other graph matching queries.\n\n(3) We invented an invisible loading technique that enables data to be loaded into our system without the user experiencing any overhead.  This technique alleviates the high \"time-to-first-analysis\" problem this is suffered by most analytical database systems.  The \"time-to-first-analysis\" problem describes the phenomenon that before data can be processed, it must be modeled and schematized (a human effort), transferred into the database's storage layer, and optionally clustered and indexed (a computational effort). This upfront effort is so significant that in many cases, the user never bothers to load the data into the system, and instead keeps the data on a file system and analyzes it using user-created scripts and mini-programs. The invisible loading technique that was developed as part of this project achieves the immediate gratification of running processing jobs directly over a file system, while still making progress towards the long-term performance benefits of loading data into an analytical system such as the one that was built as part of this project. The basic idea is to piggyback on top of user-defined scripts or Hadoop jobs, and leverage their parsing and tuple extraction operations to incrementally load and organize tuples into our system, while simultaneously processing the file system data. We found that this allows partitions of the data to be loaded into our system at almost no marginal cost in query latency. Furthermore, future queries are able to run much faster since part of the query can be redirected away fro..."
 }
}