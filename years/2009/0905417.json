{
 "awd_id": "0905417",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC: Medium: Collaborative Research: Generating Effective Dynamic Explanations in Augmented Reality",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 396404.0,
 "awd_amount": 396404.0,
 "awd_min_amd_letter_date": "2009-08-06",
 "awd_max_amd_letter_date": "2012-08-01",
 "awd_abstract_narration": "To survive and flourish, people must interact with their environment in an organized fashion. To do so, they need to learn, imagine, and perform an assortment of transformations on and in the world. Primary among these are manipulation of objects and navigation in space. This project integrates research in computer science and cognitive science to develop and evaluate augmented reality tools to create effective dynamic explanations that enhance manipulation and navigation, in conjunction with identification and visualization. Augmented reality refers to user interfaces in which virtual material is integrated with and overlaid on the user?s experience of the real world; for example, by using tracked head-worn and hand-held displays. Dynamic explanations are task-appropriate sequences of actions, presented interactively, with appropriate added information. The tools will be created in collaboration with subject matter experts for exploratory use in indoor and outdoor real world domains: navigating and identifying landmarks in a wooded park area, assembling a piece of furniture, and navigating and visualizing for planning the site of a new urban campus. Cognitive science research will determine the best ways to convey explanations and information to people. Computer science research will address the design and implementation of systems that embody the best candidate approaches for identifying objects and locations, specifying actions, and adding non-visible information. In situ experiments will be used to assess and refine the systems. \r\n\r\nManipulation, navigation, identification, and visualization are representative of important things that people do every day, ranging from fixing broken equipment to reaching a desired destination in an unfamiliar environment. The ways in which we perform these tasks could potentially be improved significantly through augmented reality systems designed using the principles to be developed by this project. Both the cognitive principles and the augmented reality tools will have broad applicability. The systems developed will inform the design of future systems that can aid the general public, for educational and recreational ends, as well as systems that can assist people with auditory, visual, or physical impairments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Barbara",
   "pi_last_name": "Tversky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Barbara Tversky",
   "pi_email_addr": "btversky@stanford.edu",
   "nsf_id": "000072811",
   "pi_start_date": "2009-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Teachers College, Columbia University",
  "inst_street_address": "525 W 120TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2126783000",
  "inst_zip_code": "100276605",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "TEACHERS COLLEGE COLUMBIA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "DBM1C8MDJ5L3"
 },
 "perf_inst": {
  "perf_inst_name": "Teachers College, Columbia University",
  "perf_str_addr": "525 W 120TH ST",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276605",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 75578.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 103205.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 106892.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 110729.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In their daily and professional lives, people need to accomplish a multitude of tasks. Many involve following explanations, to navigate from one place to another in the world or on a website, to put something together, to understand a phenomenon, to follow a recipe, to operate equipment. In order to accomplish these tasks, people need to understand each step, to find the relevant objects to act on, and to figure out how to act. Because most explanations are in language, because the world presents both relevant and irrelevant information, following explanations can be difficult. Often these tasks entail going back and forth from the explanations to the world, which is in itself difficult and error-prone. These tasks can be facilitated by superimposing information onto the viewer&rsquo;s field of view, using augmented reality. The augmented information can filter the relevant information from the irrelevant, focusing attention on the relevant, it can segment the explanation into natural steps, and it can show user&rsquo;s how to perform the critical actions or how they are performed. <br /><br />Developing effective tools for augmenting dynamic explanations requires integrating research on human perception, cognition, and action with development of technology. Jointly our labs have that expertise and have collaborated both to develop a set of tools that have proved to be effective, are firmly based in human cognition, and have and revealed new knowledge about human perception, cognition, action and embodied cognition. <br /><br />The tools we have developed that have proved to be effective for understanding and enacting dynamic explanations have value in themselves, but also provide general cognitive principles for developing similar tools for other situations. The effective tools depend on augmented reality and have their foundation in embodied cognition. For example, to enable people to find a small part in a large complex piece of machinery,&nbsp; we found that guiding movements of the body, head, and eyes directly by providing a virtual tunnel with continuous feedback was superior to traditional methods that have people keep parameters on dials aligned. To enable people to perform complex 3D actions directly, we found that virtual demonstrations of the actions that need to be performed was superior to traditional methods of giving people parameters on dials to align. Parallel basic research on human perception, cognition, and action has been important in itself as well as laying the foundation for future developments in augmented reality and human-computer interaction. For example, on the perception side, we are finding that clear, simple, direct visualizations are important for routine action and understanding, but sketchier, more ambiguous visualizations are productive for sense-making and creativity. On the cognition side, we have found increased inference, insight, and creativity from mindsets that emphasize top-down conceptual thinking and human-centric perspective-taking than from mindsets that focus in perceptual reorganization and mind-wandering. On all sides, perception, cognition, and action, we have found that both making and seeing representational gestures increase comprehension, memory, and problem-solving both in those who make the gestures and in those who see them. <br /><br />This work has been disseminated in a wide range of disciplinary and interdisciplinary workshops and conferences by both PIs and their students. In addition, both PIs have been actively interacting with researchers in computer science, psychology, philosophy, art, design, education, domain sciences, linguistics and more across the globe, as well as with technology companies, with educational institutions, and with art and science museums. <br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/28/2015<br>\n\t\t\t\t\tModified by: Barbara&nbsp;Tversky</p>\n</div>\n<div class=\"porSideCol...",
  "por_txt_cntn": "\nIn their daily and professional lives, people need to accomplish a multitude of tasks. Many involve following explanations, to navigate from one place to another in the world or on a website, to put something together, to understand a phenomenon, to follow a recipe, to operate equipment. In order to accomplish these tasks, people need to understand each step, to find the relevant objects to act on, and to figure out how to act. Because most explanations are in language, because the world presents both relevant and irrelevant information, following explanations can be difficult. Often these tasks entail going back and forth from the explanations to the world, which is in itself difficult and error-prone. These tasks can be facilitated by superimposing information onto the viewer\u00c6s field of view, using augmented reality. The augmented information can filter the relevant information from the irrelevant, focusing attention on the relevant, it can segment the explanation into natural steps, and it can show user\u00c6s how to perform the critical actions or how they are performed. \n\nDeveloping effective tools for augmenting dynamic explanations requires integrating research on human perception, cognition, and action with development of technology. Jointly our labs have that expertise and have collaborated both to develop a set of tools that have proved to be effective, are firmly based in human cognition, and have and revealed new knowledge about human perception, cognition, action and embodied cognition. \n\nThe tools we have developed that have proved to be effective for understanding and enacting dynamic explanations have value in themselves, but also provide general cognitive principles for developing similar tools for other situations. The effective tools depend on augmented reality and have their foundation in embodied cognition. For example, to enable people to find a small part in a large complex piece of machinery,  we found that guiding movements of the body, head, and eyes directly by providing a virtual tunnel with continuous feedback was superior to traditional methods that have people keep parameters on dials aligned. To enable people to perform complex 3D actions directly, we found that virtual demonstrations of the actions that need to be performed was superior to traditional methods of giving people parameters on dials to align. Parallel basic research on human perception, cognition, and action has been important in itself as well as laying the foundation for future developments in augmented reality and human-computer interaction. For example, on the perception side, we are finding that clear, simple, direct visualizations are important for routine action and understanding, but sketchier, more ambiguous visualizations are productive for sense-making and creativity. On the cognition side, we have found increased inference, insight, and creativity from mindsets that emphasize top-down conceptual thinking and human-centric perspective-taking than from mindsets that focus in perceptual reorganization and mind-wandering. On all sides, perception, cognition, and action, we have found that both making and seeing representational gestures increase comprehension, memory, and problem-solving both in those who make the gestures and in those who see them. \n\nThis work has been disseminated in a wide range of disciplinary and interdisciplinary workshops and conferences by both PIs and their students. In addition, both PIs have been actively interacting with researchers in computer science, psychology, philosophy, art, design, education, domain sciences, linguistics and more across the globe, as well as with technology companies, with educational institutions, and with art and science museums. \n\n\n\n\t\t\t\t\tLast Modified: 09/28/2015\n\n\t\t\t\t\tSubmitted by: Barbara Tversky"
 }
}