{
 "awd_id": "0931474",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Medium: Learning to Sense Robustly and Act Effectively",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 1450731.0,
 "awd_amount": 1466731.0,
 "awd_min_amd_letter_date": "2009-09-16",
 "awd_max_amd_letter_date": "2010-06-23",
 "awd_abstract_narration": "The physical environment of a cyber-physical system is unboundedly complex, changing continuously in time and space. An embodied cyber-physical system, embedded in the physical world, will receive a high bandwidth stream of sensory information, and may have multiple effectors with continuous control signals. In addition to dynamic change in the world, the properties of the cyber-physical system itself ? its sensors and effectors ? change over time. How can it cope with this complexity? The hypothesis behind this proposal is that a successful cyber-physical system will need to be a learning agent, learning the properties of its sensors, effectors, and environment from its own experience, and adapting over time. Inspired by human developmental learning, the assertion is that foundational concepts such as Space, Object, Action, etc., are essential for such a learning agent to abstract and control the complexity of its world. To bridge the gap between continuous interaction with the physical environment, and discrete symbolic descriptions that support effective planning, the agent will need multiple representations for these foundational domains, linked by abstraction relations.  To achieve this, the team is developing the Object Semantic Hierarchy (OSH), which shows how a learning agent can create a hierarchy of representations for objects it interacts with. The OSH shows how the ?object abstraction? factors the uncertainty in the sensor stream into object models and object trajectories. These object models then support the creation of action models, abstracting from low-level motor signals. To ensure generality across cyber-physical systems, these methods make only very generic assumptions about the nature of the sensors, effectors, and environment. However, to provide a physical test bed for rapid evaluation and refinement of our methods, the team has designed a model laboratory robotic system to be built from off-the-shelf components, including a stereo camera, a pan-tilt-translate base, and a manipulator arm. For dissemination and replication of  research results, the core system will be affordable and easily duplicated at other labs. There are plans to distribute the plans, the control software, and the software for experiments, to encourage other labs to replicate and extend the work. The same system will serve as a platform for an open-ended set of undergraduate laboratory tasks, ranging from classroom exercises, to term projects, to independent study projects. There is a preliminary design for a very inexpensive version of the model cyberphysical system that can be constructed from servo motors and pan-tilt webcams, for use in collaborating high schools and middle schools, to communicate the breadth and excitement of STEM research.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Kuipers",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin J Kuipers",
   "pi_email_addr": "kuipers@umich.edu",
   "nsf_id": "000324244",
   "pi_start_date": "2009-09-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Silvio",
   "pi_last_name": "Savarese",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Silvio Savarese",
   "pi_email_addr": "ssilvio@stanford.edu",
   "nsf_id": "000489619",
   "pi_start_date": "2009-09-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "Regents of the University of Michigan - Ann Arbor",
  "perf_str_addr": "1109 GEDDES AVE STE 3300",
  "perf_city_name": "ANN ARBOR",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091015",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 1450731.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A cyber-physical system exists in a physical environment that is unboundedly complex, changing continuously in time and space. &nbsp;The system is embedded in the dynamically changing physical world; it receives a high-bandwidth stream of sensory information; and it may have multiple effectors with continuous control signals. &nbsp;Our hypothesis is that, for a cyber-physical system to cope successfully with this level of complexity, it should be a learning agent, learning the properties of its sensors, effectors, and environment from its own experience, and adapting over time.&nbsp;</p>\n<p>To bridge the gap between continuous interaction with the physical environment, and discrete symbolic descriptions that support effective planning, the agent will need multiple representations for foundational domains such as Space, Objects, Actions, etc., linked by abstraction relations.&nbsp; We built on our previous successful work in representing knowledge of the spatial environment in the Spatial Semantic Hierarchy.</p>\n<p>This project supported work on sensory perception, especially vision, on learning useful conceptual organization on perceived categories of objects, and on effective and robust means of taking action in a dynamic and incompletely known environment.</p>\n<p>Grace Tsai, a doctoral student in the UM ECE department, developed methods for using robot vision to build a concise plane-based abstraction of the structure of the static background environment from a stream of visual observations during travel.&nbsp; Her method used local visual features to generate planar structure hypotheses, and then used Bayesian inference to converge rapidly to the correct hypothesis.&nbsp; Over a series of publications, she extended her original method to handle newly-revealed portions of the environment during travel, to use the set of active hypotheses to focus attention on features in the most informative regions of the image, and to separate clutter (objects not well-described by planar surfaces) from the plane-based model of the environment.&nbsp; She received her PhD in Fall 2014.</p>\n<p>Yu Xiang, a doctoral student in the UM ECE department, has developed methods for describing 3D foreground objects in terms of oriented planar surfaces (&ldquo;aspects&rdquo;), and for detecting and estimating the 3D poses of multiple objects from multiple images.&nbsp; By exploiting the constraint that the appearance of an object should be consistent when observed from different viewpoints, his method allows an object to be recognized in spite of occlusions, including self-occlusions, from the appearances of its visible parts.&nbsp; Thorough evaluation demonstrates his method to be more accurate than previous methods.</p>\n<p>Dr. Jingen Liu, a post-doctoral fellow who worked on this project, focused on attribute-based methods for human action recognition from video input.&nbsp; He developed a framework for selecting effective attributes for discriminating among viewed actions.&nbsp; He also developed and evaluated methods inspired by bilingual dictionaries for combining view-dependent features (&ldquo;visual words&rdquo;) to build consistent models of human activities across multiple views.&nbsp; This resulted in two highly-successful papers at a leading conference.&nbsp; Dr. Liu is now working at the SRI Sarnoff Research Lab.</p>\n<p>Dr. Roni Mittelman, a post-doc in Computer Science &amp; Engineering at UM, worked on Bayesian methods for learning the hierarchical conceptual structure of a collection of images.&nbsp; An uninformed agent is exposed to experience of the world in the form of a stream of instances, sensed as low-level sensory inputs (e.g., pixels, phonemes, individual words, etc.). &nbsp;To structure this experience, the agent must learn two different but interdependent higher-level structures, both of which are potentially infinite-dimensional.&nbsp; The first step uses Restric...",
  "por_txt_cntn": "\nA cyber-physical system exists in a physical environment that is unboundedly complex, changing continuously in time and space.  The system is embedded in the dynamically changing physical world; it receives a high-bandwidth stream of sensory information; and it may have multiple effectors with continuous control signals.  Our hypothesis is that, for a cyber-physical system to cope successfully with this level of complexity, it should be a learning agent, learning the properties of its sensors, effectors, and environment from its own experience, and adapting over time. \n\nTo bridge the gap between continuous interaction with the physical environment, and discrete symbolic descriptions that support effective planning, the agent will need multiple representations for foundational domains such as Space, Objects, Actions, etc., linked by abstraction relations.  We built on our previous successful work in representing knowledge of the spatial environment in the Spatial Semantic Hierarchy.\n\nThis project supported work on sensory perception, especially vision, on learning useful conceptual organization on perceived categories of objects, and on effective and robust means of taking action in a dynamic and incompletely known environment.\n\nGrace Tsai, a doctoral student in the UM ECE department, developed methods for using robot vision to build a concise plane-based abstraction of the structure of the static background environment from a stream of visual observations during travel.  Her method used local visual features to generate planar structure hypotheses, and then used Bayesian inference to converge rapidly to the correct hypothesis.  Over a series of publications, she extended her original method to handle newly-revealed portions of the environment during travel, to use the set of active hypotheses to focus attention on features in the most informative regions of the image, and to separate clutter (objects not well-described by planar surfaces) from the plane-based model of the environment.  She received her PhD in Fall 2014.\n\nYu Xiang, a doctoral student in the UM ECE department, has developed methods for describing 3D foreground objects in terms of oriented planar surfaces (\"aspects\"), and for detecting and estimating the 3D poses of multiple objects from multiple images.  By exploiting the constraint that the appearance of an object should be consistent when observed from different viewpoints, his method allows an object to be recognized in spite of occlusions, including self-occlusions, from the appearances of its visible parts.  Thorough evaluation demonstrates his method to be more accurate than previous methods.\n\nDr. Jingen Liu, a post-doctoral fellow who worked on this project, focused on attribute-based methods for human action recognition from video input.  He developed a framework for selecting effective attributes for discriminating among viewed actions.  He also developed and evaluated methods inspired by bilingual dictionaries for combining view-dependent features (\"visual words\") to build consistent models of human activities across multiple views.  This resulted in two highly-successful papers at a leading conference.  Dr. Liu is now working at the SRI Sarnoff Research Lab.\n\nDr. Roni Mittelman, a post-doc in Computer Science &amp; Engineering at UM, worked on Bayesian methods for learning the hierarchical conceptual structure of a collection of images.  An uninformed agent is exposed to experience of the world in the form of a stream of instances, sensed as low-level sensory inputs (e.g., pixels, phonemes, individual words, etc.).  To structure this experience, the agent must learn two different but interdependent higher-level structures, both of which are potentially infinite-dimensional.  The first step uses Restricted Boltzmann Machines to learn semantically meaningful attributes from low-level sensory input.  The second takes the resulting attribute vector and learns the most likely hierarchy of categories using..."
 }
}