{
 "awd_id": "0926196",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Using Neuroimaging to Test Models of Speech Motor Control",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Akaysha Tang",
 "awd_eff_date": "2009-10-01",
 "awd_exp_date": "2013-09-30",
 "tot_intn_awd_amt": 699999.0,
 "awd_amount": 699999.0,
 "awd_min_amd_letter_date": "2009-09-24",
 "awd_max_amd_letter_date": "2011-07-12",
 "awd_abstract_narration": "To speak so a listener understands, the speaker has to accurately produce the  sounds from his or her language. While this may seem effortless for most people, the actual speech production involves complex mental and physical processes involving the activation of speech muscles and the precisely timed movements in the vocal tract (e.g., the combination of movements in the mouth, jaw, and so on). The unique properties of the individual's speech organs (e.g., size of mouth), combined with the developmental changes of these properties over a lifetime, will directly influence the way speech sounds are produced by each individual. How does the human brain accomplish this feat of continually tuning the control of vocal tract so that it always produces the sounds desired? With support from the National Science Foundation, the investigators will study how the speaking process involves the brain predicting the sensory feedback and correcting the control of the vocal tract when the feedback does not match the prediction. While previous research suggests that this prediction and correction process does occur during speaking, there is little information about how the circuitry in the brain would accomplish such a process. In the proposed research the investigators will examine the timecourse of neural responses to audio feedback perturbations (brief changes in pitch, amplitude, or formant frequencies) during speaking. They will use magnetoencephalography (MEG) and electrocorticography (ECOG) methods to record normal individuals and epilepsy patients who have electrodes implanted in their brain to localize seizures. Both methods allow neural activity in the brain to be recorded at a millisecond time resolution.\r\n\r\nThe results of these experiments will allow for the testing of different models that have been proposed to explain the neural substrate of speech motor control. The outcome of the research will facilitate relating the control of speaking to what is known in other domains of motor control research, and lead to a more complete understanding of the control of movements in humans. The use of advanced functional neuroimaging to study the neural basis of speaking  will provide a special opportunity to train and educate a wide range of graduate students, post-doctoral trainees, and medical students who will get involved in the research. The proposed research will also further the development of multi-user research facilities, especially at the UCSF Biomagnetic Imaging Laboratory that has one of a limited number of MEG scanner facilities in the US.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Houde",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "John F Houde",
   "pi_email_addr": "houde@phy.ucsf.edu",
   "nsf_id": "000164988",
   "pi_start_date": "2009-09-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Srikantan",
   "pi_last_name": "Nagarajan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Srikantan Nagarajan",
   "pi_email_addr": "sri@ucsf.edu",
   "nsf_id": "000350328",
   "pi_start_date": "2009-09-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Francisco",
  "inst_street_address": "1855 FOLSOM ST STE 425",
  "inst_street_address_2": "",
  "inst_city_name": "SAN FRANCISCO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4154762977",
  "inst_zip_code": "941034249",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "CA11",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, SAN FRANCISCO, THE",
  "org_prnt_uei_num": "KMH5K9V7S518",
  "org_uei_num": "KMH5K9V7S518"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Francisco",
  "perf_str_addr": "1855 FOLSOM ST STE 425",
  "perf_city_name": "SAN FRANCISCO",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "941034249",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "CA11",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 233332.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 233334.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 233333.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>How do we learn to speak? A big part of the answer is that we listen to ourselves.&nbsp; We need to hear we need to hear the sounds that we produce, known as auditory feedback, when we first learn to speak. We know this because children born deaf don&rsquo;t learn to speak unless their hearing is restored with cochlear implants. But even after we learn to speak, we need auditory feedback to maintain our ability to speak. If speakers become deaf, the pitch and loudness of their speech degrades almost immediately, and the intelligibility of their speech begins to decline. Auditory feedback, therefore, is critically important for speaking, and so if we wish to understand how the brain controls speaking, we must understand how it processes auditory feedback.</p>\n<p>Our lab has developed a model of how the brain processes auditory feedback during speaking. This model, called the state feedback control (SFC) model, posits that as we speak, our brain generates predictions of the sounds we expect to hear given what we&rsquo;re intending to say, and that auditory feedback is compared with this prediction. If auditory feedback deviates from these expectations, the resulting error drives changes in our speaking that attempt to cancel the error between the auditory feedback and its predictions.</p>\n<p>In this grant, we tested our SFC model of how the brain controls speaking by determining where auditory feedback is processed in the brain.&nbsp; In several experiments, we had subjects produce a long, drawn out &ldquo;ah&rdquo; while wear headphones and a microphone. The experimental setup allowed us to intercept the speech at the microphone, pass it through a computer that altered how the speech sounded, and return the altered auditory feedback to the subject via the headphones in real-time.&nbsp;</p>\n<p>In the main experiments of this grant, the computer altered the auditory feedback by briefly <em>perturbing</em> it: &nbsp;it suddenly raised or lowered the pitch of the speech. This external perturbation caused the subject to change his/her production in a way that opposed the feedback perturbation &ndash; i.e. the subject <em>compensated</em> for the feedback perturbation. Using this pitch perturbation procedure, we were able to clearly show auditory feedback playing a role in speech production. When auditory feedback was perturbed, this generated a rapid sequence of neural events in the brain leading to compensatory change in speaking.</p>\n<p>We examined these neural events using two very different methods. First, we used a technique called electrocorticography (ECoG), which is the recording electric potentials directly on the brain&rsquo;s surface. These experiments were conducted in epilepsy patients who had electrode grids implanted in their brains for clinical purposes. These patients willingly participated in our pitch perturbation studies, allowing us to examine the brain activity during speech feedback compensation. Second, we used a technique called magnetoencephalography (MEG), which is the non-invasive recording of the brain&rsquo;s magnetic fields. We also conducted pitch perturbation experiments during MEG in healthy adult subjects, which allowed us to examine activity arising from the whole brain during speech compensation.</p>\n<p>Taken together, the ECoG and MEG recordings showed that auditory cortex (1) detected the feedback perturbations and then (2) signaled areas of motor cortex to compensate, and finally (3) received an updated prediction of what the auditory feedback would now sound like after compensation. These results supported the correctness of our SFC model of how the brain produces speech. But the results also yielded some additional unexpected and exciting findings. The MEG data showed that the right hemisphere appears to play an even larger role than the left in detecting feedback perturbations and generating compensations. We also found that the...",
  "por_txt_cntn": "\nHow do we learn to speak? A big part of the answer is that we listen to ourselves.  We need to hear we need to hear the sounds that we produce, known as auditory feedback, when we first learn to speak. We know this because children born deaf don\u00c6t learn to speak unless their hearing is restored with cochlear implants. But even after we learn to speak, we need auditory feedback to maintain our ability to speak. If speakers become deaf, the pitch and loudness of their speech degrades almost immediately, and the intelligibility of their speech begins to decline. Auditory feedback, therefore, is critically important for speaking, and so if we wish to understand how the brain controls speaking, we must understand how it processes auditory feedback.\n\nOur lab has developed a model of how the brain processes auditory feedback during speaking. This model, called the state feedback control (SFC) model, posits that as we speak, our brain generates predictions of the sounds we expect to hear given what we\u00c6re intending to say, and that auditory feedback is compared with this prediction. If auditory feedback deviates from these expectations, the resulting error drives changes in our speaking that attempt to cancel the error between the auditory feedback and its predictions.\n\nIn this grant, we tested our SFC model of how the brain controls speaking by determining where auditory feedback is processed in the brain.  In several experiments, we had subjects produce a long, drawn out \"ah\" while wear headphones and a microphone. The experimental setup allowed us to intercept the speech at the microphone, pass it through a computer that altered how the speech sounded, and return the altered auditory feedback to the subject via the headphones in real-time. \n\nIn the main experiments of this grant, the computer altered the auditory feedback by briefly perturbing it:  it suddenly raised or lowered the pitch of the speech. This external perturbation caused the subject to change his/her production in a way that opposed the feedback perturbation &ndash; i.e. the subject compensated for the feedback perturbation. Using this pitch perturbation procedure, we were able to clearly show auditory feedback playing a role in speech production. When auditory feedback was perturbed, this generated a rapid sequence of neural events in the brain leading to compensatory change in speaking.\n\nWe examined these neural events using two very different methods. First, we used a technique called electrocorticography (ECoG), which is the recording electric potentials directly on the brain\u00c6s surface. These experiments were conducted in epilepsy patients who had electrode grids implanted in their brains for clinical purposes. These patients willingly participated in our pitch perturbation studies, allowing us to examine the brain activity during speech feedback compensation. Second, we used a technique called magnetoencephalography (MEG), which is the non-invasive recording of the brain\u00c6s magnetic fields. We also conducted pitch perturbation experiments during MEG in healthy adult subjects, which allowed us to examine activity arising from the whole brain during speech compensation.\n\nTaken together, the ECoG and MEG recordings showed that auditory cortex (1) detected the feedback perturbations and then (2) signaled areas of motor cortex to compensate, and finally (3) received an updated prediction of what the auditory feedback would now sound like after compensation. These results supported the correctness of our SFC model of how the brain produces speech. But the results also yielded some additional unexpected and exciting findings. The MEG data showed that the right hemisphere appears to play an even larger role than the left in detecting feedback perturbations and generating compensations. We also found that the left and right cortical areas communicated quite frequently with each other in the time leading up to compensation. Finally and importantly, we also found the neural..."
 }
}