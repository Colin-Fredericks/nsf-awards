{
 "awd_id": "0846002",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Dynamic Run-Time Optimization of Parallel, Adaptive and Hybrid Applications",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2009-02-15",
 "awd_exp_date": "2016-01-31",
 "tot_intn_awd_amt": 409576.0,
 "awd_amount": 409576.0,
 "awd_min_amd_letter_date": "2009-02-05",
 "awd_max_amd_letter_date": "2012-08-20",
 "awd_abstract_narration": "CAREER: Dynamic Run-Time Tuning of Parallel, Adaptive and Hybrid Applications\r\n\r\nThe complexity of today?s High Performance Computing systems mandate significant efforts by end users and application developers to tune their code for each platform.  Processor and node architecture, network interconnect and the software stack all expose a significant number of parameters which influence the performance of an application. These parameters are furthermore often correlated, which further complicates the predictability of the performance of any application. The most popular tuning approach as of today applies a static tuning for the most time consuming operations of the code, i.e. the performance of different versions of the same operation is evaluated for certain problem sizes and the best performing version is chosen for the subsequent executions of the application. However, this approach is not practical for adaptive applications. These applications vary the problem sizes at run-time, e.g. by locally refining the computational mesh based on certain error criteria. Thus, the problem sizes are typically unknown in advance and therefore expensive operations cannot be tuned for the relevant problem sizes.\r\n\r\nThis project focuses on run-time tuning of parallel, adaptive applications utilizing either a distributed memory parallel programming model such as MPI or a hybrid shared memory/distributed memory parallelization strategy using OpenMP and MPI. The focus of the project is on introducing novel run-time selection algorithms which incorporate knowledge gathered from previous executions, algorithms from factorial design theory for very large parameter spaces and advanced algorithms from machine learning.  The project also targets the development of a recommendation system, which presents a human readable form of experiences gathered from an optimization run in order to reuse them in other applications. \r\n\r\nThis proposal tackles one of the most pressing and fundamental problems in High Performance Computing. Code portability and maintainability on one side and performance on the other side often seem to be contradicting goals. The project develops the fundamental knowledge required to develop performance portable parallel code and thus avoid the necessity to maintain multiple versions of the same code for different platforms.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Edgar",
   "pi_last_name": "Gabriel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Edgar Gabriel",
   "pi_email_addr": "gabriel@cs.uh.edu",
   "nsf_id": "000316336",
   "pi_start_date": "2009-02-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Houston",
  "inst_street_address": "4300 MARTIN LUTHER KING BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "HOUSTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7137435773",
  "inst_zip_code": "772043067",
  "inst_country_name": "United States",
  "cong_dist_code": "18",
  "st_cong_dist_code": "TX18",
  "org_lgl_bus_name": "UNIVERSITY OF HOUSTON SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "QKWEF8XLMTT3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Houston",
  "perf_str_addr": "4300 MARTIN LUTHER KING BLVD",
  "perf_city_name": "HOUSTON",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "772043067",
  "perf_ctry_code": "US",
  "perf_cong_dist": "18",
  "perf_st_cong_dist": "TX18",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "409000",
   "pgm_ele_name": "ADVANCED NET INFRA & RSCH"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 95928.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 98204.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 82418.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 133026.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The fundamental goal of this project is to create portable and efficient parallel applications by allowing an application to adjust to a given platform and application scenario by auto-tuning the occuring communication operations. As of today, applications using thousands of processes are often the result of many person-years of tuning for a particular platform, and end-users often have to maintain multiple versions of the application for different platforms. This project contributes to the solution of this problem by developing novel techniques and algorithms through dynamic run-time tuning of collective communication, and eliminate the necessity to maintain multiple code versions for performance reasons. The auto-tuning of communication operations is based on the following principles:</p>\n<ol>\n<li>Availability of alternative algorithms and/or implementations of the operation to be tuned.</li>\n<li>A parameterized description of the alternative implementations is available.</li>\n<li>Using a selection logic to choose among the alternatives based on performance data gathered during the execution of an application. </li>\n</ol>\n<p>Alternative implementations/algorithms can be evaluated within the lifetime of a single instance of the application, e.g. each function call to a particular communication operation will invoke a different alternative implementation. The algorithms developed within the project have been implemented within the Abstract Data and Communication Library (ADCL), an auto-tuning library for communication operations.</p>\n<p>The key results obtained by the project include i) demonstration of the ability of applications to achieve close to optimal performance on a wide variety of high-end computing systems using the algorithms and techniques developed in the project; ii) enormous performance improvements of parallel applications when using auto-tuning for both blocking and non-blocking collective communication operations; iii) solving the technical challenges and solutions of transferring the knowledge gathered from one application instance to another application through historic learning; iv) develop a user-friendly, human readable representation of the results of the auto-tuning operation through a recommendation system; v) an application of the algorithms developed in the project to tune internal parameters of a communication library (Open MPI); vi) successful demonstration of the benefits of integrating communication auto-tuning with automatic parallelization.</p>\n<p>The project has led to numerous conference, workshop and journal publications, contributed directly and indirectly to six PhD dissertations and two MS thesis. All software developed as part of this project is open source and available for download on the project webpages.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/30/2016<br>\n\t\t\t\t\tModified by: Edgar&nbsp;Gabriel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe fundamental goal of this project is to create portable and efficient parallel applications by allowing an application to adjust to a given platform and application scenario by auto-tuning the occuring communication operations. As of today, applications using thousands of processes are often the result of many person-years of tuning for a particular platform, and end-users often have to maintain multiple versions of the application for different platforms. This project contributes to the solution of this problem by developing novel techniques and algorithms through dynamic run-time tuning of collective communication, and eliminate the necessity to maintain multiple code versions for performance reasons. The auto-tuning of communication operations is based on the following principles:\n\nAvailability of alternative algorithms and/or implementations of the operation to be tuned.\nA parameterized description of the alternative implementations is available.\nUsing a selection logic to choose among the alternatives based on performance data gathered during the execution of an application. \n\n\nAlternative implementations/algorithms can be evaluated within the lifetime of a single instance of the application, e.g. each function call to a particular communication operation will invoke a different alternative implementation. The algorithms developed within the project have been implemented within the Abstract Data and Communication Library (ADCL), an auto-tuning library for communication operations.\n\nThe key results obtained by the project include i) demonstration of the ability of applications to achieve close to optimal performance on a wide variety of high-end computing systems using the algorithms and techniques developed in the project; ii) enormous performance improvements of parallel applications when using auto-tuning for both blocking and non-blocking collective communication operations; iii) solving the technical challenges and solutions of transferring the knowledge gathered from one application instance to another application through historic learning; iv) develop a user-friendly, human readable representation of the results of the auto-tuning operation through a recommendation system; v) an application of the algorithms developed in the project to tune internal parameters of a communication library (Open MPI); vi) successful demonstration of the benefits of integrating communication auto-tuning with automatic parallelization.\n\nThe project has led to numerous conference, workshop and journal publications, contributed directly and indirectly to six PhD dissertations and two MS thesis. All software developed as part of this project is open source and available for download on the project webpages.\n\n \n\n\t\t\t\t\tLast Modified: 03/30/2016\n\n\t\t\t\t\tSubmitted by: Edgar Gabriel"
 }
}