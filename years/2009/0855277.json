{
 "awd_id": "0855277",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "II-EN: BlueTool:  Infrastructure for Innovative Cyberphysical Data Center Management Research",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Theodore Baker",
 "awd_eff_date": "2009-08-01",
 "awd_exp_date": "2013-07-31",
 "tot_intn_awd_amt": 800000.0,
 "awd_amount": 800000.0,
 "awd_min_amd_letter_date": "2009-08-07",
 "awd_max_amd_letter_date": "2011-05-04",
 "awd_abstract_narration": "Energy consumption for data centers (a.k.a. hosting centers, server farms, clusters etc.) is rapidly increasing and is fast becoming a significant portion of the nation?s annual energy budget. Surprisingly, many of the contemporary data centers are designed and managed very inefficiently, mainly due to reliance on folklore techniques rather than those grounded in hard scientific evidence. In addition, contemporary research and experimentation in greening data centers is severely hindered by the unavailability of an experimentation test-bed, the cost of performing live tests of alternative configurations and the excessive long duration for performing simulations based on high-complexity offline models based on computational fluid dynamics.\r\nThe BlueTool project aims to resolve these issues by: 1) increasing awareness of the latest scientific and engineering research on managing data centers, and 2) providing a research and evaluation infrastructure to test and develop new methodologies to address the inefficiencies of data centers. BlueTool will promote the use of holistic cyber-physical concepts to foster the development of energy-efficient and sustainable data centers. It will leverage recent research advances in cooling technologies and cyber-physical management for data centers at Arizona State University (ASU) and provide synergy for advancing the ongoing research in this field at ASU and elsewhere. BlueTool will consist of: (a) an online tool that can simulate various configurations of data centers, in terms of physical layout, hardware and software configuration; (b) a research hub and portal, maintained by the IMPACT Lab at ASU (http://impact.asu.edu), that offers data services on various updatable archives including power and thermal profiles, multi-scale low-complexity thermal and power models, and data center management methods and software. Researchers from both academia and industry will be able to use BlueTool's online consulting services to improve the computing performance and energy consumption of conventional or existing configurations with newly developed techniques.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sandeep",
   "pi_last_name": "Gupta",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Sandeep K Gupta",
   "pi_email_addr": "sandeep.gupta@asu.edu",
   "nsf_id": "000279628",
   "pi_start_date": "2009-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Patrick",
   "pi_last_name": "Phelan",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Patrick E Phelan",
   "pi_email_addr": "phelan@asu.edu",
   "nsf_id": "000349248",
   "pi_start_date": "2009-08-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Stanzione",
   "pi_mid_init": "",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Daniel Stanzione",
   "pi_email_addr": "dan@tacc.utexas.edu",
   "nsf_id": "000193108",
   "pi_start_date": "2009-08-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "660 S MILL AVENUE STE 204",
  "perf_city_name": "TEMPE",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852813670",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 650000.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Data centers (a.k.a. hosting centers, server farms) are rooms that can be as big as warehouses that contain hundreds or thousands of connected computers called servers. Many daily activities have to do with data centers, from shopping and making phone calls, to banking and web searching. Data centers are collectively one of the leading consumers of energy in the United States, accounting for about 3% of the US&rsquo; energy budget. There is a lot of research on how to make data centers energy-efficient. However, many of these solutions haven&rsquo;t been validated because it takes an entire data center to test any of those ideas---and that is very expensive and not easy to do. The BlueTool project&rsquo;s overarching goal was to make it easy for data center researchers to validate their energy-efficiency solutions in&nbsp; a manner in which both researchers and practitioners can benefit from the infrastructure.</p>\n<p><br />The BlueTool infrastructure offers the following facilities: i) <strong>BlueCenter: </strong>A small-scale experimental data center, geared with sensors, which is the heart of the BlueTool project. ii) <strong>BlueSense: </strong>A data center assessment toolkit, which consists of wireless portable sensor nodes for on-site thermal assessment of data centers. iii) <strong>BlueSim: </strong>A simulation toolkit that provides the means to verify models and management algorithms through detailed CFD simulation. iv) <strong>BlueWeb: </strong>A repository of information and the web presence of BlueTool. <a href=\"http://impact.asu.edu/BlueTool/\">http://impact.asu.edu/BlueTool/</a>. See Figures 1 and 2 for the structure of the project.</p>\n<p><br />The main teaching of BlueTool is that Data Centers should be viewed as cyber-physical systems (CPS). A CPS is any computer system which has dominant physical interactions with its immediate environment, in terms of movement, material exchange or energy exchange. Considering and modeling a data center as CPS, we can link quantitatively the amount of computational workload to how much energy is needed, what the heat conditions are in the data center room, and how much energy is wasted from the various components (see Figure 3).</p>\n<p><br />BlueSim has been used by simulation tool called GDCSim that can simulate various configurations of data centers, in terms of physical layout, hardware and software configurations. Major features of GDCSim include: 1) automated processing, operation without user intervention (especially the necessity of expert knowledge), 2) online analysis capability, which allows real time simulation of management decisions based on changes in the physical environment in the data center, 3) iterative design analysis, enabling design time testing and analysis of different data center configurations before deployment, 4) thermal analysis capability, which characterizes the thermal effects within the data center, 5) workload and power management, which enables workload scheduling and controlling the power modes of servers in the data center, and 6) consideration of cyber-physical interdependency, which enables feedback of information on temperature and air flow patterns in the data center to the management algorithms.</p>\n<p>BlueTool has helped:</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>In model validation:</strong></p>\n<ul>\n<li>In validating the &ldquo;heat recirculation matrix&rdquo;, a model that predicts temperatures given heat output from each server: </li>\n<li>In producing power curves, i.e. utilization-to-power models. BlueTool&rsquo;s infrastructure was used to run experiments and yield what is called power curve.</li>\n<li>On developing a model for the <strong><em>computing-cooling trade-off effect</em></strong>, i.e. a condition where when consolidating the computational workload to fewer servers to save energy makes the cooling infrastructure work harder because of g...",
  "por_txt_cntn": "\nData centers (a.k.a. hosting centers, server farms) are rooms that can be as big as warehouses that contain hundreds or thousands of connected computers called servers. Many daily activities have to do with data centers, from shopping and making phone calls, to banking and web searching. Data centers are collectively one of the leading consumers of energy in the United States, accounting for about 3% of the US\u00c6 energy budget. There is a lot of research on how to make data centers energy-efficient. However, many of these solutions haven\u00c6t been validated because it takes an entire data center to test any of those ideas---and that is very expensive and not easy to do. The BlueTool project\u00c6s overarching goal was to make it easy for data center researchers to validate their energy-efficiency solutions in  a manner in which both researchers and practitioners can benefit from the infrastructure.\n\n\nThe BlueTool infrastructure offers the following facilities: i) BlueCenter: A small-scale experimental data center, geared with sensors, which is the heart of the BlueTool project. ii) BlueSense: A data center assessment toolkit, which consists of wireless portable sensor nodes for on-site thermal assessment of data centers. iii) BlueSim: A simulation toolkit that provides the means to verify models and management algorithms through detailed CFD simulation. iv) BlueWeb: A repository of information and the web presence of BlueTool. http://impact.asu.edu/BlueTool/. See Figures 1 and 2 for the structure of the project.\n\n\nThe main teaching of BlueTool is that Data Centers should be viewed as cyber-physical systems (CPS). A CPS is any computer system which has dominant physical interactions with its immediate environment, in terms of movement, material exchange or energy exchange. Considering and modeling a data center as CPS, we can link quantitatively the amount of computational workload to how much energy is needed, what the heat conditions are in the data center room, and how much energy is wasted from the various components (see Figure 3).\n\n\nBlueSim has been used by simulation tool called GDCSim that can simulate various configurations of data centers, in terms of physical layout, hardware and software configurations. Major features of GDCSim include: 1) automated processing, operation without user intervention (especially the necessity of expert knowledge), 2) online analysis capability, which allows real time simulation of management decisions based on changes in the physical environment in the data center, 3) iterative design analysis, enabling design time testing and analysis of different data center configurations before deployment, 4) thermal analysis capability, which characterizes the thermal effects within the data center, 5) workload and power management, which enables workload scheduling and controlling the power modes of servers in the data center, and 6) consideration of cyber-physical interdependency, which enables feedback of information on temperature and air flow patterns in the data center to the management algorithms.\n\nBlueTool has helped:\n\n-        In model validation:\n\nIn validating the \"heat recirculation matrix\", a model that predicts temperatures given heat output from each server: \nIn producing power curves, i.e. utilization-to-power models. BlueTool\u00c6s infrastructure was used to run experiments and yield what is called power curve.\nOn developing a model for the computing-cooling trade-off effect, i.e. a condition where when consolidating the computational workload to fewer servers to save energy makes the cooling infrastructure work harder because of gathering all the heat to one side of the room.\nIn research on models for operating data centers with energy storage.\n\n\n-        Overall research in data centers:\n\nOn how heat-activated cooling can be a feasible option forcooling data centers. This means that no significantly additional power is needed to run air conditioners.\nOn the energy-wastage effects of phase imb..."
 }
}