{
 "awd_id": "0917175",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Small: Statistical Measurement, Modeling, and Inference on Natural 3D Scenes",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2009-07-15",
 "awd_exp_date": "2013-06-30",
 "tot_intn_awd_amt": 496614.0,
 "awd_amount": 496614.0,
 "awd_min_amd_letter_date": "2009-07-10",
 "awd_max_amd_letter_date": "2011-05-20",
 "awd_abstract_narration": "This project investigates two deeply commingled and significant scientific questions on the statistical distributions of range, disparity, chrominance and luminance in natural 3D images of the world: (1) developing a comprehensive database of co-registered luminance, chrominance, range, and disparity images of natural scenes; and (2) conducting eye movement studies on stereoscopic images.. On the acquired database, the research team studies and models the bivariate statistics of luminance, chrominance, range, and disparity . In the eye movement studies, the locations of visual fixations are measured as they land in range space against where they land in luminance, chromatic, and disparity space, making it possible to develop gaze-contingent models of the statistics of luminance, chrominance, range, and disparity. The results of these studies have broad significance in vision science and image processing. To exemplify this, new approaches to computational stereo and to stereo image quality assessment are developed. New computational stereo algorithms are developed using appropriate prior and posterior distribution models on disparity. Further, new algorithms are developed for stereopair image quality assessment using the statistical models that we will develop. These new algorithms dramatically impact the emerging 3-D digital cinema, gaming, and television industries, allowing for automatic assessment of 3D presentations to human viewers. The developed 3D range-luminance databases are made available via public web portals, and the results of the work are published in the highest-profile vision science and image science journals.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lawrence",
   "pi_last_name": "Cormack",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Lawrence K Cormack",
   "pi_email_addr": "Cormack@mail.utexas.edu",
   "nsf_id": "000264254",
   "pi_start_date": "2009-07-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Bovik",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Alan C Bovik",
   "pi_email_addr": "bovik@ece.utexas.edu",
   "nsf_id": "000305764",
   "pi_start_date": "2009-07-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "110 INNER CAMPUS DR",
  "perf_city_name": "AUSTIN",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121139",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 324783.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 171831.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>(1) We discovered heretofore unknown properties of the statistics of natural 3D images. In particular we created first-of-a-kind models of the conditional probability distributions of bandpass luminance images given bandpass range or disparity images using a co-registered database of luminance and range images. We found that the magnitudes of luminance and range/disparity coefficients show a clear positive correlation, which means, at a location with larger luminance variation, there is a higher probability of a larger range/disparity variation. As an example of the usefulness of luminance statistics conditioned on range/disparity statistics, we modified a well-known Bayesian stereo ranging algorithm using our natural scene statistics models, which improved its performance quite significantly. SEE FIG 1. These results are profound for understanding 3D modeling, 3D reconstruction, 3D recognition, 3D perception, 3D image quality prediction, 3D cinema and visual comfort, and many other fields.</p>\n<p>(2) We conducted 3D eye tracking experiments on naturalistic stereo images, and found the heretofore unguessed at and surprising result that fixated disparity contrast and disparity gradient are generally lower than randomly selected disparity contrast and disparity gradient. See FIG 2. Red is luminance gradient (luminance rate of change) and Blue is depth in 3D /disparity (depth rate of change).This result has profound implications for understanding 3D saliency, 3D computer vision algorithms, 3D reconstruction, 3D image and video quality, and 3D cinema.</p>\n<p>(3) We created a very high quality data set of coregistered color and range values collected specifically for this 3D natural scene studies, and we evaluated the statistics of perceptually relevant chromatic information in addition to luminance, range, and binocular disparity information. SEE FIG 3. The most fundamental finding is that the probabilities of finding range changes depend in a systematic way on color (our prior finding, see (1)). Our chromatic&nbsp; statistical distribution models were able to much further&nbsp; improve the performance of the Bayesian stereo algorithm as considered in (1) above, resulting in even fewer errors in matching using the Middlebury stereo database and criteria. These results are profound for understanding 3D modeling, 3D reconstruction, 3D recognition, 3D perception, 3D image quality prediction, 3D cinema and visual comfort, and many other fields. These are broad theoretical results of very wide applicability.</p>\n<p>(4) We conducted two human studies aimed towards understanding the perception of distorted 3D images by analyzing subjects&rsquo; performance in locating local distortions in stereoscopically viewed images. SEE FIG 4. We found that binocular suppression of visual distortion artifacts is observed while viewing blur, JPEG, and JP2K distorted stereo 3D images. This has deep implications for 3D image and video quality assessment, for digital 3D cinema, and for the development of 3D image and video compression protocols.</p>\n<p>&nbsp;(5) Based on the human studies in (4) we developed a Full Reference (FR) model for assessing the quality of stereoscopic images that have been afflicted by possibly asymmetric distortions. The resulting 3D Full Reference Image Quality Assessment (3D FR IQA) algorithm was shown to produce significantly better results than any prior model. The new model will have tremendous impact on the 3D image and video capture, communication, display, and entertainment fields, including 3D cinema, 3D television, 3D gaming, and 3D Internet.</p>\n<p>&nbsp;(6) Also based on the human studies in (4) we developed a first-of-a-kind No Reference 3D image quality assessment model that operates in distorted 3D images with either or both symmetric- or asymmetric distortions. SEE FIG 5. The algorithm derived from the model significantly outperforms all prior 3D FR ...",
  "por_txt_cntn": "\n(1) We discovered heretofore unknown properties of the statistics of natural 3D images. In particular we created first-of-a-kind models of the conditional probability distributions of bandpass luminance images given bandpass range or disparity images using a co-registered database of luminance and range images. We found that the magnitudes of luminance and range/disparity coefficients show a clear positive correlation, which means, at a location with larger luminance variation, there is a higher probability of a larger range/disparity variation. As an example of the usefulness of luminance statistics conditioned on range/disparity statistics, we modified a well-known Bayesian stereo ranging algorithm using our natural scene statistics models, which improved its performance quite significantly. SEE FIG 1. These results are profound for understanding 3D modeling, 3D reconstruction, 3D recognition, 3D perception, 3D image quality prediction, 3D cinema and visual comfort, and many other fields.\n\n(2) We conducted 3D eye tracking experiments on naturalistic stereo images, and found the heretofore unguessed at and surprising result that fixated disparity contrast and disparity gradient are generally lower than randomly selected disparity contrast and disparity gradient. See FIG 2. Red is luminance gradient (luminance rate of change) and Blue is depth in 3D /disparity (depth rate of change).This result has profound implications for understanding 3D saliency, 3D computer vision algorithms, 3D reconstruction, 3D image and video quality, and 3D cinema.\n\n(3) We created a very high quality data set of coregistered color and range values collected specifically for this 3D natural scene studies, and we evaluated the statistics of perceptually relevant chromatic information in addition to luminance, range, and binocular disparity information. SEE FIG 3. The most fundamental finding is that the probabilities of finding range changes depend in a systematic way on color (our prior finding, see (1)). Our chromatic  statistical distribution models were able to much further  improve the performance of the Bayesian stereo algorithm as considered in (1) above, resulting in even fewer errors in matching using the Middlebury stereo database and criteria. These results are profound for understanding 3D modeling, 3D reconstruction, 3D recognition, 3D perception, 3D image quality prediction, 3D cinema and visual comfort, and many other fields. These are broad theoretical results of very wide applicability.\n\n(4) We conducted two human studies aimed towards understanding the perception of distorted 3D images by analyzing subjects\u00c6 performance in locating local distortions in stereoscopically viewed images. SEE FIG 4. We found that binocular suppression of visual distortion artifacts is observed while viewing blur, JPEG, and JP2K distorted stereo 3D images. This has deep implications for 3D image and video quality assessment, for digital 3D cinema, and for the development of 3D image and video compression protocols.\n\n (5) Based on the human studies in (4) we developed a Full Reference (FR) model for assessing the quality of stereoscopic images that have been afflicted by possibly asymmetric distortions. The resulting 3D Full Reference Image Quality Assessment (3D FR IQA) algorithm was shown to produce significantly better results than any prior model. The new model will have tremendous impact on the 3D image and video capture, communication, display, and entertainment fields, including 3D cinema, 3D television, 3D gaming, and 3D Internet.\n\n (6) Also based on the human studies in (4) we developed a first-of-a-kind No Reference 3D image quality assessment model that operates in distorted 3D images with either or both symmetric- or asymmetric distortions. SEE FIG 5. The algorithm derived from the model significantly outperforms all prior 3D FR IQA models. The new model will have tremendous impact on 3D wireless videos applications such as smartphones and table..."
 }
}