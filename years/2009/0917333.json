{
 "awd_id": "0917333",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III:   Small: Information Systems Under Schema Evolution:  Analyzing Change Histories and Management Tools",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Frank Olken",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 475453.0,
 "awd_amount": 475453.0,
 "awd_min_amd_letter_date": "2009-09-04",
 "awd_max_amd_letter_date": "2011-07-08",
 "awd_abstract_narration": "\r\nThe significant progress made by database research on \r\nschema mapping (e.g., omposition, invertibility), data exchange, \r\nand query rewriting, can provide breakthrough solutions for the Database \r\nSchema Evolution problem.  But as of today, information systems are \r\nsorely lacking the methods and tools needed to cope with the\r\nproblem, and to reduce the cost of data migration, rework of queries, \r\napplication rewriting, and downtime created by schema changes. \r\nIn fact, this old problem has been made worse by the success of \r\nscientific databases (e.g., Ensembl) and web information\r\nsystems (e.g., Wikipedia)---where the fast evolution of applications \r\nand requirements characterizing the web and the scientific discovery \r\nprocess is exacerbated by the number and diversity of users and \r\norganizations cooperating on these endeavors.. Fortunately, the openness of\r\nthese public-domain information systems (vs. corporate ones), and the \r\nabundance of their interesting evolution histories make it possible to\r\nbuilt a comprehensive testbed to determine the strengths, limitations, \r\nand potentials of candidate methods and tools proposed for the problem.\r\n \r\nThus, this project is building: (i) an open-source curated repository \r\ncontaining evolution histories from key information systems, \r\n(ii) benchmarks for a comprehensive set of tools tested therein, \r\nand (iii) instruments to collect and analyze evolution histories.\r\nThese are then used to (a) compare and evaluate existing approaches, \r\nmethods and tools, and (b) entice researchers to evaluate and improve \r\ntheir techniques and add their test cases to the benchmark.\r\nA transformative impact can be expected upon schema mapping\r\nresearch and applications, inasmuch as theoretical solutions \r\nare now validated and improved on real-life case-studies. \r\nThese in turn are expected to transform and improve significantly \r\nscientific databases and web information systems. For further\r\ninformation see the project web page:\r\nhttp://www.cs.ucla.edu/~zaniolo/nsf0917333.html \r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Carlo",
   "pi_last_name": "Zaniolo",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Carlo Zaniolo",
   "pi_email_addr": "zaniolo@CS.UCLA.EDU",
   "nsf_id": "000451654",
   "pi_start_date": "2009-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "10889 WILSHIRE BLVD STE 700",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900244200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 157583.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 158157.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 159713.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern Information Systems experience frequent revisions and changes in response to changes in their requirements and the environment they are supporting. These changes demand significant expenditures in time and resources; these are particularly serious for database schema changes which &nbsp;require upgrading of the database queries and revising the applications---a time-consuming and error-prone task involving experts and professionals.&nbsp;</p>\n<p>Schema-evolution problems occurred in traditional business-oriented databases, but they now occur even more frequently in databases designed to support scientific information systems and collaborative web systems, inasmuch as the pace of changes in these moder information is accelerated by frequent scientific discoveries, and the number and diversity of participating users and organizations. Therefore, there is a growing need to support and automate (i) the process of upgrading schemas and the applications that depend on it, and also (ii) the documentation of such upgrades and the overall schema evolution history. The importance of the second objective is &nbsp;underscored by (a) the growing interest in documenting the provenance of data and metadata and (b) the emergence of cloud computing where the management of the IS schema is delegated to third parties.&nbsp;<br /> <br />The schema evolution problem been recognized for a long time, but methods and tools been have been slow to come about.A first problem has been the lack of general techniques for automating the rewriting of queries and updates after schema changes.&nbsp;However significant advances on this problem have recently been made by researchers working on schema mapping and database integration.&nbsp;However these techniques have yet to extended to related problem of schema evolution, and&nbsp;few case studies exists that be used by researchers to frame this problem. However,&nbsp;But while in the past database administrator might have been weary of releasing this information about their corporate IS,&nbsp;the situation has improving dramatically because web information systems, such as Wikipedia, and big science projects such as Ensembl,&nbsp;which make schema evolution information available to the public. This &nbsp;made possible the realization of our research project that&nbsp;has pursued and realized the following two objectives:&nbsp;<br /> <br />A. Building a rich, curated, and integrated testbed of schema evolution histories and tools wherewith (i) the various facets and technical challenges of the problems are exposed, and (ii) the effectiveness of current and future tools and techniques in addressing these problems can be stress-tested on complex real-life information systems,&nbsp;</p>\n<p>B.&nbsp;Optimizing the documentation and management of schema histories and other metadata in our testbed to provide effective support for (i) queries about the history of past data and the provenance of current ones, and (ii) queries on the history of the schema (e.g., for DB administrators and analysts who need to learn about past upgrades before implementing new ones).&nbsp;&nbsp;</p>\n<p><br /> Therefore, to realize  objective A,  we first investigated a large  number of public-domain information systems, and then selected thirty such systems  for inclusion in our testbed.  Our selection includes popular web information systems such as Wikipedia and XOOPS, Medicine/Biology  Databases, such as Ensembl Genetic DB andBioSQL, and CERN Physics DBs, including GridCC and ATLAS.  For each  information system in our testbed,  we extracted its schema  history using various tools developed for this purpose. Then we represented   each schema evolution step using Schema Manipulation Operators (SMOs). For this, we used our SQL2SMO system, that can recognize all changes in the schema history,  along with the Schema Manipulation Operators used to transform each schema vers...",
  "por_txt_cntn": "\nModern Information Systems experience frequent revisions and changes in response to changes in their requirements and the environment they are supporting. These changes demand significant expenditures in time and resources; these are particularly serious for database schema changes which  require upgrading of the database queries and revising the applications---a time-consuming and error-prone task involving experts and professionals. \n\nSchema-evolution problems occurred in traditional business-oriented databases, but they now occur even more frequently in databases designed to support scientific information systems and collaborative web systems, inasmuch as the pace of changes in these moder information is accelerated by frequent scientific discoveries, and the number and diversity of participating users and organizations. Therefore, there is a growing need to support and automate (i) the process of upgrading schemas and the applications that depend on it, and also (ii) the documentation of such upgrades and the overall schema evolution history. The importance of the second objective is  underscored by (a) the growing interest in documenting the provenance of data and metadata and (b) the emergence of cloud computing where the management of the IS schema is delegated to third parties. \n \nThe schema evolution problem been recognized for a long time, but methods and tools been have been slow to come about.A first problem has been the lack of general techniques for automating the rewriting of queries and updates after schema changes. However significant advances on this problem have recently been made by researchers working on schema mapping and database integration. However these techniques have yet to extended to related problem of schema evolution, and few case studies exists that be used by researchers to frame this problem. However, But while in the past database administrator might have been weary of releasing this information about their corporate IS, the situation has improving dramatically because web information systems, such as Wikipedia, and big science projects such as Ensembl, which make schema evolution information available to the public. This  made possible the realization of our research project that has pursued and realized the following two objectives: \n \nA. Building a rich, curated, and integrated testbed of schema evolution histories and tools wherewith (i) the various facets and technical challenges of the problems are exposed, and (ii) the effectiveness of current and future tools and techniques in addressing these problems can be stress-tested on complex real-life information systems, \n\nB. Optimizing the documentation and management of schema histories and other metadata in our testbed to provide effective support for (i) queries about the history of past data and the provenance of current ones, and (ii) queries on the history of the schema (e.g., for DB administrators and analysts who need to learn about past upgrades before implementing new ones).  \n\n\n Therefore, to realize  objective A,  we first investigated a large  number of public-domain information systems, and then selected thirty such systems  for inclusion in our testbed.  Our selection includes popular web information systems such as Wikipedia and XOOPS, Medicine/Biology  Databases, such as Ensembl Genetic DB andBioSQL, and CERN Physics DBs, including GridCC and ATLAS.  For each  information system in our testbed,  we extracted its schema  history using various tools developed for this purpose. Then we represented   each schema evolution step using Schema Manipulation Operators (SMOs). For this, we used our SQL2SMO system, that can recognize all changes in the schema history,  along with the Schema Manipulation Operators used to transform each schema version  into its following one.  Out testbed is freely available  from: http://yellowstone.cs.ucla.edu/schemaevolution/index.php/Benchmark_home\n \n With respect to objective B, our project's mai..."
 }
}