{
 "awd_id": "0950410",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Describing the Operating System for Accurate User-mode Simulation",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Krishna Kant",
 "awd_eff_date": "2009-09-15",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 216000.0,
 "awd_min_amd_letter_date": "2009-09-21",
 "awd_max_amd_letter_date": "2010-06-24",
 "awd_abstract_narration": "This is an EAGER project that addresses a highly exploratory investigation into key elements needed to specify the characteristics of an operating system (OS) in a way that  permits an architectural model to be created that interacts fully with a suite of simulation tools.\r\n\r\nThe suite of tools, CoGenT (CoGeneration of Tools), include specification languages to allow researchers to express novel instruction sets and micro-architectures and the infrastructure for automatic generation of corresponding functional and timing co-simulators, compilers, linkers, loaders, debuggers, assemblers, disassemblers, and a fully integrated instrumentation facility to enable meaningful experimentation within this new design space. CoGenT?s ability to automatically generate a functional simulator from a specification, and other related elements, will be released this year.\r\n\r\nThis EAGER addresses the problem that, in simulating complex architectures, it is important to be able to specify OS support, not just as a set of external calls, but as a specific model that integrates with the rest of the architecture. Current architectures rely on the services and policies of the operating system, and the operating system itself needs to evolve with the radical shifts in architecture and applications that are anticipated in the next decade.\r\n\r\nWith this project, this team develops an approach that enables simultaneous research into novel hardware and software paradigms, with great flexibility, and without the heretofore prohibitive cost of manually building a complete hardware and software simulation infrastructure with a tailored OS implementation. Traditional system simulation approaches either ignored OS impact on performance or resorted to costly and inflexible full system simulation where an actual OS implementation is executed directly. The former provides unrealistic results, and the latter does not admit the kind of exploration needed for transformative paradigm shifts.\r\n\r\nThe goal of this project is to extend the relatively recent approach of functional and timing co-simulation for hardware architectures into \"pseudo-full system simulation\", where the OS becomes a first-class element in the simulation modeling and instrumentation framework. Simulating an OS model derived from a specification will also enable sensitivity and significance analyses, often neglected in current simulation-based research even though they are essential to understanding the real impact of new approaches.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "J Eliot",
   "pi_last_name": "Moss",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "J Eliot B Moss",
   "pi_email_addr": "moss@cs.umass.edu",
   "nsf_id": "000261930",
   "pi_start_date": "2009-09-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Weems",
   "pi_mid_init": "C",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Charles C Weems",
   "pi_email_addr": "weems@cs.umass.edu",
   "nsf_id": "000434762",
   "pi_start_date": "2009-09-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "101 COMMONWEALTH AVE",
  "perf_city_name": "AMHERST",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039252",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As computer systems evolve and increase in complexity, understanding how their perfomance varies becomes both more complex and more important. Small differences in behavior can cascade into significant variations in performance. One aspect of system behavior that has not been studied in a modern context is the interaction between the operating system and internal state that a computer maintains for enhanced performance. For example, the computer keeps recently accessed data in a faster memory called a cache, and the behavior of decision points in program code is recorded to enable prediction of future decisions so instructions can be fetched from memory before they are needed, etc. Most of this stored information is meant to accelerate user programs, but when a program calls upon the services of the operating system, some of the information is inadvertently displaced. As a result, when the operating system returns control to the user program, it slows down for a subsequent period until the information is restored. The goal of our research was to characterize the cost of this side effect.</p>\n<p>We began with the expectation that the internal perfomance analysis hardware (performance counters) provided by microprocessors would enable us to gather this information. But after months of trying to access them in different ways, we found that the access process itself disturbed the measurements so much that they were meaningless, and there was no way to work around the effect. We thus turned to using a software simulation of a microprocessor called MARSSx86.</p>\n<p>MARSSx86 simulates the Intel instruction set, and the internal state for a particular AMD processor. Its timing has been verified against real hardware to within a few percent. At first we found initial results that seemed reasonable, but further experiments showed inexplicable behavior. The ensuing year and a half was spent working with the MARSSx86 developers to fix numerous bugs and problems in the simulator that they were unaware of, that were causing it to produce inconsistent results. We developed a suite of tests that enabled us to both isolate the problems and to validate our own results once the simulator had been fixed.&nbsp;</p>\n<p>We modified MARSSx86 to save the internal state before each system call. We could then compare that state with the state after the call, and restore the original state to compare actual performance with performance for no distubance. We analyzed a set of benchmark applications to identify the most frequently used system calls, and then measured their impact both in isolation and in the context of applications. We also tested multiple versions of operating systems.&nbsp;</p>\n<p>Our analysis found that the primary instruction cache could suffer up to a 38% performance penalty due to disruption, and a portion of the branch prediction unit could suffer up to 32%. On average, both of these units showed an 8% variation in performance from being disturbed. Other units were impacted less. Switching from the Ubuntu Linux version 9 operating system to version 12, we found average variations on the order of 25% across all state, and as much as a 500% difference in the branch predictor state.</p>\n<p>We have shown that operating system calls have a significant effect on the internal state of user code in a processor. While the average impact is modest, it cannot be neglected because these variations are actually comparable to perfomance improvements that are often cited in computer systems research. Thus, it is possible that some of those reports are biased by this underyling variation.&nbsp;</p>\n<p>Intellectual Merit: We are the first to develop a thorough analysis of the performance side effects of operating system calls because of disruption of internal processor state. Doing so was far more challenging than we anticipated, which may be why it has not been done before. We show that...",
  "por_txt_cntn": "\nAs computer systems evolve and increase in complexity, understanding how their perfomance varies becomes both more complex and more important. Small differences in behavior can cascade into significant variations in performance. One aspect of system behavior that has not been studied in a modern context is the interaction between the operating system and internal state that a computer maintains for enhanced performance. For example, the computer keeps recently accessed data in a faster memory called a cache, and the behavior of decision points in program code is recorded to enable prediction of future decisions so instructions can be fetched from memory before they are needed, etc. Most of this stored information is meant to accelerate user programs, but when a program calls upon the services of the operating system, some of the information is inadvertently displaced. As a result, when the operating system returns control to the user program, it slows down for a subsequent period until the information is restored. The goal of our research was to characterize the cost of this side effect.\n\nWe began with the expectation that the internal perfomance analysis hardware (performance counters) provided by microprocessors would enable us to gather this information. But after months of trying to access them in different ways, we found that the access process itself disturbed the measurements so much that they were meaningless, and there was no way to work around the effect. We thus turned to using a software simulation of a microprocessor called MARSSx86.\n\nMARSSx86 simulates the Intel instruction set, and the internal state for a particular AMD processor. Its timing has been verified against real hardware to within a few percent. At first we found initial results that seemed reasonable, but further experiments showed inexplicable behavior. The ensuing year and a half was spent working with the MARSSx86 developers to fix numerous bugs and problems in the simulator that they were unaware of, that were causing it to produce inconsistent results. We developed a suite of tests that enabled us to both isolate the problems and to validate our own results once the simulator had been fixed. \n\nWe modified MARSSx86 to save the internal state before each system call. We could then compare that state with the state after the call, and restore the original state to compare actual performance with performance for no distubance. We analyzed a set of benchmark applications to identify the most frequently used system calls, and then measured their impact both in isolation and in the context of applications. We also tested multiple versions of operating systems. \n\nOur analysis found that the primary instruction cache could suffer up to a 38% performance penalty due to disruption, and a portion of the branch prediction unit could suffer up to 32%. On average, both of these units showed an 8% variation in performance from being disturbed. Other units were impacted less. Switching from the Ubuntu Linux version 9 operating system to version 12, we found average variations on the order of 25% across all state, and as much as a 500% difference in the branch predictor state.\n\nWe have shown that operating system calls have a significant effect on the internal state of user code in a processor. While the average impact is modest, it cannot be neglected because these variations are actually comparable to perfomance improvements that are often cited in computer systems research. Thus, it is possible that some of those reports are biased by this underyling variation. \n\nIntellectual Merit: We are the first to develop a thorough analysis of the performance side effects of operating system calls because of disruption of internal processor state. Doing so was far more challenging than we anticipated, which may be why it has not been done before. We show that the effect is significant, and that it varies greatly with the operating system. We also found that the effect w..."
 }
}