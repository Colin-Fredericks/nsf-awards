{
 "awd_id": "0916951",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small:  Simplifying Text for Individual Reading Needs",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2009-09-01",
 "awd_exp_date": "2015-02-28",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 463210.0,
 "awd_min_amd_letter_date": "2009-09-08",
 "awd_max_amd_letter_date": "2014-08-14",
 "awd_abstract_narration": "A surprisingly large number of Americans read below their grade level, either because of limited education or because their native language is not English. Low reading levels impact a child?s progress in school and an adult?s job opportunities as well as limiting information access. \r\nThis project aims to improve access by developing new language processing technology for selecting and transforming text to obtain material at lower reading levels, extending current paraphrasing work that focuses on summarization as compression to include explanatory expansions. In addition, the goal is to develop adaptive models that can be tuned to a specific domain and an individual's needs. The approach involves analyzing corpora of comparable text collected from the web, developing models of paraphrasing aimed at generating simplified English, developing a discourse-sensitive clause selection method for expanding or omitting details, and exploring representations of language that facilitate domain and user adaptation. The language processing contributions of this work include development of text resources to support language technology in education applications, new representations of reading difficulty, and advances in automatic methods of paraphrasing. The broader impact of this project includes making information more accessible to people with limited English reading proficiency. In addition, students working on the project will have the opportunity to interact with teachers from a local school so as to better understand the impact of their work and guide their approach, and their work will be showcased in University of Washington diversity-oriented outreach programs.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mari",
   "pi_last_name": "Ostendorf",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mari Ostendorf",
   "pi_email_addr": "ostendor@uw.edu",
   "nsf_id": "000109813",
   "pi_start_date": "2009-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 BROOKLYN AVE NE",
  "perf_city_name": "SEATTLE",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981951016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 450000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 13210.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In America, a surprisingly large number of people read below their grade level, either because of limited education or because their native language is not English.&nbsp; For children, having a lower reading level limits their progress in other subjects, and for adults it limits opportunities for job training and their ability care for themselves and their families.&nbsp; In both cases, there is the common problem of limited information access.&nbsp; This project aimed to improve access by developing new language processing technology for selecting and transforming text to obtain material at lower reading levels, taking advantage of advances in natural language processing. However, many of the most successful language processing algorithms are based on learning from large annotated (or parallel) training corpora, which simply are not available for different reading levels. Hence, our work has emphasized methods for data-driven learning that can leverage uncertain reading level labels and comparable corpora. In addition, standard methods for evaluating text summarization were not effective for simplification, so significant effort was devoted to developing new methods for characterizing text difficulty based on oral readings.&nbsp; In particular, we focus on leveraging acoustic-prosodic anomalies in reading to identify difficult regions. More specifically, project outcomes included:</p>\n<p>Major research activities and findings:</p>\n<ul>\n<li>New approaches to active learning:&nbsp; Active learning reduces the cost of annotating training data by automatically selecting the most informative samples to label. This project contributed new methods for combining active and semi-supervised learning to better leverage unlabeled data, and a new batch query technique that combines diversity and uncertainty criteria.</li>\n<li>Automatic prediction of simplification strategies: Besides paraphrasing to use more common words and shorter syntactic constructions, simplification sometimes involves omission of material or expansion to define a difficult but important concept. This work looked at how topic centrality measures could be used with other factors to select among these strategies.</li>\n<li>Improved semantic similarity models: We developed an alternative to WordNet that builds on the crowd-sourced Wiktionary for providing broader coverage knowledge-based semantic similarity, and extended the word-based score to incorporate syntactic dependencies.</li>\n<li>Identification of simplifications in comparable corpora: Leveraging advances in semantic similarity modeling, different sentence alignment methods were explored for identifications in comparable standard and simple Wikipedia articles, resulting in a large collection of comparable sentence pairs for research on simplification.</li>\n<li>A new approach to evaluation of text difficulty: Building on analysis of a large corpus of oral readings from a national literacy survey, we developed a method for identifying difficult points in a text based on the presence of reading errors or prosodically anomalous readings, averaging over multiple readings of the text by different readers.&nbsp; Further, normalizing by automatically predicted prosodic context can reduce the need for multiple readings from high-literacy subjects.</li>\n<li>Assessing text difficulty for specific readers: To demonstrate the potential for analyzing reading difficulties of individuals, we ran experiments using acoustic-prosodic features from a participant's reading of a low-level passage of text to predict the reading score that the same person would get when reading an eighth-grade passage, finding that the acoustic-prosodic features accounted for roughly 80% of the variance words correct per minute of the higher passage, and significantly better results than simply looking at word errors or speaking rate.</li>\n</ul>\n<p>Contributions to infrastructure:</p>\n<p>Wi...",
  "por_txt_cntn": "\nIn America, a surprisingly large number of people read below their grade level, either because of limited education or because their native language is not English.  For children, having a lower reading level limits their progress in other subjects, and for adults it limits opportunities for job training and their ability care for themselves and their families.  In both cases, there is the common problem of limited information access.  This project aimed to improve access by developing new language processing technology for selecting and transforming text to obtain material at lower reading levels, taking advantage of advances in natural language processing. However, many of the most successful language processing algorithms are based on learning from large annotated (or parallel) training corpora, which simply are not available for different reading levels. Hence, our work has emphasized methods for data-driven learning that can leverage uncertain reading level labels and comparable corpora. In addition, standard methods for evaluating text summarization were not effective for simplification, so significant effort was devoted to developing new methods for characterizing text difficulty based on oral readings.  In particular, we focus on leveraging acoustic-prosodic anomalies in reading to identify difficult regions. More specifically, project outcomes included:\n\nMajor research activities and findings:\n\nNew approaches to active learning:  Active learning reduces the cost of annotating training data by automatically selecting the most informative samples to label. This project contributed new methods for combining active and semi-supervised learning to better leverage unlabeled data, and a new batch query technique that combines diversity and uncertainty criteria.\nAutomatic prediction of simplification strategies: Besides paraphrasing to use more common words and shorter syntactic constructions, simplification sometimes involves omission of material or expansion to define a difficult but important concept. This work looked at how topic centrality measures could be used with other factors to select among these strategies.\nImproved semantic similarity models: We developed an alternative to WordNet that builds on the crowd-sourced Wiktionary for providing broader coverage knowledge-based semantic similarity, and extended the word-based score to incorporate syntactic dependencies.\nIdentification of simplifications in comparable corpora: Leveraging advances in semantic similarity modeling, different sentence alignment methods were explored for identifications in comparable standard and simple Wikipedia articles, resulting in a large collection of comparable sentence pairs for research on simplification.\nA new approach to evaluation of text difficulty: Building on analysis of a large corpus of oral readings from a national literacy survey, we developed a method for identifying difficult points in a text based on the presence of reading errors or prosodically anomalous readings, averaging over multiple readings of the text by different readers.  Further, normalizing by automatically predicted prosodic context can reduce the need for multiple readings from high-literacy subjects.\nAssessing text difficulty for specific readers: To demonstrate the potential for analyzing reading difficulties of individuals, we ran experiments using acoustic-prosodic features from a participant's reading of a low-level passage of text to predict the reading score that the same person would get when reading an eighth-grade passage, finding that the acoustic-prosodic features accounted for roughly 80% of the variance words correct per minute of the higher passage, and significantly better results than simply looking at word errors or speaking rate.\n\n\nContributions to infrastructure:\n\nWikipedia is potentially a good resource for text simplification, since it includes standard articles and their corresponding simple articles in English and other languages. S..."
 }
}