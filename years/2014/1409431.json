{
 "awd_id": "1409431",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Deep Neural Networks for Robust Speech Recognition through Integrated Acoustic Modeling and Separation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2014-06-01",
 "awd_exp_date": "2019-05-31",
 "tot_intn_awd_amt": 798082.0,
 "awd_amount": 798082.0,
 "awd_min_amd_letter_date": "2014-06-03",
 "awd_max_amd_letter_date": "2016-04-19",
 "awd_abstract_narration": "Over the last decade, speech recognition technology has become steadily more present in everyday life, as seen by the proliferation of applications including mobile personal agents and transcription of voicemail messages. Performance of these systems, however, degrades significantly in the presence of background noise; for example, using speech recognition technology in a noisy restaurant or on a windy street can be difficult because speech recognizers confuse the background noise with linguistic content. Compensation for noise typically involves preprocessing the acoustic signal to emphasize the speech signal (i.e. speech separation), and then feeding this processed input into the recognizer.  The innovative approach in this project is to train the recognition and separation systems in an integrated manner so that the linguistic content of the signal can inform the separation, and vice versa. \r\n\r\nGiven the impact of the recent resurgence of Deep Neural Networks (DNNs) in speech processing, this project seeks to make DNNs more resistant to noise by integrating speech separation and speech recognition, exploring three related areas.  The first research area seeks to stabilize input to DNNs by combining DNN-based suppression and acoustic modeling, integrating masking estimates across time and frequency, and using this information to improve reconstruction of speech from noisy input.  The second area seeks to examine a richer DNN structure, using multi-task learning techniques to guide the construction of DNNs better at performing all tasks and where layers have meaningful structure.  The final research area examines ways to adapt the spurious output of DNN acoustic models given acoustic noise.  With the focus of integrating speech separation and recognition, the project will be evaluated both by measuring speech recognition performance, as well as metrics that are more closely related to human speech perception.  This will ensure a broader impact of this research by providing insights not only to speech technology but also facilitating the design of next-generation hearing technology in the long run.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Fosler-Lussier",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Fosler-Lussier",
   "pi_email_addr": "fosler@cse.ohio-state.edu",
   "nsf_id": "000182577",
   "pi_start_date": "2014-06-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "DeLiang",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "DeLiang Wang",
   "pi_email_addr": "dwang@cse.ohio-state.edu",
   "nsf_id": "000486642",
   "pi_start_date": "2014-06-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Mandel",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Michael I Mandel",
   "pi_email_addr": "mim@sci.brooklyn.cuny.edu",
   "nsf_id": "000634756",
   "pi_start_date": "2014-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 514095.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 283987.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern speech recognition systems perform relatively well when there is little background noise and there is sufficient training data for the recognition task. Background noise has a significant impact on the accuracy of most speech recognition systems when there are no adaptation strategies in place. &nbsp;One approach to address this issue is to first separate the speech from the background noise (speech separation) before transcribing the acoustic signal into words (speech recognition).</p>\n<p>This project started from the hypothesis that information from the speech recognition process could be helpful in speech separation -- that is, jointly modeling the two processes could bring about a significant improvement in accuracy. &nbsp; During the course of the project, several key technological advances were made, including new multi-microphone processing techniques, new deep learning techniques for classifying speech versus noise locally, techniques for directly predicting denoised speech from a noisy signal, and a demonstration that the incorporation of phonetic information can improve speech separation and recognition performance.</p>\n<p>One key outcome was the extension of a source separation algorithm (Model-Based EM Source Separation and Localization, or MESSL) to multiple microphone inputs; this required a deeper understanding of how to coordinate MESSL models across multiple channels with time delays relative to each other. Combining multichannel MESSL with a speech model based on a neural network (LSTM) improved prediciton of time-frequency masks, which allowed better estimation of speech and noise spatial covariances.</p>\n<p>Within a single-microphone framework, another significant outcome was the development of new spatial features to train deep neural networks for time-frequency mask estimation. Two new measures were introduced, including magnitude squared coherence and directional features that provide information about both the target and noise sources. &nbsp;Another advance was the development of a novel recurrent deep stacking approach for time-frequency masking-based speech separation (predicting at a particular time and freqency whether target signal or noise is present), where the output context is explicitly employed to improve the accuracy of mask estimation.</p>\n<p>A separate line of research in this project direclty predicted the clean speech signal from noisy speech (spectral mapping), examining two particular neural architectures (Deep Neural Networks and Residual Networks). &nbsp;The studies showed that training the system to produce speech that makes a clean-speech speech recognizer behave like it has seen clean speech can improve performance significantly, as well as producing more humanly-intelligble speech representations. The phonetic information from the system can be used in concert with a new state-of-the-art speech enhancement system to train a speech enhancement system on noisy data only, eliminating or reducing the need for parallel clean/noisy data.</p>\n<p><span>In terms of broader impacts, the primary impact has been advancing the state of the art in noise-robust speech recognition, evidenced by a continued strong publication record over the duration of the project,&nbsp; Experiments for spectral separation on the CHIME-2, CHIME-3, and CHIME-4 (single-channel track) databases are currently the state-of-the-art for those databases; this project also advanced the forefront of segmental speech recognition through new techniques for training segmental conditional random fields.</span></p>\n<p><span>By showing significant advances in the twin goals of joint speech recognition and separation, this project will impact the field by getting researchers to think more about how the two processes can affect each other rather than treating separation-then-recognition as a pipeline approach. In terms of dissemination,<span>&nbsp;this project has produced 23 conference papers, 4 top workshop papers, 4 journal articles with 1 more currently under review, 3 book chapters, 2 Ph.D dissertations, and 1 undergraduate thesis. &nbsp;The project has also produced 6 public code repositories for public use.</span></span></p>\n<p>The project provided training opportunities for 13 graduate students and one undergraduate student over the course of the entire project, and allowed for an active cross-fertilization of ideas between PIs and students who traditionally worked in either speech recognition or speech separation/enhancement.&nbsp; The students who worked on this project were exposed to a wider set of ideas from across speech technology disciplines.&nbsp; Many of the students who were initially on the project have since graduated and are making significant contributions in industrial research laboratories.</p>\n<p><span>By jointly looking at speech recognition and speech separation, we not only make speech recognition available to a wider number of users in a wider number of environmental settings, we also learn how the interrelationship between language and acoustics can help inform speech separation, which in turn can inform, for example, hearing aid design.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/08/2019<br>\n\t\t\t\t\tModified by: Eric&nbsp;Fosler-Lussier</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern speech recognition systems perform relatively well when there is little background noise and there is sufficient training data for the recognition task. Background noise has a significant impact on the accuracy of most speech recognition systems when there are no adaptation strategies in place.  One approach to address this issue is to first separate the speech from the background noise (speech separation) before transcribing the acoustic signal into words (speech recognition).\n\nThis project started from the hypothesis that information from the speech recognition process could be helpful in speech separation -- that is, jointly modeling the two processes could bring about a significant improvement in accuracy.   During the course of the project, several key technological advances were made, including new multi-microphone processing techniques, new deep learning techniques for classifying speech versus noise locally, techniques for directly predicting denoised speech from a noisy signal, and a demonstration that the incorporation of phonetic information can improve speech separation and recognition performance.\n\nOne key outcome was the extension of a source separation algorithm (Model-Based EM Source Separation and Localization, or MESSL) to multiple microphone inputs; this required a deeper understanding of how to coordinate MESSL models across multiple channels with time delays relative to each other. Combining multichannel MESSL with a speech model based on a neural network (LSTM) improved prediciton of time-frequency masks, which allowed better estimation of speech and noise spatial covariances.\n\nWithin a single-microphone framework, another significant outcome was the development of new spatial features to train deep neural networks for time-frequency mask estimation. Two new measures were introduced, including magnitude squared coherence and directional features that provide information about both the target and noise sources.  Another advance was the development of a novel recurrent deep stacking approach for time-frequency masking-based speech separation (predicting at a particular time and freqency whether target signal or noise is present), where the output context is explicitly employed to improve the accuracy of mask estimation.\n\nA separate line of research in this project direclty predicted the clean speech signal from noisy speech (spectral mapping), examining two particular neural architectures (Deep Neural Networks and Residual Networks).  The studies showed that training the system to produce speech that makes a clean-speech speech recognizer behave like it has seen clean speech can improve performance significantly, as well as producing more humanly-intelligble speech representations. The phonetic information from the system can be used in concert with a new state-of-the-art speech enhancement system to train a speech enhancement system on noisy data only, eliminating or reducing the need for parallel clean/noisy data.\n\nIn terms of broader impacts, the primary impact has been advancing the state of the art in noise-robust speech recognition, evidenced by a continued strong publication record over the duration of the project,  Experiments for spectral separation on the CHIME-2, CHIME-3, and CHIME-4 (single-channel track) databases are currently the state-of-the-art for those databases; this project also advanced the forefront of segmental speech recognition through new techniques for training segmental conditional random fields.\n\nBy showing significant advances in the twin goals of joint speech recognition and separation, this project will impact the field by getting researchers to think more about how the two processes can affect each other rather than treating separation-then-recognition as a pipeline approach. In terms of dissemination, this project has produced 23 conference papers, 4 top workshop papers, 4 journal articles with 1 more currently under review, 3 book chapters, 2 Ph.D dissertations, and 1 undergraduate thesis.  The project has also produced 6 public code repositories for public use.\n\nThe project provided training opportunities for 13 graduate students and one undergraduate student over the course of the entire project, and allowed for an active cross-fertilization of ideas between PIs and students who traditionally worked in either speech recognition or speech separation/enhancement.  The students who worked on this project were exposed to a wider set of ideas from across speech technology disciplines.  Many of the students who were initially on the project have since graduated and are making significant contributions in industrial research laboratories.\n\nBy jointly looking at speech recognition and speech separation, we not only make speech recognition available to a wider number of users in a wider number of environmental settings, we also learn how the interrelationship between language and acoustics can help inform speech separation, which in turn can inform, for example, hearing aid design.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/08/2019\n\n\t\t\t\t\tSubmitted by: Eric Fosler-Lussier"
 }
}