{
 "awd_id": "1355065",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Cortical Specializations for Behavioral Discrimination of Temporal Shape and Rhythm of Sound",
 "cfda_num": "47.074",
 "org_code": "08090200",
 "po_phone": "7032924845",
 "po_email": "sraghava@nsf.gov",
 "po_sign_block_name": "Sridhar Raghavachari",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 680000.0,
 "awd_amount": 680000.0,
 "awd_min_amd_letter_date": "2014-08-11",
 "awd_max_amd_letter_date": "2017-07-12",
 "awd_abstract_narration": "Human speech and other mammalian vocalizations exhibit extensive variability in their amplitude shape and rhythm.  These sound features allow for discriminating and, ultimately, understanding the rich repertoire of vocal communications that characterize the vocal behavior of a large variety of animals including man. This proposal focuses on the roles of various stages of the mammalian forebrain ascending auditory pathway in categorizing and discriminating time variations in natural sounds, an important and open question in auditory physiology. \r\n\r\nNeural response timing properties suggest that secondary ventral auditory cortices allow for an extended range of sensory temporal processing not accounted for in primary auditory cortex. The neural response properties of primary and secondary auditory cortices will be examined to determine their functional organization and contribution to sensory discrimination behavior. The approach involves the creation of sophisticated acoustic stimuli that are analytically tractable on the one hand, while mimicking the important aspects of natural sounds on the other.  Neural responses will be characterized using electrophysiological, intrinsic-signal imaging, behavioral and computational techniques.  Pharmacological manipulations will allow reversible loss-of-function studies to determine how primary and secondary auditory areas interact to promote behavioral discrimination of temporal cues in sound. Further, this project will determine how temporal cues on multiple time scales in sound are discriminated and encoded on a neural and whole organism level for Rattus norvegicus, as this species has an expanded surface area for primary auditory cortex and secondary auditory cortex compared to other mammals commonly used in auditory studies. Another goal of the study is to determine the extent to which sound percepts are built up through parallel and hierarchal processing within auditory forebrain structures.  Finally, computer models will be developed to determine what aspects of neural activity patterns are critical for discriminating between the shape and rhythm of communication sounds.  This multifaceted approach should allow for an unprecedented understanding of vocal perception that could be broadly applicable.  Results from the study will be disseminated through presentations at scientific meetings and through peer-reviewed journal articles.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "IOS",
 "org_div_long_name": "Division Of Integrative Organismal Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Heather",
   "pi_last_name": "Read",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Heather Read",
   "pi_email_addr": "heather.read@uconn.edu",
   "nsf_id": "000593907",
   "pi_start_date": "2014-08-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Monty",
   "pi_last_name": "Escabi",
   "pi_mid_init": "A",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Monty A Escabi",
   "pi_email_addr": "escabi@engr.uconn.edu",
   "nsf_id": "000483622",
   "pi_start_date": "2014-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Connecticut",
  "inst_street_address": "438 WHITNEY RD EXTENSION UNIT 1133",
  "inst_street_address_2": "",
  "inst_city_name": "STORRS",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "8604863622",
  "inst_zip_code": "062699018",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CT02",
  "org_lgl_bus_name": "UNIVERSITY OF CONNECTICUT",
  "org_prnt_uei_num": "",
  "org_uei_num": "WNTPS995QBM7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Connecticut",
  "perf_str_addr": "406 Babbidge Rd",
  "perf_city_name": "Storrs",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "062691020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CT02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "771300",
   "pgm_ele_name": "Activation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 170000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 340000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 170000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>NSF Project 1355065 (Read, HL, PI and Escabi, MA Co-PI, Collaborator Markus Wohr).</strong>&nbsp;</p>\n<p>Animals discriminate timing cues in vocalization sequences in order to judge their meaning and communicate effectively. Timing cues become even more important when spectral (pitch) cues are degraded, masked or missing.&nbsp; Little is known about how brains are organized and underlying neural mechanisms for encoding timing cues in vocalizations.&nbsp; To address this, we systematically quantify and computationally simulate sound envelop timing cues across many types of vocalization sequences.&nbsp; We propose a novel unified scientific principle where vocalization duration sets the upper limit for vocalization rate imposing a joint (co-varying) sound statistic that uniquely defines vocalization type.&nbsp; This trend holds across all vocalizations and species examined. &nbsp;<strong>This first key outcome&nbsp;</strong>defines new ecological principles driving temporal variations in natural vocalization sequences across species.&nbsp; These findings are published in the&nbsp;<em><strong>Public Library of Science Computational Biology Journal</strong></em>&nbsp;(Khatami et al., 2018).&nbsp;&nbsp;<strong>Significantly</strong>, these ecological principles expand our basic scientific understanding and could be used to engineer more accurate computer programs for automated speech recognition.</p>\n<p>Over many generations, brains and animals have developed to optimally recognize and respond to timing variations in vocal communications and other sounds.&nbsp; Basic science, health and communication technologies would all benefit immensely from a clear understanding of how brains achieve this. <strong>A second key outcome</strong>&nbsp;of this project is the discovery that spike timing precision systematically varies with timing cues in sound providing a potential neural code for vocalization timing.&nbsp; Extensive neural response maps reveal parallel but segregated cortical pathways where primary and secondary auditory cortical neurons respond to onset and sustained components of sound bursts, respectively.&nbsp;&nbsp;These and related findings are published in the&nbsp;<strong><em>Journal of Neurophysiology</em></strong><strong>&nbsp;</strong>(Lee et al., 2016) and the&nbsp;<em><strong>Journal of Neural Engineering</strong></em>&nbsp;(Ghanbari et al., 2019).&nbsp;&nbsp;<strong>Significantly</strong>, a similar parallel cortical pathway organization exists in humans and other animals pointing towards a unified principle of organization for encoding and perceiving sound (Read and Reyes, 2018,&nbsp;<a href=\"https://doi.org/10.1007/978-3-319-71798-2_7\">https://doi.org/10.1007/978-3-319-71798-2_7</a>).&nbsp; This biological organization and encoding could inspire novel computer programs with parallel architectures and time scales for automated speech recognition.</p>\n<p>The cortical spike-timing patterns we observe could serve as unique neural codes for judging differences in time-varying vocalization sequences.&nbsp; To explore this, we create probabilistic Bayesian computer programs that simulate how spike timing responses of auditory cortical neurons could be decoded in real brain networks.&nbsp;<strong>A third key outcome</strong>&nbsp;of this project is the discovery that precise spike timing patterns (not spike rates) provide accurate neural codes for perceptual judgement of a range of time-varying sound burst sequences.&nbsp; These findings are published in the&nbsp;<strong><em>Journal of Neuroscience</em></strong>&nbsp;(Osman et al. 2018).&nbsp; In addition, ongoing behavioral studies find perceptual judgment of synthetic vocalization sequences improves with addition of naturalistic onset timing cues (Read et al.,&nbsp;<strong><em>Society for Neuroscience</em></strong><strong>, 483.18/FF11 2018</strong>). &nbsp;<strong>Significantly</strong>, these neural computation principles could be used to engineer computer programs for more accurate diagnosis of speech comprehension disorders and automated sound and speech recognition systems.</p>\n<p>Brain activity needs to be recorded as subjects actively judge vocalization sequences and other time-varying sounds in order to truly characterize underlying neural mechanisms.&nbsp; However, when mammals engage their sensory environment many factors cause spiking patterns to vary independently or conjointly with time-varying sound.&nbsp;<strong>A fourth key outcome&nbsp;</strong>of this project is the development of a novel data-driven dynamical systems approach to quantify and reconstruct large scale intrinsic and acoustically-driven brain responses.&nbsp; Using this approach, we are able to isolate and quantify significant intrinsic and sound-driven variations in brain activity more effectively than standard approaches.&nbsp; The scientific findings are published in the&nbsp;<em><strong>International Joint Conference on Neural Networks</strong></em>&nbsp;peer reviewed IEEE conference proceedings (Marrouch et al., 2018) and&nbsp;<em><strong>Annals of Mathematics and Artificial Intelligence</strong></em>&nbsp;(Marrouch et al., 2019).&nbsp;Collaborators Dimitris Giannakis and Joanna Slawinska provide custom Matlab code and tutorials on how to&nbsp;implement the Nonlinear Laplacian Spectral Analysis (NLSA) and related kernel algorithms for analyzing time series generated by dynamical systems (<a href=\"https://github.com/dg227/NLSA\">https://github.com/dg227/NLSA</a>).&nbsp;<strong>Significantly</strong>, this approach has potential to revolutionize next generation brain computer interface systems and brain signal based speech recognition systems.</p>\n<p><strong>Broader impacts</strong>.&nbsp; This project initiates international inter-disciplinary scientific collaborations and our discoveries, computer code and tutorials are publically available . The outcomes expand basic science and have high potential to impact neural and health technologies that aid humans. &nbsp;&nbsp;The results have been disseminated in public presentations at the&nbsp;<strong><em>International Behavioral Neuroscience Society</em></strong>,&nbsp;<strong><em>Gordon Research Conference</em></strong>,&nbsp;<strong><em>Kavli Institute for Theoretical Physics</em></strong>,&nbsp;<strong><em>Broad Institute</em></strong>,&nbsp;<strong><em>Society for Neuroscience</em></strong>&nbsp;and elsewhere.&nbsp; This project resulted in development of a learning community program and successful mentoring and graduation of more than 40 first generation women in science, technology, engineering and mathematics. Finally, six graduate and more than 80 undergraduate fellows mentored in computational and behavioral neuroscience resulted in published honors theses, Masters and Ph.D. theses.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/09/2020<br>\n\t\t\t\t\tModified by: Heather&nbsp;Read</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNSF Project 1355065 (Read, HL, PI and Escabi, MA Co-PI, Collaborator Markus Wohr). \n\nAnimals discriminate timing cues in vocalization sequences in order to judge their meaning and communicate effectively. Timing cues become even more important when spectral (pitch) cues are degraded, masked or missing.  Little is known about how brains are organized and underlying neural mechanisms for encoding timing cues in vocalizations.  To address this, we systematically quantify and computationally simulate sound envelop timing cues across many types of vocalization sequences.  We propose a novel unified scientific principle where vocalization duration sets the upper limit for vocalization rate imposing a joint (co-varying) sound statistic that uniquely defines vocalization type.  This trend holds across all vocalizations and species examined.  This first key outcome defines new ecological principles driving temporal variations in natural vocalization sequences across species.  These findings are published in the Public Library of Science Computational Biology Journal (Khatami et al., 2018).  Significantly, these ecological principles expand our basic scientific understanding and could be used to engineer more accurate computer programs for automated speech recognition.\n\nOver many generations, brains and animals have developed to optimally recognize and respond to timing variations in vocal communications and other sounds.  Basic science, health and communication technologies would all benefit immensely from a clear understanding of how brains achieve this. A second key outcome of this project is the discovery that spike timing precision systematically varies with timing cues in sound providing a potential neural code for vocalization timing.  Extensive neural response maps reveal parallel but segregated cortical pathways where primary and secondary auditory cortical neurons respond to onset and sustained components of sound bursts, respectively.  These and related findings are published in the Journal of Neurophysiology (Lee et al., 2016) and the Journal of Neural Engineering (Ghanbari et al., 2019).  Significantly, a similar parallel cortical pathway organization exists in humans and other animals pointing towards a unified principle of organization for encoding and perceiving sound (Read and Reyes, 2018, https://doi.org/10.1007/978-3-319-71798-2_7).  This biological organization and encoding could inspire novel computer programs with parallel architectures and time scales for automated speech recognition.\n\nThe cortical spike-timing patterns we observe could serve as unique neural codes for judging differences in time-varying vocalization sequences.  To explore this, we create probabilistic Bayesian computer programs that simulate how spike timing responses of auditory cortical neurons could be decoded in real brain networks. A third key outcome of this project is the discovery that precise spike timing patterns (not spike rates) provide accurate neural codes for perceptual judgement of a range of time-varying sound burst sequences.  These findings are published in the Journal of Neuroscience (Osman et al. 2018).  In addition, ongoing behavioral studies find perceptual judgment of synthetic vocalization sequences improves with addition of naturalistic onset timing cues (Read et al., Society for Neuroscience, 483.18/FF11 2018).  Significantly, these neural computation principles could be used to engineer computer programs for more accurate diagnosis of speech comprehension disorders and automated sound and speech recognition systems.\n\nBrain activity needs to be recorded as subjects actively judge vocalization sequences and other time-varying sounds in order to truly characterize underlying neural mechanisms.  However, when mammals engage their sensory environment many factors cause spiking patterns to vary independently or conjointly with time-varying sound. A fourth key outcome of this project is the development of a novel data-driven dynamical systems approach to quantify and reconstruct large scale intrinsic and acoustically-driven brain responses.  Using this approach, we are able to isolate and quantify significant intrinsic and sound-driven variations in brain activity more effectively than standard approaches.  The scientific findings are published in the International Joint Conference on Neural Networks peer reviewed IEEE conference proceedings (Marrouch et al., 2018) and Annals of Mathematics and Artificial Intelligence (Marrouch et al., 2019). Collaborators Dimitris Giannakis and Joanna Slawinska provide custom Matlab code and tutorials on how to implement the Nonlinear Laplacian Spectral Analysis (NLSA) and related kernel algorithms for analyzing time series generated by dynamical systems (https://github.com/dg227/NLSA). Significantly, this approach has potential to revolutionize next generation brain computer interface systems and brain signal based speech recognition systems.\n\nBroader impacts.  This project initiates international inter-disciplinary scientific collaborations and our discoveries, computer code and tutorials are publically available . The outcomes expand basic science and have high potential to impact neural and health technologies that aid humans.   The results have been disseminated in public presentations at the International Behavioral Neuroscience Society, Gordon Research Conference, Kavli Institute for Theoretical Physics, Broad Institute, Society for Neuroscience and elsewhere.  This project resulted in development of a learning community program and successful mentoring and graduation of more than 40 first generation women in science, technology, engineering and mathematics. Finally, six graduate and more than 80 undergraduate fellows mentored in computational and behavioral neuroscience resulted in published honors theses, Masters and Ph.D. theses. \n\n\t\t\t\t\tLast Modified: 01/09/2020\n\n\t\t\t\t\tSubmitted by: Heather Read"
 }
}