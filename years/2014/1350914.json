{
 "awd_id": "1350914",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Privacy-Guaranteed Distributed Interactions in Critical Infrastructure Networks",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2014-01-15",
 "awd_exp_date": "2019-12-31",
 "tot_intn_awd_amt": 455000.0,
 "awd_amount": 463000.0,
 "awd_min_amd_letter_date": "2014-01-27",
 "awd_max_amd_letter_date": "2018-02-06",
 "awd_abstract_narration": "Information sharing between operators (agents) in critical infrastructure systems such as the Smart Grid is fundamental to reliable and sustained operation. The contention, however, between sharing data for system stability and reliability (utility) and withholding data  for competitive advantage (privacy) has stymied data sharing in such systems, sometimes with catastrophic consequences. This motivates a data sharing framework that addresses the competitive interests and information leakage concerns of agents and enables timely and controlled information exchange.\r\n\r\nThis research develops a foundational approach to privacy-guaranteed information sharing among distributed self-interested agents in complex systems using information theory and game theory. This multidisciplinary project focuses on four mutually related challenges in multi-agent network abstractions of the Smart Grid: (1) characterization of the fundamental limits of distributed interaction with privacy constraints; (2) operational and practical significance of information-theoretic privacy measures; (3) formalizing the cost of privacy and the role of trust and repeated interactions for cooperation; and a direct application of these results via (4) distributed algorithms and protocols for privacy-guaranteed data sharing in the Smart Grid. The research has the broader implication of enabling information sharing in a variety of complex networks with strict privacy requirements including electronic healthcare and water distribution systems, and also engenders academic and industry collaborations in power systems. This research project incorporates carefully tailored outreach efforts including privacy awareness for middle- and high-school students, and active engagement of undergraduate and graduate students, especially females, in research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lalitha",
   "pi_last_name": "Sankar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lalitha Sankar",
   "pi_email_addr": "lalithasankar@asu.edu",
   "nsf_id": "000547715",
   "pi_start_date": "2014-01-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "PO Box 876011",
  "perf_city_name": "Tempe",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852876011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "7937",
   "pgm_ref_txt": "NETWORK CODING AND INFO THEORY"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 171815.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 185507.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 97678.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Project Outcomes: CAREER</p>\n<p>&nbsp;</p>\n<p>With the explosion of information collection across a variety of digital platforms, it is crucial to ensure both privacy of personal information and restriction of the use of such information to avoid discrimination and unfair practices. This CAREER project focused on studying the problem of quantifying and limiting the leakage of information viewed as private during interactions between two or more parties. Such measures can then affect how privacy and fairness guarantees can be made in practice. The information-theoretic approach developed in this project is context-aware, that is, it can exploit prior knowledge about the data and its statistics to design robust methods to guarantee privacy (censoring) of sensitive data in a variety of data publishing/sharing settings. The work involved four broad efforts: (a) identifying and defining meaningful measures of information leakage that are operationally motivated including understanding the underlying adversarial model for each measure; (b) formally determining how enforcing limited information leakage affects the utility of interactive data sharing and comparing this tradeoff for a variety of meaningful leakage measures; (c) developing methods to implement theoretically optimal approaches for privacy-guaranteed data sharing in practice using generative adversarial models; (d) connecting privacy via censoring or obfuscation to providing guarantees on fair machine learning across different demographic groups. These four efforts are interconnected and allow a strong coupling between theoretical guarantees and practical designs.</p>\n<p>&nbsp;</p>\n<p>This project has led to several foundational results including the following: (i) the identification and development of leakage measures that are context-free and yet motivated by adversaries that are feasible in practice using machine learning algorithms; (ii) conditions clarifying when auditing the privacy or fairness guarantees of a model using practical ML methods such as deep neural networks suffice against all possible adversaries; (iii) new techniques using relationships between information measures to give better worst-case privacy guarantees via relaxed forms of differential privacy by developing tight relationships between these measures; (iv) a formal way of providing data-driven context-aware privacy and fairness guarantees using deep learning techniques based on generative adversarial models; and finally (v) models and methods to understand how to address and alleviate consumer discomfort with the use of their private data while ensuring utility to retailers dependent on data.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>This research led to published papers in a multitude of information theory focused conferences including the flagship conference, the IEEE International Symposium on Information Theory as well as high-cited journals including the IEEE Transactions on Information Theory, IEEE Transactions on Information Forensics and Security, and the Journal on Confidentiality and Privacy. Moreover, dissemination also included the following: organizing a two-day workshop on information-theoretic privacy at the prestigious Simons Institute for the Theory for Computing at the University of California, Berkeley, in March 2019, a three hour tutorial at the International Symposium on Information Theory in July 2019 in Paris, France, plenary talk on Fair and Censored Representations and Model Auditing Guarantees at the NeurIPS workshop on Privacy in ML at Vancouver, Canada, December 2019, two poster presentations at the NeurIPS workshops on Information Theory and Machine Learning as well as Machine Learning with Guarantees in Vancouver, Canada, December 2019, and finally, an invited presentation at the Federated Learning Workshop in June 2019 at Google in Seattle. The research is well cited and continues to be cited.</p>\n<p>&nbsp;</p>\n<p>This project also involved an active outreach component across all six years of the grant. Interactive hands-on demo using a variety of social media platform have been developed to highlight the privacy breaches possible on each platform. Using fictional characters from popular books and movies, the use of various privacy settings and lack of use of such settings is made clear through many interactive efforts. Multiple female undergraduate students at ASU were involved in developing the demo and bringing it to various valley STEM outreach efforts, two big ones being the annual Girls have IT day at Xaviers Preparatory Academy for middle and high school girls and ASU&rsquo;s Open Door, the valley largest STEM event for K-12 students.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/08/2020<br>\n\t\t\t\t\tModified by: Lalitha&nbsp;Sankar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nProject Outcomes: CAREER\n\n \n\nWith the explosion of information collection across a variety of digital platforms, it is crucial to ensure both privacy of personal information and restriction of the use of such information to avoid discrimination and unfair practices. This CAREER project focused on studying the problem of quantifying and limiting the leakage of information viewed as private during interactions between two or more parties. Such measures can then affect how privacy and fairness guarantees can be made in practice. The information-theoretic approach developed in this project is context-aware, that is, it can exploit prior knowledge about the data and its statistics to design robust methods to guarantee privacy (censoring) of sensitive data in a variety of data publishing/sharing settings. The work involved four broad efforts: (a) identifying and defining meaningful measures of information leakage that are operationally motivated including understanding the underlying adversarial model for each measure; (b) formally determining how enforcing limited information leakage affects the utility of interactive data sharing and comparing this tradeoff for a variety of meaningful leakage measures; (c) developing methods to implement theoretically optimal approaches for privacy-guaranteed data sharing in practice using generative adversarial models; (d) connecting privacy via censoring or obfuscation to providing guarantees on fair machine learning across different demographic groups. These four efforts are interconnected and allow a strong coupling between theoretical guarantees and practical designs.\n\n \n\nThis project has led to several foundational results including the following: (i) the identification and development of leakage measures that are context-free and yet motivated by adversaries that are feasible in practice using machine learning algorithms; (ii) conditions clarifying when auditing the privacy or fairness guarantees of a model using practical ML methods such as deep neural networks suffice against all possible adversaries; (iii) new techniques using relationships between information measures to give better worst-case privacy guarantees via relaxed forms of differential privacy by developing tight relationships between these measures; (iv) a formal way of providing data-driven context-aware privacy and fairness guarantees using deep learning techniques based on generative adversarial models; and finally (v) models and methods to understand how to address and alleviate consumer discomfort with the use of their private data while ensuring utility to retailers dependent on data.\n\n \n\n \n\nThis research led to published papers in a multitude of information theory focused conferences including the flagship conference, the IEEE International Symposium on Information Theory as well as high-cited journals including the IEEE Transactions on Information Theory, IEEE Transactions on Information Forensics and Security, and the Journal on Confidentiality and Privacy. Moreover, dissemination also included the following: organizing a two-day workshop on information-theoretic privacy at the prestigious Simons Institute for the Theory for Computing at the University of California, Berkeley, in March 2019, a three hour tutorial at the International Symposium on Information Theory in July 2019 in Paris, France, plenary talk on Fair and Censored Representations and Model Auditing Guarantees at the NeurIPS workshop on Privacy in ML at Vancouver, Canada, December 2019, two poster presentations at the NeurIPS workshops on Information Theory and Machine Learning as well as Machine Learning with Guarantees in Vancouver, Canada, December 2019, and finally, an invited presentation at the Federated Learning Workshop in June 2019 at Google in Seattle. The research is well cited and continues to be cited.\n\n \n\nThis project also involved an active outreach component across all six years of the grant. Interactive hands-on demo using a variety of social media platform have been developed to highlight the privacy breaches possible on each platform. Using fictional characters from popular books and movies, the use of various privacy settings and lack of use of such settings is made clear through many interactive efforts. Multiple female undergraduate students at ASU were involved in developing the demo and bringing it to various valley STEM outreach efforts, two big ones being the annual Girls have IT day at Xaviers Preparatory Academy for middle and high school girls and ASU\u2019s Open Door, the valley largest STEM event for K-12 students.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 04/08/2020\n\n\t\t\t\t\tSubmitted by: Lalitha Sankar"
 }
}