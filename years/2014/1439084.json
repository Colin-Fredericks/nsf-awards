{
 "awd_id": "1439084",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: CCA: Collaborative Research: Cache-Adaptive Algorithms: How to Share Core among Many Cores",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 799999.0,
 "awd_amount": 799999.0,
 "awd_min_amd_letter_date": "2014-08-01",
 "awd_max_amd_letter_date": "2014-08-01",
 "awd_abstract_narration": "This project will develop theoretical and algorithmic foundations for writing programs that efficiently share limited memory on multi-core computers. On a multi-core computer, each process is allocated some share of the memory, but its share can fluctuate over time as other processes start, stop, and change their demands for memory. Most of today's programs do not cope well with memory fluctuations -- they have difficulty taking full advantage of additional memory freed up by other processes and they can slow to a crawl when their memory allocation decreases.\r\n\r\nBy enabling programmers to write software that can adapt to memory fluctuations, this research will provide new levels of flexibility, performance, and resource utilization for scientific and commercial applications running on shared-memory multi-core infrastructure. Cloud services will respond more rapidly to changes in workload. High-performance-computing applications will achieve higher memory utilization, enabling scientists to do more with less hardware. By creating a more efficient and flexible computing infrastructure, this project has the potential to accelerate the pace of discovery in other scientific fields.  For example, biological applications such as protein docking are likely to benefit from this research because the performance of current software is limited by contention for memory.\r\n\r\nThis project will build upon the PIs' recently proposed notion of cache-adaptive algorithms, i.e., algorithms that automatically adapt to memory fluctuations.  This project will develop cache-adaptive theory and applications in four ways:\r\n\r\n1. The PIs will extend cache-adaptive analytical techniques to apply to more algorithms, such as cache-oblivious FFT and cache-oblivious serial and parallel dynamic programs.\r\n2. The PIs will develop the foundations of cache-adaptive data structures, such as cache-adaptive priority queues.\r\n3. The PIs will measure the impact of adaptivity on actual performance, focusing on cache-adaptive sorting, serial and parallel dynamic programs, and stencil computations.\r\n4. The PIs will implement cache-adaptive parallel software for computational biology applications, such as protein-protein docking, dynamic programs, and other HPC simulations.\r\n\r\nThe PIs will offer courses on parallel algorithms, parallel programming, cache-efficient and external-memory algorithms as part of a new degree program in computational sciences that is being launched at Stony Brook through its recently established Institute for Advanced Computational Sciences (IACS).  These courses are designed to disseminate high-performance computing research results to students and faculty in other fields, such as physics, chemistry, biology and math.  The PIs will also design a course, targeted at computer science students, on theoretical and systems aspects of external memory computing in the context of big data, databases, and file systems. The PIs will use super-computing resources from the XSEDE program, giving students access to some of the world?s fastest supercomputing clusters for their programming assignments and course projects.\r\n\r\nThe PIs will engage in outreach and dissemination by organizing parallel programming workshops as part of the IACS and in collaboration for Brookhaven National Labs.  PIs will also give tutorials on parallel computing, memory-efficient computing, and big data and at conferences and at other universities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rob",
   "pi_last_name": "Johnson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rob Johnson",
   "pi_email_addr": "rob@cs.stonybrook.edu",
   "nsf_id": "000252323",
   "pi_start_date": "2014-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Bender",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Michael A Bender",
   "pi_email_addr": "bender@cs.stonybrook.edu",
   "nsf_id": "000092778",
   "pi_start_date": "2014-08-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Rezaul",
   "pi_last_name": "Chowdhury",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Rezaul A Chowdhury",
   "pi_email_addr": "rezaul@cs.stonybrook.edu",
   "nsf_id": "000599542",
   "pi_start_date": "2014-08-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "Stony Brook University",
  "perf_city_name": "Stony Brook",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117944400",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 799999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-1ef52283-7fff-f48f-95ec-b8ef20372b8f\"> </span></p>\n<p dir=\"ltr\"><span>This project explored techniques for sharing a multi-core computer&rsquo;s scarce RAM among many programs so that each program can make good progress.&nbsp; On a multi-core computer, each process is allocated some share of the memory, but its share can fluctuate over time as other processes start, stop, and change their demands for memory.&nbsp; Most of today&rsquo;s programs do not cope well with memory fluctuations -- they have difficulty taking full advantage of additional memory freed up by other processes and they can slow to a crawl when their memory allocation decreases.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>When computers and programs do a bad job of sharing RAM, they can run slowly due to excessive &ldquo;swapping&rdquo;, i.e., shuffling data between fast RAM and slower disks.&nbsp; Almost everyone has at some point experienced poor performance on their personal computer or smart phone due to swapping, and it is an important issue on cloud servers and scientific supercomputers, as well.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The project created a theoretical framework for reasoning about program performance when programs must share a fixed amount of RAM.&nbsp; Almost all prior work had assumed that every program has exclusive access to all the RAM in the computer.  The PIs used the framework to show that numerous existing algorithms are optimally &ldquo;adaptive&rdquo;, i.e., they gracefully react to changes in their share of the computer&rsquo;s RAM.&nbsp; They also showed that several important algorithms are </span><span>not</span><span> adaptive, and gave a rule-of-thumb for algorithm designers who want to make adaptive algorithms.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>The PIs developed new algorithms for a class of problems called &ldquo;dynamic programs&rdquo;.&nbsp; Dynamic programs are at the heart of many other problems, such as finding routes in computer and road networks, aligning new genetic data to existing reference genomes, and analyzing how proteins fold.&nbsp; The PIs implemented these algorithms in new software tools.  Their tools for these problems are 5-150x faster than prior solutions.  Furthermore, their algorithms are highly parallel, meaning they can utilize all the cores on modern many-core computers.&nbsp; The PIs have also developed techniques to automatically transform many classical algorithms into fast algorithms that follow their methodology.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>In summary, this project has created new tools that practitioners can use to analyze data hundreds of times faster and has created a theoretical framework to help future algorithms designers develop high-performance algorithms.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/28/2019<br>\n\t\t\t\t\tModified by: Michael&nbsp;A&nbsp;Bender</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project explored techniques for sharing a multi-core computer?s scarce RAM among many programs so that each program can make good progress.  On a multi-core computer, each process is allocated some share of the memory, but its share can fluctuate over time as other processes start, stop, and change their demands for memory.  Most of today?s programs do not cope well with memory fluctuations -- they have difficulty taking full advantage of additional memory freed up by other processes and they can slow to a crawl when their memory allocation decreases.\n\n \nWhen computers and programs do a bad job of sharing RAM, they can run slowly due to excessive \"swapping\", i.e., shuffling data between fast RAM and slower disks.  Almost everyone has at some point experienced poor performance on their personal computer or smart phone due to swapping, and it is an important issue on cloud servers and scientific supercomputers, as well.\n\n \nThe project created a theoretical framework for reasoning about program performance when programs must share a fixed amount of RAM.  Almost all prior work had assumed that every program has exclusive access to all the RAM in the computer.  The PIs used the framework to show that numerous existing algorithms are optimally \"adaptive\", i.e., they gracefully react to changes in their share of the computer?s RAM.  They also showed that several important algorithms are not adaptive, and gave a rule-of-thumb for algorithm designers who want to make adaptive algorithms.\n\n \nThe PIs developed new algorithms for a class of problems called \"dynamic programs\".  Dynamic programs are at the heart of many other problems, such as finding routes in computer and road networks, aligning new genetic data to existing reference genomes, and analyzing how proteins fold.  The PIs implemented these algorithms in new software tools.  Their tools for these problems are 5-150x faster than prior solutions.  Furthermore, their algorithms are highly parallel, meaning they can utilize all the cores on modern many-core computers.  The PIs have also developed techniques to automatically transform many classical algorithms into fast algorithms that follow their methodology.\n\n \nIn summary, this project has created new tools that practitioners can use to analyze data hundreds of times faster and has created a theoretical framework to help future algorithms designers develop high-performance algorithms.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/28/2019\n\n\t\t\t\t\tSubmitted by: Michael A Bender"
 }
}