{
 "awd_id": "1445796",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Articulate: Augmenting Data Visualization With Natural Language Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 58479.0,
 "awd_amount": 70479.0,
 "awd_min_amd_letter_date": "2014-06-30",
 "awd_max_amd_letter_date": "2015-05-06",
 "awd_abstract_narration": "Nearly one third of the human brain is devoted to processing visual information. Vision is the dominant sense for the acquisition of information from our everyday world.  It is therefore no surprise that visualization, even in its simplest forms, remains the most effective means for converting large volumes of raw data into insight, a process that can support scientific discovery. However a key challenge hindering scientific users from adopting the latest visualization tools and techniques is the steep learning curve that has to be overcome in order to make use of them. The tendency then is to resort to the simplest tools, such as bar charts and line graphs, even though they may lack the expressive power necessary to bring scientific data into focus.\r\n\r\nThe notion that scientists would ideally like to simply speak with a computer to ask questions about their data, and have the computer automatically generate visualizations that answer their queries, has been well known since at least the NSF 2007 report \"Enabling Science Discoveries through Visual Exploration.\"  This is the motivation for the current project, which involves a collaboration among researchers  at two institutions, given that scientists still are unable to do so.  The PIs' ultimate goal is to implement a Virtual Visualization Expert to translate the language of science into the language of visualization.  To demonstrate the concept is indeed viable, the PIs previously developed and evaluated a small prototype, which supported their argument that by relieving the user of the burden of having to learn how to use a complex interface one could enable them to focus on articulating better scientific questions.\r\n\r\nGiven this initial success, the focus of this exploratory research is to establish the foundations of a more generalizable approach that can encompass techniques used in scientific visualization.  To this end, the PIs will research the steps needed for mapping natural language requests, which may be accompanied by gestures, into meaningful visualizations and for enabling incremental creation and modifications of visualizations.  They will develop innovative models to understand the intent of the user and the objects s/he is referring to, and they will explore how best to design user interfaces for creating and modifying visualizations using language and direct manipulation.  The PIs' initial study showed that all these capabilities are crucial to enabling users to make the best use of a dialogic interface for data visualization.  Although project outcomes will be geared in the short term to serving the scientific community, the techniques should be applicable more broadly to consumers of information, such as citizen scientists, public policy decision makers, and students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Leigh",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jason Leigh",
   "pi_email_addr": "leighj@hawaii.edu",
   "nsf_id": "000423955",
   "pi_start_date": "2014-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Hawaii",
  "inst_street_address": "2425 CAMPUS RD SINCLAIR RM 1",
  "inst_street_address_2": "",
  "inst_city_name": "HONOLULU",
  "inst_state_code": "HI",
  "inst_state_name": "Hawaii",
  "inst_phone_num": "8089567800",
  "inst_zip_code": "968222247",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "HI01",
  "org_lgl_bus_name": "UNIVERSITY OF HAWAII",
  "org_prnt_uei_num": "",
  "org_uei_num": "NSCKLFSSABF2"
 },
 "perf_inst": {
  "perf_inst_name": "University of Hawaii",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "HI",
  "perf_st_name": "Hawaii",
  "perf_zip_code": "968222234",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "HI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 58479.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 12000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Humans communicate with each other naturally through speech and gesture. The goal of this project is to begin to collect data about how humans can create data visualizations purely through speech and gesture.&nbsp;</p>\n<p>The outcomes of this project include experimental data in the form of videos, audio and transcripts of users interacting with a computer system. It also includes research presentations and papers delivered at top internationally recognized conferences (SIGDIAL and IEEE Visualization). Software was also developed to demonstrate the efficacy of an approach toward enabling computers to understand humans and, as a result, produce visualizations of data. This project provided valuable research experience for graduate students, one of whom is female, while the other is of hispanic descent. It also provided research experience for undergraduates, some of whom were of pacific islander descent. Lastly this project enabled the completion of a Master's project for a student of hispanic descent, who is also continuing on for a PhD degree.</p>\n<p>The outcome of this project will enable future intelligent computer software to be developed that will enable non-computer-experts to make insightful discoveries with data through merely the use of natural speech commands and gestures, as if conversing with a human data analysis expert.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/15/2016<br>\n\t\t\t\t\tModified by: Jason&nbsp;Leigh</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1445796/1445796_10314448_1471237173884_manyPlots2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1445796/1445796_10314448_1471237173884_manyPlots2--rgov-800width.jpg\" title=\"Snapshot of a user study showing user interacting with Articulate via voice command\"><img src=\"/por/images/Reports/POR/2016/1445796/1445796_10314448_1471237173884_manyPlots2--rgov-66x44.jpg\" alt=\"Snapshot of a user study showing user interacting with Articulate via voice command\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This is a snapshot of an experiment to collect data on how users communicate with the computer verbally when creating visualizations.</div>\n<div class=\"imageCredit\">University of Illinois at Chicago, University of Hawaii at Manoa</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Jason&nbsp;Leigh</div>\n<div class=\"imageTitle\">Snapshot of a user study showing user interacting with Articulate via voice command</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nHumans communicate with each other naturally through speech and gesture. The goal of this project is to begin to collect data about how humans can create data visualizations purely through speech and gesture. \n\nThe outcomes of this project include experimental data in the form of videos, audio and transcripts of users interacting with a computer system. It also includes research presentations and papers delivered at top internationally recognized conferences (SIGDIAL and IEEE Visualization). Software was also developed to demonstrate the efficacy of an approach toward enabling computers to understand humans and, as a result, produce visualizations of data. This project provided valuable research experience for graduate students, one of whom is female, while the other is of hispanic descent. It also provided research experience for undergraduates, some of whom were of pacific islander descent. Lastly this project enabled the completion of a Master's project for a student of hispanic descent, who is also continuing on for a PhD degree.\n\nThe outcome of this project will enable future intelligent computer software to be developed that will enable non-computer-experts to make insightful discoveries with data through merely the use of natural speech commands and gestures, as if conversing with a human data analysis expert.\n\n\t\t\t\t\tLast Modified: 08/15/2016\n\n\t\t\t\t\tSubmitted by: Jason Leigh"
 }
}