{
 "awd_id": "1439007",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: DSD: Collaborative Research: Rapid Prototyping HPC Environment for Deep Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tevfik Kosar",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 315000.0,
 "awd_amount": 315000.0,
 "awd_min_amd_letter_date": "2014-08-05",
 "awd_max_amd_letter_date": "2014-08-05",
 "awd_abstract_narration": "The impact of Big Data is all around us and is enabling a plethora of commercial services. Further it is establishing the fourth paradigm of scientific investigation where discovery is based on mining data rather than from theories verified by observation. Big Data has established a new discipline (Data Science) with vibrant research activities across several areas of computer science. This ?Rapid Python Deep Learning Infrastructure? (RaPyDLI) project advances Deep Learning (DL) which is a novel exciting artificial intelligence approach to Big Data problems, which also involves a sophisticated model and a corresponding ?big compute? needing high end supercomputer architectures. DL has already seen success in areas like speech recognition, drug discovery and computer vision where self-driving cars are an early target. DL uses a very general unbiased way of analyzing large data sets inspired by the brain as a set of connected neurons. As with the brain, the artificial neurons learn from experience corresponding to a ?training dataset? and the ?trained network? can be used to make decisions. Trained on voices, the DL network can enhance voice recognition and trained on images, the DL network can recognize objects in the image. A recent study by the Stanford participants in this project trained 10 billion connections on 10 million images to recognize objects in an image. This study involved a dataset that was approximately 0.1% the size of data ?learnt? by an adult human in their lifetime and one billionth of the total digital data stored in the world today. Note the 1.5 billion images uploaded to social media sites every day emphasize the staggering size of big data. The project aims to enhance by DL by allowing it to use large supercomputers efficiently and by providing a convenient DL computing environment that enables rapid prototyping i.e. interactive experimentation with new algorithms. This will enable DL to be applied to much larger datasets such as those ?seen? by a human in their lifetime. The RaPyDLI partnership of Indiana University, University of Tennessee, and Stanford enables this with expertise in parallel computing algorithms and run times, big data, clouds, and DL itself.\r\nRaPyDLI will reach out to DL practitioners with workshops both to gather requirements for and feedback on its software. Further it will proactively reach out to under-represented communities with summer experiences and DL curriculum modules that include demonstrations built as ?Deep Learning as a Service?.\r\nRaPyDLI will be built as a set of open source modules that can be accessed from a Python user interface but executed interoperably in a C/C++ or Java environment on the largest supercomputers or clouds with interactive analysis and visualization. RaPyDLI will support GPU accelerators and Intel Phi coprocessors and a broad range of storage approaches including files, NoSQL, HDFS and databases. RaPyDLI will include benchmarks as well as software and will offer a repository so users can contribute the high level code for a range of neural networks with benefits to research and education.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Geoffrey",
   "pi_last_name": "Fox",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Geoffrey C Fox",
   "pi_email_addr": "vxj6mb@virginia.edu",
   "nsf_id": "000231257",
   "pi_start_date": "2014-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregor",
   "pi_last_name": "von Laszewski",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gregor von Laszewski",
   "pi_email_addr": "thf2bn@virginia.edu",
   "nsf_id": "000275379",
   "pi_start_date": "2014-08-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Judy",
   "pi_last_name": "Fox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Judy Fox",
   "pi_email_addr": "ckw9mp@virginia.edu",
   "nsf_id": "000555330",
   "pi_start_date": "2014-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "901 E 10th Street",
  "perf_city_name": "Bloomington",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474083912",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 315000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-80a0b129-3c28-6d1d-c0a8-3ebb8eb4e36b\"> </span></p>\n<p dir=\"ltr\"><span>The Rapid Python Deep Learning Infrastructure (RaPyDLI) project, a collaboration between the University of Tennessee, Indiana University, and Stanford University, delivers productivity and performance to the deep learning (DL) community by combining high-level Python, C/C++, and Java environments with carefully designed libraries for graphics processing units (GPUs) and Intel Xeon Phi coprocessors.</span></p>\n<p dir=\"ltr\"><span>DL has major impacts in areas like speech recognition, drug discovery, computer vision, and natural language processing. In general, learning networks take input data, such as a set of pixels from an image, and map them into decisions, such as labeling animals in an image, or deciding how best to drive a car given a video image from a vehicle-mounted camera. The input and output are linked by multiple nodes in a layered network, where each node corresponds to a parameterized transformation&mdash;a function of weights and thresholds. A (very) deep learning network has multiple layers (intermediate nodes), perhaps 10 or more. Introduced around 25 years ago, DL networks were originally unsuccessful, but improved algorithms and drastic increases in available compute power led to practical breakthroughs around 10 years ago. DLs are now used in all major commercial speech recognition systems and outperform previous Hidden Markov Model-based systems.</span></p>\n<p dir=\"ltr\"><span>The quality of a DL network depends on the number of layers and the nature of the nodes in the network. RaPyDLI&rsquo;s approach, which leverages hardware accelerators (GPUs), is of prime importance because these accelerators increase the available computational power by more than an order of magnitude, thereby enabling much more sophisticated neural networks. Further, this success relies on training large neural nets&mdash;currently, up to 10 billion connections trained on 10 million images&mdash;using either large-scale commodity clusters or smaller high-performance computing systems, where these GPU accelerators perform with high efficiency.</span></p>\n<p dir=\"ltr\"><span>With its rapid prototyping features, RaPyDLI is also used as an onramp to DL. It serves as an aggregator for modules related to DL while simplifying access through a service interface and hosted services that provide access to fast computing resources. We offer simple interfaces for the deployment framework while also offering access to the actual algorithmic functionality through a variety of interfaces. This includes a Python application programming interface (based on REST), a command shell, and web services. In addition, we provide an experiment management framework that enables us to monitor the services and resources.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/20/2017<br>\n\t\t\t\t\tModified by: Geoffrey&nbsp;C&nbsp;Fox</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe Rapid Python Deep Learning Infrastructure (RaPyDLI) project, a collaboration between the University of Tennessee, Indiana University, and Stanford University, delivers productivity and performance to the deep learning (DL) community by combining high-level Python, C/C++, and Java environments with carefully designed libraries for graphics processing units (GPUs) and Intel Xeon Phi coprocessors.\nDL has major impacts in areas like speech recognition, drug discovery, computer vision, and natural language processing. In general, learning networks take input data, such as a set of pixels from an image, and map them into decisions, such as labeling animals in an image, or deciding how best to drive a car given a video image from a vehicle-mounted camera. The input and output are linked by multiple nodes in a layered network, where each node corresponds to a parameterized transformation&mdash;a function of weights and thresholds. A (very) deep learning network has multiple layers (intermediate nodes), perhaps 10 or more. Introduced around 25 years ago, DL networks were originally unsuccessful, but improved algorithms and drastic increases in available compute power led to practical breakthroughs around 10 years ago. DLs are now used in all major commercial speech recognition systems and outperform previous Hidden Markov Model-based systems.\nThe quality of a DL network depends on the number of layers and the nature of the nodes in the network. RaPyDLI?s approach, which leverages hardware accelerators (GPUs), is of prime importance because these accelerators increase the available computational power by more than an order of magnitude, thereby enabling much more sophisticated neural networks. Further, this success relies on training large neural nets&mdash;currently, up to 10 billion connections trained on 10 million images&mdash;using either large-scale commodity clusters or smaller high-performance computing systems, where these GPU accelerators perform with high efficiency.\nWith its rapid prototyping features, RaPyDLI is also used as an onramp to DL. It serves as an aggregator for modules related to DL while simplifying access through a service interface and hosted services that provide access to fast computing resources. We offer simple interfaces for the deployment framework while also offering access to the actual algorithmic functionality through a variety of interfaces. This includes a Python application programming interface (based on REST), a command shell, and web services. In addition, we provide an experiment management framework that enables us to monitor the services and resources.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/20/2017\n\n\t\t\t\t\tSubmitted by: Geoffrey C Fox"
 }
}