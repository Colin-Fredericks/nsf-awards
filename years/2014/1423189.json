{
 "awd_id": "1423189",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Small: Looking Across the Uncanny Valley: Procedural and Data-Driven Methods for Gaze Modeling",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 494347.0,
 "awd_amount": 520513.0,
 "awd_min_amd_letter_date": "2014-07-31",
 "awd_max_amd_letter_date": "2020-06-15",
 "awd_abstract_narration": "Eye movements play a key role in human communication, yet they remain a significant stumbling block for humanoid animation.  Computer generated avatars currently lack realistic gaze, which subconsciously distracts viewers and thereby detracts from the usefulness of the many applications   that employ these graphical actors (e.g., educational / tutoring / training systems that incorporate animated agents), a perceptual issue that has been dubbed the \"uncanny valley.\"  This project represents a collaboration between two investigators with complementary skills who will tackle the problem by developing a holistic model of gaze dynamics for avatars, which combines detailed real-world measurements (with the aid of binocular eye-tracking) and signal analysis of both eye motions and the periocular skin region around the eyes, to generate improved synthetic eye movement animations.  Project activities will include creation of a database of gaze motions,  perceptual experiments to improve our understanding of the saliency of different components of human gaze (eyeball rotation, vergence, jitter, periocular skin motion) and their signal properties in physical space, and the implementation of exemplary tasks (such as reading and conversations) along with a software tool which will enable animators to more easily design convincing gaze in frequently encountered situations.  The PI intends to open-source the software to be developed in this research.  \r\n\r\nThe gaze model under development by the PI differs from previous approaches in the following ways.   First, while others have modeled cyclopean avatar gaze few have accessed the steadily growing eye tracking literature for inclusion of binocular eye movement.  The PI argues that binocular eye tracking in three-dimensional physical space allows estimation of where the subject is fixating in depth and, consequently, recording, analysis, and modeling of gaze vergence, which will yield more believable characters.  Second, the proposed model provides a component of subtle gaze jitter, which is critical for dynamic realism as the eyes are never perfectly still.  Third, the description of the rotations of the eyes is mathematically concise, follows physiological laws, and is easy to implement.  Finally, the proposed modeling effort includes perceptual studies designed to investigate the influence of each model component and to optimize the parameters of the resulting complete model.  The procedural model is two-staged and reminiscent of the functionality of human vision: a bottom-up stage of eye rotation, which will be used to represent gaze when selecting a series of look points, followed by a top-down stage of gaze orientation dependent on a given task.  Building a perceptual science underlying gaze modeling will foster the believability of synthetic actors, and will more broadly impact diverse areas such as social robotics where realistic gaze simulation is crucial for creating likable robots.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sophie",
   "pi_last_name": "Joerg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sophie Joerg",
   "pi_email_addr": "sjoerg@clemson.edu",
   "nsf_id": "000626746",
   "pi_start_date": "2014-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Duchowski",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew T Duchowski",
   "pi_email_addr": "duchowski@clemson.edu",
   "nsf_id": "000492350",
   "pi_start_date": "2014-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Clemson University",
  "inst_street_address": "201 SIKES HALL",
  "inst_street_address_2": "",
  "inst_city_name": "CLEMSON",
  "inst_state_code": "SC",
  "inst_state_name": "South Carolina",
  "inst_phone_num": "8646562424",
  "inst_zip_code": "296340001",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "SC03",
  "org_lgl_bus_name": "CLEMSON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "H2BMNX7DSKU8"
 },
 "perf_inst": {
  "perf_inst_name": "Clemson University",
  "perf_str_addr": "300 Brackett Hall",
  "perf_city_name": "Clemson",
  "perf_st_code": "SC",
  "perf_st_name": "South Carolina",
  "perf_zip_code": "296340001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "SC03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 177609.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 321738.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 21166.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Eye movements play a key role in human communication. Correspondingly, convincing gaze motion is crucial for creating engaging virtual characters and avatars. With this award, we have developed models to synthesize gaze for virtual characters and realized a series of perceptual experiments to create and evaluate our models.</p>\n<p>Our models for gaze synthesis include a method to create a synthetic stream of raw gaze position coordinates, a model of saccadic velocity, and procedural models to synthesize emotionally expressive reading behavior and emotionally expressive face-scanning behavior. We furthermore model subtle gaze jitter, which is critical for dynamic realism as the eyes are never perfectly still. We show that microsaccadic eye motions can be simulated by adding jitter to the movements of the eyeball and pupil. We found the scaling factors influencing the amplitude of eyeball and pupil size jitter that are perceived as most realistic for three different models: a realistic human, a cartoony human, and an anthropomorphic robot.</p>\n<p>Our perceptual experiments revealed that microsaccadic jitter is important when modeling realistic eye motions. When absent, eye motions are perceived as less natural. We compared four techniques to create eye motion jitter and pupil dilation ? data-driven, keyframed, motion captured, and procedural ? viewers considered our keyframed animation which did not include jitter to be significantly less natural than all other animation techniques, whereas we did not find any significant differences between the other animation techniques. These findings hold across the different character models and motion types we tested.Further findings include that viewers are able to discriminate between reading and face-scanning motions but are not able to recognize the emotional valence in these motions based on our model and stimuli and that participants are able to recognize fear based on pupil motions.</p>\n<p>Our work connects the knowledge and methods in the computer graphics and eye tracking communities and has been published in computer animation, perception, eye-tracking, and virtual reality journals and conference proceedings. It contributes to creating more realistic and expressive virtual characters, virtual agents, and avatars and can thus also have a broader impact in areas such as education, virtual reality, and social robotics.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2021<br>\n\t\t\t\t\tModified by: Sophie&nbsp;Joerg</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nEye movements play a key role in human communication. Correspondingly, convincing gaze motion is crucial for creating engaging virtual characters and avatars. With this award, we have developed models to synthesize gaze for virtual characters and realized a series of perceptual experiments to create and evaluate our models.\n\nOur models for gaze synthesis include a method to create a synthetic stream of raw gaze position coordinates, a model of saccadic velocity, and procedural models to synthesize emotionally expressive reading behavior and emotionally expressive face-scanning behavior. We furthermore model subtle gaze jitter, which is critical for dynamic realism as the eyes are never perfectly still. We show that microsaccadic eye motions can be simulated by adding jitter to the movements of the eyeball and pupil. We found the scaling factors influencing the amplitude of eyeball and pupil size jitter that are perceived as most realistic for three different models: a realistic human, a cartoony human, and an anthropomorphic robot.\n\nOur perceptual experiments revealed that microsaccadic jitter is important when modeling realistic eye motions. When absent, eye motions are perceived as less natural. We compared four techniques to create eye motion jitter and pupil dilation ? data-driven, keyframed, motion captured, and procedural ? viewers considered our keyframed animation which did not include jitter to be significantly less natural than all other animation techniques, whereas we did not find any significant differences between the other animation techniques. These findings hold across the different character models and motion types we tested.Further findings include that viewers are able to discriminate between reading and face-scanning motions but are not able to recognize the emotional valence in these motions based on our model and stimuli and that participants are able to recognize fear based on pupil motions.\n\nOur work connects the knowledge and methods in the computer graphics and eye tracking communities and has been published in computer animation, perception, eye-tracking, and virtual reality journals and conference proceedings. It contributes to creating more realistic and expressive virtual characters, virtual agents, and avatars and can thus also have a broader impact in areas such as education, virtual reality, and social robotics. \n\n \n\n\t\t\t\t\tLast Modified: 11/29/2021\n\n\t\t\t\t\tSubmitted by: Sophie Joerg"
 }
}