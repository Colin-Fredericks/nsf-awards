{
 "awd_id": "1415954",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Object Pose Estimation System for Pick and Place Robots",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Muralidharan Nair",
 "awd_eff_date": "2014-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 115750.0,
 "awd_amount": 138800.0,
 "awd_min_amd_letter_date": "2014-05-20",
 "awd_max_amd_letter_date": "2014-12-15",
 "awd_abstract_narration": "The broader impact/commercial potential of this project will enable improved cost-efficiency and industrial automation in manufacturing, increasing worker productivity and reducing injuries. The end-users of the robots, i.e., automotive original-equipment-manufacturers and subassembly suppliers, will be able to achieve significant cost advantages by automating new assembly tasks with more inexpensive systems. Of the non-fatal injuries and illness cases reported in the U.S. workforce, 43% of injuries were due to bodily reaction/exertion, and 62% of illness cases were due to repetitive trauma. This innovative solution will facilitate the automation of repetitive, injury-prone manual tasks and greatly improve the speed, accuracy, and cost-efficiency of current robotic handling systems.  Beyond handling, there is significant market potential in packaging and warehousing, hazardous materials handling, medical device and other precision manufacturing, and military applications such as bomb defusal and evacuation robots. By enabling new robotic applications and increasing productivity in current automation, this sensor will help the U.S. (and other developed economies) maintain a competitive domestic manufacturing sector.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase I project?s goal is to develop a visual-tactile sensing package for parts handling.  The solution is two-fold: (1) A new flexible tactile sensor that can be tailored to a wide variety of form factors; (2) Software to fuse the tactile data with a vision system to estimate pose of objects in pick-and-place tasks.  Object grasping and manipulation by robotic hands in unstructured environments demands a sensor that is durable, compliant, and responsive to various force and slip conditions. The goal is to be the first commercially available sensing package that integrates tactile and visual data with accompanying software for state estimation.  A large software and gaming company was able to greatly impact the machine vision space by introducing an inexpensive, easily calibrated robust visual sensor; this will do the same for touch sensing ? our team has studied the desirable properties of such tactile sensors for years and discovered a way to produce them in an inexpensive, robust format.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicholas",
   "pi_last_name": "Wettels",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nicholas Wettels",
   "pi_email_addr": "nwettels@perceptionrobotics.com",
   "nsf_id": "000600356",
   "pi_start_date": "2014-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Hargrave",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brian Hargrave",
   "pi_email_addr": "hargrave@somatistech.com",
   "nsf_id": "000660696",
   "pi_start_date": "2014-05-20",
   "pi_end_date": "2014-08-28"
  }
 ],
 "inst": {
  "inst_name": "Perception Robotics",
  "inst_street_address": "525 S. Hewitt St",
  "inst_street_address_2": "",
  "inst_city_name": "Los Angeles",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2134770710",
  "inst_zip_code": "900132215",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "JHLRJZ3JD563"
 },
 "perf_inst": {
  "perf_inst_name": "Perceptra Robotics",
  "perf_str_addr": "411 S. Hewitt St.",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900132215",
  "perf_ctry_code": "US",
  "perf_cong_dist": "34",
  "perf_st_cong_dist": "CA34",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "163E",
   "pgm_ref_txt": "SBIR Phase IB"
  },
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "8035",
   "pgm_ref_txt": "Hardware Devices"
  },
  {
   "pgm_ref_code": "9139",
   "pgm_ref_txt": "INFORMATION INFRASTRUCTURE & TECH APPL"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 115750.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 23050.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p style=\"text-align: left;\">We have built an innovative combined visual-tactile prototype that will give robots an integrated sense of touch and vision, much like the hand-eye coordination of humans. Current industrial robots are restricted in their ability to handle small, irregularly shaped, soft, or fragile parts. Existing solutions rely on expensive and complex 3D-vision systems or repetitive manual labor. Our solution incorporates a technically novel compliant tactile sensing solution&mdash;a rubber &ldquo;skin&rdquo; that can be molded into any form factor and is inexpensive and durable. &nbsp;This advanced skin technology can resolve object shape, contact/slip events, and forces of contacted objects. &nbsp;We will uniquely fuse visual and tactile information for object handling resulting in flexible robotic system that handles objects more like humans do. &nbsp;Our approach addresses key weaknesses in vision-based robotic manufacturing, such as occlusion and dislodging when parts are grasped. The result of this research will produce data on conductive elastomer fabrication and software methods to fuse visual and tactile data for more agile manufacturing processes.</p>\n<p style=\"text-align: left;\">The end-users of the robots will be able to achieve significant cost advantages by automating new assembly tasks with more inexpensive systems. Of the non-fatal injuries and illness cases reported in the U.S. workforce, 43% of injuries were due to bodily reaction/exertion, and 62% of illness cases were due to repetitive trauma. Our innovative system will facilitate the automation of repetitive, injury-prone manual tasks and greatly improve the speed and accuracy of current robotic handling systems at 3-4 times less the cost of existing solutions &ndash; if existing solutions can be applied at all.</p>\n<p style=\"text-align: left;\">This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/04/2015<br>\n\t\t\t\t\tModified by: Nicholas&nbsp;Wettels</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "We have built an innovative combined visual-tactile prototype that will give robots an integrated sense of touch and vision, much like the hand-eye coordination of humans. Current industrial robots are restricted in their ability to handle small, irregularly shaped, soft, or fragile parts. Existing solutions rely on expensive and complex 3D-vision systems or repetitive manual labor. Our solution incorporates a technically novel compliant tactile sensing solution&mdash;a rubber \"skin\" that can be molded into any form factor and is inexpensive and durable.  This advanced skin technology can resolve object shape, contact/slip events, and forces of contacted objects.  We will uniquely fuse visual and tactile information for object handling resulting in flexible robotic system that handles objects more like humans do.  Our approach addresses key weaknesses in vision-based robotic manufacturing, such as occlusion and dislodging when parts are grasped. The result of this research will produce data on conductive elastomer fabrication and software methods to fuse visual and tactile data for more agile manufacturing processes.\nThe end-users of the robots will be able to achieve significant cost advantages by automating new assembly tasks with more inexpensive systems. Of the non-fatal injuries and illness cases reported in the U.S. workforce, 43% of injuries were due to bodily reaction/exertion, and 62% of illness cases were due to repetitive trauma. Our innovative system will facilitate the automation of repetitive, injury-prone manual tasks and greatly improve the speed and accuracy of current robotic handling systems at 3-4 times less the cost of existing solutions &ndash; if existing solutions can be applied at all.\nThis Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.\n\n \n\n\t\t\t\t\tLast Modified: 08/04/2015\n\n\t\t\t\t\tSubmitted by: Nicholas Wettels"
 }
}