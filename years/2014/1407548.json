{
 "awd_id": "1407548",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Flexible Statistical Modeling",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2019-10-31",
 "tot_intn_awd_amt": 499996.0,
 "awd_amount": 499996.0,
 "awd_min_amd_letter_date": "2014-06-06",
 "awd_max_amd_letter_date": "2017-06-09",
 "awd_abstract_narration": "The abundance of data in science, medicine and commerce, and the current state of computing technologies gives us opportunities in statistical modeling never seen before. We are able to build powerful predictive models for the risk of breast cancer, heart disease or stroke, for example, using genomic markers. We can predict the risk of credit-card default or fraudulent insurance claims. Predictive models are able to recommend movies or music to a customer, based on their past behavior and preferences and that of customers like them. Using data on locations of sightings of multiple animal or plant species, we can build distribution maps over a geographical domain. With large amounts of data, it becomes necessary that these models are built in an automatic way; the goal of this project is to ensure that the resulting products remain interpretable.\r\n\r\nGeneralized additive models are both interpretable and somewhat powerful, but were originally intended for a relatively small set of predictor variables. This project will use methods in convex optimization to automatically build such models using potentially thousands of variables. The method will automatically omit irrelevant variables, as well as select the amount of nonlinearity needed for all those retained. Convex methods will also be used to incorporate side information in matrix completion problems, as well as a variety of multivariate methods where we have traditionally worked with low-rank representations. Ecologists often struggle with combining data from multiple species and different sampling schemes. This project will provide a unified framework using inhomogeneous Poisson process models for combining these data, and producing high-quality distribution.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Trevor",
   "pi_last_name": "Hastie",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Trevor J Hastie",
   "pi_email_addr": "hastie@stanford.edu",
   "nsf_id": "000447619",
   "pi_start_date": "2014-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "CA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 96998.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 129999.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 134999.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 138000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>My research is in applied statistical modeling. This includes: developing new approaches to modeling data, along with software implementations; reinterpreting and debunking existing popular techniques; and teaching the important techniques in an accessible way. I made substantial progress in all of these areas during the granting period 2014-2019.</p>\n<div>&nbsp;</div>\n<div>I developed a number of new techniques (and accompanying software) using state-of-the-art methods in convex optimization for building sparse models: additive models, low-rank models for matrix completion, and elastic-net models for very large data. These are critical in the era of wide data, which frequently arises in genomics and other similar environments. In one application we fit polygenic risk scores using genomic data from 500K individuals with measurements on 800K SNPs. I also developed some new approaches to matrix completion (missing data estimation) that scale to very large problems. For both of these projects, I developed high-quality software in R which is publicly available to the wider community.</div>\n<p>&nbsp;</p>\n<div>&nbsp;</div>\n<div>With coauthors I wrote two textbooks \"Statistical Learning with Sparsity\" (Chapman and Hall 2015, with Rob Tibshirani and Martin Wainwright) and \"Computer Age Statistical Inference\" (Cambridge 2016, with Bradley Efron). By agreement with the publishers, digital versions of these books are freely available on my website.</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/14/2019<br>\n\t\t\t\t\tModified by: Trevor&nbsp;Hastie</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMy research is in applied statistical modeling. This includes: developing new approaches to modeling data, along with software implementations; reinterpreting and debunking existing popular techniques; and teaching the important techniques in an accessible way. I made substantial progress in all of these areas during the granting period 2014-2019.\n \nI developed a number of new techniques (and accompanying software) using state-of-the-art methods in convex optimization for building sparse models: additive models, low-rank models for matrix completion, and elastic-net models for very large data. These are critical in the era of wide data, which frequently arises in genomics and other similar environments. In one application we fit polygenic risk scores using genomic data from 500K individuals with measurements on 800K SNPs. I also developed some new approaches to matrix completion (missing data estimation) that scale to very large problems. For both of these projects, I developed high-quality software in R which is publicly available to the wider community.\n\n \n \nWith coauthors I wrote two textbooks \"Statistical Learning with Sparsity\" (Chapman and Hall 2015, with Rob Tibshirani and Martin Wainwright) and \"Computer Age Statistical Inference\" (Cambridge 2016, with Bradley Efron). By agreement with the publishers, digital versions of these books are freely available on my website.\n\n \n\n\t\t\t\t\tLast Modified: 11/14/2019\n\n\t\t\t\t\tSubmitted by: Trevor Hastie"
 }
}