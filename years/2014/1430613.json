{
 "awd_id": "1430613",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Enabling Taskability: Research on Teaching Computers new Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2014-04-01",
 "awd_exp_date": "2017-03-31",
 "tot_intn_awd_amt": 298619.0,
 "awd_amount": 298619.0,
 "awd_min_amd_letter_date": "2014-03-19",
 "awd_max_amd_letter_date": "2014-03-19",
 "awd_abstract_narration": "This study investigates a new line of research into the \"taskability\" of cognitive agents. Taskability is the ability of an agent to accept high level, task-oriented instructions from a human and translate those goal-oriented directives into a suitably decomposed plan of sensing, reasoning, or action. Little to no research has been conducted on this subject to date.  This research defines taskability in a manner sufficiently formal to allow for scientific research, establishes some of the initial conditions needed to evaluate taskable agents, and advances theories and prototype agents that meet those requirements.  Specifically, the four research activities being undertaken in this study include: 1. A review, analysis, and synthesis of prior work from multiple disciplines that that can lay groundwork for focused cognitive systems research in this area; 2) An analysis of the different types of tasks and their structure, as well as the associated types of knowledge that must be learned by taskable agents; 3) Research on how people use instruction with taskable agents for such activities, performing user studies to determine the required task knowledge and agent capabilities; and 4) An extension of current cognitive agent capabilities to support the behaviors observed in the aforementioned studies, resulting in a software system that can be evaluated and co-developed with theory.  \r\n\r\nThe current state of the art in research for cognitive systems and agents in general allows human handlers to issue directions to agents either in terms of lower level action primitives that correspond to the agent's particular design, or constrained by a higher-order action language (or even visual repertoire) that is engineered to suffice as a shorthand for some sequence of the same kinds of agent-specific behaviors. Taskability requires that an agent be able to interpret a richer, less constrained set of instructions from a user and, despite a lack of precompiled task decomposition instructions, dynamically formulate an accurate representation of the task to be performed, and even learn new tasks via this sort of interaction.  Taskable agents would fundamentally change the way humans interact with intelligent agents and robotic systems in a broad range of disciplines and environments, leading to richer interactions with more capable cars, phones, and almost any device containing a computer, all of which might respond to human interaction without resort to preprogrammed interaction modes. This in turn promises transformational advances in a range of application domains such as health care, industry, government, and home automation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Laird",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "John E Laird",
   "pi_email_addr": "laird@umich.edu",
   "nsf_id": "000331801",
   "pi_start_date": "2014-03-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, 3753 Beyster Bldg.",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7484",
   "pgm_ref_txt": "IIS SPECIAL PROJECTS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 298619.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Over the last 30 years, there has been significant progress in AI, Cognitive Science, and Robotics in research on reasoning, problem solving, and learning. We now have a greater understanding of how to extract regularities from data and behavior, and use that knowledge to drive behavior. There has also been significant progress in research on developing autonomous systems, where agents that can select appropriate tasks and pursue them as they arise. However, all these agents, even those that have learning capabilities, are prisoners to the tasks for which they are programmed. They cannot be taught new activities or novel problems to be solved. As extreme examples, Deep Blue became the world&rsquo;s chess champion and Watson won the Jeopardy Challenge, but neither of them can play Tic-Tac-Toe or any other game, nor can they be taught how to play a new game. The goal of this project was to take the first steps in research on <em>interactive task learning</em> (ITL) &ndash; the ability of an agent to learn <em>new</em> tasks from a human through natural interaction. ITL has the potential to fundamentally change the way we interact with intelligent agents and robotic systems across science, health care, industry, government, home, and entertainment.</p>\n<p>Our major outcomes have been to develop theories and implementations of the general mechanisms and structures necessary for agents to learn how to learn to formulate and master tasks, moving to a future where intelligent agents are no longer limited to performing only pre-programmed tasks.</p>\n<p>&nbsp;Associated with this project, we have organized a one-day workshop associated with the International Conference on Cognitive Modeling, which was held on April 8, 2015 in Groningen, Netherlands. We are also organizing an Ernst Str&uuml;ngmann Forum on Interactive Task Learning for May 2017. These forums &ldquo;provides a creative environment within which top international scientists discuss themes that transcend classic disciplinary boundaries.&rdquo; Both of these activities were done in collaboration with Kevin Gluck (AFRL).</p>\n<p>We have made significant strides in understanding the computational processes involved in learning new tasks through natural interaction, requiring advances in AI, Cognitive Science, and Robotics. The most concrete evidence of our achievements is the development of an AI agent, called Rosie, that is implemented in the Soar cognitive architecture, and is embodied in both a tabletop robot and a small robotic platform. That agent can learn over 30 different puzzles and games and a variety of navigation and delivery tasks through natural language instruction. Developing this agent involved many advancements.</p>\n<p>1. A novel integrated parser based on embodied construction grammar (ECG) that supports a wide variety of syntactic and semantic structures for grounded instruction. In addition, the instructor can refer to objects that are not immediately visible &ndash; some that the agent has seen in the past, and some it has never experienced.</p>\n<p>2. The agent can learn new concepts through instruction (including new verbs, nouns, and adjectives), and can then use those concepts for interacting with the instructor and also to learn additional concepts.</p>\n<p>3. The agent is achieves significant transfer of learned knowledge between different tasks, both within similar games and similar mobile navigation tasks.</p>\n<p>4. We have extended our agent so it can learn goals not only from language descriptions, but also from demonstrations of goal states.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/05/2017<br>\n\t\t\t\t\tModified by: John&nbsp;E&nbsp;Laird</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1430613/1430613_10295443_1491401095773__DSC0120--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1430613/1430613_10295443_1491401095773__DSC0120--rgov-800width.jpg\" title=\"Interactive Task Learning in Rosie\"><img src=\"/por/images/Reports/POR/2017/1430613/1430613_10295443_1491401095773__DSC0120--rgov-66x44.jpg\" alt=\"Interactive Task Learning in Rosie\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Rosie interactive task learning agent learning the Tower of Hanoi puzzle.</div>\n<div class=\"imageCredit\">John Laird</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">John&nbsp;E&nbsp;Laird</div>\n<div class=\"imageTitle\">Interactive Task Learning in Rosie</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nOver the last 30 years, there has been significant progress in AI, Cognitive Science, and Robotics in research on reasoning, problem solving, and learning. We now have a greater understanding of how to extract regularities from data and behavior, and use that knowledge to drive behavior. There has also been significant progress in research on developing autonomous systems, where agents that can select appropriate tasks and pursue them as they arise. However, all these agents, even those that have learning capabilities, are prisoners to the tasks for which they are programmed. They cannot be taught new activities or novel problems to be solved. As extreme examples, Deep Blue became the world?s chess champion and Watson won the Jeopardy Challenge, but neither of them can play Tic-Tac-Toe or any other game, nor can they be taught how to play a new game. The goal of this project was to take the first steps in research on interactive task learning (ITL) &ndash; the ability of an agent to learn new tasks from a human through natural interaction. ITL has the potential to fundamentally change the way we interact with intelligent agents and robotic systems across science, health care, industry, government, home, and entertainment.\n\nOur major outcomes have been to develop theories and implementations of the general mechanisms and structures necessary for agents to learn how to learn to formulate and master tasks, moving to a future where intelligent agents are no longer limited to performing only pre-programmed tasks.\n\n Associated with this project, we have organized a one-day workshop associated with the International Conference on Cognitive Modeling, which was held on April 8, 2015 in Groningen, Netherlands. We are also organizing an Ernst Str&uuml;ngmann Forum on Interactive Task Learning for May 2017. These forums \"provides a creative environment within which top international scientists discuss themes that transcend classic disciplinary boundaries.\" Both of these activities were done in collaboration with Kevin Gluck (AFRL).\n\nWe have made significant strides in understanding the computational processes involved in learning new tasks through natural interaction, requiring advances in AI, Cognitive Science, and Robotics. The most concrete evidence of our achievements is the development of an AI agent, called Rosie, that is implemented in the Soar cognitive architecture, and is embodied in both a tabletop robot and a small robotic platform. That agent can learn over 30 different puzzles and games and a variety of navigation and delivery tasks through natural language instruction. Developing this agent involved many advancements.\n\n1. A novel integrated parser based on embodied construction grammar (ECG) that supports a wide variety of syntactic and semantic structures for grounded instruction. In addition, the instructor can refer to objects that are not immediately visible &ndash; some that the agent has seen in the past, and some it has never experienced.\n\n2. The agent can learn new concepts through instruction (including new verbs, nouns, and adjectives), and can then use those concepts for interacting with the instructor and also to learn additional concepts.\n\n3. The agent is achieves significant transfer of learned knowledge between different tasks, both within similar games and similar mobile navigation tasks.\n\n4. We have extended our agent so it can learn goals not only from language descriptions, but also from demonstrations of goal states.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 04/05/2017\n\n\t\t\t\t\tSubmitted by: John E Laird"
 }
}