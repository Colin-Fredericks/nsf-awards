{
 "awd_id": "1423004",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: EvalFest (Evaluation Use, Value and Learning through Festivals of Science and Technology)",
 "cfda_num": "47.076",
 "org_code": "11090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Robert Russell",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 1633659.0,
 "awd_amount": 1922969.0,
 "awd_min_amd_letter_date": "2014-08-05",
 "awd_max_amd_letter_date": "2018-08-15",
 "awd_abstract_narration": "EvalFest (Evaluation Use, Value, and Learning through Festivals of Science and Technology) will test innovative evaluation methods in science festivals that are being held across the country and assess in what ways and how effectively they are used. Morehead Planetarium and Science Center (at the University of North Carolina-Chapel Hill) and the University of California, San Francisco, in collaboration with over twenty science festivals, will (1) investigate whether a multisite evaluation approach is an effective model for creating common metrics for informal STEM education, (2) develop common methods to measure the effects of Festivals, (3) create a query-able database of 50,000 Festival attendees to share with the informal STEM learning field, and (4) document whether these efforts also result in new knowledge related to informal STEM education. \r\n\r\nThe project will develop the Enterprise Feedback Management (EFM) system and query-able database for the festival community. EFMs are systems, including processes and software, that enable groups (such as the festival network) to collect, organize, analyze and share data. The EFM system will be designed to integrate data across sites and to allow users to extract data of interest. The project will refine evaluation tools currently used within the Science Festival Alliance that assess self-reported festival learning, and the effects of festival attendance, motivation, and future science participation. It will collect economic impact data and longitudinal festival attendee data. The project will also develop some new evaluation tools such as secret shopper observational protocols. Data from festival attendees will be collected onsite at participating festivals.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DRL",
 "org_div_long_name": "Division of Research on Learning in Formal and Informal Settings (DRL)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Denise",
   "pi_last_name": "Young",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Denise Young",
   "pi_email_addr": "dlyoung@email.unc.edu",
   "nsf_id": "000365779",
   "pi_start_date": "2014-08-05",
   "pi_end_date": "2016-09-07"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Todd",
   "pi_last_name": "Boyette",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Todd R Boyette",
   "pi_email_addr": "tboy@email.unc.edu",
   "nsf_id": "000250276",
   "pi_start_date": "2016-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Peterman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karen Peterman",
   "pi_email_addr": "karenpetermanphd@gmail.com",
   "nsf_id": "000556467",
   "pi_start_date": "2014-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993480",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725900",
   "pgm_ele_name": "AISL"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9177",
   "pgm_ref_txt": "ELEMENTARY/SECONDARY EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0414",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001415DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0416",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001617DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 796524.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 546998.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 579447.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The overarching goal of EvalFest was to create a community of practice within the science festival community to test innovative evaluation methods in order to study impacts related to evaluation use, capacity building and the generation of new knowledge related to informal STEM learning. Over the course of the EvalFest project (9/1/2014 &ndash; 8/31/2021), organizers and their evaluation partners from 24 science festivals from across the U.S. worked together to strengthen their evaluation capacity and practice, test innovative methods and share lessons learned with the broader science festival field. The development of the community of practice itself is one of the major outcomes of EvalFest. The 24 participating festivals represent a large portion of all of the science festivals in the country, therefore the work of the community naturally impacts the United States science festival landscape. Participants expressed the importance of gathering in-person to strengthen their work, which resulted in the EvalFest project team organizing three additional annual meetings (2017, 2018 and 2019) beyond the two (2015 and 2016) proposed in the original EvalFest project plan. These annual meetings were devoted to project planning activities and professional development experiences such as training in data collection and interpretation, finding and effectively reporting stories from data, data visualization, science learning activation and the ActApp system &ndash; a suite of programs used to assess efficacy of science education programs.</p>\n<p>&nbsp;</p>\n<p>EvalFest was centered on a centralized evaluation model, therefore initiating and sustaining multisite data collection was another major project outcome. EvalFest partners agreed upon a set of core questions that all participating festivals would use in their evaluation efforts. This allowed for the project team to analyze and compare data from all participating festivals. In addition to the core</p>\n<p>questions, each festival had the opportunity to select other items that they wanted to</p>\n<p>include in their attendee evaluation. More than 60,000 surveys were conducted and analyzed during the course of the EvalFest project.</p>\n<p>&nbsp;</p>\n<p>The EvalFest project team utilized the community of practice to test innovative evaluation methods. These studies were conducted by one of the EvalFest partner sites or by a member of the project advisory board. Methods tested include: Using a &ldquo;Secret Shopper&rdquo; to evaluate science festival events, using embedded assessment methods to collect evaluation data at festival events, using coding methods to assess interactions between scientists and the lay public, the development of outgroup survey instruments to determine why people choose not to participate, using Q-sort methods to evaluate visitor behavior, and the development of an instrument to determine environmental impact of science festivals.</p>\n<p>&nbsp;</p>\n<p>Analysis of the 60K+ surveys and the studies conducted to test innovative methods led to numerous conference presentations and peer-reviewed journal articles. Over the course of the EvalFest project, project team members made 17 conference presentations and published 11 articles in peer-reviewed journals using EvalFest data.</p>\n<p>&nbsp;</p>\n<p>Videos of EvalFest webinars have been viewed on YouTube nearly 1000 times. These videos and all other assets developed through the EvalFest project are now accessible through the website of the Science Festival Alliance: <a href=\"https://www.sciencefestivals.org/\">https://www.sciencefestivals.org/</a></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Todd&nbsp;R&nbsp;Boyette</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe overarching goal of EvalFest was to create a community of practice within the science festival community to test innovative evaluation methods in order to study impacts related to evaluation use, capacity building and the generation of new knowledge related to informal STEM learning. Over the course of the EvalFest project (9/1/2014 &ndash; 8/31/2021), organizers and their evaluation partners from 24 science festivals from across the U.S. worked together to strengthen their evaluation capacity and practice, test innovative methods and share lessons learned with the broader science festival field. The development of the community of practice itself is one of the major outcomes of EvalFest. The 24 participating festivals represent a large portion of all of the science festivals in the country, therefore the work of the community naturally impacts the United States science festival landscape. Participants expressed the importance of gathering in-person to strengthen their work, which resulted in the EvalFest project team organizing three additional annual meetings (2017, 2018 and 2019) beyond the two (2015 and 2016) proposed in the original EvalFest project plan. These annual meetings were devoted to project planning activities and professional development experiences such as training in data collection and interpretation, finding and effectively reporting stories from data, data visualization, science learning activation and the ActApp system &ndash; a suite of programs used to assess efficacy of science education programs.\n\n \n\nEvalFest was centered on a centralized evaluation model, therefore initiating and sustaining multisite data collection was another major project outcome. EvalFest partners agreed upon a set of core questions that all participating festivals would use in their evaluation efforts. This allowed for the project team to analyze and compare data from all participating festivals. In addition to the core\n\nquestions, each festival had the opportunity to select other items that they wanted to\n\ninclude in their attendee evaluation. More than 60,000 surveys were conducted and analyzed during the course of the EvalFest project.\n\n \n\nThe EvalFest project team utilized the community of practice to test innovative evaluation methods. These studies were conducted by one of the EvalFest partner sites or by a member of the project advisory board. Methods tested include: Using a \"Secret Shopper\" to evaluate science festival events, using embedded assessment methods to collect evaluation data at festival events, using coding methods to assess interactions between scientists and the lay public, the development of outgroup survey instruments to determine why people choose not to participate, using Q-sort methods to evaluate visitor behavior, and the development of an instrument to determine environmental impact of science festivals.\n\n \n\nAnalysis of the 60K+ surveys and the studies conducted to test innovative methods led to numerous conference presentations and peer-reviewed journal articles. Over the course of the EvalFest project, project team members made 17 conference presentations and published 11 articles in peer-reviewed journals using EvalFest data.\n\n \n\nVideos of EvalFest webinars have been viewed on YouTube nearly 1000 times. These videos and all other assets developed through the EvalFest project are now accessible through the website of the Science Festival Alliance: https://www.sciencefestivals.org/\n\n \n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Todd R Boyette"
 }
}