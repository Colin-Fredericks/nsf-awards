{
 "awd_id": "1419240",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: CDS&E: Investigating a Self-Assembling Data Paradigm for Detector Arrays",
 "cfda_num": "47.049",
 "org_code": "03010000",
 "po_phone": "7032928235",
 "po_email": "bmihaila@nsf.gov",
 "po_sign_block_name": "Bogdan Mihaila",
 "awd_eff_date": "2014-07-15",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 142851.0,
 "awd_amount": 171351.0,
 "awd_min_amd_letter_date": "2014-07-08",
 "awd_max_amd_letter_date": "2016-06-21",
 "awd_abstract_narration": "A host of problems in scientific research, security, and commerce involve events registered by many devices in multiple locations. The result is fragmented information that must be gathered and built into a coherent whole. In addition, these events may come in rapid succession. When the event rate is high and the number of fragments large, the problem comes to resemble that of assembling tens, hundreds or even thousands of puzzle pieces that are continually being dumped into a common container. Further, puzzle pieces can become damaged or lost, introducing errors into the puzzle assembly process. These challenges are well-studied in the field of computational (nanoscale) self-assembly, which models processes such as the growth of crystals from organic molecules in solution. This project adapts computational self-assembly models to create a new paradigm that treats pieces of information from multiple sensors like molecules randomly meeting and assembling in solution. The result is a dynamic, fluid database of information chunks that evolve over time to form complete, accurate associations. This approach is applied to assemble data from the telescope arrays of very-high-energy gamma-ray observatories. A successful proof of concept in this domain is of interest to more than high-energy astrophysicists. The methods developed here are relevant to high data-volume experiments in other areas of physics and may have further applications to data transport and mining problems in the economic and security sectors. \r\n\r\nThis radically different method of fault-tolerant association of information from distributed sensors requires a proof-of-concept study, which will take place over a two-year period. The chosen test case is scientific. Very-high-energy gamma rays and cosmic rays initiate showers of charged particles in Earth's atmosphere, which in turn produce light due to an effect known as Cherenkov radiation. Arrays of atmospheric Cherenkov telescopes sample the light from a shower from multiple directions in order to more accurately infer the origin and energy of a given gamma ray. Assembling data from these telescopes into a description of a single gamma- or cosmic-ray shower (event-building) is typically done only once. Since revisiting the event-building process is impractical for a large (up to 100 petabytes per year) volume of data, errors become frozen into the data archive. This problem is addressed by the algorithmic self-assembly paradigm. Real and simulated data from the operating gamma-ray observatory VERITAS and simulated data from a planned next-generation observatory, the Cherenkov Telescope Array (CTA), are used to develop the concept and iteratively design, prototype, and test simple implementations for these instruments. Novel signal processing techniques will be exploited to rapidly extract information used in the association process. A series of use-case-dependent benchmarks are used to assess the performance. CTA's size, roughly 100 telescopes distributed over a square kilometer, and high (30 gigabytes per second) data rates make it a particularly apt test case and a successful proof of concept could lead to adoption of this model by CTA.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "PHY",
 "org_div_long_name": "Division Of Physics",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lucy",
   "pi_last_name": "Fortson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lucy Fortson",
   "pi_email_addr": "fortson@physics.umn.edu",
   "nsf_id": "000079170",
   "pi_start_date": "2014-07-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jarvis",
   "pi_last_name": "Haupt",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Jarvis D Haupt",
   "pi_email_addr": "jdhaupt@umn.edu",
   "nsf_id": "000588489",
   "pi_start_date": "2014-07-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "116 Church Str SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554550149",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164300",
   "pgm_ele_name": "Particle Astrophysics/Cosmic P"
  },
  {
   "pgm_ele_code": "724400",
   "pgm_ele_name": "COMPUTATIONAL PHYSICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 70561.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 72290.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 28500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-a0e72060-c6a1-df25-bfd2-abd25a84fa5e\"> </span></p>\n<p><span id=\"docs-internal-guid-a0e72060-c6ac-497e-620c-6109ae57a474\"> </span></p>\n<p dir=\"ltr\"><span>Many problems in scientific research, security, and commerce involve events registered by many devices in multiple locations. This fragmented information must be gathered and correctly assembled into coherent descriptions of unique events. When these events come in rapid succession and the number of fragments is large, the problem comes to resemble that of assembling tens, hundreds or even thousands of ``puzzle pieces&rsquo;&rsquo; that are continually being dumped into a common container. Further, puzzle pieces can become damaged or lost, introducing errors into the puzzle assembly process. These challenges are well-studied in the field of computational (nanoscale) self-assembly, which models processes such as the growth of crystals from organic molecules in solution. This project adapts computational self-assembly models to create a revolutionary paradigm that treats pieces of information from multiple sensors like molecules randomly meeting and assembling in solution. The result is a dynamic, fluid database of information chunks that evolve over time to complete, accurate associations. Here, we applied this approach to data from the telescope arrays of very-high-energy gamma-ray observatories, where multiple telescopes provide separate but correlated time-streams of celestial phenomena.</span></p>\n<p dir=\"ltr\"><span>Intellectual Merit: We report here on the development of a proof-of-concept fluid database over the three-year period of this grant. The fluid database is not only a structure for storing data, but also a novel paradigm for gathering data from multiple sensors and sorting that data into groups that correspond to specific physical events. (Sensors in this context can refer to anything from pixels of a camera to an array of telescopes). This approach is suited to situations where the data sorting needs to be done on multiple timescales with varying degrees of quality (e.g. a less perfect association is done in real time, to provide immediate feedback to those using an instrument, while the database archive organically corrects for errors in the association on a timescale of a few hours to a day). Errors in association usually arise when one or more of the identifiers used to establish data packets as belonging to the same physical event are compromised or corrupted.</span></p>\n<p dir=\"ltr\"><span>In our fluid database concept &ldquo;bonds&rdquo; are defined between data packets based on identifying tags attached to those data packets at the time of their creation. Such tags---which we collectively call metadata---include such quantities as data timestamps and identifying numbers. The fluid database, which plays the role of a beaker of solution, continually selects pairs of assemblies to test for bonds. Single data packets and small sets of of data packets aggregate into larger assemblies.</span></p>\n<p dir=\"ltr\"><span>The prototype fluid database was developed for and tested on raw data from the VERITAS gamma-ray observatory. We found that while random association (as is the case for true nanoscale self-assembly) was relativelyinefficient for our application, we could achieve reasonable performance by doing the following: pre-seeding the database with &ldquo;first guess&rdquo; assemblies, rather than starting from individual packets; using quasi-random selection of pairs of assemblies to test for a bonding interaction; parallelizing the pair selection and bonding process; and removing assemblies or decreasing their draw probability if they have a high likelihood of already being correct and complete.</span></p>\n<p dir=\"ltr\"><span>The prototype achieved a pre-seeding rate that approximated but did not exceed the speed of the normal VERITAS data archiving process. The fluid database was able to repair association faults with reasonable convergence times (between 1 and 24 hours) for samples of VERITAS data with modest to high rates of metadata corruption. In all cases a non-zero floor of incomplete assemblies was reached, primarily due to packet loss during the original recording of the test data. The rate of irreversible packet loss (and hence the size of the incomplete assembly residue) also correlated with the frequency of metadata corruption in the data.</span></p>\n<p dir=\"ltr\"><span>In addition to developing the fluid database prototype, we focused significant effort on exploring feature extraction methods that could be used to speed up the bonding process by providing additional metadata tags. In gamma-ray astronomy, these features can be represented by quantities that characterize the shape of an image recorded on the cameras. We investigated several techniques from the field of signal processing. A slight improvement on the signal extraction was found when temporal information from individual telescope-level events was included. Future work would establish whether this signal extraction method and resulting metadata would enhance the self-assembly process of the fluid database.</span></p>\n<p dir=\"ltr\"><span>Broader Impacts: The program fostered a strong cross-domain collaboration between groups working in physics and astronomy, computer science, and electrical engineering, trained a graduate student and postdocs, and supported an outreach program to under-represented minorities in STEM by providing high school student research internships and undergraduate research opportunities.The methods developed are also relevant to high-data-volume experiments in other areas of physics and may have further applications to data transport and mining problems in the economic and security sectors.</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/16/2017<br>\n\t\t\t\t\tModified by: Lucy&nbsp;Fortson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\n \nMany problems in scientific research, security, and commerce involve events registered by many devices in multiple locations. This fragmented information must be gathered and correctly assembled into coherent descriptions of unique events. When these events come in rapid succession and the number of fragments is large, the problem comes to resemble that of assembling tens, hundreds or even thousands of ``puzzle pieces?? that are continually being dumped into a common container. Further, puzzle pieces can become damaged or lost, introducing errors into the puzzle assembly process. These challenges are well-studied in the field of computational (nanoscale) self-assembly, which models processes such as the growth of crystals from organic molecules in solution. This project adapts computational self-assembly models to create a revolutionary paradigm that treats pieces of information from multiple sensors like molecules randomly meeting and assembling in solution. The result is a dynamic, fluid database of information chunks that evolve over time to complete, accurate associations. Here, we applied this approach to data from the telescope arrays of very-high-energy gamma-ray observatories, where multiple telescopes provide separate but correlated time-streams of celestial phenomena.\nIntellectual Merit: We report here on the development of a proof-of-concept fluid database over the three-year period of this grant. The fluid database is not only a structure for storing data, but also a novel paradigm for gathering data from multiple sensors and sorting that data into groups that correspond to specific physical events. (Sensors in this context can refer to anything from pixels of a camera to an array of telescopes). This approach is suited to situations where the data sorting needs to be done on multiple timescales with varying degrees of quality (e.g. a less perfect association is done in real time, to provide immediate feedback to those using an instrument, while the database archive organically corrects for errors in the association on a timescale of a few hours to a day). Errors in association usually arise when one or more of the identifiers used to establish data packets as belonging to the same physical event are compromised or corrupted.\nIn our fluid database concept \"bonds\" are defined between data packets based on identifying tags attached to those data packets at the time of their creation. Such tags---which we collectively call metadata---include such quantities as data timestamps and identifying numbers. The fluid database, which plays the role of a beaker of solution, continually selects pairs of assemblies to test for bonds. Single data packets and small sets of of data packets aggregate into larger assemblies.\nThe prototype fluid database was developed for and tested on raw data from the VERITAS gamma-ray observatory. We found that while random association (as is the case for true nanoscale self-assembly) was relativelyinefficient for our application, we could achieve reasonable performance by doing the following: pre-seeding the database with \"first guess\" assemblies, rather than starting from individual packets; using quasi-random selection of pairs of assemblies to test for a bonding interaction; parallelizing the pair selection and bonding process; and removing assemblies or decreasing their draw probability if they have a high likelihood of already being correct and complete.\nThe prototype achieved a pre-seeding rate that approximated but did not exceed the speed of the normal VERITAS data archiving process. The fluid database was able to repair association faults with reasonable convergence times (between 1 and 24 hours) for samples of VERITAS data with modest to high rates of metadata corruption. In all cases a non-zero floor of incomplete assemblies was reached, primarily due to packet loss during the original recording of the test data. The rate of irreversible packet loss (and hence the size of the incomplete assembly residue) also correlated with the frequency of metadata corruption in the data.\nIn addition to developing the fluid database prototype, we focused significant effort on exploring feature extraction methods that could be used to speed up the bonding process by providing additional metadata tags. In gamma-ray astronomy, these features can be represented by quantities that characterize the shape of an image recorded on the cameras. We investigated several techniques from the field of signal processing. A slight improvement on the signal extraction was found when temporal information from individual telescope-level events was included. Future work would establish whether this signal extraction method and resulting metadata would enhance the self-assembly process of the fluid database.\nBroader Impacts: The program fostered a strong cross-domain collaboration between groups working in physics and astronomy, computer science, and electrical engineering, trained a graduate student and postdocs, and supported an outreach program to under-represented minorities in STEM by providing high school student research internships and undergraduate research opportunities.The methods developed are also relevant to high-data-volume experiments in other areas of physics and may have further applications to data transport and mining problems in the economic and security sectors.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/16/2017\n\n\t\t\t\t\tSubmitted by: Lucy Fortson"
 }
}