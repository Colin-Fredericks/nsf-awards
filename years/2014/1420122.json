{
 "awd_id": "1420122",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Sampling and Reconstruction for Computer Graphics Rendering and Imaging",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 249999.0,
 "awd_amount": 249999.0,
 "awd_min_amd_letter_date": "2014-06-25",
 "awd_max_amd_letter_date": "2014-06-25",
 "awd_abstract_narration": "Sampling of high-dimensional signals is at the heart of graphical rendering and computational photography, but current approaches unfortunately still tend to be brute-force and require large numbers of samples, which is time-consuming and costly.  In this project, which involves researchers at two institutions, the Principal Investigators will build on their prior work to develop a comprehensive theoretical, algorithmic and systems foundation for sampling and reconstruction in computer graphics rendering and imaging.  A key goal is a unified sampling theory that considers the type of coherence in the visual signal (such as low rank, locally low rank, low frequency, sparsity) and the type of measurement (such as point samples in rendering or projection of generic patterns for light transport acquisition, or acquisition of full light field imagery).  This will provide a unified framework for choosing the best sampling strategy, and for comparing different approaches.  It will also enable the establishment of rigorous lower bounds and optimality results.  The work has immediate connections to signal-processing, applied mathematics and photography, and will have broad impact in connecting these domains with computer graphics.  The Principal Investigators will disseminate project outcomes in part by incorporating the findings into their online courses that have large enrolments. They will also make datasets and software available, and will work to include them in industrial applications by exploiting their strong ties with a number of high-tech companies. \r\n\r\nPhysically-based rendering algorithms are now widespread in production, but photorealistic rendering is still inefficient since it involves the evaluation of a high-dimensional 4D-8D Monte Carlo integral for each pixel considering antialiasing, lens effects, motion blur, soft shadows and global illumination.  Typically, each pixel is treated separately, with many samples needed for each integral dimension.  Similar challenges arise in other areas of computer graphics, such as precomputed rendering (explicit tabulation of a 4D-8D light transport operator), light transport acquisition (measurement of high-dimensional 4D-8D functions like the BRDF or BSSRDF), and computational photography or imaging that acquires higher-dimensional 4D functions in consumer light field cameras.  The traditional approach is to (pre)compute or measure the data by brute force, followed by compression.  However, this incurs unacceptable costs given the size and dimensionality of current visual appearance datasets.  In this work the Principal Investigators will leverage the sparsity in the continuous (rather than discrete Fourier) domain, coherence and structure of light transport to sample, reconstruct and integrate, reducing the amount of data needed by orders of magnitude, while developing new reconstruction schemes for computational imaging.  Within rendering, the PIs will explore a novel method that combines motion blur, depth of field, and global illumination in a single algorithm for real-time rendering based on adaptive Monte Carlo sampling and filtering of different effects.  A key challenge in such approaches is robust sampling of difficult paths; the Principal Investigators will address this issue with conservative adaptive sampling and Graduated Metropolis.  Finally, new systems-level software will be developed that enables easy integration and implementation of light transport simulation methods for rendering and imaging.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Fredo",
   "pi_last_name": "Durand",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fredo Durand",
   "pi_email_addr": "fredo@graphics.lcs.mit.edu",
   "nsf_id": "000107364",
   "pi_start_date": "2014-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Ave.",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 249999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Simulating light in 3D scenes is extremely challenging because light at any point in the scene can come from any other point and after an arbitrary number of bounces. This can make lighting simulation incredibly costly to take into account all these interactions. In this work, we have analyzed and leveraged structure in the interplay of light to dramatically accelerate such computations.&nbsp;</p>\n<p>We have created techniques that leverage the derivatives of the light transport equation to better sample light contributions and focus efforts where there is much light. In particular, we address the challenge posed by regions of light space that have high contributions but are narrow, therefore hard to sample. We use the second derivative to get an estimate of local function variation and steer sampling. We derived closed-form solution baed on &nbsp;Hamiltonian-Monte-Carlo and show dramatic improvements for challenging configurations.&nbsp;</p>\n<p>We have leveraged the temporal coherence of video sequences to reduce the cost of lighting simulation and amortize it over time. For this, we propose to compute the temporal derivative of the frames rather than pixel values directly.&nbsp;</p>\n<p>We have developed a new programming language to facilitate the implementation of complex probabilistic calculation involved in light transport. Methods implement in our language are correct by construction because it ensures that sampling and probability computations are consistent. It also keeps track of all the different ways a sample can be achieved, enabling advanced technqiues such as multiple importance sampling without the error-prone hassles.&nbsp;</p>\n<p>We have reduced the cost of volumetric light simulations by proposing new downsampling approaches for volumetric material appearance representations. The challenge raised by tehse approaches is teh non-linear behavior, which makes it hard to use standard downsampling.&nbsp;</p>\n<p><span>Bidirectional path tracing (BDPT) with Multiple Importance Sampling is one of the most versatile unbiased rendering&nbsp;algorithms today. BDPT repeatedly generates sub-paths from the eye and the lights, which are connected&nbsp;for each pixel and then discarded. Unfortunately, many such bidirectional connections turn out to have low contribution&nbsp;to the solution. Our key observation is that we can importance sample connections to an eye sub-path&nbsp;by considering multiple light sub-paths at once and creating connections probabilistically. We do this by storing&nbsp;light paths, and estimating probability mass functions of the discrete set of possible connections to all light paths.&nbsp;This has two key advantages: we efficiently create connections with low variance by Monte Carlo sampling, and&nbsp;we reuse light paths across different eye paths.&nbsp;</span></p>\n<p><span><span>We introduce gradient-domain rendering for Monte Carlo image synthesis. While previous gradient-domain Metropolis Light Transport sought to distribute more samples in areas of high gradients, we show, in contrast, that estimating image gradients is also possible using standard (non-Metropolis) Monte Carlo algorithms, and furthermore, that even without changing the sample distribution, this often leads to significant error reduction. This broadens the applicability of gradient rendering considerably. To gain insight into the conditions under which gradient-domain sampling is beneficial, we present a frequency analysis that compares Monte Carlo sampling of gradients followed by Poisson reconstruction to traditional Monte Carlo sampling. Finally, we describe Gradient-Domain Path Tracing (G-PT), a relatively simple modification of the standard path tracing algorithm that can yield far superior results.</span><br /></span></p>\n<p><span>Soft shadows, depth of field, and diffuse global illumination are common distribution effects, usually rendered by Monte Carlo ray tracing. Physically correct, noise-free images can require hundreds or thousands of ray samples per pixel, and take a long time to compute. Recent approaches have exploited sparse sampling and filtering; the filtering is either fast (axis-aligned), but requires more input samples, or needs fewer input samples but is very slow (sheared). We present a new approach for fast sheared filtering on the GPU. Our algorithm factors the 4D sheared filter into four 1D filters.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/23/2020<br>\n\t\t\t\t\tModified by: Fredo&nbsp;Durand</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSimulating light in 3D scenes is extremely challenging because light at any point in the scene can come from any other point and after an arbitrary number of bounces. This can make lighting simulation incredibly costly to take into account all these interactions. In this work, we have analyzed and leveraged structure in the interplay of light to dramatically accelerate such computations. \n\nWe have created techniques that leverage the derivatives of the light transport equation to better sample light contributions and focus efforts where there is much light. In particular, we address the challenge posed by regions of light space that have high contributions but are narrow, therefore hard to sample. We use the second derivative to get an estimate of local function variation and steer sampling. We derived closed-form solution baed on  Hamiltonian-Monte-Carlo and show dramatic improvements for challenging configurations. \n\nWe have leveraged the temporal coherence of video sequences to reduce the cost of lighting simulation and amortize it over time. For this, we propose to compute the temporal derivative of the frames rather than pixel values directly. \n\nWe have developed a new programming language to facilitate the implementation of complex probabilistic calculation involved in light transport. Methods implement in our language are correct by construction because it ensures that sampling and probability computations are consistent. It also keeps track of all the different ways a sample can be achieved, enabling advanced technqiues such as multiple importance sampling without the error-prone hassles. \n\nWe have reduced the cost of volumetric light simulations by proposing new downsampling approaches for volumetric material appearance representations. The challenge raised by tehse approaches is teh non-linear behavior, which makes it hard to use standard downsampling. \n\nBidirectional path tracing (BDPT) with Multiple Importance Sampling is one of the most versatile unbiased rendering algorithms today. BDPT repeatedly generates sub-paths from the eye and the lights, which are connected for each pixel and then discarded. Unfortunately, many such bidirectional connections turn out to have low contribution to the solution. Our key observation is that we can importance sample connections to an eye sub-path by considering multiple light sub-paths at once and creating connections probabilistically. We do this by storing light paths, and estimating probability mass functions of the discrete set of possible connections to all light paths. This has two key advantages: we efficiently create connections with low variance by Monte Carlo sampling, and we reuse light paths across different eye paths. \n\nWe introduce gradient-domain rendering for Monte Carlo image synthesis. While previous gradient-domain Metropolis Light Transport sought to distribute more samples in areas of high gradients, we show, in contrast, that estimating image gradients is also possible using standard (non-Metropolis) Monte Carlo algorithms, and furthermore, that even without changing the sample distribution, this often leads to significant error reduction. This broadens the applicability of gradient rendering considerably. To gain insight into the conditions under which gradient-domain sampling is beneficial, we present a frequency analysis that compares Monte Carlo sampling of gradients followed by Poisson reconstruction to traditional Monte Carlo sampling. Finally, we describe Gradient-Domain Path Tracing (G-PT), a relatively simple modification of the standard path tracing algorithm that can yield far superior results.\n\n\nSoft shadows, depth of field, and diffuse global illumination are common distribution effects, usually rendered by Monte Carlo ray tracing. Physically correct, noise-free images can require hundreds or thousands of ray samples per pixel, and take a long time to compute. Recent approaches have exploited sparse sampling and filtering; the filtering is either fast (axis-aligned), but requires more input samples, or needs fewer input samples but is very slow (sheared). We present a new approach for fast sheared filtering on the GPU. Our algorithm factors the 4D sheared filter into four 1D filters. \n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/23/2020\n\n\t\t\t\t\tSubmitted by: Fredo Durand"
 }
}