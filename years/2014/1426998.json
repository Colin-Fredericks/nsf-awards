{
 "awd_id": "1426998",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "NRI: Large-Scale Collaborative Semantic Mapping using 3D Structure from Motion",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 397836.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2014-08-22",
 "awd_max_amd_letter_date": "2016-06-24",
 "awd_abstract_narration": "The project develops techniques to advance the state of the art in tackling the challenges associated with creating such representations using robots, namely issues related to the scalability and semantic interpretability of such maps. The research activities include advancement of knowledge in multiple fields, such as computer vision, structure from motion, robotics, and semantic mapping. The results have the potential for many societal applications including city planning, asset management, creation of historical records, and support for autonomous driving. The demonstration of the developed theoretical techniques for real-time interaction between humans and robots facilitated by a semantic map enables even greater societal benefit, for example for emergency management, crime prevention, and traffic management. Direct educational impact is anticipated for graduate students and the results are disseminated through both publications and software, allowing the community to leverage the results.\r\n\r\nThis research program advances real-time large-scale distributed semantic mapping of outdoor environments. Specifically, the research team is enabling real-time large-scale semantic mapping by using unsupervised object discovery, obviating the need for large sets of annotated videos for each object category which becomes prohibitive when dealing with hundreds of object categories. The research team frames this process within the structure from motion optimization framework, thereby leveraging geometric and multi-view constraints and features to increase reliability of object track association as well as category clustering. In addition to address scalability, the project develops a distributed, multi-robot system, allowing large teams of air and ground vehicles to cooperatively build a map of large geographic areas in reasonable time frames. Furthermore, the project develops techniques to make the maps more semantically-meaningful and hence interpretable by humans. To accomplish this objective, the research team uses automatic techniques to attach semantic labels to objects discovered in an unsupervised manner. Moreover, humans can interact with the system at multiple levels. Human users can refine both the object categories and semantic labels to increase their accuracy, as well as designate dynamic targets of interest and task robots to track them.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zsolt",
   "pi_last_name": "Kira",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zsolt Kira",
   "pi_email_addr": "zkira@gatech.edu",
   "nsf_id": "000664443",
   "pi_start_date": "2014-08-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Dellaert",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frank Dellaert",
   "pi_email_addr": "dellaert@cc.gatech.edu",
   "nsf_id": "000274425",
   "pi_start_date": "2014-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Applied Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH APPLIED RESEARCH CORP",
  "org_prnt_uei_num": "L3G5SBQ2PLK5",
  "org_uei_num": "L3G5SBQ2PLK5"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Tech Research Institute",
  "perf_str_addr": "250 14th Street NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303185304",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 391990.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 208010.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The ability to build rich, coherent maps is crucial for a large number of robotics applications, ranging from self-driving cars to service robotics. These maps must represent a complex world that contains not only a rich three-dimensional structure but also very dynamic elements such as pedestrians or vehicles. Further, the world is continuously changing and robots must therefore be capable of discovering new objects in a manner that does not require manual labeling by humans. The research supported by this award studied the development of such representations through optimization and machine learning methods in a computationally efficient manner. <br /><br />To represent large-scale static scenes, we have improved mapping and localization methods to be more scalable and robust, especially to support mapping across many variations (e.g. across seasons), facilitating the aggregation of maps from multiple robots. To represent dynamic scenes, we have developed novel optimization-based methods that can jointly represent the structure and motion of objects (scene flow) as the robot moves through the environment, disentangling its own motion from that of the dynamic scene. We subsequently developed newer deep learning-based methods that combine geometric reasoning within data-driven neural network architectures that are able to achieve state of the art results for this problem while being significantly more efficient. One of the key findings from this work is that the problem of representing dynamic 3D scenes can be mathematically described as the relationship between the motion of the robot (ego-motion), the motion of the objects, and optical flow (movement of the pixels) arising from both. Such relationships can be expressed within deep neural networks to incorporate this domain knowledge and support efficient learning of how to extract this information from an image stream.<br /><br />While representations of static and dynamics scenes can be used for planning and decision-making, the semantics of the scene is also important both for incorporating such knowledge as context as well as for human interpretability. One of the key challenges, however, is that current machine learning methods that classify objects into semantic categories (e.g. 'pedestrian' or 'vehicle') require an abundance of labeled data and a-priori training using human supervision. Our work therefore has studied how new object categories in unlabeled data can be automatically discovered in order to group semantically similar objects. One of the key insights resulting from this project is that rather than learning how to classify objects into specific categories the models can learn how to compare objects instead (i.e. determine if two objects in an image are similar or dissimilar). This ability to compare can then be transferred to unlabeled data to generate weak constraints, determining whether pairs of data are similar or not. We have developed a novel deep-learning based clustering algorithm that is able to take these estimates for pairs and learn how to group the data instances, resulting in the ability to discover new objects in the world. We have shown that such methods can obtain state of art performance across a number of machine learning problems, ranging from when the categories are already known but the statistics of the data changes (again for example due to seasonal variations) or to discover entirely new object categories. <br /><br />These two sets of work, namely learning how to model dynamic 3D scenes and discover new object categories, has advanced the field to enable building large-scale representations of environments in an efficient manner. The results have been publicly disseminated through numerous conference and journal publications, release of source code for some of the methods, and release of data. The resulting capabilities has implications for numerous applications ranging from self-driving cars to mapping of outdoor infrastructure for city planning. Along the way, the award has also supported significant mentoring of students at all levels (including one graduated and two in-progress Ph.D. students), several educational events such as bootcamps in machine learning, and STEM outreach.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/18/2018<br>\n\t\t\t\t\tModified by: Zsolt&nbsp;Kira</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147288679_projected_image_pose_1298--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147288679_projected_image_pose_1298--rgov-800width.jpg\" title=\"Dense 3D Reconstruction of an Urban Environment\"><img src=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147288679_projected_image_pose_1298--rgov-66x44.jpg\" alt=\"Dense 3D Reconstruction of an Urban Environment\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A sample view of a dense billion-point pointcloud from a 3D reconstruction of an urban scene.</div>\n<div class=\"imageCredit\">Frank Dellaert and Zhaoyang Lv</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Zsolt&nbsp;Kira</div>\n<div class=\"imageTitle\">Dense 3D Reconstruction of an Urban Environment</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147360960_sceneflow--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147360960_sceneflow--rgov-800width.jpg\" title=\"Scene Flow Visualization\"><img src=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147360960_sceneflow--rgov-66x44.jpg\" alt=\"Scene Flow Visualization\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Visualization of 3D motion fields estimated for an urban scene. (From Lv et al., ECCV 2016)</div>\n<div class=\"imageCredit\">Frank Dellaert, Zsolt Kira, and Zhaoyang Lv</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Zsolt&nbsp;Kira</div>\n<div class=\"imageTitle\">Scene Flow Visualization</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147716338_ObjectDiscovery--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147716338_ObjectDiscovery--rgov-800width.jpg\" title=\"Coherent Object Category Discovery in Unlabeled Data\"><img src=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545147716338_ObjectDiscovery--rgov-66x44.jpg\" alt=\"Coherent Object Category Discovery in Unlabeled Data\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Representation of semantically-meaningful object categories discovered from unlabeled data.</div>\n<div class=\"imageCredit\">Zsolt Kira, Zhaoyang Lv, and Yen-Chang Hsu</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Zsolt&nbsp;Kira</div>\n<div class=\"imageTitle\">Coherent Object Category Discovery in Unlabeled Data</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545153950093_LearningRigidity_v2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545153950093_LearningRigidity_v2--rgov-800width.jpg\" title=\"Extraction of Structure and Motion in Dynamic Scenes\"><img src=\"/por/images/Reports/POR/2018/1426998/1426998_10336377_1545153950093_LearningRigidity_v2--rgov-66x44.jpg\" alt=\"Extraction of Structure and Motion in Dynamic Scenes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Depiction of our scene flow results in extracting structure and motion of dynamic objects in complex synthetic scenes.</div>\n<div class=\"imageCredit\">Zhaoyang Lv, Kihwan Kim, Alejandro Troccoli, Deqing Sun, James M. Rehg, Jan Kautz  (Georgia Tech and NVIDIA, from ECCV 2018)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Zsolt&nbsp;Kira</div>\n<div class=\"imageTitle\">Extraction of Structure and Motion in Dynamic Scenes</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe ability to build rich, coherent maps is crucial for a large number of robotics applications, ranging from self-driving cars to service robotics. These maps must represent a complex world that contains not only a rich three-dimensional structure but also very dynamic elements such as pedestrians or vehicles. Further, the world is continuously changing and robots must therefore be capable of discovering new objects in a manner that does not require manual labeling by humans. The research supported by this award studied the development of such representations through optimization and machine learning methods in a computationally efficient manner. \n\nTo represent large-scale static scenes, we have improved mapping and localization methods to be more scalable and robust, especially to support mapping across many variations (e.g. across seasons), facilitating the aggregation of maps from multiple robots. To represent dynamic scenes, we have developed novel optimization-based methods that can jointly represent the structure and motion of objects (scene flow) as the robot moves through the environment, disentangling its own motion from that of the dynamic scene. We subsequently developed newer deep learning-based methods that combine geometric reasoning within data-driven neural network architectures that are able to achieve state of the art results for this problem while being significantly more efficient. One of the key findings from this work is that the problem of representing dynamic 3D scenes can be mathematically described as the relationship between the motion of the robot (ego-motion), the motion of the objects, and optical flow (movement of the pixels) arising from both. Such relationships can be expressed within deep neural networks to incorporate this domain knowledge and support efficient learning of how to extract this information from an image stream.\n\nWhile representations of static and dynamics scenes can be used for planning and decision-making, the semantics of the scene is also important both for incorporating such knowledge as context as well as for human interpretability. One of the key challenges, however, is that current machine learning methods that classify objects into semantic categories (e.g. 'pedestrian' or 'vehicle') require an abundance of labeled data and a-priori training using human supervision. Our work therefore has studied how new object categories in unlabeled data can be automatically discovered in order to group semantically similar objects. One of the key insights resulting from this project is that rather than learning how to classify objects into specific categories the models can learn how to compare objects instead (i.e. determine if two objects in an image are similar or dissimilar). This ability to compare can then be transferred to unlabeled data to generate weak constraints, determining whether pairs of data are similar or not. We have developed a novel deep-learning based clustering algorithm that is able to take these estimates for pairs and learn how to group the data instances, resulting in the ability to discover new objects in the world. We have shown that such methods can obtain state of art performance across a number of machine learning problems, ranging from when the categories are already known but the statistics of the data changes (again for example due to seasonal variations) or to discover entirely new object categories. \n\nThese two sets of work, namely learning how to model dynamic 3D scenes and discover new object categories, has advanced the field to enable building large-scale representations of environments in an efficient manner. The results have been publicly disseminated through numerous conference and journal publications, release of source code for some of the methods, and release of data. The resulting capabilities has implications for numerous applications ranging from self-driving cars to mapping of outdoor infrastructure for city planning. Along the way, the award has also supported significant mentoring of students at all levels (including one graduated and two in-progress Ph.D. students), several educational events such as bootcamps in machine learning, and STEM outreach.\n\n\t\t\t\t\tLast Modified: 12/18/2018\n\n\t\t\t\t\tSubmitted by: Zsolt Kira"
 }
}