{
 "awd_id": "1440332",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EarthCube Building Blocks: Collaborative Proposal: GeoSoft: Collaborative Open Source Software Sharing for Geosciences",
 "cfda_num": "47.050",
 "org_code": "06010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Eva Zanzerkia",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 330000.0,
 "awd_amount": 330000.0,
 "awd_min_amd_letter_date": "2014-08-14",
 "awd_max_amd_letter_date": "2014-08-14",
 "awd_abstract_narration": "Geosciences software embodies crucial scientific knowledge, and as such it should be explicitly captured, curated, managed, and disseminated. The goal of this project is to create a system for software stewardship in geosciences that will empower scientists to manage their software as valuable scientific assets. Scientific software stewardship requires a combination of cyberinfrastructure, social infrastructure, and professional development infrastructure. The  framework will result in an open transparent and broader access to scientific software to other scientists, software professionals, students, and decision makers. It will significantly improve the adoption of open data and open software initiatives, improve reproducibility, and advance scientific scholarship.\r\n\r\nThe proposed research will advance knowledge and understanding of scientific software as a valuable community asset that is worth sharing, curating, cataloging, validating, reusing, and maintaining.\r\n 1) Facilitating software publication through TurboSoft, a personal assistant (analogous to TurboTax) that guides a user through best practices. Users will choose the degree of investment they are willing to make in componentizing, describing, licensing, and maintaining their software. The system will encourage open source publication, the formation of communities around the software, and set up mechanisms for software citation and credit. \r\n2) Enabling broad software dissemination through GeoSoft, a \"software commons\" for geosciences that will support software contributions (prepared through TurboSoft or otherwise), software discovery through multi-faceted search, and foster social interactions through dynamic formation of communities of interest. GeoSoft will interoperate with existing software repositories and modeling frameworks in geosciences. \r\n3) Providing just-in-time training materials through GeoCamp, an annotated collection of educational units ranging from basic education to professional training on all aspects of software stewardship. GeoCamp will be seamlessly integrated with TurboSoft and GeoSoft, and present a wide range of options for learning in the context of a user?s context of interaction with the framework or independently.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "GEO",
 "org_dir_long_name": "Directorate for Geosciences",
 "div_abbr": "RISE",
 "org_div_long_name": "Integrative and Collaborative Education and Research (ICER)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Scott",
   "pi_last_name": "Peckham",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Scott D Peckham",
   "pi_email_addr": "Scott.Peckham@colorado.edu",
   "nsf_id": "000473356",
   "pi_start_date": "2014-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Colorado at Boulder",
  "inst_street_address": "3100 MARINE ST",
  "inst_street_address_2": "STE 481 572 UCB",
  "inst_city_name": "Boulder",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3034926221",
  "inst_zip_code": "803090001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "THE REGENTS OF THE UNIVERSITY OF COLORADO",
  "org_prnt_uei_num": "",
  "org_uei_num": "SPVKK1RC2MZ3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Colorado Boulder",
  "perf_str_addr": "3100 Marine Street, room 479",
  "perf_city_name": "Boulder",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803090572",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "807400",
   "pgm_ele_name": "EarthCube"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 330000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Currently, scientists spend most of their time (estimated by studies at 60 to 80 percent) dealing with issues that stem from the Findability, Accessibility, Interoperability and Reusability of the digital resources that they need to do their research. &nbsp;In the last few years, this issue has received increased attention (e.g. the NSF EarthCube initiative among others) and there has been a push towards making sure that digital resources adhere to what are called FAIR principles, where each letter in FAIR corresponds to one of these four broad issues. &nbsp;The phrase \"digital resources\" refers to data sets of all kinds, computational models, other software (e.g. for data preparation, analysis and visualization) and publications. &nbsp;While there have been many efforts to address findability and accessibility -- such as Dublin Core, DataCite and schema.org -- addressing interoperability and reusability is a much more difficult problem, and requires much richer metadata. &nbsp;Interoperability is of critical importance because it is usually necessary to connect the input and output variables of many digital resources into a workflow in order to solve a given scientific problem. &nbsp;But interoperability is difficult due to the heterogeneity of the resources that need to be connected, a problem that has been called \"data friction\". &nbsp;Heterogeneity here refers to all of the ways resources can differ, such as variable names and units, assumptions, computational grid, file formats, programming language, map projection and so on.</p>\n<p>The two key strategies for addressing the related problems of interoperability and reusability are: (1) standardized interfaces for interacting with resources and (2) standardized metadata that provides a deep description of each resource. &nbsp;These strategies make it possible to write software that automatically deals with heterogeneity so that resources can easily be coupled into workflows. &nbsp;That is, rich, standardized metadata is essential for taming heterogeneity and enabling interoperability. &nbsp;Note that domain-specific scientific metadata, while having the potential to enable interoperability within a given scientific domain, is insufficient to enable interoperability between or across disciplines. &nbsp;These facts point to the need for carefully-constructed, cross-domain ontologies that provide the concepts and terms needed to describe science resources, because the ability of interoperability software to automatically perform mundane tasks for the scientist (or assist them) is determined by the \"depth\" or \"richness\" of the standardized metadata. &nbsp;Very basic metadata may allow a scientist to find and download a resource, but is typically inadequate in order for them (or their software) to use the resource, either alone or as part of a composition or workflow.</p>\n<p>In view of the previous paragraphs, the purpose of this project has been to develop a powerful, new, cross-domain ontology called the Scientific Variables Ontology (SVO) for describing scientific data and models, at the \"granular\" level of variables, equations, assumptions, grids, etc. &nbsp;The SVO covers measurement concepts across the sciences, is built on patterns gleaned from analyzing a large and diverse set of models and data sets, and is rooted in ISO 80000, the International System of Quantities. &nbsp;The SVO also utilizes Semantic Web best practices and technologies (e.g. RDF, SPARQL, SKOS and OWL) making it easy to incorporate into other systems such as OntoSoft (formerly GeoSoft). &nbsp;The SVO is something that didn't exist before but which is needed to support the development of advanced software applications that automatically perform a wide variety of mundane tasks on behalf of scientists. &nbsp;Despite the focus on interoperability, the SVO also greatly improves the findability of resources that address a specific measurement concept of interest. &nbsp;This ambitious project therefore has the potential to have an enormous impact that extends across science domains, providing scientists with more time to make scientific discoveries.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/01/2019<br>\n\t\t\t\t\tModified by: Scott&nbsp;D&nbsp;Peckham</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCurrently, scientists spend most of their time (estimated by studies at 60 to 80 percent) dealing with issues that stem from the Findability, Accessibility, Interoperability and Reusability of the digital resources that they need to do their research.  In the last few years, this issue has received increased attention (e.g. the NSF EarthCube initiative among others) and there has been a push towards making sure that digital resources adhere to what are called FAIR principles, where each letter in FAIR corresponds to one of these four broad issues.  The phrase \"digital resources\" refers to data sets of all kinds, computational models, other software (e.g. for data preparation, analysis and visualization) and publications.  While there have been many efforts to address findability and accessibility -- such as Dublin Core, DataCite and schema.org -- addressing interoperability and reusability is a much more difficult problem, and requires much richer metadata.  Interoperability is of critical importance because it is usually necessary to connect the input and output variables of many digital resources into a workflow in order to solve a given scientific problem.  But interoperability is difficult due to the heterogeneity of the resources that need to be connected, a problem that has been called \"data friction\".  Heterogeneity here refers to all of the ways resources can differ, such as variable names and units, assumptions, computational grid, file formats, programming language, map projection and so on.\n\nThe two key strategies for addressing the related problems of interoperability and reusability are: (1) standardized interfaces for interacting with resources and (2) standardized metadata that provides a deep description of each resource.  These strategies make it possible to write software that automatically deals with heterogeneity so that resources can easily be coupled into workflows.  That is, rich, standardized metadata is essential for taming heterogeneity and enabling interoperability.  Note that domain-specific scientific metadata, while having the potential to enable interoperability within a given scientific domain, is insufficient to enable interoperability between or across disciplines.  These facts point to the need for carefully-constructed, cross-domain ontologies that provide the concepts and terms needed to describe science resources, because the ability of interoperability software to automatically perform mundane tasks for the scientist (or assist them) is determined by the \"depth\" or \"richness\" of the standardized metadata.  Very basic metadata may allow a scientist to find and download a resource, but is typically inadequate in order for them (or their software) to use the resource, either alone or as part of a composition or workflow.\n\nIn view of the previous paragraphs, the purpose of this project has been to develop a powerful, new, cross-domain ontology called the Scientific Variables Ontology (SVO) for describing scientific data and models, at the \"granular\" level of variables, equations, assumptions, grids, etc.  The SVO covers measurement concepts across the sciences, is built on patterns gleaned from analyzing a large and diverse set of models and data sets, and is rooted in ISO 80000, the International System of Quantities.  The SVO also utilizes Semantic Web best practices and technologies (e.g. RDF, SPARQL, SKOS and OWL) making it easy to incorporate into other systems such as OntoSoft (formerly GeoSoft).  The SVO is something that didn't exist before but which is needed to support the development of advanced software applications that automatically perform a wide variety of mundane tasks on behalf of scientists.  Despite the focus on interoperability, the SVO also greatly improves the findability of resources that address a specific measurement concept of interest.  This ambitious project therefore has the potential to have an enormous impact that extends across science domains, providing scientists with more time to make scientific discoveries.\n\n\t\t\t\t\tLast Modified: 02/01/2019\n\n\t\t\t\t\tSubmitted by: Scott D Peckham"
 }
}