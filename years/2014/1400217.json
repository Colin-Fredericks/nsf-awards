{
 "awd_id": "1400217",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Resolving Parametric Misspecification: Joint Schemes for Computation and Learning",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2014-02-24",
 "awd_max_amd_letter_date": "2014-02-24",
 "awd_abstract_narration": "The objective of this research is to consider the solution of convex optimization and variational inequality problems complicated by misspecified parameters. A possibly naive sequential approach would first (i) estimate the problem parameters accurately through learning; and then (ii) solve the associated computational problem. Unfortunately, as the learning problems grow in size and complexity, such a framework is hampered by the significant increase in solution time to obtain accurate parameter estimates. Furthermore, any parameter estimation error propagates to the computational problem, possibly with devastating impact. Accordingly, on contrary to the na\u00efve sequential approach, this research will aim to develop gradient-based algorithms that can simultaneously learn the misspecified parameter and solve the computational problem corresponding to the correct parameter. More generally, these coupled schemes will be shown to be capable of contending with problem intricacies such as uncertainty and nonsmoothness. Importantly, this methodology will cope with situations where the observations used in the learning problem may depend on the computational process. The research will emphasize the development of global convergence statements, iteration complexity results, regret bounds with reference to offline algorithms, and trade-offs between exploration and exploitation.\r\n\r\nIf successful, this research will find impact at several levels. At a fundamental level, this research is expected to lead to new truly adaptive algorithms that can both learn parameters and optimize the associated systems simultaneously. The algorithms will make impact through addressing a wide range of large-scale application problems that are complicated by misspecification, including portfolio selection problems and distributed optimization of networked systems. Finally, the educational component of this project will comprise of undergraduate research projects and high school course modules.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Uday",
   "pi_last_name": "Shanbhag",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Uday V Shanbhag",
   "pi_email_addr": "udaybag@psu.edu",
   "nsf_id": "000106424",
   "pi_start_date": "2014-02-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Necdet",
   "pi_last_name": "Aybat",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Necdet S Aybat",
   "pi_email_addr": "nsa10@psu.edu",
   "nsf_id": "000654670",
   "pi_start_date": "2014-02-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "Leonhard Building",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168026817",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Traditionally, optimization problems have&nbsp;been characterized by perfect knowledge regarding problem primitives (objectives, constraints, etc.). Examples of such misspecification include machine efficiencies, parameters of demand functions, and covariance matrices of financial assets in a portfolio, all of which may be learnt.&nbsp;This proposal focused on computational problems complicated by a parametric misspecification with a view towards developing an implementable &nbsp;framework with theoretical convergence guarantees, in contrast with what&nbsp;would be a naive approach: (i) Estimate the parameter through learning; and (ii) Solve the associated computational problem. Unfortunately, as systems grow in size and complexity, this sequential and dichotomous framework is hindered by the requirement that accurate solutions of the learning problem are required in finite time, often a highly stringent assumption. Furthermore, any resulting parametric error propagates to the solution of the computational problem, possibly with devastating impact. This research grant helped the PIs develop&nbsp;a new paradigm for computation and learning with the following outcomes: (a) Schemes for resolving misspecified deterministic and stochastic optimization problems where the misspecification arose in the objective. Notably, the resulting schemes displayed no degradation of the convergence rate &nbsp;under some conditions. (b) Schemes for addressing misspecified constraints were developed by overlaying a learning framework within an augmented Lagrangian scheme; we showed &nbsp;that such schemes still converge &nbsp;with no degradation in the rate of convergence in certain regimes. (iii) Variance-reduced&nbsp;accelerated proximal schemes and stochastic quasi-Newton schemes were also developed for nonsmooth convex and strongly convex optimization; our &nbsp;key finding &nbsp;is that in many of the settings, the deterministic rate of convergence was provable. (iv) New optimization methods for large-scale learning problems: we examined rate analysis for distributed algorithms (primal-dual methods and ADMM) to solve large-scale learning problems as well as variable sample-size stochastic approximation schemes. (v) Robustness of a first-order algorithm is defined as the H2 system norm of its equivalent system representation when gradient evaluations are subject to noise. Many of the methods developed in our simultaneous learning &amp; optimization framework can be seen as optimization methods with inexact gradient computations. Using this control theoretic perspective, we analyzed the trade-off between rate and robustness of first-order methods in order to appropriately select tunable algorithmic parameters to ensure certain convergence guarantees with given robustness bound.</p>\n<p>The intellectual merit of this work is founded on providing a new paradigm for decision-making and the provision of a foundation for contending with misspecification in constrained, stochastic, and variational regimes. In addition, techniques have been developed for a broad array of related schemes that can be easily extended to account for misspecification. This work is expected to find impact in a &nbsp;range of problem settings in diverse areas including control theory, financial optimization, supply-chain management, amongst others.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/30/2019<br>\n\t\t\t\t\tModified by: Uday&nbsp;V&nbsp;Shanbhag</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nTraditionally, optimization problems have been characterized by perfect knowledge regarding problem primitives (objectives, constraints, etc.). Examples of such misspecification include machine efficiencies, parameters of demand functions, and covariance matrices of financial assets in a portfolio, all of which may be learnt. This proposal focused on computational problems complicated by a parametric misspecification with a view towards developing an implementable  framework with theoretical convergence guarantees, in contrast with what would be a naive approach: (i) Estimate the parameter through learning; and (ii) Solve the associated computational problem. Unfortunately, as systems grow in size and complexity, this sequential and dichotomous framework is hindered by the requirement that accurate solutions of the learning problem are required in finite time, often a highly stringent assumption. Furthermore, any resulting parametric error propagates to the solution of the computational problem, possibly with devastating impact. This research grant helped the PIs develop a new paradigm for computation and learning with the following outcomes: (a) Schemes for resolving misspecified deterministic and stochastic optimization problems where the misspecification arose in the objective. Notably, the resulting schemes displayed no degradation of the convergence rate  under some conditions. (b) Schemes for addressing misspecified constraints were developed by overlaying a learning framework within an augmented Lagrangian scheme; we showed  that such schemes still converge  with no degradation in the rate of convergence in certain regimes. (iii) Variance-reduced accelerated proximal schemes and stochastic quasi-Newton schemes were also developed for nonsmooth convex and strongly convex optimization; our  key finding  is that in many of the settings, the deterministic rate of convergence was provable. (iv) New optimization methods for large-scale learning problems: we examined rate analysis for distributed algorithms (primal-dual methods and ADMM) to solve large-scale learning problems as well as variable sample-size stochastic approximation schemes. (v) Robustness of a first-order algorithm is defined as the H2 system norm of its equivalent system representation when gradient evaluations are subject to noise. Many of the methods developed in our simultaneous learning &amp; optimization framework can be seen as optimization methods with inexact gradient computations. Using this control theoretic perspective, we analyzed the trade-off between rate and robustness of first-order methods in order to appropriately select tunable algorithmic parameters to ensure certain convergence guarantees with given robustness bound.\n\nThe intellectual merit of this work is founded on providing a new paradigm for decision-making and the provision of a foundation for contending with misspecification in constrained, stochastic, and variational regimes. In addition, techniques have been developed for a broad array of related schemes that can be easily extended to account for misspecification. This work is expected to find impact in a  range of problem settings in diverse areas including control theory, financial optimization, supply-chain management, amongst others. \n\n \n\n\t\t\t\t\tLast Modified: 05/30/2019\n\n\t\t\t\t\tSubmitted by: Uday V Shanbhag"
 }
}