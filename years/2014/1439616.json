{
 "awd_id": "1439616",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Small: STAAR: Spatial Touch Audio Annotator and Reader for Individuals with Blindness or Severe Visual Impairment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2013-11-20",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 302067.0,
 "awd_amount": 305097.0,
 "awd_min_amd_letter_date": "2014-06-05",
 "awd_max_amd_letter_date": "2017-09-22",
 "awd_abstract_narration": "The PI's goal in this research is to develop tools on a state-of-the-art platform (the Apple iPad) that will afford access to textual information for individuals who are blind or who suffer from severe visual impairments (IBSVI). The PI's approach is to use an embossed screen overlay to provide spatial and tactile correlates to text read aloud, and to engage the spatial cognition and memory resources of the target population for navigating through a document and annotating it if/as desired.  The PI argues that from the invention of print media forward, information has been formulated and optimized for consumption by beings (people) with a dominant visual capability.  This visuo-spatial bias is not well-understood or studied in the context of information access by and delivery to the IBSVI community; most technological information aids funnel information to them as sequential aural streams, obviating the use of broader spatial cognitive resources.  In this project the PI will develop a Spatial Touch Audio Annotator and Reader (STAAR) testbed to explore a multimodal alternative that enables the user to fuse spatial layout and informational content through touch location on a slate-type device and audio rendering of text to speech, respectively.  STAAR will enable self-paced reading using a tactile overlay pattern on an iPad surface, which will be designed to provide tactile landmarks to help the user navigate the \"page.\"  STAAR will render the text chunk touched audibly.  The use of touch gestures to enable contextualized highlighting and note-taking will also be investigated.  The PI will study how the target population may employ spatial strategies and exploration to re-find and re-access information both in the act of reading and for recall after some time interval.  \r\n\r\nBroader Impacts:  A new generation of slate-type devices exemplified by the Apple iPad threatens to widen the accessibility gap between the IBSVI community and the majority of the population.  By supporting the use of spatial cognitive and memory resource for both reading and contextualized annotation, the PI hopes to ameliorate this endemic barrier to participation.  Project outcomes will contribute to our understanding of the role of space in information design and representation, and the spatial cognition and memory resources needed for uptake of such information.  And they will contribute to the domain of mobile computing, for the IBSVI community in particular but ultimately for the population in general, through the STAAR system, which will be designed and implemented from the ground up as a portable device on a state-of-art interaction form-factor.  The multimodal fusion of haptics and speech in STAAR will have implications for the designs of such things as navigational aids and service delivery systems for the non-sighted as well as the sighted.  The project will in addition provide unique cross-disciplinary educational and learning opportunities for undergraduate and graduate students.  All software developed in this project will be placed in open source.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Francis",
   "pi_last_name": "Quek",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Francis Quek",
   "pi_email_addr": "quek@tamu.edu",
   "nsf_id": "000324474",
   "pi_start_date": "2014-06-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M University",
  "inst_street_address": "400 HARVEY MITCHELL PKY S STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778454375",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A & M UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JF6XLNB4CDJ5"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M University Main Campus",
  "perf_str_addr": "400 Harvey Mitchell Pkwy South Suite 300",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778431260",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 299321.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 5775.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-c621789b-7fff-5d6b-3d0c-3e431efe55fa\"><span>The goal of this project is to understand the spatial nature of written information and its implications on the design of an active reading support tool for Individuals with Blindness or Severe Visual Impairment (IBSVI). The Spatial Tactile and Audio Annotator and Reader (STAAR) system and technology was developed that combines spatial-tactile interaction with audio to enable IBSVI to have access to the spatial organization of printed content. This is critical because modern literature employs space and &lsquo;visual access&rsquo; to page content to help readers to keep place and maintain context in reading. Without spatial access, the IBSVI reader would have to overload their cognitive and short-term memory while reading. STAAR employs an embossed tactile landmark overlay over an iPad and reads the text on the screen as words are touched. This allows the IBSVI to fuse the spatial and content information as they read. STAAR supports the kind of active reading that is needed for individuals to learn and study by enabling IBSVI to read at their own pace, and to embed in-place dictated notes. For documents to be read by the STAAR system employs a STAAR page description format containing a database of the words on the page, their locations and their machine-generated audio snippets. This allows STAAR to vary the speed of reading in real-time response to the speed at which the IBSVI&rsquo;s finger scans the text, enabling self-paced reading and exploration of the page. STAAR was tested with IBSVI readers to find the specific features and enablements that are necessary to support real active reading and note-taking. These studies show that STAAR needs a minimum of the landmark overlay, intelligent reading support (to help the IBSVI stay on a line of text when it is determined that they are reading a line), realtime sonification and word transitions to support effective reading. A major finding is that our STAAR reader system requires a degree of literacy before it can be effectively used -- just as regular books require literacy in the reader to read its contents. IBSVI who do not have a habit of mind to read would rather have literature read to them rather than reading using the system. Readers (either Braille readers or late-blind IBSVI who read visually before) could learn and use the system effectively with little training. Those who read for study, learn to use the note-taking feature very quickly, and express enthusiasm for it. This indicates that the approach is not a just an interactive system to read words to IBSVI. It has the potential to support new reading literacy for IBSVI. This project has resulted in an effective technology to support active reading by IBSVI and contributes knowledge to the design of spatial/tactile-audio reading systems.</span></span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/31/2018<br>\n\t\t\t\t\tModified by: Francis&nbsp;Quek</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project is to understand the spatial nature of written information and its implications on the design of an active reading support tool for Individuals with Blindness or Severe Visual Impairment (IBSVI). The Spatial Tactile and Audio Annotator and Reader (STAAR) system and technology was developed that combines spatial-tactile interaction with audio to enable IBSVI to have access to the spatial organization of printed content. This is critical because modern literature employs space and ?visual access? to page content to help readers to keep place and maintain context in reading. Without spatial access, the IBSVI reader would have to overload their cognitive and short-term memory while reading. STAAR employs an embossed tactile landmark overlay over an iPad and reads the text on the screen as words are touched. This allows the IBSVI to fuse the spatial and content information as they read. STAAR supports the kind of active reading that is needed for individuals to learn and study by enabling IBSVI to read at their own pace, and to embed in-place dictated notes. For documents to be read by the STAAR system employs a STAAR page description format containing a database of the words on the page, their locations and their machine-generated audio snippets. This allows STAAR to vary the speed of reading in real-time response to the speed at which the IBSVI?s finger scans the text, enabling self-paced reading and exploration of the page. STAAR was tested with IBSVI readers to find the specific features and enablements that are necessary to support real active reading and note-taking. These studies show that STAAR needs a minimum of the landmark overlay, intelligent reading support (to help the IBSVI stay on a line of text when it is determined that they are reading a line), realtime sonification and word transitions to support effective reading. A major finding is that our STAAR reader system requires a degree of literacy before it can be effectively used -- just as regular books require literacy in the reader to read its contents. IBSVI who do not have a habit of mind to read would rather have literature read to them rather than reading using the system. Readers (either Braille readers or late-blind IBSVI who read visually before) could learn and use the system effectively with little training. Those who read for study, learn to use the note-taking feature very quickly, and express enthusiasm for it. This indicates that the approach is not a just an interactive system to read words to IBSVI. It has the potential to support new reading literacy for IBSVI. This project has resulted in an effective technology to support active reading by IBSVI and contributes knowledge to the design of spatial/tactile-audio reading systems.\n\n\t\t\t\t\tLast Modified: 10/31/2018\n\n\t\t\t\t\tSubmitted by: Francis Quek"
 }
}