{
 "awd_id": "1426744",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Jointly Learning Language and Affordances",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 341548.0,
 "awd_amount": 341548.0,
 "awd_min_amd_letter_date": "2014-08-06",
 "awd_max_amd_letter_date": "2015-07-09",
 "awd_abstract_narration": "The investigators of this project envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people by delivering medicine, preparing food, and assembling objects. Achieving this vision requires robots to communicate with people about their needs, and then plan their activities to help meet those needs.  Previous research has addressed these two problems separately, leading to technical solutions that do not work reliably in real-world situations, and to difficulties in human-robot communication.  To solve these problems, we are developing the Physically-Grounded Language with Affordances (PGLA) framework and concentrate our research into two thrusts: 1) enable a robot to observe a patient, then answer a nurse's questions about the patient's activity, and 2) enable a robot to respond to natural language requests in a collaborative cooking task and in a manufacturing setting. We will release our open-source data sets and code, which will have impact in other technical areas beyond robotics, such as computer vision and machine learning.  The results of our proposed research will find direct applications in industries such as manufacturing and assistive robotics.\r\n\r\nThis project takes a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions.  Since the affordance map is grounded to perceptual data, our robots will learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words.  Our learning approach enables the robot to infer cross-model knowledge from large data sets of people carrying out activities paired with natural language descriptions of the activities, leveraging the strength of each modality to inform the others. Our novel learning algorithms will integrate and learn from multi-domain databases such as the semantic web, visual scenes, and a novel activity database paired with natural language descriptions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bart",
   "pi_last_name": "Selman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bart Selman",
   "pi_email_addr": "selman@cs.cornell.edu",
   "nsf_id": "000191386",
   "pi_start_date": "2015-07-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Ashutosh",
   "pi_last_name": "Saxena",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ashutosh Saxena",
   "pi_email_addr": "asaxena@cs.cornell.edu",
   "nsf_id": "000539644",
   "pi_start_date": "2014-08-06",
   "pi_end_date": "2015-07-09"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Bart",
   "pi_last_name": "Selman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bart Selman",
   "pi_email_addr": "selman@cs.cornell.edu",
   "nsf_id": "000191386",
   "pi_start_date": "2015-05-27",
   "pi_end_date": "2015-07-09"
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "Gates Hall",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 341548.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people in many ways, from delivering medicine, to preparing food, to assembling objects. Achieving this vision requires the capability for robots to communicate with people about their needs, and then plan complex behaviors that meet those needs. Previous work has addressed these two problems separately, leading to brittle solutions that do not scale to real-world environments and open-ended natural language input. To bridge this gap, we developed a new framework, called the Physically Grounded Language with Affordances (PGLA) framework. We take a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions. Since the affordance map is grounded to perceptual data, our robots learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words. Our learning approach enables the robot to infer cross-model knowledge about perception, planning, and language from large datasets of people carrying out activities paired with natural language descriptions of the activity, leveraging the strength of each modality to inform the others. We developed Tell Me Dave, a large system for grounding natural language into environment and controllers. This system learns from large-scale user interactions and enables the robots to perform tasks in new situations. Broader Impacts: To develop assistive robotics platforms, needed in for example elderly care, robots need to be able to fluently interact with humans in natural language. Our project represents a significant advance in reaching this goal.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/19/2017<br>\n\t\t\t\t\tModified by: Bart&nbsp;Selman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people in many ways, from delivering medicine, to preparing food, to assembling objects. Achieving this vision requires the capability for robots to communicate with people about their needs, and then plan complex behaviors that meet those needs. Previous work has addressed these two problems separately, leading to brittle solutions that do not scale to real-world environments and open-ended natural language input. To bridge this gap, we developed a new framework, called the Physically Grounded Language with Affordances (PGLA) framework. We take a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions. Since the affordance map is grounded to perceptual data, our robots learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words. Our learning approach enables the robot to infer cross-model knowledge about perception, planning, and language from large datasets of people carrying out activities paired with natural language descriptions of the activity, leveraging the strength of each modality to inform the others. We developed Tell Me Dave, a large system for grounding natural language into environment and controllers. This system learns from large-scale user interactions and enables the robots to perform tasks in new situations. Broader Impacts: To develop assistive robotics platforms, needed in for example elderly care, robots need to be able to fluently interact with humans in natural language. Our project represents a significant advance in reaching this goal.\n\n \n\n\t\t\t\t\tLast Modified: 11/19/2017\n\n\t\t\t\t\tSubmitted by: Bart Selman"
 }
}