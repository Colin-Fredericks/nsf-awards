{
 "awd_id": "1407670",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RUI: A Bayesian Approach to Sequential Change Point Detection",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2014-07-15",
 "awd_exp_date": "2018-06-30",
 "tot_intn_awd_amt": 135490.0,
 "awd_amount": 135490.0,
 "awd_min_amd_letter_date": "2014-07-03",
 "awd_max_amd_letter_date": "2014-07-03",
 "awd_abstract_narration": "This project will develop an efficient change point algorithm that not only will indicate when change points occur, but also provide uncertainty estimates as to the number and exact timing of these changes.  Applications of this model are widespread and include any field where long sequences of data are collected such as medicine (e.g. EEG readings), economics (e.g. stock market data, coal mining disasters), and climate (e.g. temperature readings, glacial records).  More specifically, a 5 million year record of global ice volume shows at least two distinct changes. The first, around 2.7 million years ago, represents an increase in the amount of ice volume on the Earth as permanent glaciers began to form in the northern hemisphere, whereas a more recent change around 0.8 million years ago represents a gradual change in the frequency of major glacial melting events from every 40,000 to every 100,000 years.  A more prominent example concerns NCDC's global temperature anomalies data set that many have cited as evidence of global warming.  This record indicates three changes in the rate of temperature increases on the Earth over the last 133 years - in 1906, 1945, and either 1963 or 1976.  The algorithm will be able to handle sequential data, giving it the ability to quickly update itself as each new observation is recorded, and will be able to accurately predict where in the data set a change point has occurred.  \r\n \r\nIt is well known that long time series are often heterogeneous in nature, any attempt to model these data sets may have to account for parameters that change through time.  The difference can be as simple as a change in the mean, slope, or frequency of the underlying signal.  However, the identification of ?change points? is not always a trivial task as the number of potential solutions grows exponentially with the length of the data set, rendering brute force attempts to solve the problem infeasible.  Previous work on a Bayesian change point algorithm has produced an efficient and exact probabilistic solution to the multiple change point problem by using dynamic programming-like recursions to reduce the computational complexity from exponential to quadratic.  Samples drawn from the joint posterior distribution of the change point locations quantify the uncertainty in both the number and timing of changes in the data set.  In this project, the existing change point model will be modified to handle sequential data.  Once this initial objective is complete, research will turn towards further modifications that include the ability to handle correlated error terms and an approximate algorithm that has linear complexity, bringing the computational complexity down to a point where a time series of any length can be analyzed.  The project fits naturally with undergraduate education and will serve as the basis of summer research projects, senior theses, and a potential seminar course for a new statistics program.  The software developed through this project will be made publicly available so as to make this cutting-edge statistical methodology accessible to researchers in a wide variety of fields.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Ruggieri",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Eric R Ruggieri",
   "pi_email_addr": "eruggier@holycross.edu",
   "nsf_id": "000631946",
   "pi_start_date": "2014-07-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "College of the Holy Cross",
  "inst_street_address": "1 COLLEGE ST",
  "inst_street_address_2": "",
  "inst_city_name": "WORCESTER",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "5087932741",
  "inst_zip_code": "016102322",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "COLLEGE OF HOLY CROSS",
  "org_prnt_uei_num": "",
  "org_uei_num": "G4TUM6J1ZM24"
 },
 "perf_inst": {
  "perf_inst_name": "College of the Holy Cross",
  "perf_str_addr": "1 College Street",
  "perf_city_name": "Worcester",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "016102395",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9229",
   "pgm_ref_txt": "RES IN UNDERGRAD INST-RESEARCH"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 135490.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A change point is defined as the point at which the statistical properties of a data set change.&nbsp; In the context of a time series, this can represent a change in the mean, trend, or variance of the data.&nbsp; Perhaps the most high-profile example is global warming.&nbsp; Here, the argument is that there has been a large spike in global surface temperatures over the last several decades.&nbsp; The time at which this temperature change begins is a change point.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The goal of change point analysis is to fit a piecewise regression model to a data set.&nbsp; Change point analysis is easy if the change points are known, but this is almost never the case.&nbsp; Instead, the location of change points must be inferred from the data.&nbsp; Because the number of potential solutions increases exponentially with the length of the data set, an efficient algorithm must be developed in order to make the problem tractable.&nbsp; The goal of this project was to develop a Bayesian sequential change point algorithm.&nbsp; A Bayesian approach to change point analysis allows us to quantify the uncertainty in both the number and location of change points in a data set.&nbsp; &lsquo;Sequential&rsquo; implies that we can analyze the data as it comes in, providing for a quick update to the inference with each new observation.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The algorithm that was developed built off previous work of the PI.&nbsp; By changing the order in which calculations are performed, a sequential analysis of the data can now be performed.&nbsp; Several detection criteria for the existence of a change point were also developed and tested.&nbsp; Simulation studies showed that the algorithm performed favorably against existing change point algorithms in that it was highly accurate in its placement of change points with a low rate of false positives.&nbsp;</p>\n<p>&nbsp;</p>\n<p>The recursive structure of the algorithm reduced the number of calculations needed from exponential to quadratic in the number of observations.&nbsp; However, a quadratic algorithm can still be too slow for very long data sets.&nbsp; To remedy this problem, an approximate Bayesian algorithm was developed that sought to remove calculations unlikely to be part of the final solution.&nbsp; When analyzing the data set sequentially, we place change points one at a time from the beginning to the end of the data set.&nbsp; If we can assume that any two segments of the data are independent of one another, then we can remove from consideration calculations which span multiple change points.&nbsp; This &lsquo;pruning&rsquo; procedure further reduced the number of calculations needed with almost no loss of information.&nbsp;</p>\n<p>&nbsp;</p>\n<p>A main focus of this project was to involve undergraduate students in summer research projects.&nbsp; This grant directly supported four summer research students, including a stipend, summer housing, and travel money to present their work at the Joint Mathematics Meetings, a national mathematics conference.&nbsp; The College of the Holy Cross provided matching funding for two additional summer research students, and the PI also had one additional student work on a senior thesis project during the academic year.&nbsp; Students learned to formulate concrete statistical questions, break down a complex problem into more easily accessible pieces, investigate these sub-problems both analytically and numerically, and gather and utilize the relevant work of other researchers.&nbsp; In particular, each student gained proficiency in using Matlab and/or R to simulate data sets, sampling from probability distributions, and writing recursive algorithms, as well as organizational skills when working on large problems.&nbsp; Several of the students also learned LaTeX and Beamer for their presentation in the department's student/faculty summer research seminar.&nbsp; In addition, each student had an opportunity to practice his or her communication skills when they participated in a college-wide poster session held on campus each September to highlight student research.&nbsp; The PI met with each student at least three times each week over the summer to discuss the progress of his project, work through technical difficulties, and outline the next steps to be taken.</p>\n<p>&nbsp;</p>\n<p>The work done by these students has also added to the growing interest in statistics at the College of the Holy Cross.&nbsp; Over the last several years, we have been developing a new program in statistics, culminating with the approval of a minor in the spring of 2017. &nbsp;Each year, we have had the opportunity to offer new elective courses in support of this minor, including a course in Statistical Computing offered by the PI in the fall of 2017.&nbsp; Students in this class completed a final project (studying the statistical properties of board games through simulation) in lieu of a final exam.&nbsp; Knowing that some of their classmates were planning to attend the Joint Mathematics Meetings, several of these students also decided to attend and present posters on their projects.&nbsp; In short, the grant had a far-reachiung impact at the College of the Holy Cross.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/13/2018<br>\n\t\t\t\t\tModified by: Eric&nbsp;R&nbsp;Ruggieri</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nA change point is defined as the point at which the statistical properties of a data set change.  In the context of a time series, this can represent a change in the mean, trend, or variance of the data.  Perhaps the most high-profile example is global warming.  Here, the argument is that there has been a large spike in global surface temperatures over the last several decades.  The time at which this temperature change begins is a change point. \n\n \n\nThe goal of change point analysis is to fit a piecewise regression model to a data set.  Change point analysis is easy if the change points are known, but this is almost never the case.  Instead, the location of change points must be inferred from the data.  Because the number of potential solutions increases exponentially with the length of the data set, an efficient algorithm must be developed in order to make the problem tractable.  The goal of this project was to develop a Bayesian sequential change point algorithm.  A Bayesian approach to change point analysis allows us to quantify the uncertainty in both the number and location of change points in a data set.  ?Sequential? implies that we can analyze the data as it comes in, providing for a quick update to the inference with each new observation. \n\n \n\nThe algorithm that was developed built off previous work of the PI.  By changing the order in which calculations are performed, a sequential analysis of the data can now be performed.  Several detection criteria for the existence of a change point were also developed and tested.  Simulation studies showed that the algorithm performed favorably against existing change point algorithms in that it was highly accurate in its placement of change points with a low rate of false positives. \n\n \n\nThe recursive structure of the algorithm reduced the number of calculations needed from exponential to quadratic in the number of observations.  However, a quadratic algorithm can still be too slow for very long data sets.  To remedy this problem, an approximate Bayesian algorithm was developed that sought to remove calculations unlikely to be part of the final solution.  When analyzing the data set sequentially, we place change points one at a time from the beginning to the end of the data set.  If we can assume that any two segments of the data are independent of one another, then we can remove from consideration calculations which span multiple change points.  This ?pruning? procedure further reduced the number of calculations needed with almost no loss of information. \n\n \n\nA main focus of this project was to involve undergraduate students in summer research projects.  This grant directly supported four summer research students, including a stipend, summer housing, and travel money to present their work at the Joint Mathematics Meetings, a national mathematics conference.  The College of the Holy Cross provided matching funding for two additional summer research students, and the PI also had one additional student work on a senior thesis project during the academic year.  Students learned to formulate concrete statistical questions, break down a complex problem into more easily accessible pieces, investigate these sub-problems both analytically and numerically, and gather and utilize the relevant work of other researchers.  In particular, each student gained proficiency in using Matlab and/or R to simulate data sets, sampling from probability distributions, and writing recursive algorithms, as well as organizational skills when working on large problems.  Several of the students also learned LaTeX and Beamer for their presentation in the department's student/faculty summer research seminar.  In addition, each student had an opportunity to practice his or her communication skills when they participated in a college-wide poster session held on campus each September to highlight student research.  The PI met with each student at least three times each week over the summer to discuss the progress of his project, work through technical difficulties, and outline the next steps to be taken.\n\n \n\nThe work done by these students has also added to the growing interest in statistics at the College of the Holy Cross.  Over the last several years, we have been developing a new program in statistics, culminating with the approval of a minor in the spring of 2017.  Each year, we have had the opportunity to offer new elective courses in support of this minor, including a course in Statistical Computing offered by the PI in the fall of 2017.  Students in this class completed a final project (studying the statistical properties of board games through simulation) in lieu of a final exam.  Knowing that some of their classmates were planning to attend the Joint Mathematics Meetings, several of these students also decided to attend and present posters on their projects.  In short, the grant had a far-reachiung impact at the College of the Holy Cross.\n\n\t\t\t\t\tLast Modified: 07/13/2018\n\n\t\t\t\t\tSubmitted by: Eric R Ruggieri"
 }
}