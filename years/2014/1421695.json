{
 "awd_id": "1421695",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Cognitive models of the acquisition of vowels in context",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 240000.0,
 "awd_amount": 240000.0,
 "awd_min_amd_letter_date": "2014-08-18",
 "awd_max_amd_letter_date": "2015-06-15",
 "awd_abstract_narration": "Models of infant language acquisition allow researchers to test hypotheses about how infants learn their native language from speech.  This project focuses on how infants determine which sounds are meaningful in their language and concentrates on co-articulation, a process in which sounds are produced differently depending on qualities of neighboring sounds. Co-articulation is closely tied to the historical creation, change, and loss of sound patterns in language.  As a result, the models developed in this project further our understanding of both infant language acquisition and historical language change. Knowing how different aspects of the learning process interact can also help pinpoint deficits that might underlie developmental language disorders. Finally, building models of language acquisition that can work on speech data can potentially help create speech recognition technology that adapts robustly to novel languages, accents, and domains of discourse in the same way that human learners do.\r\n \r\nA series of unsupervised phonetic learning models are created to examine how learners take into account co-articulation.  Whereas previous models of phonetic learning have categorized each sound independently, these models begin with a set of constraints that capture possible dependences between sounds, formalized using a Markov random field. They then learn from the data which constraints characterize a particular language.  Recordings of child-directed speech from the CHILDES database are annotated through forced alignment to serve as test corpora for comparing the newly developed models with previous models. The project yields a rigorous evaluation of where existing models fall short, a new framework for accounting for phonetic variability, and corpora to support the development and evaluation of future phonetic learning models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Naomi",
   "pi_last_name": "Feldman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Naomi Feldman",
   "pi_email_addr": "nhf@umd.edu",
   "nsf_id": "000622351",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "1401 Marie Mount Hall",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 36282.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 203718.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Children can learn a language just by hearing it spoken around them, but it is not yet known how they achieve this. &nbsp;One of the things that makes language learning challenging is figuring out which differences between speech sounds are meaningful (e.g., the different vowels in the words \"bit\" and \"bat\" change the identity of the word), and which are not (e.g., you can say the word \"bit\" faster or slower, but it is still the same word). &nbsp;Crucially, the same difference can be meaningful in one language but not in another; for example, in Japanese, pronouncing a word with a shorter or longer vowel duration can change its meaning.</p>\n<p>&nbsp;</p>\n<p>We studied this learning problem by conducting computer simulations of proposed learning strategies. &nbsp;If the strategy works, the computer will be able to learn the correct sound categories---otherwise, it will learn too many. This is exactly what we found when testing a standard categorization model on samples of American English and of Japanese. &nbsp;The model learns correctly from careful speech recorded under controlled conditions, but does not learn from real speech.</p>\n<p>&nbsp;</p>\n<p>In one line of followup studies, we looked carefully at Japanese vowels, which fall into \"long\" and \"short\" categories. &nbsp;We looked at recordings of parents to understand the data that babies learn from, and at recordings of toddlers to see what they learn. &nbsp;\"Short\" vowels are not always acoustically shorter than \"long\" vowels in the parents' speech. &nbsp;Researchers used to believe that children could normalize out irrelevant factors---e.g., by mentally \"slowing down\" the fast speech so that it corresponded to the slow speech. &nbsp;However, we found that this strategy does not make the short and long categories any more clearly separated. &nbsp;On the other hand, we showed that, by the age of two, toddlers have learned which words have long versus short vowels. &nbsp;It is clear that the previous theory does not account for their successful learning.<br />We then proposed a new theory of how speech sound learning works. &nbsp;The basic idea of our proposal is that babies can learn that vowel length encodes meaningful differences in Japanese by noticing that the proportion of acoustically longer versus shorter vowels varies as a function of context (e.g., neighboring sounds, or position in a sentence). &nbsp;Through computer simulations, we showed that this learning strategy works on real speech, unlike previous approaches. &nbsp;It successfully distinguishes languages that do have a vowel length contrast (such as Japanese) from languages that don't (such as French).</p>\n<p>&nbsp;</p>\n<p>In another line of inquiry, we explored more powerful computer learning mechanisms called \"neural networks\". Neural networks can learn complex interactions between many different dimensions of a signal, and so could potentially learn to integrate evidence for the identity of a speech sound with evidence about its context, how fast the speaker is talking, and other mediating factors. &nbsp;We showed that trying to memorize and repeat back the audio recording was enough to force the network to learn some meaningful properties of speech, like whether it was listening to a vowel or a consonant. But so far, our results still fall short of human performance.</p>\n<p>&nbsp;</p>\n<p>Finally, we asked how social context influences children's learning of word pronunciations. &nbsp;Speakers may sound different than one another because of where they live, their cultural background, and other factors based on who they are. &nbsp;We built a computer model that assumes that learners are trying to figure out who are the best informants to learn words from---i.e., who speaks their target dialect---at the same time that they learn the pronunciations of individual words. &nbsp;This model explained aspects of 14-month-olds' word learning that had previously been difficult to explain, suggesting that very young children may already be using social context to decide which speakers are good informants for learning their target dialect. &nbsp;This could be particularly relevant for understanding how children who are exposed to multiple dialects learn language.</p>\n<p>&nbsp;</p>\n<p>Along the way, our project made other contributions. We annotated a large database of American English mothers speaking to their babies, using speech recognition to label the sound categories in the recordings. This will help us to understand the kinds of variations babies hear when listening to natural speech. We gave a variety of talks to professional and non-professional audiences, including high school and college students, to explain our findings and discuss how computer simulations can help us to understand language learning. Several graduate students participated in the project, including several who belong to groups that are underrepresented in computational fields. All these students gained experience in writing computer programs, analyzing the results and linking their findings to experimental research on how babies learn.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2019<br>\n\t\t\t\t\tModified by: Naomi&nbsp;Feldman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nChildren can learn a language just by hearing it spoken around them, but it is not yet known how they achieve this.  One of the things that makes language learning challenging is figuring out which differences between speech sounds are meaningful (e.g., the different vowels in the words \"bit\" and \"bat\" change the identity of the word), and which are not (e.g., you can say the word \"bit\" faster or slower, but it is still the same word).  Crucially, the same difference can be meaningful in one language but not in another; for example, in Japanese, pronouncing a word with a shorter or longer vowel duration can change its meaning.\n\n \n\nWe studied this learning problem by conducting computer simulations of proposed learning strategies.  If the strategy works, the computer will be able to learn the correct sound categories---otherwise, it will learn too many. This is exactly what we found when testing a standard categorization model on samples of American English and of Japanese.  The model learns correctly from careful speech recorded under controlled conditions, but does not learn from real speech.\n\n \n\nIn one line of followup studies, we looked carefully at Japanese vowels, which fall into \"long\" and \"short\" categories.  We looked at recordings of parents to understand the data that babies learn from, and at recordings of toddlers to see what they learn.  \"Short\" vowels are not always acoustically shorter than \"long\" vowels in the parents' speech.  Researchers used to believe that children could normalize out irrelevant factors---e.g., by mentally \"slowing down\" the fast speech so that it corresponded to the slow speech.  However, we found that this strategy does not make the short and long categories any more clearly separated.  On the other hand, we showed that, by the age of two, toddlers have learned which words have long versus short vowels.  It is clear that the previous theory does not account for their successful learning.\nWe then proposed a new theory of how speech sound learning works.  The basic idea of our proposal is that babies can learn that vowel length encodes meaningful differences in Japanese by noticing that the proportion of acoustically longer versus shorter vowels varies as a function of context (e.g., neighboring sounds, or position in a sentence).  Through computer simulations, we showed that this learning strategy works on real speech, unlike previous approaches.  It successfully distinguishes languages that do have a vowel length contrast (such as Japanese) from languages that don't (such as French).\n\n \n\nIn another line of inquiry, we explored more powerful computer learning mechanisms called \"neural networks\". Neural networks can learn complex interactions between many different dimensions of a signal, and so could potentially learn to integrate evidence for the identity of a speech sound with evidence about its context, how fast the speaker is talking, and other mediating factors.  We showed that trying to memorize and repeat back the audio recording was enough to force the network to learn some meaningful properties of speech, like whether it was listening to a vowel or a consonant. But so far, our results still fall short of human performance.\n\n \n\nFinally, we asked how social context influences children's learning of word pronunciations.  Speakers may sound different than one another because of where they live, their cultural background, and other factors based on who they are.  We built a computer model that assumes that learners are trying to figure out who are the best informants to learn words from---i.e., who speaks their target dialect---at the same time that they learn the pronunciations of individual words.  This model explained aspects of 14-month-olds' word learning that had previously been difficult to explain, suggesting that very young children may already be using social context to decide which speakers are good informants for learning their target dialect.  This could be particularly relevant for understanding how children who are exposed to multiple dialects learn language.\n\n \n\nAlong the way, our project made other contributions. We annotated a large database of American English mothers speaking to their babies, using speech recognition to label the sound categories in the recordings. This will help us to understand the kinds of variations babies hear when listening to natural speech. We gave a variety of talks to professional and non-professional audiences, including high school and college students, to explain our findings and discuss how computer simulations can help us to understand language learning. Several graduate students participated in the project, including several who belong to groups that are underrepresented in computational fields. All these students gained experience in writing computer programs, analyzing the results and linking their findings to experimental research on how babies learn.\n\n\t\t\t\t\tLast Modified: 11/29/2019\n\n\t\t\t\t\tSubmitted by: Naomi Feldman"
 }
}