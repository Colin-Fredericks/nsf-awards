{
 "awd_id": "1427030",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Modeling and Verification of Language-based Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Miller",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 700000.0,
 "awd_amount": 700000.0,
 "awd_min_amd_letter_date": "2014-08-18",
 "awd_max_amd_letter_date": "2014-08-18",
 "awd_abstract_narration": "Many autonomous systems today, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this  interaction with a model that guarantees robot performance.  This research brings together key elements that are just now reaching a sufficient level of maturity for integration: firstly, natural language processing and probabilistic modeling to capture human input, and secondly probabilistic synthesis and verification of the combined human-robot systems to ensure correct performance. The outcome will be theory and software to enable correct, effective and natural interactions between robots and humans to be realized. This research will impact most future autonomous systems which require interactions with humans, including service, personal and planetary robots. \r\n\r\nThe goal of this research is to develop models and algorithms for synthesizing and verifying an integrated human-plus-robot system based on natural language interaction. Algorithms are being developed for probabilistic modeling and inference of natural language, including the grounding of the constituents of the language into the physical world and the human's expectations. These models will enable the development of a distribution over specifications for control synthesis, which will in turn enable the development and verification of correct-by-construction controllers to a particular level of probability. The out years will consider interactive human-robot dialogue to resolve conflicts, and \"open world\" scenarios to enable on-line learning of new models over time. It is expected that this research will enable high reliability and performance in many autonomous systems because of the inherent interaction with humans.  Outcomes include open source data and software; community workshops; and undergraduate and graduate student education in the unique area of language, modeling and verification for robotics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Campbell",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Mark E Campbell",
   "pi_email_addr": "mc288@cornell.edu",
   "nsf_id": "000214501",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hadas",
   "pi_last_name": "Kress Gazit",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hadas Kress Gazit",
   "pi_email_addr": "hadaskg@cornell.edu",
   "nsf_id": "000521463",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "105 Upson Hall",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 700000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Many current autonomous systems, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this interaction with a model that guarantees robot performance.&nbsp;&nbsp;This research project developed and validated models and algorithms for synthesizing and verifying an integrated human+robot system based on natural language interaction. The following summarizes the specific outcomes of the project:</span><span>&nbsp;</span></p>\n<p><strong><span>Robot generating natural language for human:</span></strong><span>&nbsp;The project developed an algorithm to generate natural language from complex beliefs, and validated the approach via an extensive set of human trials. The model uses a Dirichlet mixture of sentences, and both semantic correctness and information preservation metrics are optimized. Thus, the number of statements to match the belief is also inferred at run time. The natural language generated is based on grounding to a mutually known map and belief (e.g. the object is near the tree). Validation results compare information loss compared to the original sentence, human collaborative task performances, and correctness rating scores indicating that the proposed method for generating belief expressions is an effective approach for communicating probabilistic information between robots and humans.</span></p>\n<p><strong><span>Automatic inference of pre-conditions:&nbsp;</span></strong><span>The project developed an algorithm for learning obvious pre-conditions for synthesis, which are rules governing when a task can be synthesized (or not). For example, an object cannot be placed in a bin if the bin is covered, or if the object is too large for the bin. The approach uses topic modeling to group and define similar constraints/rules; these groups are inferred via data offline, prior to synthesis.</span></p>\n<p><strong><span>Synthesizing robot controllers from natural language:</span></strong><span>&nbsp;The project developed an algorithm for probabilistic grounding of natural language instructions into logical formulas and automatically synthesizing robot controllers from the grounded language. The approach uses a training corpus for grounding natural language instructions both to the physical environment and logical formulae. Robot-specific templates are used to form a specification which can be synthesized into a grounded controller. The project also developed a natural language instruction grounding framework which uses formal synthesis to enable the robot to identify necessary environment assumptions for the task to be successful</span></p>\n<p><strong><span>Human feedback about robot understanding:</span></strong><span>&nbsp;The project developed an algorithm for automatically suggesting to untrained users additional assumptions about the environment relating to the robot&rsquo;s task. These suggested instructions are required for correct robot execution and users are prompted to either accept or decline them. Different methods of prompting feedback using natural language (repeating the original sentence, creating a new sentence) were developed and evaluated the methods in user studies.</span></p>\n<p><strong><span>Data Collection:</span></strong><span>&nbsp;The project integrated natural language parsing and controller synthesis from logic formulas into a system (simulation and experiment) that was used for data collection of a corpus of natural language instructions, and validation experiments. Given natural language instructions and a world model, the end-to-end system synthesizes robot controllers that a physical robot (Rethink Robotics&rsquo; Baxter) then executes. The simulator and experiment are based on the Baxter robot and a flexible manufacturing environment where objects are constructed building blocks delivered over a conveyor belt.&nbsp;</span></p>\n<p><strong><span>Validation experiments via Baxter:</span></strong><span>&nbsp;This project designed and validated integrated algorithmic approaches for human-robot collaboration via the Baxter robot. The task is a Baxter robot collaboratively working with a human to take a mixture of blocks, arrange them in an order, and then build an object. Human natural language is used to define specifications and describe the scene. The robot perceives the environment and determines the logical values of the prepositions. Automatic controllers are generated for the robot. Run time evaluations are conducted to understand if/when assumptions are not met; if not met, then specifications are changed and controllers are re-generated. Feedback to the human includes an unrealizable specification, added assumptions about the environment that are automatically inferred (if any), and feedback regarding failure of action.</span></p>\n<p><strong><span>Transition and Training:</span></strong><span>&nbsp;In addition to the generation of algorithms and data, a total of 10 conference and journal papers and one PhD thesis were published. Training included funding and advising three PhD students and two postdocs on the project. In addition, undergraduates and Masters of Engineering students supported the data collections and validation experiments.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2019<br>\n\t\t\t\t\tModified by: Mark&nbsp;E&nbsp;Campbell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany current autonomous systems, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this interaction with a model that guarantees robot performance.  This research project developed and validated models and algorithms for synthesizing and verifying an integrated human+robot system based on natural language interaction. The following summarizes the specific outcomes of the project: \n\nRobot generating natural language for human: The project developed an algorithm to generate natural language from complex beliefs, and validated the approach via an extensive set of human trials. The model uses a Dirichlet mixture of sentences, and both semantic correctness and information preservation metrics are optimized. Thus, the number of statements to match the belief is also inferred at run time. The natural language generated is based on grounding to a mutually known map and belief (e.g. the object is near the tree). Validation results compare information loss compared to the original sentence, human collaborative task performances, and correctness rating scores indicating that the proposed method for generating belief expressions is an effective approach for communicating probabilistic information between robots and humans.\n\nAutomatic inference of pre-conditions: The project developed an algorithm for learning obvious pre-conditions for synthesis, which are rules governing when a task can be synthesized (or not). For example, an object cannot be placed in a bin if the bin is covered, or if the object is too large for the bin. The approach uses topic modeling to group and define similar constraints/rules; these groups are inferred via data offline, prior to synthesis.\n\nSynthesizing robot controllers from natural language: The project developed an algorithm for probabilistic grounding of natural language instructions into logical formulas and automatically synthesizing robot controllers from the grounded language. The approach uses a training corpus for grounding natural language instructions both to the physical environment and logical formulae. Robot-specific templates are used to form a specification which can be synthesized into a grounded controller. The project also developed a natural language instruction grounding framework which uses formal synthesis to enable the robot to identify necessary environment assumptions for the task to be successful\n\nHuman feedback about robot understanding: The project developed an algorithm for automatically suggesting to untrained users additional assumptions about the environment relating to the robot?s task. These suggested instructions are required for correct robot execution and users are prompted to either accept or decline them. Different methods of prompting feedback using natural language (repeating the original sentence, creating a new sentence) were developed and evaluated the methods in user studies.\n\nData Collection: The project integrated natural language parsing and controller synthesis from logic formulas into a system (simulation and experiment) that was used for data collection of a corpus of natural language instructions, and validation experiments. Given natural language instructions and a world model, the end-to-end system synthesizes robot controllers that a physical robot (Rethink Robotics? Baxter) then executes. The simulator and experiment are based on the Baxter robot and a flexible manufacturing environment where objects are constructed building blocks delivered over a conveyor belt. \n\nValidation experiments via Baxter: This project designed and validated integrated algorithmic approaches for human-robot collaboration via the Baxter robot. The task is a Baxter robot collaboratively working with a human to take a mixture of blocks, arrange them in an order, and then build an object. Human natural language is used to define specifications and describe the scene. The robot perceives the environment and determines the logical values of the prepositions. Automatic controllers are generated for the robot. Run time evaluations are conducted to understand if/when assumptions are not met; if not met, then specifications are changed and controllers are re-generated. Feedback to the human includes an unrealizable specification, added assumptions about the environment that are automatically inferred (if any), and feedback regarding failure of action.\n\nTransition and Training: In addition to the generation of algorithms and data, a total of 10 conference and journal papers and one PhD thesis were published. Training included funding and advising three PhD students and two postdocs on the project. In addition, undergraduates and Masters of Engineering students supported the data collections and validation experiments. \n\n \n\n\t\t\t\t\tLast Modified: 10/30/2019\n\n\t\t\t\t\tSubmitted by: Mark E Campbell"
 }
}