{
 "awd_id": "1451830",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Collaborative Research: Sampling and Reconstruction for Computer Graphics Rendering and Imaging",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2014-08-04",
 "awd_max_amd_letter_date": "2014-08-04",
 "awd_abstract_narration": "Sampling of high-dimensional signals is at the heart of graphical rendering and computational photography, but current approaches unfortunately still tend to be brute-force and require large numbers of samples, which is time-consuming and costly.  In this project, which involves researchers at two institutions, the Principal Investigators will build on their prior work to develop a comprehensive theoretical, algorithmic and systems foundation for sampling and reconstruction in computer graphics rendering and imaging.  A key goal is a unified sampling theory that considers the type of coherence in the visual signal (such as low rank, locally low rank, low frequency, sparsity) and the type of measurement (such as point samples in rendering or projection of generic patterns for light transport acquisition, or acquisition of full light field imagery).  This will provide a unified framework for choosing the best sampling strategy, and for comparing different approaches.  It will also enable the establishment of rigorous lower bounds and optimality results.  The work has immediate connections to signal-processing, applied mathematics and photography, and will have broad impact in connecting these domains with computer graphics.  The Principal Investigators will disseminate project outcomes in part by incorporating the findings into their online courses that have large enrolments. They will also make datasets and software available, and will work to include them in industrial applications by exploiting their strong ties with a number of high-tech companies. \r\n\r\nPhysically-based rendering algorithms are now widespread in production, but photorealistic rendering is still inefficient since it involves the evaluation of a high-dimensional 4D-8D Monte Carlo integral for each pixel considering antialiasing, lens effects, motion blur, soft shadows and global illumination.  Typically, each pixel is treated separately, with many samples needed for each integral dimension.  Similar challenges arise in other areas of computer graphics, such as precomputed rendering (explicit tabulation of a 4D-8D light transport operator), light transport acquisition (measurement of high-dimensional 4D-8D functions like the BRDF or BSSRDF), and computational photography or imaging that acquires higher-dimensional 4D functions in consumer light field cameras.  The traditional approach is to (pre)compute or measure the data by brute force, followed by compression.  However, this incurs unacceptable costs given the size and dimensionality of current visual appearance datasets.  In this work the Principal Investigators will leverage the sparsity in the continuous (rather than discrete Fourier) domain, coherence and structure of light transport to sample, reconstruct and integrate, reducing the amount of data needed by orders of magnitude, while developing new reconstruction schemes for computational imaging.  Within rendering, the PIs will explore a novel method that combines motion blur, depth of field, and global illumination in a single algorithm for real-time rendering based on adaptive Monte Carlo sampling and filtering of different effects.  A key challenge in such approaches is robust sampling of difficult paths; the Principal Investigators will address this issue with conservative adaptive sampling and Graduated Metropolis.  Finally, new systems-level software will be developed that enables easy integration and implementation of light transport simulation methods for rendering and imaging.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ravi",
   "pi_last_name": "Ramamoorthi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ravi Ramamoorthi",
   "pi_email_addr": "ravir@cs.ucsd.edu",
   "nsf_id": "000486826",
   "pi_start_date": "2014-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930404",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This proposal addresses a number of problems in realistic computer graphics, most prominently that of efficiently creating realistic images from a three-dimensional scene description.&nbsp; This is important in creating realistic images for computer-generated animations and movies, predictive rendering for product visualizations in catalogs, or interactive applications like video games.&nbsp;&nbsp;</p>\n<p><br />For many years, it has been known that the most accurate approaches simulate the actual propagation of light in the scene, using methods known as ray or path tracing.&nbsp; These methods simulate the rays or paths of light in the scene, thereby rendering the most accurate and realistic images by a process known as Monte Carlo rendering, which randomly samples the paths or rays to produce an accurate final result.&nbsp;&nbsp;</p>\n<p><br />However, Monte Carlo rendering has historically been very expensive computationally, making it difficult to use in practical settings, because many thousands of rays or sample paths must be generated at each pixel.&nbsp; In this proposal, we seek to address these issues by novel methods that can sparse sample the space of rays, followed by adaptive reconstruction using sophisticated computer algorithms.&nbsp;&nbsp;</p>\n<p><br />The effort has been largely successful with a number of relevant papers at the top computer graphics venues (in particular, the ACM SIGGRAPH conference).&nbsp; These results have shown that often using a few tens of samples instead of hundreds or thousands suffices, if we couple sparse sampling with the reconstruction algorithms developed in this proposal.&nbsp; This increases the speed of realistic image synthesis in computer graphics by orders of magnitude.&nbsp;&nbsp;</p>\n<p>As a result of our work and that of others, this general methodology of physically-accurate Monte Carlo Rendering, followed by denoising, has been widely adopted by the industry.&nbsp; In fact, almost any realistic image made for a movie today includes this pipeline.&nbsp;&nbsp;</p>\n<p>More interesting, over the course of this project, the same ideas have even entered into the real-time domain, and this is a major milestone for this research and the outcomes of this proposal.&nbsp; In particular, it is now possible to use hardware-accelerated systems to trace a few rays per pixel (picture element) of the image in real-time (at about 30 frames per second, which is how fast the human observer can see) followed by a real-time reconstruction and denoising step.&nbsp; Recent developments in graphics software and hardware have seen this vision come to fruition in the past year, with the general pipeline being integrated into state of the art commercial products.&nbsp;&nbsp;</p>\n<p>Perhaps most interesting, the research in this project has demonstrated accurate results with only 4 samples per pixel, which is one to two orders of magnitude than the state of the art prior to the proposal.&nbsp; These developments have created great excitement for real-time computer graphics image synthesis.</p>\n<p>Of course, the basic ideas of sampling and reconstruction are relevant in a number of other domains in computer graphics, and we have developed new methods for computing images under general lighting from only five input images under specific lighting directions.&nbsp; We have shown how to recover the reflectance properties of a surface or how it reflects light from only two images, to name a few results.&nbsp; All of these results open up new potential for applications like appearance capture, virtual reality, and accurate representation of objects for e-commerce and visualization.&nbsp;&nbsp;</p>\n<p>In summary, this proposal has introduced a new paradigm in the way realistic computer graphics images are calculated and displayed in practical applications, and the research results have inspired an important shift in the thinking of software developers and commercial users, to adopt our framework of sparse Monte Carlo Sampling followed by reconstruction.&nbsp; The benefits are significant for the industry, and have been enjoyed by millions of viewers in the general public whenever they see movies with computer-generated visual effects, view advertisements for products with computer-generated imagery, or enjoy interacting with an playing video games on high-end personal computers.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/07/2018<br>\n\t\t\t\t\tModified by: Ravi&nbsp;Ramamoorthi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis proposal addresses a number of problems in realistic computer graphics, most prominently that of efficiently creating realistic images from a three-dimensional scene description.  This is important in creating realistic images for computer-generated animations and movies, predictive rendering for product visualizations in catalogs, or interactive applications like video games.  \n\n\nFor many years, it has been known that the most accurate approaches simulate the actual propagation of light in the scene, using methods known as ray or path tracing.  These methods simulate the rays or paths of light in the scene, thereby rendering the most accurate and realistic images by a process known as Monte Carlo rendering, which randomly samples the paths or rays to produce an accurate final result.  \n\n\nHowever, Monte Carlo rendering has historically been very expensive computationally, making it difficult to use in practical settings, because many thousands of rays or sample paths must be generated at each pixel.  In this proposal, we seek to address these issues by novel methods that can sparse sample the space of rays, followed by adaptive reconstruction using sophisticated computer algorithms.  \n\n\nThe effort has been largely successful with a number of relevant papers at the top computer graphics venues (in particular, the ACM SIGGRAPH conference).  These results have shown that often using a few tens of samples instead of hundreds or thousands suffices, if we couple sparse sampling with the reconstruction algorithms developed in this proposal.  This increases the speed of realistic image synthesis in computer graphics by orders of magnitude.  \n\nAs a result of our work and that of others, this general methodology of physically-accurate Monte Carlo Rendering, followed by denoising, has been widely adopted by the industry.  In fact, almost any realistic image made for a movie today includes this pipeline.  \n\nMore interesting, over the course of this project, the same ideas have even entered into the real-time domain, and this is a major milestone for this research and the outcomes of this proposal.  In particular, it is now possible to use hardware-accelerated systems to trace a few rays per pixel (picture element) of the image in real-time (at about 30 frames per second, which is how fast the human observer can see) followed by a real-time reconstruction and denoising step.  Recent developments in graphics software and hardware have seen this vision come to fruition in the past year, with the general pipeline being integrated into state of the art commercial products.  \n\nPerhaps most interesting, the research in this project has demonstrated accurate results with only 4 samples per pixel, which is one to two orders of magnitude than the state of the art prior to the proposal.  These developments have created great excitement for real-time computer graphics image synthesis.\n\nOf course, the basic ideas of sampling and reconstruction are relevant in a number of other domains in computer graphics, and we have developed new methods for computing images under general lighting from only five input images under specific lighting directions.  We have shown how to recover the reflectance properties of a surface or how it reflects light from only two images, to name a few results.  All of these results open up new potential for applications like appearance capture, virtual reality, and accurate representation of objects for e-commerce and visualization.  \n\nIn summary, this proposal has introduced a new paradigm in the way realistic computer graphics images are calculated and displayed in practical applications, and the research results have inspired an important shift in the thinking of software developers and commercial users, to adopt our framework of sparse Monte Carlo Sampling followed by reconstruction.  The benefits are significant for the industry, and have been enjoyed by millions of viewers in the general public whenever they see movies with computer-generated visual effects, view advertisements for products with computer-generated imagery, or enjoy interacting with an playing video games on high-end personal computers.  \n\n\t\t\t\t\tLast Modified: 09/07/2018\n\n\t\t\t\t\tSubmitted by: Ravi Ramamoorthi"
 }
}