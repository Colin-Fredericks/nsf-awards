{
 "awd_id": "1414030",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Large: Collaborative Research: Exploiting the Naturalness of Software",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2014-07-01",
 "awd_exp_date": "2018-06-30",
 "tot_intn_awd_amt": 666667.0,
 "awd_amount": 666667.0,
 "awd_min_amd_letter_date": "2014-06-06",
 "awd_max_amd_letter_date": "2016-09-16",
 "awd_abstract_narration": "This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. \r\n \r\nThe project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Cohen",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "William W Cohen",
   "pi_email_addr": "wcohen@cs.cmu.edu",
   "nsf_id": "000148698",
   "pi_start_date": "2014-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 Forbes Avenue",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 489653.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 177014.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>What does it mean for a computer to \"understand\" a technical domain,such as software? To test understanding for a person, one wouldprobably try and measure performance on a number of different tasks.</p>\n<p>One test of knowledge would be to explain how entities from the domainare related.&nbsp; For example, a \"button\", a \"link\", and an \"option\" are all similar in that they are names of GUI elements.&nbsp; We looked at ways to infer relationships between software entities from text aboutsoftware.&nbsp; Specifically, we collected data from the Q&amp;A websiteStackOverflow, and used NLP methods and statistical machine-learningmethods to find a set of relationships that were frequentllymentioned, and internally consistent. The quality of the top-scoringrelationship was good (as much as 90% for certain measures) whenevaluated by domain experts.</p>\n<p>As another test, one might ask a person read some computer code and explain in English what it does--i.e., to produce meaningful comments for the code.&nbsp; We built a system could produce English comments on a new program that it was presented with.&nbsp; This system was developedu sing machine learning methods: it was trained on actual comments, written by programmers to explain code that they had written.&nbsp; The learning method we used works by reading the program into a neural memory (\"encoding\" it), then producing the comment one word at a time, updating the neural memory after each word is produced (\"decoding\").The best-performing model for comment generation was a novel learning system we called \"encoder-reviewer-decoder\", which allows some additional neural processing to be performed after the source code isloaded, but before the first word of the comments are produced.&nbsp; The encoder-reviewer-decoder model is intended to model a person's mentalprocess of thinking about the program after reading it, but before explaining it.&nbsp; Although this model cannot produce good-enough program comments on its own, it is accurate enough to help a programmersubstantially, filling in enough of a comment automatically to reduce typing by over a third.</p>\n<p>Another common way to test understanding for people would be to ask them to answer question about terms from the domain.&nbsp; We looked at answering \"fill-in-the-blank\" or \"cloze\" questions, such as</p>\n<p><br />&nbsp; <em>Django is a free and open-source web framework, written in ____,&nbsp; which follows the model-view-template (MVT) architectural pattern.</em></p>\n<p><br />(The correct answer here is \"Python\").&nbsp; From StackOverflow, a web site with questions and answers about computer programming, we extracted thousands of definitions, and constructed over 35,000 cloze questions. These questions are very hard: two experienced, senior programmers answered only 46.8% correctly, on average.</p>\n<p>We explored a number of approaches to answering these questions automatically.&nbsp; The first approach modeled what a human might do.&nbsp; The system first searches for documents that might contain the answer,then \"reads\" the documents returned by the search, and finally produces the answer.&nbsp; To \"read\" the documents use existing methods called reading comprehension (RC) systems.&nbsp; These methods performed reasonably well, with the best existing question-answering methods reaching accuracy of 32.1%.&nbsp; However, further experiments showed than one could do about as well (accuracy of 33.6%) using another approach called a \"neural language model\" (LM), which simply guesses a plausible word to fill in the blank using general statistics about English sentences in the software domain.</p>\n<p>Motivated by this, we explored a new approach to answering cloze questions about software. The insight we had was that often, related software entities (e.g., \"Django\" and \"Python\") were used together as tags for StackOverFlow posts. (Every posted question can be tagged by the user with a set of software entities, to make it easier for possible question-answerers to find relevant questions.)&nbsp; Often entities that are semantically related are used together as tags ofthe same post.&nbsp; We thus designed a new method specifically designed for answering Cloze (fill-in-the-blank) questions using this sort of text with tagged entities.</p>\n<p>Our method is called CASE (Context-Adjusted Syntax Embeddings).&nbsp; It's a&nbsp; hybrid of an neural LM and a co-occurrence model, which predicts an answer based on co-occurrence with the entity.&nbsp; This allows a useful \"division of labor\" between the two models.&nbsp; The LM can predict the appropriate &ldquo;type&rdquo; of the answer entity based on the question syntax (in the example, a programming language is needed), while the co-occurrence model picks out the entity of that \"type\" based on co-occurrence with the term defined.&nbsp; CASE far outperforms the other methods: it achieved an accuracy of 44.9%, which is close to the performance of human experts.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/26/2018<br>\n\t\t\t\t\tModified by: William&nbsp;Cohen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWhat does it mean for a computer to \"understand\" a technical domain,such as software? To test understanding for a person, one wouldprobably try and measure performance on a number of different tasks.\n\nOne test of knowledge would be to explain how entities from the domainare related.  For example, a \"button\", a \"link\", and an \"option\" are all similar in that they are names of GUI elements.  We looked at ways to infer relationships between software entities from text aboutsoftware.  Specifically, we collected data from the Q&amp;A websiteStackOverflow, and used NLP methods and statistical machine-learningmethods to find a set of relationships that were frequentllymentioned, and internally consistent. The quality of the top-scoringrelationship was good (as much as 90% for certain measures) whenevaluated by domain experts.\n\nAs another test, one might ask a person read some computer code and explain in English what it does--i.e., to produce meaningful comments for the code.  We built a system could produce English comments on a new program that it was presented with.  This system was developedu sing machine learning methods: it was trained on actual comments, written by programmers to explain code that they had written.  The learning method we used works by reading the program into a neural memory (\"encoding\" it), then producing the comment one word at a time, updating the neural memory after each word is produced (\"decoding\").The best-performing model for comment generation was a novel learning system we called \"encoder-reviewer-decoder\", which allows some additional neural processing to be performed after the source code isloaded, but before the first word of the comments are produced.  The encoder-reviewer-decoder model is intended to model a person's mentalprocess of thinking about the program after reading it, but before explaining it.  Although this model cannot produce good-enough program comments on its own, it is accurate enough to help a programmersubstantially, filling in enough of a comment automatically to reduce typing by over a third.\n\nAnother common way to test understanding for people would be to ask them to answer question about terms from the domain.  We looked at answering \"fill-in-the-blank\" or \"cloze\" questions, such as\n\n\n  Django is a free and open-source web framework, written in ____,  which follows the model-view-template (MVT) architectural pattern.\n\n\n(The correct answer here is \"Python\").  From StackOverflow, a web site with questions and answers about computer programming, we extracted thousands of definitions, and constructed over 35,000 cloze questions. These questions are very hard: two experienced, senior programmers answered only 46.8% correctly, on average.\n\nWe explored a number of approaches to answering these questions automatically.  The first approach modeled what a human might do.  The system first searches for documents that might contain the answer,then \"reads\" the documents returned by the search, and finally produces the answer.  To \"read\" the documents use existing methods called reading comprehension (RC) systems.  These methods performed reasonably well, with the best existing question-answering methods reaching accuracy of 32.1%.  However, further experiments showed than one could do about as well (accuracy of 33.6%) using another approach called a \"neural language model\" (LM), which simply guesses a plausible word to fill in the blank using general statistics about English sentences in the software domain.\n\nMotivated by this, we explored a new approach to answering cloze questions about software. The insight we had was that often, related software entities (e.g., \"Django\" and \"Python\") were used together as tags for StackOverFlow posts. (Every posted question can be tagged by the user with a set of software entities, to make it easier for possible question-answerers to find relevant questions.)  Often entities that are semantically related are used together as tags ofthe same post.  We thus designed a new method specifically designed for answering Cloze (fill-in-the-blank) questions using this sort of text with tagged entities.\n\nOur method is called CASE (Context-Adjusted Syntax Embeddings).  It's a  hybrid of an neural LM and a co-occurrence model, which predicts an answer based on co-occurrence with the entity.  This allows a useful \"division of labor\" between the two models.  The LM can predict the appropriate \"type\" of the answer entity based on the question syntax (in the example, a programming language is needed), while the co-occurrence model picks out the entity of that \"type\" based on co-occurrence with the term defined.  CASE far outperforms the other methods: it achieved an accuracy of 44.9%, which is close to the performance of human experts.\n\n\t\t\t\t\tLast Modified: 07/26/2018\n\n\t\t\t\t\tSubmitted by: William Cohen"
 }
}