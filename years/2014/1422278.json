{
 "awd_id": "1422278",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Statistical Data Privacy: Fundamental Limits and Efficient Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2014-07-24",
 "awd_max_amd_letter_date": "2014-07-24",
 "awd_abstract_narration": "Privacy is a fundamental individual right. In the era of big data, large amounts of data about individuals are collected both voluntarily (e.g., frequent flier/shopper incentives)  and  involuntarily  (e.g. US Census or medical records).  With the ready ability to  search for information and  correlate it across distinct sources (using data analytics and/or recommender systems), privacy violation takes on an ominous note in this information age. \r\n\r\nAnonymization of user information is a classical technique, but is susceptible to correlation attacks: by correlating the anonymized database with  another (perhaps publicly available) deanonymized database, a user's privacy could still be divulged. A way out of the limitations of anonymization is to release a randomized database; this offers plausible deniability of any user identity breached via the data release. A systematic way of  providing guarantees for the deniability of user presence/absence is the technical field of differential privacy, providing strong privacy guarantees against adversaries with arbitrary side information. It is of fundamental interest to characterize privacy mechanisms that  randomize \"just enough\" to keep the released database as true to the intended one as possible, providing maximal utility.  \r\n\r\nBased on recent work connecting the areas of information theory and statistical data privacy (via a hypothesis testing context) and demonstrating novel privacy mechanisms that exponentially improve (in terms of variance of noise added, say) upon the state of the art  for medium and low privacy regimes, the objective of the project is threefold: (a) characterize the fundamental limits to tradeoffs between privacy and utility in a variety of canonical setting; (b) discover (near) optimal mechanisms that can be efficiently implemented in practice; and (c) seek natural notions of statistical data privacy (beyond differential privacy) using the operational context of hypothesis testing. \r\n\r\n\r\nPrivacy is a central, and multifaceted, social and technological issue of today's information age. This project is focused on the technical aspect of this multifaceted area, and seeks to discover fundamental limits to privacy-utility tradeoffs in the context of currently well established notions of privacy (differential privacy). The expected results expected are fundamental and immediately applicable to a variety of practical settings. Specifically,  two concrete practical settings involving genomic data release and smart meter data release will be studied in detail. Due to privacy concerns, genomic and smart meter data is simply unavailable at large -- depriving widespread data analytics and practical implications of such analysis. This project will  build and release a software suite of sanitization tools, involving the privacy mechanisms discovered as part of this project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Pramod",
   "pi_last_name": "Viswanath",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Pramod Viswanath",
   "pi_email_addr": "pramodv@princeton.edu",
   "nsf_id": "000486611",
   "pi_start_date": "2014-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Privacy is a fundamental right. In the era of big data and social networking, privacy of individuals and their data has rapidly eroded. In this project we take a first principles view of data privacy and how to formally guarantee it under statistical (i.e., plausibility) contexts. Differential privacy is a leading proposal for a formal characterization of statistical data privacy. &nbsp;In this project we enlarge the scope of differential privacy by deriving an equivalent operational hypothesis testing interpretation. This allows us to establish exact optimality results both for canonical differential privacy settings but also an enlarged class. In the first result, we derive the exact noise distribution that achieves optimal utlity (expecxted value of any monotonically increasing function of the noise): the optimal noise has a &nbsp;staircase distribution with parameters that depend on the function being queried and the privacy required. In the context of &nbsp;binary data being exposed potentially via a real valued description, we show that the optimal privatization mechanism is simply the randomized response (where the output is also simply binary). This optimality is shown in a very general setting, utilizing the hypothesis testing framework we have established. &nbsp;In the second result, we show precise characterizations of the privacy leakage due to additional querying (composition of multiple data releases), for every finite number of queries and exact values of the differential privacy parameters. Finally, we show that randomized response is optimal in any &nbsp;multiparty setting where the individual users all have binary data and are computing some boolean function of each other's data with differential privacy constraints.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/06/2018<br>\n\t\t\t\t\tModified by: Pramod&nbsp;Viswanath</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPrivacy is a fundamental right. In the era of big data and social networking, privacy of individuals and their data has rapidly eroded. In this project we take a first principles view of data privacy and how to formally guarantee it under statistical (i.e., plausibility) contexts. Differential privacy is a leading proposal for a formal characterization of statistical data privacy.  In this project we enlarge the scope of differential privacy by deriving an equivalent operational hypothesis testing interpretation. This allows us to establish exact optimality results both for canonical differential privacy settings but also an enlarged class. In the first result, we derive the exact noise distribution that achieves optimal utlity (expecxted value of any monotonically increasing function of the noise): the optimal noise has a  staircase distribution with parameters that depend on the function being queried and the privacy required. In the context of  binary data being exposed potentially via a real valued description, we show that the optimal privatization mechanism is simply the randomized response (where the output is also simply binary). This optimality is shown in a very general setting, utilizing the hypothesis testing framework we have established.  In the second result, we show precise characterizations of the privacy leakage due to additional querying (composition of multiple data releases), for every finite number of queries and exact values of the differential privacy parameters. Finally, we show that randomized response is optimal in any  multiparty setting where the individual users all have binary data and are computing some boolean function of each other's data with differential privacy constraints. \n\n\t\t\t\t\tLast Modified: 08/06/2018\n\n\t\t\t\t\tSubmitted by: Pramod Viswanath"
 }
}