{
 "awd_id": "1445610",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Developing a System for Tracking State Assessment Policies in Science and Mathematics Education",
 "cfda_num": "47.076",
 "org_code": "11010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Karen King",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 299049.0,
 "awd_amount": 299049.0,
 "awd_min_amd_letter_date": "2014-07-29",
 "awd_max_amd_letter_date": "2014-07-29",
 "awd_abstract_narration": "The National Research Council in 2013 released the report Monitoring Progress toward Successful K-12 STEM Education: A Nation Advancing?, which outlined 14 Indicators as well as the needed research and development to create a system to monitor the quality of STEM education in the nation. This project is funded in response to a Dear Colleague Letter request for research in the Promoting Innovation in Measurement and Evaluation program focused on developing research and tools to advance the nation's capability to measure these indicators. The researchers in this project will address Indicator 12 - States' use of assessments that measure the core concepts and practices of science and mathematics disciplines. Determining the alignment of state assessments with the core concepts in the current college and career readiness standards adopted by states is a complex process that requires coordination of state policy and assessment practices. This project will design and test an online method and develop a database for tracking and reporting on policies regarding student assessment systems of the 50 states. In addition, the project will develop cross-state criteria for reporting on the content alignment of state assessments to state-adopted standards to improve methodologies for such alignment.\r\n\r\nResearch activities will be carried out in three phases. In phase one, the project will gather available information about assessment policies of the states and the two national consortia to develop the database of existing policy documents and to structure the online information gathering portal. In the second stage, the researchers will work with expert advisors to refine the online process for expanding the database to make it easily updatable. In the third phase the researchers will work with a subset of state science and mathematics supervisors to test out the collection system. Parallel to the development of the system to report state assessment policies will be the development of frameworks for determining the alignment of state assessments and standards based on the work of the two assessment consortia, state-contracted studies, and the models of state assessments systems recommended by work conducted by the National Research Council science assessment committee.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DGE",
 "org_div_long_name": "Division Of Graduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rolf",
   "pi_last_name": "Blank",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rolf Blank",
   "pi_email_addr": "Rolfb444@gmail.com",
   "nsf_id": "000340781",
   "pi_start_date": "2014-07-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "National Opinion Research Center",
  "inst_street_address": "55 E MONROE ST",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7732566000",
  "inst_zip_code": "606035713",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "NATIONAL OPINION RESEARCH CENTER",
  "org_prnt_uei_num": "MPYFY5UMSDP4",
  "org_uei_num": "MPYFY5UMSDP4"
 },
 "perf_inst": {
  "perf_inst_name": "National Opinion Research Center",
  "perf_str_addr": "1155 E. 60th Street",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372745",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "726100",
   "pgm_ele_name": "Project & Program Evaluation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0414",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001415DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 299049.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The design pilot study (2014&ndash;15 school year) tested a design for collection of state policies information from three sources&mdash;state websites, survey of state assessment staff (focusing on design and methods of reporting), and state content specialists in math and science (focusing on content and alignment of assessments and standards).&nbsp; The design options and issues of focus in the pilot study are summarized in a paper, developed from the recommendations of a project expert panel (NORC, 2015).&nbsp;</p><p>A key issue for the design study was how to track states use of innovative features of the mathematics assessments being provided through the consortia--Smarter Balanced (SB) and PARCC, including performance tasks (with multiple steps and explanation of work), balance of items across the different levels of depth of knowledge, use of computer-based testing to improve turnaround time for scoring and reporting, measures of learning content and skills, use of adaptive testing to measure a full range of student knowledge, benchmark testing to track progress through the year, and a digital assessment library for formative classroom assessment.&nbsp; To work toward an indicator of state assessments alignment to standards especially relative to content coverage and levels of cognitive demand, NORC and the advisers considered the Criteria for High-Quality Assessments defined by the chief state school officers (CCSSO, 2014).&nbsp;</p><p>Reporting of state policies and trends over time is equally important for science education as part of the proposed state policies tracking system because of the recent shifts in science education standards and the strong interest in new forms of student assessment to match the direction of the standards.&nbsp; Some states are following the recommendations of the NRC committee on NGSS science assessments (NRC, 2014), including moving toward a system of science assessment with varied methods, assessing the three dimensions of science instruction, and maintaining annual reporting of student progress.&nbsp;</p><p>Objectives of Design Study</p><div><h1><span style=\"color: #000000; font-size: 12px; font-weight: normal;\">The design study and pilot project addressed two questions about state assessment policies for mathematics and science and development of a system for tracking and reporting across the states:</span></h1></div><p class=\"ListNumber1\">1)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What core information on state assessment programs and policies should be collected and reported across states, i.e., what information on types of assessments and characteristics of assessment programs are important to have available in a 50-state report or online resource?</p><p class=\"ListNumber112ptAfter\">2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; What is an effective method for reporting comparable information on the extent to which student assessments in math and science are aligned to state-adopted content standards, and what is the relationship to the Common Core State Standards-Mathematics and Next Generation Science Standards?</p><p class=\"ListNumber112ptAfter\">Results</p><p>The design pilot study was carried out with support and participation by staff of ten state departments of education (ID, KY, KS, MA, MI, MN, NC, RI, UT, WA).&nbsp; Each of the states voluntarily participated in the study after a request was sent by NORC to state assessment directors.&nbsp; Information was collected during May and June 2015, with reporting covering assessments used with schools and students during the 2014&ndash;15 school year (NORC, 2015).</p><p>A key decision in planning for a state assessment policies indicator is the degree to which reporting and analysis across states should be comparable and quantifiable. &nbsp;</p><p>The panel recommended collecting and reporting information for the following categories of state assessment policies:</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Types of state assessments in mathematics and science</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Intended uses of assessment data</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Item or task design</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Timing and methods of administration</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cost of assessments</p><p class=\"ListBullet12ptAfter\">?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Methods of assessment reporting and dissemination.</p><p>Under the scope of the NSF grant, it would not be possible to conduct a new study of the alignment of each state&rsquo;s assessments in math and science.&nbsp; However, the panel recommended that NORC report indicators of the degree of alignment by reporting on assessments <em>content</em> and <em>depth </em>relative to standards &ndash; that is, reporting on grade-specific content topics, cognitive levels of assessment items, and the range of methods used in assessing content knowledge and practices.&nbsp; The indicators would be selected to provide analysis and comparison of state assessments&rsquo; content rigor and breadth specific to a grade, and the degree to which the items/tasks address the depth of knowledge and cognitive demand of the standards. &nbsp;</p><p>The following indicators of content alignment were identified for testing in the pilot study:</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; State policy on content standards, and relationship to CCSS and NGSS</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; When standards were developed and approved</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Source of state assessment framework or blueprint (consortia, state, other)</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Percentage of grade 5 math assessments on selected content topics</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cognitive complexity/demand categories represented in assessment items/tasks</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Elementary grades at which selected topics were assessed</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Percentage of grade 11 math assessments on selected content topics</p><p>?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Timeline and methods of development of new science assessments</p><p class=\"ListBullet12ptAfter\">?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Major content topics for new science assessments.</p><p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/18/2016<br>\n\t\t\t\t\tModified by: Rolf&nbsp;Blank</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>\n<div class=\"porColContainerHR\">\n<div class=\"porContentCol\">\n<h2>Addendum # 1</h2><p>Summary on Implementing the Survey Design Online and Next Steps</p>\n<p>On reviewing the reporting of assessment content topics in math, advisors recommended that further informaiton be collected on the assessments being reported.&nbsp; The end of course assessments and comprehensive math assessments should be reported for role in school accountability and high school graduation requirements. The content of these assesments should be summarized across broad content categories comparable by states.</p>\n<p>The science assessments content section should summarize the transition process for standards and assessment by states.&nbsp; The policy position in relation to NGSS should be indicated and the relation to the NRC Framwork for K-12 education.&nbsp; The content and design of new assessments should be summarized as planning decisions being made and the timeline for implementing new assessments.&nbsp; Then, at a later point&nbsp;information should be reported on the three dimensions of science content and how they are being assessed.&nbsp;</p>\n<p>The review of the design study by advisers resulted in recommendations to NORC for reporting on degree of alignment of math and science assessment in relation to state content standards.&nbsp; Many states have conducted alignment studies in the past decade under federal requirements, and states will continue to do studies to meet new peer review requirements of the US ED.&nbsp; Under the scope of the NSF STEM indicators grant, i will not be possible to conduct new studies of each state's assessments.&nbsp; The panel recommended that NORC report indicator measures of&nbsp;alignment by focusing on specific content and depth of assessments relative to standards, and by selecitng specific grade levels.&nbsp; The indicators will&nbsp;provide analysis and reporting across states on the rigor and breadth of assessments as compared to the standards expectations for content topics and cognitive demand.&nbsp;&nbsp;Regardless of the&nbsp;standards defined by state policy,&nbsp;or the assessments selected by a state, the alignment indicators will provide a common basis for reporting across states.</p>\n<div class=\"mcePaste\" style=\"left: -10000px; top: 0px; width: 1px; height: 1px; overflow: hidden; position: absolute;\" id=\"_mcePaste\"><span style=\"font-family: Times New Roman; font-size: small;\"> </span>\n<p style=\"margin: 0in 0in 12pt;\"><span style=\"font-family: Times New Roman;\"><span style=\"font-size: small;\">The pilot study instrument asked that states report on the purposes or uses of specific assessments.</span><span style=\"font-size: small;\"><span style=\"mso-spacerun: yes;\">&nbsp; </span>All of the states reported the use of requirement mathematics for school accountability under federal and state policies, and eight states reported science assessments were used for school accountability.<span style=\"mso-spacerun: yes;\">&nbsp; </span>Four of the pilot states indicated that data were used for teacher accountability and five states reported data were used for student accountability, primarily for graduation requirements.<span style=\"mso-spacerun: yes;\">&nbsp; </span>All the states reported data were used to inform instruction and curriculum.<span style=\"mso-spacerun: yes;\">&nbsp; </span>In future revisions the advisers recommended asking for an example of local use of assessment data with instruction and curriculum.<span style=\"mso-spacerun: yes;\">&nbsp; </span></span></span></p>\n<span style=\"font-family: Times New Roman; font-size: small;\"> </span></div><br>\n<p>Added: 11/01/2016<br>Submitted by: Rolf&nbsp;Blank</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe design pilot study (2014&ndash;15 school year) tested a design for collection of state policies information from three sources&mdash;state websites, survey of state assessment staff (focusing on design and methods of reporting), and state content specialists in math and science (focusing on content and alignment of assessments and standards).  The design options and issues of focus in the pilot study are summarized in a paper, developed from the recommendations of a project expert panel (NORC, 2015). \nA key issue for the design study was how to track states use of innovative features of the mathematics assessments being provided through the consortia--Smarter Balanced (SB) and PARCC, including performance tasks (with multiple steps and explanation of work), balance of items across the different levels of depth of knowledge, use of computer-based testing to improve turnaround time for scoring and reporting, measures of learning content and skills, use of adaptive testing to measure a full range of student knowledge, benchmark testing to track progress through the year, and a digital assessment library for formative classroom assessment.  To work toward an indicator of state assessments alignment to standards especially relative to content coverage and levels of cognitive demand, NORC and the advisers considered the Criteria for High-Quality Assessments defined by the chief state school officers (CCSSO, 2014). \nReporting of state policies and trends over time is equally important for science education as part of the proposed state policies tracking system because of the recent shifts in science education standards and the strong interest in new forms of student assessment to match the direction of the standards.  Some states are following the recommendations of the NRC committee on NGSS science assessments (NRC, 2014), including moving toward a system of science assessment with varied methods, assessing the three dimensions of science instruction, and maintaining annual reporting of student progress. \nObjectives of Design StudyThe design study and pilot project addressed two questions about state assessment policies for mathematics and science and development of a system for tracking and reporting across the states:1)      What core information on state assessment programs and policies should be collected and reported across states, i.e., what information on types of assessments and characteristics of assessment programs are important to have available in a 50-state report or online resource?2)      What is an effective method for reporting comparable information on the extent to which student assessments in math and science are aligned to state-adopted content standards, and what is the relationship to the Common Core State Standards-Mathematics and Next Generation Science Standards?Results\nThe design pilot study was carried out with support and participation by staff of ten state departments of education (ID, KY, KS, MA, MI, MN, NC, RI, UT, WA).  Each of the states voluntarily participated in the study after a request was sent by NORC to state assessment directors.  Information was collected during May and June 2015, with reporting covering assessments used with schools and students during the 2014&ndash;15 school year (NORC, 2015).\nA key decision in planning for a state assessment policies indicator is the degree to which reporting and analysis across states should be comparable and quantifiable.  \nThe panel recommended collecting and reporting information for the following categories of state assessment policies:\n?         Types of state assessments in mathematics and science\n?         Intended uses of assessment data\n?         Item or task design\n?         Timing and methods of administration\n?         Cost of assessments?         Methods of assessment reporting and dissemination.\nUnder the scope of the NSF grant, it would not be possible to conduct a new study of the alignment of each state?s assessments in math and science.  However, the panel recommended that NORC report indicators of the degree of alignment by reporting on assessments content and depth relative to standards &ndash; that is, reporting on grade-specific content topics, cognitive levels of assessment items, and the range of methods used in assessing content knowledge and practices.  The indicators would be selected to provide analysis and comparison of state assessments? content rigor and breadth specific to a grade, and the degree to which the items/tasks address the depth of knowledge and cognitive demand of the standards.  \nThe following indicators of content alignment were identified for testing in the pilot study:\n?         State policy on content standards, and relationship to CCSS and NGSS\n?         When standards were developed and approved\n?         Source of state assessment framework or blueprint (consortia, state, other)\n?         Percentage of grade 5 math assessments on selected content topics\n?         Cognitive complexity/demand categories represented in assessment items/tasks\n?         Elementary grades at which selected topics were assessed\n?         Percentage of grade 11 math assessments on selected content topics\n?         Timeline and methods of development of new science assessments?         Major content topics for new science assessments.\n \n\n\t\t\t\t\tLast Modified: 05/18/2016\n\n\t\t\t\t\tSubmitted by: Rolf Blank\nSummary on Implementing the Survey Design Online and Next Steps\n\nOn reviewing the reporting of assessment content topics in math, advisors recommended that further informaiton be collected on the assessments being reported.  The end of course assessments and comprehensive math assessments should be reported for role in school accountability and high school graduation requirements. The content of these assesments should be summarized across broad content categories comparable by states.\n\nThe science assessments content section should summarize the transition process for standards and assessment by states.  The policy position in relation to NGSS should be indicated and the relation to the NRC Framwork for K-12 education.  The content and design of new assessments should be summarized as planning decisions being made and the timeline for implementing new assessments.  Then, at a later point information should be reported on the three dimensions of science content and how they are being assessed. \n\nThe review of the design study by advisers resulted in recommendations to NORC for reporting on degree of alignment of math and science assessment in relation to state content standards.  Many states have conducted alignment studies in the past decade under federal requirements, and states will continue to do studies to meet new peer review requirements of the US ED.  Under the scope of the NSF STEM indicators grant, i will not be possible to conduct new studies of each state's assessments.  The panel recommended that NORC report indicator measures of alignment by focusing on specific content and depth of assessments relative to standards, and by selecitng specific grade levels.  The indicators will provide analysis and reporting across states on the rigor and breadth of assessments as compared to the standards expectations for content topics and cognitive demand.  Regardless of the standards defined by state policy, or the assessments selected by a state, the alignment indicators will provide a common basis for reporting across states.\n \nThe pilot study instrument asked that states report on the purposes or uses of specific assessments.  All of the states reported the use of requirement mathematics for school accountability under federal and state policies, and eight states reported science assessments were used for school accountability.  Four of the pilot states indicated that data were used for teacher accountability and five states reported data were used for student accountability, primarily for graduation requirements.  All the states reported data were used to inform instruction and curriculum.  In future revisions the advisers recommended asking for an example of local use of assessment data with instruction and curriculum.  \n \nLast Modified: 11/01/2016\nSubmitted by: Rolf Blank"
 }
}