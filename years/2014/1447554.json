{
 "awd_id": "1447554",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "BIGDATA: F: DKA: Collaborative Research: Dealing Efficiently with Big Social Network Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2014-08-27",
 "awd_max_amd_letter_date": "2015-09-02",
 "awd_abstract_narration": "The past decade has seen dramatic growth in systems that collect data from human activities. Online social networks record not just friendships, but interactions, messages, photos, and interests. Mobile devices track location via GPS information. Online stores monitor millions of customers as they explore and transact. Sensors, wearable and otherwise, produce detailed behavioral data. Collectively, this provides ever-larger collections of human social-activity information -- we refer to this as Big Social Data. While Big Social Data is growing rapidly, the available processing resources -- CPU, memory, communication -- are growing at a slower pace. To realize the promise of big social data, we need algorithms that use only sublinear resources, that is, resources growing much less than the growth of the data in suitable parameters. Designing these algorithms will be the core activity of this research project. This work will be in consultation with practitioners handling Big Social Data, leading to many opportunities for technology transfer. The research program both enables and benefits from an education and outreach program that will help develop the new breed of algorithmically-trained data scientists for Big Social Data.\r\n\r\nEmerging systems -- MapReduce, Hadoop, Spark, Storm, etc. -- use large scale distributed computation: clusters of machines not only gathering and storing data in parallel, but also working together to perform computations. Often, these systems and applications work via incremental processing, storing and returning only approximate solutions, trading off quality and certainty for efficiency. In addition, these systems take a data-centric view, wherein the data is stored as <Key, Value> pairs. This project will address fundamental problems with Big Social Data -- search, ranking, and optimization, etc. in these modern computing and data models. For these problems, this project will design algorithms that are sublinear in the relevant parameter -- number of keys, size of values, computing time per key or over all keys, and other variations that map to underlying storage, number of machines, bandwidth and other computational constraints.\r\n\r\nFor further information, see the project web site at http://www.stanford.edu/~ashishg/socialdata.html .",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kameshwar",
   "pi_last_name": "Munagala",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kameshwar Munagala",
   "pi_email_addr": "kamesh@cs.duke.edu",
   "nsf_id": "000487108",
   "pi_start_date": "2014-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277080129",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 150000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Overview. &nbsp;&nbsp;</strong>The main goal of this project is to further our theoretical understanding and algorithms to process massive datasets generated by social/economic and geometric applications, where computational efficiency and storage space are paramount considerations. We ideally require algorithms that run in time that is linear or sub-linear in the size of the data, and in addition, we seek algorithms that can be effectively run on large-scale distributed computing systems such as MapReduce. A theme that cuts across our work is making our algorithms \"data dependent\" by exploiting and abstracting properties that real data sets should satisfy. In addition, a running theme is a focus on objectives such as fairness that have become increasingly important in reasoning about massive data.</p>\n<p><strong>Intellectual Merit. </strong>We discuss the research highlights of this project. We presented the first provably efficient algorithms to compute, store, and query data structures for the basic geometric problems of range queries and approximate nearest neighbor queries in a popular parallel computing abstraction that captures the salient features of MapReduce and other massively parallel communication models. In particular, we presented algorithms for KD-trees, range trees, and BBD-trees that only require a constant rounds of communication for both preprocessing and querying while staying competitive in terms of running time and workload to their classical counterparts. Similarly, we presented algorithms that run in a constant number of rounds for basic geographic information systems problems, such as computing Delauney triangulations, arising processing large terrain elevation data (represented as a 3D point cloud) that are too big to fit on one machine.</p>\n<p>We&nbsp;considered several problems that arise in processing trajectory (or curve) data, for instance, that is obtained by GPS traces in a road network. We presented the first sub-quadratic time algorithms for the basic question of computing the similarity between a pair of trajectories, specified as point sequences, in any number of dimensions. We also proposed a model and associated algorithms for clustering multiple trajectories, that is, grouping them based on the shared portions between them, and tested this algorithm on real trajectory data.</p>\n<p>We finally considered the problem of clustering datasets more generally. A major question in clustering data is to develop linear time algorithms for well-known objective functions such as K-means. We presented a nearly linear time algorithm for optimally clustering data via the K-means objective in Euclidean spaces when the data satisfies the property that the optimal clustering does not change significantly when the data is perturbed. We also presented a new model for fairness in clustering, where every group of data points of sufficient size should feel the cluster centers they are assigned to are optimal in a certain sense. We presented simple algorithms for clustering under such a notion of fairness and studied its trade-off with the classical K-means objective.</p>\n<p><strong>Broader Impacts. </strong>Our work impacts the development of near-linear time and massively parallel algorithms for basic data analysis problems. This contributes to the canon of research in computational geometry, parallel computing, and geographic information systems. On the more applied front, to further our research agenda in the field of algorithmic fairness, as part of this project, we have partnered with the cities of Durham and Greensboro NC to implement advertising campaigns targeting various minority demographics as part of their Participatory Budgeting elections.<strong> </strong>In addition to curricular improvements such as developing a new course on Algorithmic Decision Making, we have developed and co-organized workshops at the Simons Institute of Theoretical Computer Science, and the ACM EC conference. We have supervised several graduate and undergraduate students (including female students) as part of this grant. These students have won best thesis awards at Duke, best paper awards at leading conferences, and mentions at the CRA outstanding undergraduate researcher award. We have also supervised high school students leading to published papers in leading conferences.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/06/2020<br>\n\t\t\t\t\tModified by: Kameshwar&nbsp;Munagala</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOverview.   The main goal of this project is to further our theoretical understanding and algorithms to process massive datasets generated by social/economic and geometric applications, where computational efficiency and storage space are paramount considerations. We ideally require algorithms that run in time that is linear or sub-linear in the size of the data, and in addition, we seek algorithms that can be effectively run on large-scale distributed computing systems such as MapReduce. A theme that cuts across our work is making our algorithms \"data dependent\" by exploiting and abstracting properties that real data sets should satisfy. In addition, a running theme is a focus on objectives such as fairness that have become increasingly important in reasoning about massive data.\n\nIntellectual Merit. We discuss the research highlights of this project. We presented the first provably efficient algorithms to compute, store, and query data structures for the basic geometric problems of range queries and approximate nearest neighbor queries in a popular parallel computing abstraction that captures the salient features of MapReduce and other massively parallel communication models. In particular, we presented algorithms for KD-trees, range trees, and BBD-trees that only require a constant rounds of communication for both preprocessing and querying while staying competitive in terms of running time and workload to their classical counterparts. Similarly, we presented algorithms that run in a constant number of rounds for basic geographic information systems problems, such as computing Delauney triangulations, arising processing large terrain elevation data (represented as a 3D point cloud) that are too big to fit on one machine.\n\nWe considered several problems that arise in processing trajectory (or curve) data, for instance, that is obtained by GPS traces in a road network. We presented the first sub-quadratic time algorithms for the basic question of computing the similarity between a pair of trajectories, specified as point sequences, in any number of dimensions. We also proposed a model and associated algorithms for clustering multiple trajectories, that is, grouping them based on the shared portions between them, and tested this algorithm on real trajectory data.\n\nWe finally considered the problem of clustering datasets more generally. A major question in clustering data is to develop linear time algorithms for well-known objective functions such as K-means. We presented a nearly linear time algorithm for optimally clustering data via the K-means objective in Euclidean spaces when the data satisfies the property that the optimal clustering does not change significantly when the data is perturbed. We also presented a new model for fairness in clustering, where every group of data points of sufficient size should feel the cluster centers they are assigned to are optimal in a certain sense. We presented simple algorithms for clustering under such a notion of fairness and studied its trade-off with the classical K-means objective.\n\nBroader Impacts. Our work impacts the development of near-linear time and massively parallel algorithms for basic data analysis problems. This contributes to the canon of research in computational geometry, parallel computing, and geographic information systems. On the more applied front, to further our research agenda in the field of algorithmic fairness, as part of this project, we have partnered with the cities of Durham and Greensboro NC to implement advertising campaigns targeting various minority demographics as part of their Participatory Budgeting elections. In addition to curricular improvements such as developing a new course on Algorithmic Decision Making, we have developed and co-organized workshops at the Simons Institute of Theoretical Computer Science, and the ACM EC conference. We have supervised several graduate and undergraduate students (including female students) as part of this grant. These students have won best thesis awards at Duke, best paper awards at leading conferences, and mentions at the CRA outstanding undergraduate researcher award. We have also supervised high school students leading to published papers in leading conferences.\n\n\t\t\t\t\tLast Modified: 11/06/2020\n\n\t\t\t\t\tSubmitted by: Kameshwar Munagala"
 }
}