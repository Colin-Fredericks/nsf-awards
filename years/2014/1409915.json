{
 "awd_id": "1409915",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Medium: Collaborative: Aspire: Leveraging Automated Synthesis Technologies for Enhancing System Security",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 800000.0,
 "awd_amount": 800000.0,
 "awd_min_amd_letter_date": "2014-08-05",
 "awd_max_amd_letter_date": "2014-08-05",
 "awd_abstract_narration": "Designing secure systems and validating security of existing systems are hard challenges facing our society. For implementing secure applications, a serious stumbling block lies in the generation of a correct system specification for a security policy. It is non-trivial for both system designers and end users to express their intent in terms of formal logic. Similar challenges plague users' trying to validate security properties of existing applications, such as web or cloud based services, which often have no formal specifications. Thus, there is an urgent need for mechanisms that can bridge the gap between expressions of user intent and system specifications. This research designs an approach and a system called Aspire that is able to translate user intent into security specifications. \r\n\r\nAspire takes as input, expressions of user intent such as a system demonstration,  application input-output examples, or natural language. Aspire leverages recent  developments in the field of automated synthesis technologies that can consider such examples of user intent as input to the synthesis of security specifications. Aspire combines such inputs, along with a domain specific language for security applications, to synthesize a candidate set of possible outputs. The user can either choose a candidate output or provide more examples to guide the synthesis process. In this iterative fashion, the user can generate system specifications, policies, or properties. Aspire uses concepts from the domain of formal methods, machine learning, and programming languages to perform synthesis. Aspire is applicable to a variety of domains including web, mobile, and cloud applications. The output of Aspire's synthesis can either be used for analyzing security vulnerabilities, or for compilation and testing with real systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dawn",
   "pi_last_name": "Song",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dawn Song",
   "pi_email_addr": "dawnsong@cs.berkeley.edu",
   "nsf_id": "000079467",
   "pi_start_date": "2014-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "675 Soda Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201776",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 800000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-e3b68e5e-7fff-0d93-2c39-46cb0f4d325c\">\n<p dir=\"ltr\"><span>We have made significant progress in advancing the field of neural program synthesis, where we leverage deep neural networks to generate programs. We have proposed novel techniques to synthesize programs from user intents specified in diverse formats, including natural language and input-output examples. We have also developed techniques for software engineering applications.</span></p>\n<br />\n<p dir=\"ltr\"><span>First, we proposed new model architectures and training algorithms to learn programs that generalize to test cases far from the training distribution. The key insight is to incorporate recursion into the approach design, which provides effective regularization of the program space. By utilizing recursive traces to train the Neural Programmer-Interpreter (NPI) architecture, the model achieves 100% accuracy on learning sorting and several other simple algorithms. We also proposed new model architectures to support hierarhical program generation, and improved the model expressiveness to handle more complex tasks. In particular, we proposed novel techniques to learn a context-free parser from pairs of input programs and their parse trees. Our neural parsing programs can be learned to achieve 100% test accuracy on test inputs that are 500X longer than the training samples. We published related papers on these topics at ICLR 2017 and ICLR 2018, and the paper on recursive NPI won the Best Paper Award at ICLR 2017.</span></p>\n<br />\n<p dir=\"ltr\"><span>Second, we have developed a series of techniques to leverage execution results for program synthesis. Specifically, we incorporate an execution machine into our neural program synthesis framework, so that we can utilize the execution traces to provide richer information for model prediction. We conduct the evaluation on a benchmark program synthesis task based on the Karel programming language (an educational programming language, where imperative programs control an agent in a grid-world), and our approaches achieve significant performance gain compared to previous work. We have published related papers at NeurIPS 2018 and ICLR 2019.</span></p>\n<br />\n<p dir=\"ltr\"><span>Third, we have developed novel deep neural network architectures for software engineering applications. We have developed a novel method for using tree-structured neural networks to interpret and synthesize programs, with the application of translating code from one programming language to another. We created a dataset of JavaScript and CoffeeScript program pairs and evaluated our system on it; our tree-to-tree method achieves significantly higher performance than baselines based on sequence-based neural networks. We also evaluated the performance of our methodology on a Java to C# translation task proposed by prior work, where our system was 1.5-3x more accurate than the prior work. In addition, we developed a framework to incorporate program repair as a sub-routine for program synthesis, where the repair module aims at fixing the errors in the programs predicted by the neural program synthesizer. We designed neural networks for specification-guided program repair, and demonstrated that the repair module provides further improvement for program synthesis. We have published the paper on program translation at NeurIPS 2018.</span></p>\n<br />\n<p dir=\"ltr\"><span>Fourth, we have developed techniques for synthesizing programs from natural language, including logical forms in domain-specific languages, SQL queries, and Python. For example, we have developed a system for synthesizing a restricted class of programs called &ldquo;If-Then&rdquo; programs. Our system determines which services an If-Then program interacts with given a natural language description of the program. We proposed the Latent Attention neural network architecture, which takes into account the embedded meanings of words and their positions in the description, in addition to the relative importance of each word in the vocabulary. We demonstrated superior performance on the public data available on IFTTT and Zapier. We have also developed a neural-symbolic approach for reading comprehension. Specifically, given a question and a passage as the context, instead of directly generating the final answer, our model generates a program for multi-step reasoning, which can be executed to produce the answer. We demonstrated superior performance on reading comprehension benchmarks that require advanced numerical reasoning. We have published related papers at NeurIPS 2016 and ICLR 2020.</span></p>\n<br />\n<p dir=\"ltr\"><span>Finally, we have studied the effect of training data bias for neural program synthesis. For input-output program synthesis, a common practice for training dataset construction is to randomly generate programs and their corresponding input-output pairs. However, we showed that such type of processes may lead to subtle biases in the distribution of the resulting data, and models trained on such data will fail to work on input-output specifications that fall outside of the distribution. Through more careful generation of the training data, we were able to obtain significant performance improvement on synthesizing programs in Karel language. For our work on If-Then program synthesis, we also considered the few-shot learning scenario, where the test samples may call APIs that do not appear frequently in the training set, and we proposed a 2-step training method to improve the model performance in this more challenging setting. We have published related papers at NeurIPS 2016 and ICLR 2019.</span></p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/06/2020<br>\n\t\t\t\t\tModified by: Dawn&nbsp;Song</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nWe have made significant progress in advancing the field of neural program synthesis, where we leverage deep neural networks to generate programs. We have proposed novel techniques to synthesize programs from user intents specified in diverse formats, including natural language and input-output examples. We have also developed techniques for software engineering applications.\n\n\nFirst, we proposed new model architectures and training algorithms to learn programs that generalize to test cases far from the training distribution. The key insight is to incorporate recursion into the approach design, which provides effective regularization of the program space. By utilizing recursive traces to train the Neural Programmer-Interpreter (NPI) architecture, the model achieves 100% accuracy on learning sorting and several other simple algorithms. We also proposed new model architectures to support hierarhical program generation, and improved the model expressiveness to handle more complex tasks. In particular, we proposed novel techniques to learn a context-free parser from pairs of input programs and their parse trees. Our neural parsing programs can be learned to achieve 100% test accuracy on test inputs that are 500X longer than the training samples. We published related papers on these topics at ICLR 2017 and ICLR 2018, and the paper on recursive NPI won the Best Paper Award at ICLR 2017.\n\n\nSecond, we have developed a series of techniques to leverage execution results for program synthesis. Specifically, we incorporate an execution machine into our neural program synthesis framework, so that we can utilize the execution traces to provide richer information for model prediction. We conduct the evaluation on a benchmark program synthesis task based on the Karel programming language (an educational programming language, where imperative programs control an agent in a grid-world), and our approaches achieve significant performance gain compared to previous work. We have published related papers at NeurIPS 2018 and ICLR 2019.\n\n\nThird, we have developed novel deep neural network architectures for software engineering applications. We have developed a novel method for using tree-structured neural networks to interpret and synthesize programs, with the application of translating code from one programming language to another. We created a dataset of JavaScript and CoffeeScript program pairs and evaluated our system on it; our tree-to-tree method achieves significantly higher performance than baselines based on sequence-based neural networks. We also evaluated the performance of our methodology on a Java to C# translation task proposed by prior work, where our system was 1.5-3x more accurate than the prior work. In addition, we developed a framework to incorporate program repair as a sub-routine for program synthesis, where the repair module aims at fixing the errors in the programs predicted by the neural program synthesizer. We designed neural networks for specification-guided program repair, and demonstrated that the repair module provides further improvement for program synthesis. We have published the paper on program translation at NeurIPS 2018.\n\n\nFourth, we have developed techniques for synthesizing programs from natural language, including logical forms in domain-specific languages, SQL queries, and Python. For example, we have developed a system for synthesizing a restricted class of programs called \"If-Then\" programs. Our system determines which services an If-Then program interacts with given a natural language description of the program. We proposed the Latent Attention neural network architecture, which takes into account the embedded meanings of words and their positions in the description, in addition to the relative importance of each word in the vocabulary. We demonstrated superior performance on the public data available on IFTTT and Zapier. We have also developed a neural-symbolic approach for reading comprehension. Specifically, given a question and a passage as the context, instead of directly generating the final answer, our model generates a program for multi-step reasoning, which can be executed to produce the answer. We demonstrated superior performance on reading comprehension benchmarks that require advanced numerical reasoning. We have published related papers at NeurIPS 2016 and ICLR 2020.\n\n\nFinally, we have studied the effect of training data bias for neural program synthesis. For input-output program synthesis, a common practice for training dataset construction is to randomly generate programs and their corresponding input-output pairs. However, we showed that such type of processes may lead to subtle biases in the distribution of the resulting data, and models trained on such data will fail to work on input-output specifications that fall outside of the distribution. Through more careful generation of the training data, we were able to obtain significant performance improvement on synthesizing programs in Karel language. For our work on If-Then program synthesis, we also considered the few-shot learning scenario, where the test samples may call APIs that do not appear frequently in the training set, and we proposed a 2-step training method to improve the model performance in this more challenging setting. We have published related papers at NeurIPS 2016 and ICLR 2019.\n\n\n\t\t\t\t\tLast Modified: 08/06/2020\n\n\t\t\t\t\tSubmitted by: Dawn Song"
 }
}