{
 "awd_id": "1443054",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF21 DIBBs: Middleware and High Performance Analytics Libraries for Scalable Data Science",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924538",
 "po_email": "awalton@nsf.gov",
 "po_sign_block_name": "Amy Walton",
 "awd_eff_date": "2014-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 5000000.0,
 "awd_amount": 5283170.0,
 "awd_min_amd_letter_date": "2014-09-05",
 "awd_max_amd_letter_date": "2020-06-02",
 "awd_abstract_narration": "Many scientific problems depend on the ability to analyze and compute on large amounts of data.  This analysis often does not scale well; its effectiveness is hampered by the increasing volume, variety and rate of change (velocity) of big data.  This project will design, develop and implement building blocks that enable a fundamental improvement in the ability to support data intensive analysis on a broad range of cyberinfrastructure, including that supported by NSF for the scientific community. The project will integrate features of traditional high-performance computing, such as scientific libraries, communication and resource management middleware, with the rich set of capabilities found in the commercial Big Data ecosystem. The latter includes many important software systems such as Hadoop, available from the Apache open source community.  A collaboration between university teams at Arizona, Emory, Indiana (lead), Kansas, Rutgers, Virginia Tech, and Utah provides the broad expertise needed to design and successfully execute the project.  The project will engage scientists and educators with annual workshops and activities at discipline-specific meetings, both to gather requirements for and feedback on its software.  It will include under-represented communities with summer experiences, and will develop curriculum modules that include demonstrations built as 'Data Analytics as a Service.'\r\n\r\nThe project will design and implement a software Middleware for Data-Intensive Analytics and Science (MIDAS) that will enable scalable applications with the performance of HPC (High Performance Computing) and the rich functionality of the commodity Apache Big Data Stack.  Further, this project will design and implement a set of cross-cutting high-performance data-analysis libraries; SPIDAL (Scalable Parallel Interoperable Data Analytics Library) will support new programming and execution models for data-intensive analysis in a wide range of science and engineering applications.   The project addresses major data challenges in seven different communities: Biomolecular Simulations, Network and Computational Social Science, Epidemiology, Computer Vision, Spatial Geographical Information Systems, Remote Sensing for Polar Science, and Pathology Informatics.  The project libraries will have the same beneficial impact on data analytics that scientific libraries such as PETSc, MPI and ScaLAPACK have had for supercomputer simulations.  These libraries will be implemented to be scalable and interoperable across a range of computing systems including clouds, clusters and supercomputers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Geoffrey",
   "pi_last_name": "Fox",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Geoffrey C Fox",
   "pi_email_addr": "vxj6mb@virginia.edu",
   "nsf_id": "000231257",
   "pi_start_date": "2014-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Madhav",
   "pi_last_name": "Marathe",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Madhav V Marathe",
   "pi_email_addr": "mvm7hz@virginia.edu",
   "nsf_id": "000148286",
   "pi_start_date": "2014-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shantenu",
   "pi_last_name": "Jha",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shantenu Jha",
   "pi_email_addr": "shantenu.jha@rutgers.edu",
   "nsf_id": "000163942",
   "pi_start_date": "2014-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Judy",
   "pi_last_name": "Fox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Judy Fox",
   "pi_email_addr": "ckw9mp@virginia.edu",
   "nsf_id": "000555330",
   "pi_start_date": "2014-09-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Fusheng",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Fusheng Wang",
   "pi_email_addr": "fusheng.wang@stonybrook.edu",
   "nsf_id": "000622066",
   "pi_start_date": "2014-09-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "901 E. 10th Street",
  "perf_city_name": "Bloomington",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474083912",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736100",
   "pgm_ele_name": "EDUCATION AND WORKFORCE"
  },
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  },
  {
   "pgm_ele_code": "174400",
   "pgm_ele_name": "Tribal College & Univers Prog"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8048",
   "pgm_ref_txt": "Data Infrstr Bldg Blocks-DIBBs"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0420",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04002021DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 5000000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 51969.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 49894.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 52395.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 53785.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 55127.0
  },
  {
   "fund_oblg_fiscal_yr": 2020,
   "fund_oblg_amt": 20000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-798ba6c8-7fff-f031-2adb-2f444fc70188\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>The SPIDAL (Scalable Parallel Interoperable Data Analytics Library) project&nbsp; begun Fall 2014 and completed Fall 2020 with outreach activities continuing in 2021. The perspectives summary [1] summarizes the 2020 status with previous work through September 2018&nbsp; summarized in a book chapter </span><a href=\"https://paperpile.com/c/uvoIl7/Q3s1\"><span>[3]</span></a><span> with extensive references. This builds on our 21-month report </span><a href=\"https://paperpile.com/c/uvoIl7/ZgJTM\"><span>[4]</span></a><span>. Our project-wide workshop paper [2] was an early identification of the importance of AI surrogates for simulations. Institutions and key people involved were Arizona State (Beckstein), Indiana (Fox, Qiu, von Laszewski), Kansas (Paden), Rutgers (Jha), Stony Brook (Wang), Virginia (Marathe, Vullikanti), and Utah (Cheatham).</span></p>\n<h2 dir=\"ltr\"><strong>&#8203;Architecture</strong></h2>\n<p dir=\"ltr\"><span>The project was built around community-driven High Performance Big Data biophysical applications using HPC, distributed systems, network science, GIS, and machine/deep learning. It involved cyberinfrastructure, algorithms, and applications with seven participating organizations. The project has an overall architecture built around the twin concepts of HPC-ABDS (High-Performance Computing Enhanced Apache Big Data Stack) software and classification of Big data applications, the Ogres, that defined the key qualities exhibited by applications and required to be supported in software. These ideas led to a sophisticated discussion of Big Data ? Big Simulation and HPC-Cloud convergence. The original big data Ogres work was a collaboration between Indiana University and the NIST Public Big Data Working Group that collected 54 use cases ? each with 26 properties. The Ogres are a set of 50 features that categorized applications and allowed one to identify common classes such as Global GML and Local LML Machine Learning. GML is highly suitable for HPC systems while the very common LML and MapReduce categories also perform well on more commodity systems. Again, the ?Streaming? feature appeared in 80% of the NIST applications.</span></p>\n<h2 dir=\"ltr\"><strong>Cyberinfrastructure</strong></h2>\n<p dir=\"ltr\"><span>Our approach to data-intensive applications relies on Apache Big Data stack ABDS for the core software building blocks adding an interface layer MIDAS ? the Middleware for Data-Intensive Analytics and Science, that enables scalable applications with the performance of HPC (High-Performance Computing) and the rich functionality of the commodity ABDS (Apache Big Data Stack). Here we developed major HPC enhancements to the ABDS software including Harp based on Hadoop and Cylon/Twister2 based on Heron, Spark, and Flink for both batch and streaming scenarios. Pilot jobs from Rutgers were very successful in resource management and scheduling for high throughput parallel computing on NSF and DoE systems. We contributed with new techniques to get high performance across C++, Java and Python coded systems. MIDAS allows our libraries to be scalable and interoperable across a range of computing systems including clouds, clusters, and supercomputers. We also recognized [2] and contributed to two important broad categories HPCforML (CIforAI) or MLforHPC (AIforCI).</span></p>\n<h2 dir=\"ltr\"><strong>Community Applications and Algorithms</strong></h2>\n<p dir=\"ltr\"><span>Another major project product was a cross-cutting high-performance data-analysis library ? SPIDAL (Scalable Parallel Interoperable Data Analytics Library). The library has 4 components: a) a core library covering well-established functionality such as optimization and clustering; b) parallel graph and network algorithms; c) analysis of biomolecular simulations (high-performance versions of existing libraries from Utah and Arizona State) and d) image processing in both Polar Science and Pathology.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The project has also led to significant algorithmic advances in machine learning methods for networks, including motif detection, anomaly detection, explainability of clustering, deep learning for epidemic forecasting (TDEFSI in MLforHPC category), and the foundations of dynamical systems on networks.&nbsp; We supported the mitigation of the Coronavirus outbreak with the simulation of different spreading scenarios and possible interventions. For Polar Science, we developed operational ML/DL to locate ice sheet boundaries and snow layers from radar data. In Public Health GIS, we researched and implemented spatial big data query for opioid epidemic prevention and intervention while for pathology, we developed DL based image analysis tools for image segmentation, 3D registration, reconstruction, and spatial analysis. For the major Biomolecular Simulation community, SPIDAL developed PMDA which parallelizes the widely used MDAnalysis Python package for MD (Molecular Dynamics) trajectory analysis. In this area, recent MLforHPC research by us has shown surrogates that improve molecular dynamics simulation performance by very large factors for both short times (using recurrent neural nets) and long time scales (with fully connected networks). This broad impact was enhanced by the over 50 REU undergraduate students who were mentored by our project over its full duration.&nbsp;</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><strong>Project-wide References</strong></p>\n<p dir=\"ltr\"><span>[1]</span><span><span> </span></span><span>?Summary Perspectives of the SPIDAL Project NSF #1443054 from 2014-2020,? http://dx.doi.org/10.13140/RG.2.2.16245.65764</span></p>\n<p dir=\"ltr\"><span>[2]</span><span><span> </span></span><span>?Learning Everywhere: Pervasive Machine Learning for Effective High-Performance Computation,? in HPDC Workshop at IPDPS 2019, Rio de Janeiro, 2019 https://arxiv.org/abs/1902.10810</span></p>\n<p dir=\"ltr\"><span>[3]</span><span><span> </span></span><span>?Contributions to High-Performance Big Data Computing,? in Future Trends of HPC in a Disruptive Scenario, Grandinetti, L., Joubert, G.R., Michielsen, K., Mirtaheri, S.L., Taufer, M., Yokota, R., Ed. IOS, 2019 http://dx.doi.org/10.13140/RG.2.2.25192.11528</span></p>\n<p dir=\"ltr\"><span>[4]</span><span><span> </span></span><span>?Datanet: CIF21 DIBBs: Middleware and High Performance Analytics Libraries for Scalable Data Science NSF14-43054 Progress Report. A 21 month Project Report,? Sep. 2016 http://dx.doi.org/10.13140/RG.2.2.23559.47524</span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/02/2022<br>\n\t\t\t\t\tModified by: Geoffrey&nbsp;C&nbsp;Fox</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643738061841_Logo--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643738061841_Logo--rgov-800width.jpg\" title=\"NSF 1443054 SPIDAL Project Logo\"><img src=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643738061841_Logo--rgov-66x44.jpg\" alt=\"NSF 1443054 SPIDAL Project Logo\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Arizona State (Beckstein), Indiana (Fox, Qiu, von Laszewski), Kansas (Paden), Rutgers (Jha), Stony Brook (Wang), Virginia (Marathe, Vullikanti), and Utah (Cheatham).</div>\n<div class=\"imageCredit\">NSF 1443054 SPIDAL Project</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox</div>\n<div class=\"imageTitle\">NSF 1443054 SPIDAL Project Logo</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643823642015_Harp--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643823642015_Harp--rgov-800width.jpg\" title=\"Subgraph Algorithms in Harp Framework\"><img src=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643823642015_Harp--rgov-66x44.jpg\" alt=\"Subgraph Algorithms in Harp Framework\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">NSF 1443054 SPIDAL Project Harp Cloud-HPC interoperable software for High Performance Big Data Analytics at Scale applied to Network Science   Subgraph algorithms with excellent parallel performance.</div>\n<div class=\"imageCredit\">NSF 1443054 SPIDAL Project</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox</div>\n<div class=\"imageTitle\">Subgraph Algorithms in Harp Framework</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643823980485_Picture2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643823980485_Picture2--rgov-800width.jpg\" title=\"Biomolecular Simulation Tools\"><img src=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643823980485_Picture2--rgov-66x44.jpg\" alt=\"Biomolecular Simulation Tools\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">NSF 1443054 SPIDAL Project for Biomolecular Simulations. (Left) DeepDriveMD based on Radical Pilot MIDAS software and (Right) Parallel Molecular Dynamics PMDA with Python simulation result analysis tools.</div>\n<div class=\"imageCredit\">NSF 1443054 SPIDAL Project</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox</div>\n<div class=\"imageTitle\">Biomolecular Simulation Tools</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643824593840_Picture3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643824593840_Picture3--rgov-800width.jpg\" title=\"Deep Learning Tools Tools for Digital Pathology\"><img src=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643824593840_Picture3--rgov-66x44.jpg\" alt=\"Deep Learning Tools Tools for Digital Pathology\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">NSF 1443054 SPIDAL Project Tools for Digital Pathology 3D images based on deep learning with application to cancer diagnosis and treatment.</div>\n<div class=\"imageCredit\">NSF 1443054 SPIDAL Project</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox</div>\n<div class=\"imageTitle\">Deep Learning Tools Tools for Digital Pathology</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643824831853_Picture4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643824831853_Picture4--rgov-800width.jpg\" title=\"Polar Science ice-sheet bed and snow layer algorithms\"><img src=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643824831853_Picture4--rgov-66x44.jpg\" alt=\"Polar Science ice-sheet bed and snow layer algorithms\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">NSF 1443054 SPIDAL Project Polar Science algorithms for estimating ice sheet bed depths and identifying multiple snow layers from radar data from CReSIS project.</div>\n<div class=\"imageCredit\">NSF 1443054 SPIDAL Project</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox</div>\n<div class=\"imageTitle\">Polar Science ice-sheet bed and snow layer algorithms</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643825306004_Picture5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643825306004_Picture5--rgov-800width.jpg\" title=\"64 Convergence Diamonds\"><img src=\"/por/images/Reports/POR/2022/1443054/1443054_10340923_1643825306004_Picture5--rgov-66x44.jpg\" alt=\"64 Convergence Diamonds\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">NSF 1443054 SPIDAL Project Convergence diamonds as facets in four views defining Big Data Ogres. 64 Features in 4 views for Unified Classification of Big Data and Simulation Applications</div>\n<div class=\"imageCredit\">NSF 1443054 SPIDAL Project</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Geoffrey&nbsp;C&nbsp;Fox</div>\n<div class=\"imageTitle\">64 Convergence Diamonds</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThe SPIDAL (Scalable Parallel Interoperable Data Analytics Library) project  begun Fall 2014 and completed Fall 2020 with outreach activities continuing in 2021. The perspectives summary [1] summarizes the 2020 status with previous work through September 2018  summarized in a book chapter [3] with extensive references. This builds on our 21-month report [4]. Our project-wide workshop paper [2] was an early identification of the importance of AI surrogates for simulations. Institutions and key people involved were Arizona State (Beckstein), Indiana (Fox, Qiu, von Laszewski), Kansas (Paden), Rutgers (Jha), Stony Brook (Wang), Virginia (Marathe, Vullikanti), and Utah (Cheatham).\n&#8203;Architecture\nThe project was built around community-driven High Performance Big Data biophysical applications using HPC, distributed systems, network science, GIS, and machine/deep learning. It involved cyberinfrastructure, algorithms, and applications with seven participating organizations. The project has an overall architecture built around the twin concepts of HPC-ABDS (High-Performance Computing Enhanced Apache Big Data Stack) software and classification of Big data applications, the Ogres, that defined the key qualities exhibited by applications and required to be supported in software. These ideas led to a sophisticated discussion of Big Data ? Big Simulation and HPC-Cloud convergence. The original big data Ogres work was a collaboration between Indiana University and the NIST Public Big Data Working Group that collected 54 use cases ? each with 26 properties. The Ogres are a set of 50 features that categorized applications and allowed one to identify common classes such as Global GML and Local LML Machine Learning. GML is highly suitable for HPC systems while the very common LML and MapReduce categories also perform well on more commodity systems. Again, the ?Streaming? feature appeared in 80% of the NIST applications.\nCyberinfrastructure\nOur approach to data-intensive applications relies on Apache Big Data stack ABDS for the core software building blocks adding an interface layer MIDAS ? the Middleware for Data-Intensive Analytics and Science, that enables scalable applications with the performance of HPC (High-Performance Computing) and the rich functionality of the commodity ABDS (Apache Big Data Stack). Here we developed major HPC enhancements to the ABDS software including Harp based on Hadoop and Cylon/Twister2 based on Heron, Spark, and Flink for both batch and streaming scenarios. Pilot jobs from Rutgers were very successful in resource management and scheduling for high throughput parallel computing on NSF and DoE systems. We contributed with new techniques to get high performance across C++, Java and Python coded systems. MIDAS allows our libraries to be scalable and interoperable across a range of computing systems including clouds, clusters, and supercomputers. We also recognized [2] and contributed to two important broad categories HPCforML (CIforAI) or MLforHPC (AIforCI).\nCommunity Applications and Algorithms\nAnother major project product was a cross-cutting high-performance data-analysis library ? SPIDAL (Scalable Parallel Interoperable Data Analytics Library). The library has 4 components: a) a core library covering well-established functionality such as optimization and clustering; b) parallel graph and network algorithms; c) analysis of biomolecular simulations (high-performance versions of existing libraries from Utah and Arizona State) and d) image processing in both Polar Science and Pathology. \nThe project has also led to significant algorithmic advances in machine learning methods for networks, including motif detection, anomaly detection, explainability of clustering, deep learning for epidemic forecasting (TDEFSI in MLforHPC category), and the foundations of dynamical systems on networks.  We supported the mitigation of the Coronavirus outbreak with the simulation of different spreading scenarios and possible interventions. For Polar Science, we developed operational ML/DL to locate ice sheet boundaries and snow layers from radar data. In Public Health GIS, we researched and implemented spatial big data query for opioid epidemic prevention and intervention while for pathology, we developed DL based image analysis tools for image segmentation, 3D registration, reconstruction, and spatial analysis. For the major Biomolecular Simulation community, SPIDAL developed PMDA which parallelizes the widely used MDAnalysis Python package for MD (Molecular Dynamics) trajectory analysis. In this area, recent MLforHPC research by us has shown surrogates that improve molecular dynamics simulation performance by very large factors for both short times (using recurrent neural nets) and long time scales (with fully connected networks). This broad impact was enhanced by the over 50 REU undergraduate students who were mentored by our project over its full duration. \n\n \nProject-wide References\n[1] ?Summary Perspectives of the SPIDAL Project NSF #1443054 from 2014-2020,? http://dx.doi.org/10.13140/RG.2.2.16245.65764\n[2] ?Learning Everywhere: Pervasive Machine Learning for Effective High-Performance Computation,? in HPDC Workshop at IPDPS 2019, Rio de Janeiro, 2019 https://arxiv.org/abs/1902.10810\n[3] ?Contributions to High-Performance Big Data Computing,? in Future Trends of HPC in a Disruptive Scenario, Grandinetti, L., Joubert, G.R., Michielsen, K., Mirtaheri, S.L., Taufer, M., Yokota, R., Ed. IOS, 2019 http://dx.doi.org/10.13140/RG.2.2.25192.11528\n[4] ?Datanet: CIF21 DIBBs: Middleware and High Performance Analytics Libraries for Scalable Data Science NSF14-43054 Progress Report. A 21 month Project Report,? Sep. 2016 http://dx.doi.org/10.13140/RG.2.2.23559.47524\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 02/02/2022\n\n\t\t\t\t\tSubmitted by: Geoffrey C Fox"
 }
}