{
 "awd_id": "1414172",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Large: Collaborative Research: Exploiting the Naturalness of Software",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2014-07-01",
 "awd_exp_date": "2022-06-30",
 "tot_intn_awd_amt": 1000108.0,
 "awd_amount": 1264108.0,
 "awd_min_amd_letter_date": "2014-06-06",
 "awd_max_amd_letter_date": "2021-05-18",
 "awd_abstract_narration": "This inter-disciplinary project has its roots in Natural Language (NL) processing. Languages such as English allow intricate, lovely and complex constructions; yet, everyday, ``natural? speech and writing is simple, prosaic, and repetitive, and thus amenable to statistical modeling. Once large NL corpora became available, computational muscle and algorithmic insight led to rapid advances in the statistical modeling of natural utterances, and revolutionized tasks such as translation, speech recognition, text summarization, etc.  While programming languages, like NL, are flexible and powerful, in theory allowing a great variety of complex programs to be written, we find that ``natural? programs that people actually write are regular, repetitive and predictable. This project will use statistical models to capture and exploit this regularity to create a new generation of software engineering tools to achieve transformative improvements in software quality and productivity. \r\n \r\nThe project will exploit language modeling techniques to capture the regularity in natural programs at the lexical, syntactic, and semantic levels. Statistical modeling will also be used to capture alignment regularities in ``bilingual? corpora such as code with comments, or explanatory text (e.g., Stackoverflow) and in systems developed on two platforms such as Java and C#.  These statistical models will help drive novel, data-driven approaches for applications such as code suggestion and completion, and assistive devices for programmers with movement or visual challenges. These models will also be exploited to correct simple errors in programs. Models of bilingual data will used to build code summarization and code retrieval tools, as well as tools for porting across platforms. Finally, this project will create a large, curated corpus of  software, and code analysis products, as well as a corpus of alignments within software bilingual corpora, to help create and nurture a research community in this area.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Zhendong",
   "pi_last_name": "Su",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhendong Su",
   "pi_email_addr": "su@cs.ucdavis.edu",
   "nsf_id": "000336300",
   "pi_start_date": "2014-06-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Premkumar",
   "pi_last_name": "Devanbu",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Premkumar T Devanbu",
   "pi_email_addr": "devanbu@cs.ucdavis.edu",
   "nsf_id": "000195677",
   "pi_start_date": "2014-06-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vladimir",
   "pi_last_name": "Filkov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vladimir Filkov",
   "pi_email_addr": "filkov@cs.ucdavis.edu",
   "nsf_id": "000217276",
   "pi_start_date": "2014-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Davis",
  "inst_street_address": "1850 RESEARCH PARK DR STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "DAVIS",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5307547700",
  "inst_zip_code": "956186153",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "CA04",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, DAVIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "TX2DAGQPENZ5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Davis",
  "perf_str_addr": "1 Shields Way",
  "perf_city_name": "Davis",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "956165270",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "CA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "8624",
   "pgm_ref_txt": "Societal Implications (ELSI)"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0121",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01002122DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 477251.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 738857.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 24000.0
  },
  {
   "fund_oblg_fiscal_yr": 2021,
   "fund_oblg_amt": 24000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project introduced the concept of \"naturalness\", <em>viz., </em>that software was not only highly repetitive, it was also highly <em>predictable, </em>and could be modeled using statistical models. This means that computers can <em>learn </em>how programmers write code by essentially <em>reading</em> large amounts of code. They can capture this knowledge internally, using algorithms called \"language models\" and then use this knowledge to help programmers with programming-related tasks. <br /><br />This allows various kinds of software tools to be now based on statistical algorithms, many inspired by prior work in natural language processing. The main contributions of the project include:</p>\n<ol>\n<li>This project introduced and developed the idea of using language models to help software engineering. This has now become a major undertaking, with products (such as <a title=\"Codex\" href=\"https://openai.com/blog/openai-codex/\" target=\"_blank\">Codex</a> from Microsoft/OpenAI, and <a title=\"CodeGuru\" href=\"https://aws.amazon.com/codeguru/\" target=\"_blank\">CodeGuru</a> from Amazon) from several major vendors,&nbsp; numerous startups, and of course thousands of follow-on papers. We believe that our decision <strong><span style=\"text-decoration: underline;\"><em>not</em></span></strong> to patent this idea has helped this area flourish. <br /><br /></li>\n<li>We introduced a plugin for Eclipse in 2015 that helped developers complete code.&nbsp; <br /><br /></li>\n<li>Code is very hierarchical, structured into functions, modules, libraries, systems. Vocabulary, coding patterns, etc, all are reflected in this hiearchical structure, and reflect varying levels of specialized knowledge. We developed specialized language models that can leverage this structure. <br /><br /></li>\n<li>Because code is <strong><span style=\"text-decoration: underline;\">so</span></strong> repetitive and predictable, unusual code constructions are wrong. We showed that language models can actually predict when code is wrong. <br /><br /></li>\n<li>Programmers encode and retrieve a lot of information about the design and function of code by choosing helpful names for bits of the code that represent data and function. By the same token, sometimes these names are \"obfuscated\" protect or hide meaning. We developed ways to \"de-obfuscate\" and retrieve helpful names using language models. </li>\n</ol>\n<p>Several PhD students and postdocs were supported under this project; several have gone on to Academic positions. The PI has received a SIGSOFT Outstanding Research Award, and a Alexander von Humboldt Research Award for the work done under this project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/11/2022<br>\n\t\t\t\t\tModified by: Premkumar&nbsp;T&nbsp;Devanbu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project introduced the concept of \"naturalness\", viz., that software was not only highly repetitive, it was also highly predictable, and could be modeled using statistical models. This means that computers can learn how programmers write code by essentially reading large amounts of code. They can capture this knowledge internally, using algorithms called \"language models\" and then use this knowledge to help programmers with programming-related tasks. \n\nThis allows various kinds of software tools to be now based on statistical algorithms, many inspired by prior work in natural language processing. The main contributions of the project include:\n\nThis project introduced and developed the idea of using language models to help software engineering. This has now become a major undertaking, with products (such as Codex from Microsoft/OpenAI, and CodeGuru from Amazon) from several major vendors,  numerous startups, and of course thousands of follow-on papers. We believe that our decision not to patent this idea has helped this area flourish. \n\n\nWe introduced a plugin for Eclipse in 2015 that helped developers complete code.  \n\n\nCode is very hierarchical, structured into functions, modules, libraries, systems. Vocabulary, coding patterns, etc, all are reflected in this hiearchical structure, and reflect varying levels of specialized knowledge. We developed specialized language models that can leverage this structure. \n\n\nBecause code is so repetitive and predictable, unusual code constructions are wrong. We showed that language models can actually predict when code is wrong. \n\n\nProgrammers encode and retrieve a lot of information about the design and function of code by choosing helpful names for bits of the code that represent data and function. By the same token, sometimes these names are \"obfuscated\" protect or hide meaning. We developed ways to \"de-obfuscate\" and retrieve helpful names using language models. \n\n\nSeveral PhD students and postdocs were supported under this project; several have gone on to Academic positions. The PI has received a SIGSOFT Outstanding Research Award, and a Alexander von Humboldt Research Award for the work done under this project.\n\n\t\t\t\t\tLast Modified: 11/11/2022\n\n\t\t\t\t\tSubmitted by: Premkumar T Devanbu"
 }
}