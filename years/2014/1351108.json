{
 "awd_id": "1351108",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Algorithms for understanding data",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2014-07-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2014-01-22",
 "awd_max_amd_letter_date": "2014-01-22",
 "awd_abstract_narration": "Given samples from some unknown distribution, what can one infer about the underlying distribution, and how efficiently can these inferences be made? \u00a0In many of the most fundamental settings, our understanding of the computational and information theoretic possibilities and barriers is still startlingly poor. \u00a0This project tackles two broad research objectives: developing efficient algorithms for probing data, and understanding how to efficiently estimate properties of distributions. \u00a0The first line of research seeks to understand which questions about a dataset can be answered extremely efficiently, requiring computational resources (time, or memory) that are sublinear in the size of the dataset or distribution. \u00a0The second research objective is to understand the minimal amount of information necessary to ascertain, with high probability, whether or not a distribution or dataset possesses a given property. \u00a0In the context of statistical property estimation, this problem asks how few samples are needed to estimate the property in question to a desired accuracy, with high probability. \u00a0This research pursues both new estimation algorithms, and new information theoretic tools and lower bounds.\r\n\r\nWith vast and important datasets emerging across many disciplines, from genetic, biological, and medical databases, to databases documenting our economic and social behaviors, the challenge of how to make sense of them has particular immediate relevance and has rapidly become the bottleneck in scientific understanding. \u00a0 The specific problems investigated in this project arise in the analysis of these datasets; algorithmic advances on these problems have the potential to very quickly be adopted and transform ongoing data analysis efforts. \u00a0 Beyond the immediate implications for the data sciences, these questions are extremely basic and foundational. As such, new techniques, perspectives, and insights gleaned from their study are likely to have broad implications for other problems throughout computer science, statistics, information theory, and the data sciences.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Valiant",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory J Valiant",
   "pi_email_addr": "gvaliant@cs.stanford.edu",
   "nsf_id": "000603941",
   "pi_start_date": "2014-01-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": null,
  "perf_city_name": "Palo Alto",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943041212",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research advances funded by this project fall into three related veins: 1) the development of new, theoretically principled and practically relevant machine learning and statistical analysis tools that extract as much information from available data as is possible--particularly focussing on settings where the data is high dimensional or has large support; 2) the development of algorithms for learning and estimation in the presence of a significant fraction of bad data--for example where some of the data might be extremely noisy, corrupted, or even maliciously generated by an adversary to mislead the learning system; and 3) developing our understanding of how limitations on the communication or memory available to a learning algorithm inherently affects how quickly or how much data is required to achieve a given level of accuracy.&nbsp;</p>\n<p>The results of this research have been shared publicly, via available software, and publications in top theoretical computer science, machine learning, and statistics venues, including Nature Methods, Annals of Statistics, Journal of the ACM, Foundations of Computer Science (FOCS), Symposium on Theory of Computing (STOC), International Conference on Machine Learning (ICML), Conference on Neural Information Processing Systems (NeurIPS), and Conference on Learning Theory.&nbsp; Some of the techniques developed during this project have also yielded surprising insights into meaningful applied settings, for example, quantifying the likely value of obtaining genetic sequences for larger cohorts by providing accurate predictions for the number of novel genetic mutations that will likely be observed.</p>\n<p>This project directly supported two PhD students, a Masters' student, and several undergraduate researchers, many of whom are continuing in research-related roles, either in academia or industry.&nbsp; The funding also supported curriculum development for three new courses at Stanford, including a Modern Algorithmic Toolbox course that covers a variety of practically important and theoretically deep algorithms that have been developed over the past two decades.&nbsp; Such material is generally not included in the traditional undergraduate computer science curriculum, despite being extremely helpful for students who plan to work closely with data, either in industry or academia.&nbsp; This advanced elective course is very successful, with enrollments growing to over 150 students this year.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/19/2019<br>\n\t\t\t\t\tModified by: Gregory&nbsp;J&nbsp;Valiant</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe research advances funded by this project fall into three related veins: 1) the development of new, theoretically principled and practically relevant machine learning and statistical analysis tools that extract as much information from available data as is possible--particularly focussing on settings where the data is high dimensional or has large support; 2) the development of algorithms for learning and estimation in the presence of a significant fraction of bad data--for example where some of the data might be extremely noisy, corrupted, or even maliciously generated by an adversary to mislead the learning system; and 3) developing our understanding of how limitations on the communication or memory available to a learning algorithm inherently affects how quickly or how much data is required to achieve a given level of accuracy. \n\nThe results of this research have been shared publicly, via available software, and publications in top theoretical computer science, machine learning, and statistics venues, including Nature Methods, Annals of Statistics, Journal of the ACM, Foundations of Computer Science (FOCS), Symposium on Theory of Computing (STOC), International Conference on Machine Learning (ICML), Conference on Neural Information Processing Systems (NeurIPS), and Conference on Learning Theory.  Some of the techniques developed during this project have also yielded surprising insights into meaningful applied settings, for example, quantifying the likely value of obtaining genetic sequences for larger cohorts by providing accurate predictions for the number of novel genetic mutations that will likely be observed.\n\nThis project directly supported two PhD students, a Masters' student, and several undergraduate researchers, many of whom are continuing in research-related roles, either in academia or industry.  The funding also supported curriculum development for three new courses at Stanford, including a Modern Algorithmic Toolbox course that covers a variety of practically important and theoretically deep algorithms that have been developed over the past two decades.  Such material is generally not included in the traditional undergraduate computer science curriculum, despite being extremely helpful for students who plan to work closely with data, either in industry or academia.  This advanced elective course is very successful, with enrollments growing to over 150 students this year.\n\n\t\t\t\t\tLast Modified: 08/19/2019\n\n\t\t\t\t\tSubmitted by: Gregory J Valiant"
 }
}