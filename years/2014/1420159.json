{
 "awd_id": "1420159",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Small: Direct physical grasping, manipulation, and tooling of simulated objects",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 496858.0,
 "awd_amount": 496858.0,
 "awd_min_amd_letter_date": "2014-07-31",
 "awd_max_amd_letter_date": "2017-02-24",
 "awd_abstract_narration": "This project will develop and evaluate an approach to physical grasping, manipulating, and applying tools to simulated objects, and a means of exploring of three-dimensional information. The project is founded on the concept of tool use, in which handheld tool objects are used to modify the properties or appearance of target objects. The project integrates a wide range of findings in human-computer interaction and visualization, from bimanual and tangible user interfaces to augmented reality. At the core of this research endeavor is a desktop augmented reality system with a stereoscopic or monoscopic display, a haptic pointing device, and a camera focused on the user's hands. In one hand the user holds a physical wireframe cube that contains virtual objects, and in the other hand the pointing device, its tip visually augmented to show its function, which will relate to one of several possible tools, including: (a) a probe for pointing at, selecting, and moving objects, (b) a magnifying or semantic lens for filtering, recoding, and elaborating information, and (c) a cutting plane that shows slices or projection views. On the display, users watch the immediate, direct effects of their actions with the tools on the simulated object. The system will support visualization with fluid and natural interaction techniques, improving the ability of users to explore and understand 3D objects to some extent as if they were holding the objects in their hands. The project will provide societal benefits by generating theoretical and practical advances across multiple disciplines, such as a deeper understanding of the psychological theory and computer algorithms that are needed to give a person a seamless impression that he or she is manipulating an object that appears to be physically in front of the person, but which is only there virtually. It should be relatively straightforward to transition the practical outcome of this work to low-cost hardware components such as head-mounted 3D displays, such that the project could ultimately provide everyday users with a means of directly creating and modifying objects that they have in mind for 3D printing. University students will help to develop and evaluate the system, thus exposing these students to a new and potentially transformative approach to augmenting a simulated space with physical tools, and thus providing a larger population of students with opportunities for exciting hands-on computer science learning. Orthopedic surgeons have been recruited to assist in exploring the use of the system for preoperative surgical planning such as by permitting the exploration of complex 3D bone structures.\r\n\r\nDevelopment and evaluation of the novel interaction techniques in this hands-on augmented reality system will lead to a better understanding of how performance may be improved in data exploration with augmented tool use.  The research will investigate: the spatial collocation of user actions and system effects; physical constraints between the hands, objects, and the environment; and a greater role for proprioception. Experimental results will give insight into questions that cross the boundaries between relatively disparate areas of research in HCI and visualization, specifically the potential benefits of proprioception in bimanual tasks, the extent to which mechanical constraints and stabilization can improve performance in precise interaction tasks, and how these factors may compensate for the visual errors associated with presenting 3D information on a stereo or monoscopic display. The research will extend beyond the laboratory to a real and important medical domain, which will help validate our work in practice.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "St. Amant",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Robert A St. Amant",
   "pi_email_addr": "stamant@csc.ncsu.edu",
   "nsf_id": "000451837",
   "pi_start_date": "2014-07-31",
   "pi_end_date": "2017-02-24"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Healey",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher G Healey",
   "pi_email_addr": "healey@ncsu.edu",
   "nsf_id": "000486229",
   "pi_start_date": "2017-02-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Healey",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher G Healey",
   "pi_email_addr": "healey@ncsu.edu",
   "nsf_id": "000486229",
   "pi_start_date": "2014-07-31",
   "pi_end_date": "2017-02-24"
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276957214",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 496858.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>When we work with objects in a 3D environment on a computer, it is often difficult to position and rotate the objects to exactly the pose we want. But in the real world, it is easy to take a physical object and align it in a precise way. This research buildings a system that allows users to position a physical object, and have that position applied to a virtual object on a computer. No expensive hardware is needed. A simple tinker-toy-like wireframe cube with different colors at its corners, and an on-the-shelf web camera, together with our software, is all that is needed to setup our system. A camera captures a working environment that includes the cube, then embeds the virtual object in the cube's center, and displays the result (the physical environment and cube together with the virtual object) on the compute screen. As the user moves and twists the cube, the virtual object follows these manipulations in lock-step. Experiments show that users find it much easier and more efficient to use a \"tangible\" physical object for position on-screen objects, versus direct on-screen manipulation with a mouse or other type of input device. Our system holds potential for a variety of domains, including manipulating medical images, computer-aided designs of physical objects, and entertainment system (e.g., video games). We are providing our software as unrestricted open-source material, so anyone who is interested in building their own system can do so, to see how it benefits their own work.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/30/2018<br>\n\t\t\t\t\tModified by: Christopher&nbsp;G&nbsp;Healey</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302388769_switch--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302388769_switch--rgov-800width.jpg\" title=\"Interaction With Objects\"><img src=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302388769_switch--rgov-66x44.jpg\" alt=\"Interaction With Objects\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Two different cubes are used to control two different objects, a wooden cube for a robot (left), and a 3D printed cube for a teapot (right), images show the computer screen with the background environment and the virtual object super-imposed within the scene</div>\n<div class=\"imageCredit\">Zeyuan Chen, Christopher G. Healey</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Christopher&nbsp;G&nbsp;Healey</div>\n<div class=\"imageTitle\">Interaction With Objects</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302503663_1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302503663_1--rgov-800width.jpg\" title=\"CAPTIVE 3D Tangible Cube\"><img src=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302503663_1--rgov-66x44.jpg\" alt=\"CAPTIVE 3D Tangible Cube\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A 3D wireframe cube, used to manipulate on-screen objects; when a user physically moves or rotates the cube, the on-screen object mimics exactly the same actions</div>\n<div class=\"imageCredit\">Zeyuan Chen, Christopher G. Healey</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Christopher&nbsp;G&nbsp;Healey</div>\n<div class=\"imageTitle\">CAPTIVE 3D Tangible Cube</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302624394_fighter--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302624394_fighter--rgov-800width.jpg\" title=\"Controlling Multiple Objects\"><img src=\"/por/images/Reports/POR/2018/1420159/1420159_10326272_1538302624394_fighter--rgov-66x44.jpg\" alt=\"Controlling Multiple Objects\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our system allows a user to switch between the objects they want to control; in this image, a user manipulates a single cube to position two different virtual spaceships</div>\n<div class=\"imageCredit\">Zeyuan Chen, Christopher G. Healey</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Christopher&nbsp;G&nbsp;Healey</div>\n<div class=\"imageTitle\">Controlling Multiple Objects</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWhen we work with objects in a 3D environment on a computer, it is often difficult to position and rotate the objects to exactly the pose we want. But in the real world, it is easy to take a physical object and align it in a precise way. This research buildings a system that allows users to position a physical object, and have that position applied to a virtual object on a computer. No expensive hardware is needed. A simple tinker-toy-like wireframe cube with different colors at its corners, and an on-the-shelf web camera, together with our software, is all that is needed to setup our system. A camera captures a working environment that includes the cube, then embeds the virtual object in the cube's center, and displays the result (the physical environment and cube together with the virtual object) on the compute screen. As the user moves and twists the cube, the virtual object follows these manipulations in lock-step. Experiments show that users find it much easier and more efficient to use a \"tangible\" physical object for position on-screen objects, versus direct on-screen manipulation with a mouse or other type of input device. Our system holds potential for a variety of domains, including manipulating medical images, computer-aided designs of physical objects, and entertainment system (e.g., video games). We are providing our software as unrestricted open-source material, so anyone who is interested in building their own system can do so, to see how it benefits their own work.\n\n\t\t\t\t\tLast Modified: 09/30/2018\n\n\t\t\t\t\tSubmitted by: Christopher G Healey"
 }
}