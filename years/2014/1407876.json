{
 "awd_id": "1407876",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Quality of Configuration in Large Scale Data Centers",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2014-05-01",
 "awd_exp_date": "2018-04-30",
 "tot_intn_awd_amt": 299353.0,
 "awd_amount": 299353.0,
 "awd_min_amd_letter_date": "2014-04-28",
 "awd_max_amd_letter_date": "2014-04-28",
 "awd_abstract_narration": "The management of the emerging data centers involves substantial complexity due to numerous resources that must be properly configured at all levels from individual devices to entire systems and services. The complexity of configuration management leads to numerous opportunities for misconfigurations and attacks. It has been estimated that misconfigurations are responsible for 62% of downtime and 65% of security exploits in current computer systems. These, already high percentages, are expected to continue increasing due to current trends of extensive virtualization, architectural heterogeneity, and increasing size. \r\n\r\nThis project attempts to devise quality metrics to characterize the vulnerability of the given configuration to bad parameter values (set accidentally or by a malicious entity). The quality metrics are also expected to provide guidance for evaluating alternatives to proposed or required configuration changes. The main challenge in characterizing configuration quality is that in practice specifications of correct configuration are not available and the viability of the configuration must be determined by analyzing the application behavior. The key issue to consider in devising the quality metrics relates to the configuration management structure and the direct/indirect dependencies that it implies. The project will examine methods ranging from discovering the configuration structure to flow analysis of the source code along with methods to correlate the discovered information. In many cases, the dependencies are not definitive and hence the quality metrics need to consider fuzzy values and their composition.  The project will evaluate the usefulness of the metrics in the context of current open-source software such as Apache web server.\r\n\r\nReducing misconfigurations and detecting them quickly is expected to have a substantial impact on computing systems with respect to their availability, functional correctness, usability, and resistance against hacker attacks.  The metrics explored in this project are expected to provide important insights into improving the configuration of systems at various stages including design, deployment, and dynamic reconfiguration.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Krishna",
   "pi_last_name": "Kant",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Krishna Kant",
   "pi_email_addr": "kkant@temple.edu",
   "nsf_id": "000656409",
   "pi_start_date": "2014-04-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Temple University",
  "inst_street_address": "1805 N BROAD ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2157077547",
  "inst_zip_code": "191226104",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "PA02",
  "org_lgl_bus_name": "TEMPLE UNIVERSITY-OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "QD4MGHFDJKU1",
  "org_uei_num": "QD4MGHFDJKU1"
 },
 "perf_inst": {
  "perf_inst_name": "Temple University",
  "perf_str_addr": "1938 Liacouras Walk",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191226027",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "PA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 299353.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The availability and dependability of data center services are critical to our digital economy and society, and the vendors often advertise them as providing five 9&rsquo;s (i.e., 99.999 percent) or better availability; which means less than 5.25 minutes per year downtime. Unfortunately, downtime episodes that violate these lofty goals are too common and result in substantial impacts in one or more areas, including unsatisfactory online customer experience, lost revenue, lost customer goodwill, high infrastructure, and operational costs, etc. Many of these are caused by inadequate or flawed operating procedures coupled with hardware and software misconfigurations and human mistakes. Often, ad-hoc procedures are used in the first place, or ad-hoc fixes are implemented to fix problems. However, the complexity and scale of modern data centers make such an approach largely ineffective.</p>\n<p>It has been estimated that misconfigurations are responsible for 62% of downtime and 65% of security exploits in current computer systems. These already high percentages are expected to continue increasing due to current trends of extensive virtualization, architectural heterogeneity, and increasing size. Misconfiguration errors are the most significant contributors to service-level failures, but other reasons include hardware failure, operating system or software failure, intrusion, virus outbreak, disasters, and relocation of services or the data center. &nbsp;These disruptions could be partial, i.e., impact some applications, or full, i.e., impact the entire data center. When such disruptions occur, they cause significant downtime which may lead to a substantial financial and legal impact.</p>\n<p>The goal of this project was to examine a core set of operational issues which arise in almost all data center environments and study mechanisms to improve the data center availability and resilience. One such problem is a fast restoration of data center services in the event of large-scale failures or downtimes. We have shown that to address this problem; it is crucial to understand the expertise of the workers and how best they can be deployed in an environment where there are complex dependencies across services and between services and machines. Another aspect we explored is the conflicts between the IP addresses when IT systems from different organizations are joined as a result of mergers and acquisitions. We showed that by addressing the conflict problem systematically, we could remove conflicts with only a small number of changes in IP addresses and in the process significantly reduce the size of the routing tables. We also examined the problem of failure diagnosis schemes in enterprise networks and data centers which are becoming very complex due to multiple locations, complex arrangements concerning the provision of access to the network and IT services in various locations, and dependencies between services which are often not known fully. We show that a careful selection of probing locations and probes launched from them can significantly reduce the effort in performing the diagnosis.</p>\n<p>Overall the project has provided us the opportunity to examine many complex problems in data center configuration management and the need for effective tools for them. While we have examined some of these problems in detail, there are others that remain to be analyzed. Similarly, we have ideas for developing tools for such diagnosis and will be looking for ways of making such tool development a reality.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/22/2018<br>\n\t\t\t\t\tModified by: Krishna&nbsp;Kant</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1407876/1407876_10300656_1532270386881_3layer--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1407876/1407876_10300656_1532270386881_3layer--rgov-800width.jpg\" title=\"Multilevel Service Dependencies\"><img src=\"/por/images/Reports/POR/2018/1407876/1407876_10300656_1532270386881_3layer--rgov-66x44.jpg\" alt=\"Multilevel Service Dependencies\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Dependencies between user requests, services and machines that must be considered for service restoration and fault diagnosis</div>\n<div class=\"imageCredit\">Ibrahim El-Shekeil</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Krishna&nbsp;Kant</div>\n<div class=\"imageTitle\">Multilevel Service Dependencies</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe availability and dependability of data center services are critical to our digital economy and society, and the vendors often advertise them as providing five 9?s (i.e., 99.999 percent) or better availability; which means less than 5.25 minutes per year downtime. Unfortunately, downtime episodes that violate these lofty goals are too common and result in substantial impacts in one or more areas, including unsatisfactory online customer experience, lost revenue, lost customer goodwill, high infrastructure, and operational costs, etc. Many of these are caused by inadequate or flawed operating procedures coupled with hardware and software misconfigurations and human mistakes. Often, ad-hoc procedures are used in the first place, or ad-hoc fixes are implemented to fix problems. However, the complexity and scale of modern data centers make such an approach largely ineffective.\n\nIt has been estimated that misconfigurations are responsible for 62% of downtime and 65% of security exploits in current computer systems. These already high percentages are expected to continue increasing due to current trends of extensive virtualization, architectural heterogeneity, and increasing size. Misconfiguration errors are the most significant contributors to service-level failures, but other reasons include hardware failure, operating system or software failure, intrusion, virus outbreak, disasters, and relocation of services or the data center.  These disruptions could be partial, i.e., impact some applications, or full, i.e., impact the entire data center. When such disruptions occur, they cause significant downtime which may lead to a substantial financial and legal impact.\n\nThe goal of this project was to examine a core set of operational issues which arise in almost all data center environments and study mechanisms to improve the data center availability and resilience. One such problem is a fast restoration of data center services in the event of large-scale failures or downtimes. We have shown that to address this problem; it is crucial to understand the expertise of the workers and how best they can be deployed in an environment where there are complex dependencies across services and between services and machines. Another aspect we explored is the conflicts between the IP addresses when IT systems from different organizations are joined as a result of mergers and acquisitions. We showed that by addressing the conflict problem systematically, we could remove conflicts with only a small number of changes in IP addresses and in the process significantly reduce the size of the routing tables. We also examined the problem of failure diagnosis schemes in enterprise networks and data centers which are becoming very complex due to multiple locations, complex arrangements concerning the provision of access to the network and IT services in various locations, and dependencies between services which are often not known fully. We show that a careful selection of probing locations and probes launched from them can significantly reduce the effort in performing the diagnosis.\n\nOverall the project has provided us the opportunity to examine many complex problems in data center configuration management and the need for effective tools for them. While we have examined some of these problems in detail, there are others that remain to be analyzed. Similarly, we have ideas for developing tools for such diagnosis and will be looking for ways of making such tool development a reality.\n\n \n\n\t\t\t\t\tLast Modified: 07/22/2018\n\n\t\t\t\t\tSubmitted by: Krishna Kant"
 }
}