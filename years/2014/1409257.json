{
 "awd_id": "1409257",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Write A Classifier: Learning Fine-Grained Visual Classifiers from Text and Images",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2014-06-15",
 "awd_exp_date": "2019-05-31",
 "tot_intn_awd_amt": 463208.0,
 "awd_amount": 463208.0,
 "awd_min_amd_letter_date": "2014-06-16",
 "awd_max_amd_letter_date": "2016-06-10",
 "awd_abstract_narration": "This project develops the learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.  The research team investigates computational models for joint learning of visual concepts from images and textual descriptions of fine-grained categories, for example, discriminating between bird species.  The research activities have broader impact in three fields: computer vision, natural language processing, and machine learning. There is a huge need to develop algorithms to automatically understand the content of images and videos, with numerous potential applications in web searches, image and video archival and retrieval, surveillance applications, robot navigation and others. There are various applications for developing an intelligent system that can use narrative to define and recognize categories.\r\n\r\nThis project addresses two research questions:  First, given a visual corpus and a textual corpus about a specific domain, how to jointly and effectively learn visual concepts? Second, given these two modalities how to facilitate learning novel visual concepts using only pure textual descriptions of novel categories in the domain? The research team approaches the problem on three integrated fronts: Learning, Natural Language Processing (NLP), and Computer Vision. On the learning front, the project investigates and develops algorithms suitable for learning and predicting visual classifiers with side textual information. On the NLP front, the project aims to develop novel methods for learning global and local discriminative category-level attributes and their values from text, with feedback from human computation and visual signal. The project investigates supervised and unsupervised methods for detecting visual text, and learning methods for deep language understanding to build such rich domain models from the noisy visual text. On the Vision front, the project addresses the tasks of detection and classification with side textual information. The project investigates models for the shape and appearance of a general category that can specialize to different subordinates, in a way that allows interpreting information from text within a proper geometric context, and handle variability in viewpoints and articulation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Smaranda",
   "pi_last_name": "Muresan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Smaranda Muresan",
   "pi_email_addr": "smuresan@barnard.edu",
   "nsf_id": "000542607",
   "pi_start_date": "2014-06-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 147708.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 153081.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 162419.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>When students learn from a teacher about different species, for example birds or flowers, the teacher will not just give you sample images of each species and their class labels; the teacher will tell you about discriminative visual or non-visual features for each species, similarity and differences between species, hierarchal relations between species, and many other aspects. The same learning experience takes place when you read a book or a web page to learn about the different species. Typically, the text will tell you about the taxonomy, highlights discriminative features and discusses similarities and differences between species, as well as within-species variations (male vs. female). This learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.</p>\n<p>The main aim of the proposal is to investigate models for joint learning of visual concepts from images and textual descriptions of fine-grained categories. One of the main challenges of zero-shot learning for fine-grained image classification is the detection of discriminative semantic attributes correlated with images. Using full-length articles such as Wikipedia pages as textual descriptions for an object has obtained limited success. Our team has developed a novel approach of automatically identifying visually relevant sentence with respect to an object from documents that may contain predominantly non-visual text detecting. Using such visually relevant text has proven to be more effective in zero-shot learning for fine-grained classification. However, an important aspect for fine- grained object classification in particular is the ability to capture cases where two objects differ by a slight variation in one of the properties, e.g., a small deviation in color. In textual descriptions, this differentiation is most often described using comparative adjectives (&ldquo;less orangish&rdquo;, &ldquo;lighter brown wings&rdquo;). To address this problem our team developed a new paradigm of grounding comparative adjectives for color description in the color space of an image. Given a reference color and a comparative term, the model learns to ground the comparative as a direction in the color space such that the colors along the vector, rooted at the reference color, satisfy the comparison. This approach opens the road of modeling subtle differences in objects, specifically when used together with part-based learning representations.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/19/2019<br>\n\t\t\t\t\tModified by: Smaranda&nbsp;Muresan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWhen students learn from a teacher about different species, for example birds or flowers, the teacher will not just give you sample images of each species and their class labels; the teacher will tell you about discriminative visual or non-visual features for each species, similarity and differences between species, hierarchal relations between species, and many other aspects. The same learning experience takes place when you read a book or a web page to learn about the different species. Typically, the text will tell you about the taxonomy, highlights discriminative features and discusses similarities and differences between species, as well as within-species variations (male vs. female). This learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.\n\nThe main aim of the proposal is to investigate models for joint learning of visual concepts from images and textual descriptions of fine-grained categories. One of the main challenges of zero-shot learning for fine-grained image classification is the detection of discriminative semantic attributes correlated with images. Using full-length articles such as Wikipedia pages as textual descriptions for an object has obtained limited success. Our team has developed a novel approach of automatically identifying visually relevant sentence with respect to an object from documents that may contain predominantly non-visual text detecting. Using such visually relevant text has proven to be more effective in zero-shot learning for fine-grained classification. However, an important aspect for fine- grained object classification in particular is the ability to capture cases where two objects differ by a slight variation in one of the properties, e.g., a small deviation in color. In textual descriptions, this differentiation is most often described using comparative adjectives (\"less orangish\", \"lighter brown wings\"). To address this problem our team developed a new paradigm of grounding comparative adjectives for color description in the color space of an image. Given a reference color and a comparative term, the model learns to ground the comparative as a direction in the color space such that the colors along the vector, rooted at the reference color, satisfy the comparison. This approach opens the road of modeling subtle differences in objects, specifically when used together with part-based learning representations.\n\n \n\n\t\t\t\t\tLast Modified: 12/19/2019\n\n\t\t\t\t\tSubmitted by: Smaranda Muresan"
 }
}