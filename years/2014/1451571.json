{
 "awd_id": "1451571",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: CortiCore - Exploring the Use of An Automata Processor as an MISD Accelerator",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2017-12-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 179995.0,
 "awd_min_amd_letter_date": "2014-08-06",
 "awd_max_amd_letter_date": "2016-06-14",
 "awd_abstract_narration": "A novel computational accelerator architecture - the Automata Processor -has recently been introduced by Micron, that extends the computational paradigm of non-deterministic finite automata with important new capabilities. This architecture is particularly well suited for tasks involving  pattern matching.  Preliminary results suggest speedups as high as 1000X are possible, especially applications that entail combinatorial search, i.e., searching among many possible patterns to find the best match.  This project evaluates the suitability of this novel architecture for accelerating combinatorial search, using cortical learning algorithms (i.e., algorithms for machine learning that are inspired by observations and/or theories of how the brain works) as a case study. Until now, cortical learning algorithms have primarily been implemented only in software, which leads to solutions that are slow, large, expensive and power hungry, and thus limits their applicability. In particular, this project initially focuses on accelerating hierarchical temporal memory, a cortical learning algorithm that has recently been shown to be highly effective for analysis and integration of high-data-rate, multi-modal sensor and video data. It embodies many characteristics of a variety of combinatorial search tasks, combining and extending techniques from Bayesian networks, clustering, and decision trees.  This project is the first to evaluate the ability of the \"enhanced automata\" paradigm to accelerate cortical learning algorithms, and one of the first to explore the capabilities of the Automata Processor. In the process of evaluating the best way to accelerate cortical learning algorithms, this project will yield insights into the suitability of the Automata Processor for other artificial intelligence algorithms. It will also lead to development of new algorithms, software libraries, programming guidelines, and a new programming interface, to help speed the mapping of other applications to the Automata Processor and future accelerators. It will also yield techniques to improve the performance, flexibility, and energy efficiency of future accelerators, and new insights into the design and programming of heterogeneous systems with diverse accelerator hardware units.\r\n\r\n\tThis project has potential to lay the foundations for a novel acceleration framework that enables efficient solutions to a large set of intractable problems, with orders-of-magnitude improvements in performance and energy efficiency, and to guide development of future accelerators. As a consequence of these acceleration capabilities, portable, low-power artificial intelligence solutions could become ubiquitous.  This project creates tools that facilitate research and product development involving accelerator-based computing.  This project contributes to education and outreach through new course materials and assignments, hands-on research and training opportunities in cutting-edge acceleration paradigms, and new academic-industry collaborations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mircea",
   "pi_last_name": "Stan",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Mircea R Stan",
   "pi_email_addr": "mircea@virginia.edu",
   "nsf_id": "000105637",
   "pi_start_date": "2016-06-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Skadron",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin Skadron",
   "pi_email_addr": "skadron@cs.virginia.edu",
   "nsf_id": "000393383",
   "pi_start_date": "2014-08-06",
   "pi_end_date": "2015-01-23"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Mircea",
   "pi_last_name": "Stan",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Mircea R Stan",
   "pi_email_addr": "mircea@virginia.edu",
   "nsf_id": "000105637",
   "pi_start_date": "2014-08-06",
   "pi_end_date": "2015-01-23"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Skadron",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin Skadron",
   "pi_email_addr": "skadron@cs.virginia.edu",
   "nsf_id": "000393383",
   "pi_start_date": "2016-06-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Virginia Main Campus",
  "inst_street_address": "1001 EMMET ST N",
  "inst_street_address_2": "",
  "inst_city_name": "CHARLOTTESVILLE",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "4349244270",
  "inst_zip_code": "229034833",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "VA05",
  "org_lgl_bus_name": "RECTOR & VISITORS OF THE UNIVERSITY OF VIRGINIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "JJG6HU8PA4S5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Virginia",
  "perf_str_addr": "P. O. Box 400195",
  "perf_city_name": "Charlottesville",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "229044195",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "VA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "7943",
   "pgm_ref_txt": "PROGRAMMING LANGUAGES"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "8206",
   "pgm_ref_txt": "Formal Methods and Verification"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 150000.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 29995.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project started when we noticed several intriguing similarities (see Table 1) between the concept of Hierarchical Temporal Memory (HTM) [1] and the hardware implementation of the Automata Processor (AP) introduced by Micron [2]. We were lucky enough to obtain some of the very first (beta) AP development boards as we started the Center for Automata Processing (CAP) at the University of Virginia [3] so we were able to explore both the AP hardware and tools, but also the mapping of HTM to the AP. This has led to several publications [4,5] which have shown significant speed-ups of the AP implementation of HTM (see Table 2).&nbsp;Two even more important outcomes though were unplanned:&nbsp;</p>\n<p>- First, expanding the scope of CAP from mostly focusing on the AP, to a much broader set of platforms, with special emphasis on FPGAs</p>\n<p>- Second, expanding the scope of our foray into Artificial Intelligence/Machine Learning (AI/ML) from mostly focusing on HTM to a much broader set of approaches, including Random Forest (RF), Deep Neural Networks (DNNs) with special emphasis on hardware optimizations and reconfigurability</p>\n<p>Expanding the scope beyond the APThe AP was introduced by Micron several years ago, but although showing a lot of promise, for various reasons that have mostly to do with business, not technology, Micron has decided to drop it as a product and to spin-off the core AP group into a separate commercial entity that took the name of Natural Intelligence Semiconductor (NIS) [6]. Our CAP center still has strong ties both with Micron and with NIS, but as part of our efforts to develop new ideas we also found out that Field Programmable Gate Arrays (FPGAs) have many features that are desirable when implementing deterministic and nondeterministic finite automata (DFAs and NFAs) in hardware similar to the AP. Since FPGAs are ubiquitous this has opened many more opportunities for research for the CAP center [7,8].&nbsp;HTM is a very interesting AI paradigm, but represents just a niche approach in the big AI/ML landscape. Working on HTM opened our eyes to the many other ways to tackle the fascinating world of AI and especially the problem of hardware optimization. The main discovery was that since there are so many variations out there, there is no single solution optimal in all cases, so reconfigurability is a desirable trait [9].&nbsp;</p>\n<p>References:</p>\n<p>[1] Hawkins, Jeff, and Dileep George. Hierarchical temporal memory: Concepts, theory and terminology. Technical report, Numenta, 2006.</p>\n<p>[2] Dlugosch, Paul, Dave Brown, Paul Glendenning, Michael Leventhal, and Harold Noyes. \"An efficient and scalable semiconductor architecture for parallel automata processing.\" IEEE Transactions on Parallel and Distributed Systems 25, no. 12 (2014): 3088-3098.</p>\n<p>[3] https://engineering.virginia.edu/center-automata-processing-cap</p>\n<p>[4] Putic, M., Varshneya, A.J. and Stan, M.R., 2017. Hierarchical temporal memory on the automata processor. IEEE Micro, 37(1), pp.52-59.</p>\n<p>[5] Putic, M., Varshneya, A.J. and Stan, M.R., Dendroplex: Synthesis, Simulation, and Validation of Hierarchical Temporal Memory on the Automata Processor. Design Automation Conference (DAC) 2017.</p>\n<p>[6] http://www.naturalsemi.com/</p>\n<p>[7] Wadden, J., Dang, V., Brunelle, N., Tracy II, T., Guo, D., Sadredini, E., Wang, K., Bo, C., Robins, G., Stan, M. and Skadron, K., 2016, September. ANMLzoo: a benchmark suite for exploring bottlenecks in automata processing engines and architectures. In Workload Characterization (IISWC), 2016 IEEE International Symposium on (pp. 1-12). IEEE.</p>\n<p>[8] Xie, T., Dang, V., Wadden, J., Skadron, K. and Stan, M., 2017, September. REAPR: Reconfigurable engine for automata processing. In Field Programmable Logic and Applications (FPL), 2017 27th International Conference on (pp. 1-8). IEEE.</p>\n<p>[9] Putic, M., Venkataramani, S., Eldridge, S., Buyuktosunoglu, A., Bose, P. and Stan, M., 2018, June. Dyhard-DNN: even more DNN acceleration with dynamic hardware reconfiguration. In Proceedings of the 55th Annual Design Automation Conference (p. 14). ACM.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/07/2018<br>\n\t\t\t\t\tModified by: Mircea&nbsp;R&nbsp;Stan</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1451571/1451571_10329263_1533684543584_HTMAP--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1451571/1451571_10329263_1533684543584_HTMAP--rgov-800width.jpg\" title=\"Table 1\"><img src=\"/por/images/Reports/POR/2018/1451571/1451571_10329263_1533684543584_HTMAP--rgov-66x44.jpg\" alt=\"Table 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">HTM to AP mapping</div>\n<div class=\"imageCredit\">IEEE Micro</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mircea&nbsp;R&nbsp;Stan</div>\n<div class=\"imageTitle\">Table 1</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1451571/1451571_10329263_1533684624371_APspeedup--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1451571/1451571_10329263_1533684624371_APspeedup--rgov-800width.jpg\" title=\"Table 2\"><img src=\"/por/images/Reports/POR/2018/1451571/1451571_10329263_1533684624371_APspeedup--rgov-66x44.jpg\" alt=\"Table 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">HTM Speedup on the AP</div>\n<div class=\"imageCredit\">IEEE Micro</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mircea&nbsp;R&nbsp;Stan</div>\n<div class=\"imageTitle\">Table 2</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project started when we noticed several intriguing similarities (see Table 1) between the concept of Hierarchical Temporal Memory (HTM) [1] and the hardware implementation of the Automata Processor (AP) introduced by Micron [2]. We were lucky enough to obtain some of the very first (beta) AP development boards as we started the Center for Automata Processing (CAP) at the University of Virginia [3] so we were able to explore both the AP hardware and tools, but also the mapping of HTM to the AP. This has led to several publications [4,5] which have shown significant speed-ups of the AP implementation of HTM (see Table 2). Two even more important outcomes though were unplanned: \n\n- First, expanding the scope of CAP from mostly focusing on the AP, to a much broader set of platforms, with special emphasis on FPGAs\n\n- Second, expanding the scope of our foray into Artificial Intelligence/Machine Learning (AI/ML) from mostly focusing on HTM to a much broader set of approaches, including Random Forest (RF), Deep Neural Networks (DNNs) with special emphasis on hardware optimizations and reconfigurability\n\nExpanding the scope beyond the APThe AP was introduced by Micron several years ago, but although showing a lot of promise, for various reasons that have mostly to do with business, not technology, Micron has decided to drop it as a product and to spin-off the core AP group into a separate commercial entity that took the name of Natural Intelligence Semiconductor (NIS) [6]. Our CAP center still has strong ties both with Micron and with NIS, but as part of our efforts to develop new ideas we also found out that Field Programmable Gate Arrays (FPGAs) have many features that are desirable when implementing deterministic and nondeterministic finite automata (DFAs and NFAs) in hardware similar to the AP. Since FPGAs are ubiquitous this has opened many more opportunities for research for the CAP center [7,8]. HTM is a very interesting AI paradigm, but represents just a niche approach in the big AI/ML landscape. Working on HTM opened our eyes to the many other ways to tackle the fascinating world of AI and especially the problem of hardware optimization. The main discovery was that since there are so many variations out there, there is no single solution optimal in all cases, so reconfigurability is a desirable trait [9]. \n\nReferences:\n\n[1] Hawkins, Jeff, and Dileep George. Hierarchical temporal memory: Concepts, theory and terminology. Technical report, Numenta, 2006.\n\n[2] Dlugosch, Paul, Dave Brown, Paul Glendenning, Michael Leventhal, and Harold Noyes. \"An efficient and scalable semiconductor architecture for parallel automata processing.\" IEEE Transactions on Parallel and Distributed Systems 25, no. 12 (2014): 3088-3098.\n\n[3] https://engineering.virginia.edu/center-automata-processing-cap\n\n[4] Putic, M., Varshneya, A.J. and Stan, M.R., 2017. Hierarchical temporal memory on the automata processor. IEEE Micro, 37(1), pp.52-59.\n\n[5] Putic, M., Varshneya, A.J. and Stan, M.R., Dendroplex: Synthesis, Simulation, and Validation of Hierarchical Temporal Memory on the Automata Processor. Design Automation Conference (DAC) 2017.\n\n[6] http://www.naturalsemi.com/\n\n[7] Wadden, J., Dang, V., Brunelle, N., Tracy II, T., Guo, D., Sadredini, E., Wang, K., Bo, C., Robins, G., Stan, M. and Skadron, K., 2016, September. ANMLzoo: a benchmark suite for exploring bottlenecks in automata processing engines and architectures. In Workload Characterization (IISWC), 2016 IEEE International Symposium on (pp. 1-12). IEEE.\n\n[8] Xie, T., Dang, V., Wadden, J., Skadron, K. and Stan, M., 2017, September. REAPR: Reconfigurable engine for automata processing. In Field Programmable Logic and Applications (FPL), 2017 27th International Conference on (pp. 1-8). IEEE.\n\n[9] Putic, M., Venkataramani, S., Eldridge, S., Buyuktosunoglu, A., Bose, P. and Stan, M., 2018, June. Dyhard-DNN: even more DNN acceleration with dynamic hardware reconfiguration. In Proceedings of the 55th Annual Design Automation Conference (p. 14). ACM.\n\n\t\t\t\t\tLast Modified: 08/07/2018\n\n\t\t\t\t\tSubmitted by: Mircea R Stan"
 }
}