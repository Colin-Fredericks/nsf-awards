{
 "awd_id": "1422211",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC: Small: Understanding Network Level Malicious Activities: Classification, Community Detection and Inference of Security Interdependence",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 499952.0,
 "awd_amount": 499952.0,
 "awd_min_amd_letter_date": "2014-08-21",
 "awd_max_amd_letter_date": "2014-09-10",
 "awd_abstract_narration": "This goal of this project is development of a formal method to quantitatively assess the security posture of large networks and assign them a numeric score. Large networks are made up of a collection of individual machines, which exhibit more stable behavior and features as a group than at the IP level, where each host is inspected separately. Networks at an aggregate level thus carry more predictive power, enabling a more robust and accurate policy design. \r\nA large-scale statistical analysis of network data forms the basis of two sets of metrics. The first concerns a network as a standalone entity irrespective of other networks in the same ecosystem. The second concerns a network as one of many inter-connected networks. This second set is crucial due to the fact that the actions of one network affect its neighbor networks. If a network tolerates malicious behavior, its network neighbors feel the impact.\r\nThis project enables network operators to design network security policies that can be meaningfully applied at a network or organizational level, for example peering arrangements between Internet Service Providers, traffic routing decisions, and incentive mechanisms (e.g., cyber insurance) aimed at encouraging better security practices and investment by organizations. The outcome of this project is thus expected to have significant impact on security and incentive policy design across the very core of the Internet.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mingyan",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mingyan Liu",
   "pi_email_addr": "mingyan@eecs.umich.edu",
   "nsf_id": "000230014",
   "pi_start_date": "2014-08-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Bailey",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Bailey",
   "pi_email_addr": "mdbailey@illinois.edu",
   "nsf_id": "000300596",
   "pi_start_date": "2014-08-21",
   "pi_end_date": "2014-09-10"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Kallitsis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Kallitsis",
   "pi_email_addr": "mgkallit@merit.edu",
   "nsf_id": "000647852",
   "pi_start_date": "2014-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092122",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 499952.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project seeks to develop a method to quantitatively assess the security posture of a network entity, i.e., at an aggregate level as opposed to at the IP level with each host inspected separately.&nbsp; Empirical evidence as well as intuition suggests that at an aggregate level entities exhibit more stable behavior and features, thus carry more predictive power, thereby enabling more proactive policy design.&nbsp; There are two main domains where such an assessment framework can be extremely useful.&nbsp; The first is the design of network/security policies that can only be meaningfully applied at a network or organizational level, e.g., peering arrangements and traffic routing decisions, and incentive mechanisms (e.g., cyber insurance) aimed at encouraging better security practices and investment by organizations. The second involves security practices under severe resource limitation, e.g., deep packet inspection, whereby network-level assessment can enable hierarchical inspection by allocating more resources in examining traffic from more malicious networks.&nbsp; Our technical approach includes a data collection and measurement study and comprehensive large-scale statistical data analysis.&nbsp; Under the latter we developed two sets of metrics. The first concerns a network as a standalone entity irrespective of other networks in the same ecosystem. The second concerns a network as one of many inter-connected networks. This second set is crucial due to the interdependence or externality nature of network security, i.e., what one network does affects others.&nbsp;<br />The main outcomes of this project are described as follows.&nbsp;</p>\n<p><br />We developed various metrics, features extracted from our dataset, to capture the security posture of a network entity, and put them to use in forecasting cyber security incidents at an organizational level.&nbsp; In a prototype study, we collected hundreds of externally measurable features about an organization&rsquo;s network from two main categories: mismanagement symptoms, such as misconfigured DNS or BGP within a network, and malicious activity time series, which include spam, phishing, and scanning activities sourced from these organizations. Using these features we trained and tested a Random Forest (RF) classifier against more than 1,000 incident reports taken from the VERIS community database, Hackmageddon, and the Web Hacking Incidents Database that occurred between mid-2013 and the end of 2014. The resulting classifier is able to achieve a 90% True Positive (TP) rate, a 10% False Positive (FP) rate.&nbsp; &nbsp;In a second prototype study, we developed another classifier using additional features extracted from an organization's business details to predict the most likely type of cyber incidents, i.e., the distribution of cyber risks over a spectrum of incident types.&nbsp;&nbsp;</p>\n<p><br />We also performed various community detection analysis over our datasets to understand cybersecurity risk dependencies. This has led to the detection of phishing activities in the form of propagating signals among networks using an epidemic modeling approach earlier on in the project.&nbsp; More recently, we combined a spam dataset and a host patching dataset to perform early detection of actively exploited vulnerabilities.&nbsp; Specifically, on one hand, we use symptomatic botnet data (in the form of a set of spam blacklists) to discover a community structure which reveals how similar Internet entities behave in terms of their malicious activities. On the other hand, we analyze the risk behavior of end-hosts through a set of patch deployment measurements that allow us to assess their risk to different vulnerabilities. The latter is then compared to the former to quantify whether the underlying risks are consistent with the observed global symptomatic community structure, which then allows us to statistically determine whether a given vulnerability is being actively exploited in the wild. Our results show that by observing up to 10 days&rsquo; worth of data, we can successfully detect vulnerability exploitation, and this is shown to be much earlier than the standard discovery time records for most vulnerabilities.&nbsp;&nbsp;</p>\n<p><br />To summarize, the outcome of this project has had significant impact on security and incentive policy design. Our ability to perform fine-grained breach prediction enables more proactive and preventative measures at an organizational level and informs strategic decision making involving resource allocation within the organization.&nbsp; This allows organizations to focus on a sparser set of incidents, thus achieving the same level of protection by spending less resources on security through more judicious prioritization.&nbsp; Our ability to perform early detection of exploited vulnerabilities is equally impactful.&nbsp; For context, at any given time there exist a large number of software vulnerabilities in our computing systems, but only a fraction of them are ultimately exploited in the wild. Advanced knowledge of which vulnerabilities are being or likely to be exploited would allow system administrators to prioritize patch deployments, enterprises to assess their security risk more precisely, and security companies to develop intrusion protection for those vulnerabilities.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/16/2018<br>\n\t\t\t\t\tModified by: Mingyan&nbsp;Liu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537144558364_new_rf1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537144558364_new_rf1--rgov-800width.jpg\" title=\"Breach prediction performance\"><img src=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537144558364_new_rf1--rgov-66x44.jpg\" alt=\"Breach prediction performance\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Prediction results. There are variations between the datasets, but an operating point ? combined (TP, FP) values ? of (90%, 10%) or (80%, 5%) is achievable. In particular, when we use all three datasets together, we can achieve an accuracy level of (88%, 4%).</div>\n<div class=\"imageCredit\">Liu et al. \"Cloudy with a Chance of Breach: Forecasting Cyber Security Incidents\", USENIX Security, August 2015.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mingyan&nbsp;Liu</div>\n<div class=\"imageTitle\">Breach prediction performance</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537144738917_2015--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537144738917_2015--rgov-800width.jpg\" title=\"Case studies of 2015\"><img src=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537144738917_2015--rgov-66x44.jpg\" alt=\"Case studies of 2015\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">There is a clear separation between the predicted outcome of victim and non-victim groups.  Select examples of high profile breaches during 2015 and their predicted risk are shown.</div>\n<div class=\"imageCredit\">Mingyan Liu</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mingyan&nbsp;Liu</div>\n<div class=\"imageTitle\">Case studies of 2015</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537145408401_roc_features3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537145408401_roc_features3--rgov-800width.jpg\" title=\"Exploit-in-the-wild vulnerability detection performance.\"><img src=\"/por/images/Reports/POR/2018/1422211/1422211_10335645_1537145408401_roc_features3--rgov-66x44.jpg\" alt=\"Exploit-in-the-wild vulnerability detection performance.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Exploit-in-the-wild vulnerability detection performance measured by ROC and AUC, using different sets of features.</div>\n<div class=\"imageCredit\">Xiao et al. \"From Patching Delays to Infection Symptoms: Using Risk Profiles for an Early Discovery of Vulnerabilities Exploited in the Wild\", USENIX Security, August 2018.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Mingyan&nbsp;Liu</div>\n<div class=\"imageTitle\">Exploit-in-the-wild vulnerability detection performance.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project seeks to develop a method to quantitatively assess the security posture of a network entity, i.e., at an aggregate level as opposed to at the IP level with each host inspected separately.  Empirical evidence as well as intuition suggests that at an aggregate level entities exhibit more stable behavior and features, thus carry more predictive power, thereby enabling more proactive policy design.  There are two main domains where such an assessment framework can be extremely useful.  The first is the design of network/security policies that can only be meaningfully applied at a network or organizational level, e.g., peering arrangements and traffic routing decisions, and incentive mechanisms (e.g., cyber insurance) aimed at encouraging better security practices and investment by organizations. The second involves security practices under severe resource limitation, e.g., deep packet inspection, whereby network-level assessment can enable hierarchical inspection by allocating more resources in examining traffic from more malicious networks.  Our technical approach includes a data collection and measurement study and comprehensive large-scale statistical data analysis.  Under the latter we developed two sets of metrics. The first concerns a network as a standalone entity irrespective of other networks in the same ecosystem. The second concerns a network as one of many inter-connected networks. This second set is crucial due to the interdependence or externality nature of network security, i.e., what one network does affects others. \nThe main outcomes of this project are described as follows. \n\n\nWe developed various metrics, features extracted from our dataset, to capture the security posture of a network entity, and put them to use in forecasting cyber security incidents at an organizational level.  In a prototype study, we collected hundreds of externally measurable features about an organization?s network from two main categories: mismanagement symptoms, such as misconfigured DNS or BGP within a network, and malicious activity time series, which include spam, phishing, and scanning activities sourced from these organizations. Using these features we trained and tested a Random Forest (RF) classifier against more than 1,000 incident reports taken from the VERIS community database, Hackmageddon, and the Web Hacking Incidents Database that occurred between mid-2013 and the end of 2014. The resulting classifier is able to achieve a 90% True Positive (TP) rate, a 10% False Positive (FP) rate.   In a second prototype study, we developed another classifier using additional features extracted from an organization's business details to predict the most likely type of cyber incidents, i.e., the distribution of cyber risks over a spectrum of incident types.  \n\n\nWe also performed various community detection analysis over our datasets to understand cybersecurity risk dependencies. This has led to the detection of phishing activities in the form of propagating signals among networks using an epidemic modeling approach earlier on in the project.  More recently, we combined a spam dataset and a host patching dataset to perform early detection of actively exploited vulnerabilities.  Specifically, on one hand, we use symptomatic botnet data (in the form of a set of spam blacklists) to discover a community structure which reveals how similar Internet entities behave in terms of their malicious activities. On the other hand, we analyze the risk behavior of end-hosts through a set of patch deployment measurements that allow us to assess their risk to different vulnerabilities. The latter is then compared to the former to quantify whether the underlying risks are consistent with the observed global symptomatic community structure, which then allows us to statistically determine whether a given vulnerability is being actively exploited in the wild. Our results show that by observing up to 10 days? worth of data, we can successfully detect vulnerability exploitation, and this is shown to be much earlier than the standard discovery time records for most vulnerabilities.  \n\n\nTo summarize, the outcome of this project has had significant impact on security and incentive policy design. Our ability to perform fine-grained breach prediction enables more proactive and preventative measures at an organizational level and informs strategic decision making involving resource allocation within the organization.  This allows organizations to focus on a sparser set of incidents, thus achieving the same level of protection by spending less resources on security through more judicious prioritization.  Our ability to perform early detection of exploited vulnerabilities is equally impactful.  For context, at any given time there exist a large number of software vulnerabilities in our computing systems, but only a fraction of them are ultimately exploited in the wild. Advanced knowledge of which vulnerabilities are being or likely to be exploited would allow system administrators to prioritize patch deployments, enterprises to assess their security risk more precisely, and security companies to develop intrusion protection for those vulnerabilities. \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/16/2018\n\n\t\t\t\t\tSubmitted by: Mingyan Liu"
 }
}