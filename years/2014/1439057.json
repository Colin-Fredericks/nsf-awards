{
 "awd_id": "1439057",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: DSD: End-to-end Acceleration of Genomic Workflows on Emerging Heterogeneous Supercomputers",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2020-02-29",
 "tot_intn_awd_amt": 849984.0,
 "awd_amount": 849984.0,
 "awd_min_amd_letter_date": "2014-08-20",
 "awd_max_amd_letter_date": "2019-03-15",
 "awd_abstract_narration": "The proposed research harnesses parallelism to accelerate the\r\npervasive bioinformatics workflow of detecting genetic variations.\r\nThis workflow determines the genetic variants present in an\r\nindividual, given DNA sequencing data. The variant detection workflow\r\nis an integral part of current genomic data analysis, and several\r\nstudies have linked genetic variants to diseases. Typical instances\r\nof this workflow currently take several hours to multiple days to\r\ncomplete with state-of-the-art software, and current algorithms and\r\nsoftware are unable to exploit and benefit from even modest levels of\r\nhardware parallelism. Most prior approaches to parallelization and\r\nperformance tuning of genomic data analysis pipelines have targeted\r\ncomputation, I/O, or network data transfer bottlenecks in isolation,\r\nand consequently, are limited in the overall performance improvement\r\nthey can achieve. This project targets end-to-end acceleration\r\nmethodologies and uses emerging heterogeneous supercomputers to\r\nreduce workflow time-to-completion.\r\n\r\nThe project focuses on holistic methodologies to accelerate multiple\r\ncomponents within the genetic variant detection workflow. It explores\r\nlightweight data reorganizations at multiple granularities to enhance\r\nlocality, investigates compute-, communication-, and I/O task\r\ncotuning, locality-aware load-balancing, and coordinated resource\r\npartitioning to exploit high-performance computing platforms. A key\r\ngoal of the proposed research is to design domain-specific\r\noptimizations targeting the massive parallelism and scalability\r\npotential of current heterogeneous supercomputers, so that the\r\ndeveloped techniques can be easily transferred and applied to \r\ndedicated academic cluster and commercial computational environments.\r\nOutreach efforts target undergraduate students through recruiting\r\nworkshops and attract them to interdisciplinary graduate programs.\r\nCurriculum development activities emphasize cross-layer parallelism.\r\n\r\nFor further information, see project web site at \r\nhttp://sites.psu.edu/XPSGenomics",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kamesh",
   "pi_last_name": "Madduri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kamesh Madduri",
   "pi_email_addr": "madduri@cse.psu.edu",
   "nsf_id": "000598267",
   "pi_start_date": "2014-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Padma",
   "pi_last_name": "Raghavan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Padma Raghavan",
   "pi_email_addr": "padma.raghavan@vanderbilt.edu",
   "nsf_id": "000097691",
   "pi_start_date": "2014-08-20",
   "pi_end_date": "2019-03-15"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mahmut",
   "pi_last_name": "Kandemir",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Mahmut T Kandemir",
   "pi_email_addr": "mtk2@psu.edu",
   "nsf_id": "000163936",
   "pi_start_date": "2014-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "Medvedev",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Paul Medvedev",
   "pi_email_addr": "pashadag@cse.psu.edu",
   "nsf_id": "000636101",
   "pi_start_date": "2014-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State University",
  "perf_str_addr": "343E IST Building",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 849984.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern genomic data analysis workflows are complex and resource-intensive. Typical instances of genomic workflows currently take several hours to complete with state-of-the-art software, and current algorithms and software are unable to exploit and benefit from even modest levels of hardware parallelism. Most prior approaches to parallelization and performance tuning of genomic workflows have targeted computation, I/O, or network data transfer bottlenecks in isolation, and consequently, are limited in the overall performance improvement they can achieve. This project focused on end-to-end acceleration methodologies and uses supercomputers to reduce workflow time-to-completion.</p>\n<p>The project has resulted in several technical contributions. We have developed new software tools (SPRITE4, MetaPartMin, MetaPrep) that exploit multi-node distributed memory parallelism to accelerate data-intensive genomic workflows. Some common optimizations underlying their design include lightweight preprocessing routines for static load balancing, parallel I/O, inter-node communication-reducing mechanisms, and fully utilizing shared-memory multicore resources.</p>\n<p>SPRITE4 is a FASTQ-to-VCF variant detection pipeline with highly scalable methods for sequence alignment, SAM to BAM file processing, and variant calling. Specifically, we parallelize Minimap2 and Strelka2 for multi-node environments and reduce intermediate file I/O. Using 32 compute nodes (1536 compute cores) of the Stampede2 supercomputer, SPRITE4 can process a 159 gigabase human genome in under six minutes, while retaining Strelka2's state-of-the-art variant detection quality. MetaPartMin is a read partitioning tool, with the goal of partitioning metagenomic reads into disjoint components that can be assembled independently. MetaPartMin is the successor to MetaPrep, and includes novel memory-reducing optimizations to drastically reduce aggregate main memory use, enabling the partitioning of massive datasets on a modest number of compute nodes. All steps of MetaPartMin exploit hybrid multicore and distributed-memory parallelism. On 32 compute nodes of Stampede2, MetaPartMin can partition a 1.25 terabase soil metagenome in six minutes.</p>\n<p>We also investigated new manycore-centric parallelization and optimization of a popular machine learning method with natural language processing applications, called Word2Vec Skip gram with negative sampling (SGNS). This tool can be used for unsupervised literature-based discovery of concepts and relationships. There also exist extensions to this method for bioinformatics (e.g, BioVectors). We introduce a new optimization called context combining to boost SGNS training performance on multicore and manycore systems. We also show that our accuracy on benchmark queries is comparable to state-of-the-art implementations. For processing the One Billion Word benchmark dataset on a 16-core platform, our approach SGNScc is 3.53X faster than the original multithreaded WordVec implementation and 1.35X faster than an Intel-optimized Word2Vec implementation.</p>\n<p>To complement the peer-reviewed research publications describing these new methods, we made the source code for the software programs freely available online.</p>\n<p>The project partially supported seven doctoral students and three Masters students in Computer Science and Engineering. Three of the seven doctoral students and the three Masters students have graduated. The dissertation of four PhD students is in progress. Three of the ten students are from groups traditionally underrepresented in computing. The project's research outcomes have been incorporated into undergraduate and graduate classes.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/13/2020<br>\n\t\t\t\t\tModified by: Kamesh&nbsp;Madduri</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern genomic data analysis workflows are complex and resource-intensive. Typical instances of genomic workflows currently take several hours to complete with state-of-the-art software, and current algorithms and software are unable to exploit and benefit from even modest levels of hardware parallelism. Most prior approaches to parallelization and performance tuning of genomic workflows have targeted computation, I/O, or network data transfer bottlenecks in isolation, and consequently, are limited in the overall performance improvement they can achieve. This project focused on end-to-end acceleration methodologies and uses supercomputers to reduce workflow time-to-completion.\n\nThe project has resulted in several technical contributions. We have developed new software tools (SPRITE4, MetaPartMin, MetaPrep) that exploit multi-node distributed memory parallelism to accelerate data-intensive genomic workflows. Some common optimizations underlying their design include lightweight preprocessing routines for static load balancing, parallel I/O, inter-node communication-reducing mechanisms, and fully utilizing shared-memory multicore resources.\n\nSPRITE4 is a FASTQ-to-VCF variant detection pipeline with highly scalable methods for sequence alignment, SAM to BAM file processing, and variant calling. Specifically, we parallelize Minimap2 and Strelka2 for multi-node environments and reduce intermediate file I/O. Using 32 compute nodes (1536 compute cores) of the Stampede2 supercomputer, SPRITE4 can process a 159 gigabase human genome in under six minutes, while retaining Strelka2's state-of-the-art variant detection quality. MetaPartMin is a read partitioning tool, with the goal of partitioning metagenomic reads into disjoint components that can be assembled independently. MetaPartMin is the successor to MetaPrep, and includes novel memory-reducing optimizations to drastically reduce aggregate main memory use, enabling the partitioning of massive datasets on a modest number of compute nodes. All steps of MetaPartMin exploit hybrid multicore and distributed-memory parallelism. On 32 compute nodes of Stampede2, MetaPartMin can partition a 1.25 terabase soil metagenome in six minutes.\n\nWe also investigated new manycore-centric parallelization and optimization of a popular machine learning method with natural language processing applications, called Word2Vec Skip gram with negative sampling (SGNS). This tool can be used for unsupervised literature-based discovery of concepts and relationships. There also exist extensions to this method for bioinformatics (e.g, BioVectors). We introduce a new optimization called context combining to boost SGNS training performance on multicore and manycore systems. We also show that our accuracy on benchmark queries is comparable to state-of-the-art implementations. For processing the One Billion Word benchmark dataset on a 16-core platform, our approach SGNScc is 3.53X faster than the original multithreaded WordVec implementation and 1.35X faster than an Intel-optimized Word2Vec implementation.\n\nTo complement the peer-reviewed research publications describing these new methods, we made the source code for the software programs freely available online.\n\nThe project partially supported seven doctoral students and three Masters students in Computer Science and Engineering. Three of the seven doctoral students and the three Masters students have graduated. The dissertation of four PhD students is in progress. Three of the ten students are from groups traditionally underrepresented in computing. The project's research outcomes have been incorporated into undergraduate and graduate classes.\n\n\t\t\t\t\tLast Modified: 07/13/2020\n\n\t\t\t\t\tSubmitted by: Kamesh Madduri"
 }
}