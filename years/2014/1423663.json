{
 "awd_id": "1423663",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Structured Signal Recovery from Noisy Measurements via Convex Programming: A Framework for Analyzing Performance",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Richard Brown",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2014-08-04",
 "awd_max_amd_letter_date": "2014-08-04",
 "awd_abstract_narration": "With the advent of ubiquitous sensing (multi-modal sensors, imaging systems and cameras, etc.), various complex social networks, and the deluge of health-care data (DNA sequences, micro-arrays, etc.), society is now officially in the era of Big Data. In such a setting, the ability to systematically and efficiently derive structured models, and recover reliable and actionable information, from barrages of high dimensional data will have far-reaching impact on engineering challenges and on everyday life. Unfortunately, the data is often noisy, inaccurate, or partially missing. This research will develop a comprehensive theory to assess the performance of a very wide class of algorithms designed for this purpose which are based on convex programming techniques.  Such performance guarantees will assist practitioners in a wide array of applications in signal processing, machine learning, statistics and data analysis.\r\n\r\nRecent years has witnessed some spectacular theoretical and algorithmic advances in convex optimization and compressed sensing that have changed how large noisy data sets are handled. Despite these successes, key challenges remain, including the need for a comprehensive theory that accurately predicts the performance of the algorithms and goes beyond the customary ?order-wise? performance guarantees. The investigators will pursue an ambitious research program to give exact performance evaluations for a wide variety of convex-optimization-based signal recovery methods, including the classical LASSO and its variants.  The framework can deal with a wide array of signal-to-noise ratios, different measurement matrix ensembles, and a variety of cost functions and signal structures. The techniques draw upon a host of ideas in high-dimensional geometry, statistics, and signal processing and are the culmination of a flurry of activity by several different research communities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Babak",
   "pi_last_name": "Hassibi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Babak Hassibi",
   "pi_email_addr": "hassibi@caltech.edu",
   "nsf_id": "000490528",
   "pi_start_date": "2014-08-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "California Institute of Technology",
  "inst_street_address": "1200 E CALIFORNIA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "PASADENA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6263956219",
  "inst_zip_code": "911250001",
  "inst_country_name": "United States",
  "cong_dist_code": "28",
  "st_cong_dist_code": "CA28",
  "org_lgl_bus_name": "CALIFORNIA INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "U2JMKHNS5TG4"
 },
 "perf_inst": {
  "perf_inst_name": "California Institute of Technology",
  "perf_str_addr": "1200 E. California Blvd",
  "perf_city_name": "Pasadena",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "911250001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "28",
  "perf_st_cong_dist": "CA28",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In the era of Big Data and ubiquitous computing, the ability to systematically and efficiently derive structured models from deluges of high dimensional data will have far-reaching impact on engineering challenges and on our everyday lives. Unfortunately, the data is often noisy, inaccurate, or partially missing.</p>\n<p>The main goal of this project is to develop a comprehensive theory to assess the performance of a very wide class of algorithms designed for this purpose which are based on convex programming techniques.&nbsp; Convex programming is of interest, because it yields unique optima and because it is often computationally efficient. Being able to theoretically assess the performance of the algorithms is critical, since in signal processing, machine learning, and data analysis, optimization is often used as a surrogate for learning, parameter estimation, etc. The quality of the learning process, estimate, etc., cannot be inferred from the solution itself. One either has to resort to cross-validation (i.e., to reserve part of the data for evaluation purposes) or to have a theoretical framework to assess the performance of the method even before it is carried out. This project focuses on the latter approach.</p>\n<p>Developing a general theory for assessing the performance (mean-square-error, probability-of-error, etc.) for convex-optimization-based signal recovery methods will assist practitioners in a wide array of applications in signal processing, machine learning and data analysis. It will also add to our understanding of optimization problems, and the domain-specific areas they are applied to. This goes well beyond what one can achieve will cross-validation. The theory is based on a very powerful extension of classical results on comparing Gaussian proceses (due to Slepian and Gordon). &nbsp;We refer to the main theorem developed as the convex Gaussian minimax theorem (CGMT).</p>\n<p>The specific goals of the project were.</p>\n<p>1. To derive exact formulas for the mean-square error, probability of error, etc., of generalized LASSO algorithms in the regime of arbitrary noise.</p>\n<p>The project was successful in doing so and the answer was given in terms of geometrical quantities that naturally arise, since as the Gaussian width of the descent cone of the objective function, as well as various Moreau envelopes.</p>\n<p>2. Going beyond the Gaussian measurement matrix ensemble.</p>\n<p>Being based on Gaussian comparison lemmas, our results apply directly to Gaussian measurement matrices. One goal was to extend the results beyond Gaussian measurement matrices. This was successfully done for complex Gaussian measurement matrices, as well as for nonlinear measurements. For example, in the latter case, the performance of one-bit-compressed sensing was exactly analyzed. The results were also somewhat extended to quadratic Gaussian measurements (through the development of certain upper bounds), which has applications in covariance estimation, phase retrieval, and graphical LASSO. One fruitful extension was the demonstration of a universality result which demonstrated that all the asymptotic results hold when the measurement matrix with iid Gaussian entries is replaced with one with iid sub-Gaussian entries.</p>\n<p>3. Cost functions beyond least-squares.</p>\n<p>The generalized LASSO algorithm considers a least-squares cost, along with a structure-inducing regularizer. The least-squares cost is most natural when the noise is additive and Gaussian. However, for other noise distributions, say bounded noise, or sparse noise, other cost functions are more appropriate. The project was able to generalize these results to a very wide class of optimization problems which, in the statistics literature, are referred to as M-estimators.</p>\n<p>Specific application areas where the project obtained strikingly new results include phase retrieval, massive MIMO signal detection, compressed sensing with quantized measurements, blind data transmission, and structured covariance estimation.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/19/2018<br>\n\t\t\t\t\tModified by: Babak&nbsp;Hassibi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn the era of Big Data and ubiquitous computing, the ability to systematically and efficiently derive structured models from deluges of high dimensional data will have far-reaching impact on engineering challenges and on our everyday lives. Unfortunately, the data is often noisy, inaccurate, or partially missing.\n\nThe main goal of this project is to develop a comprehensive theory to assess the performance of a very wide class of algorithms designed for this purpose which are based on convex programming techniques.  Convex programming is of interest, because it yields unique optima and because it is often computationally efficient. Being able to theoretically assess the performance of the algorithms is critical, since in signal processing, machine learning, and data analysis, optimization is often used as a surrogate for learning, parameter estimation, etc. The quality of the learning process, estimate, etc., cannot be inferred from the solution itself. One either has to resort to cross-validation (i.e., to reserve part of the data for evaluation purposes) or to have a theoretical framework to assess the performance of the method even before it is carried out. This project focuses on the latter approach.\n\nDeveloping a general theory for assessing the performance (mean-square-error, probability-of-error, etc.) for convex-optimization-based signal recovery methods will assist practitioners in a wide array of applications in signal processing, machine learning and data analysis. It will also add to our understanding of optimization problems, and the domain-specific areas they are applied to. This goes well beyond what one can achieve will cross-validation. The theory is based on a very powerful extension of classical results on comparing Gaussian proceses (due to Slepian and Gordon).  We refer to the main theorem developed as the convex Gaussian minimax theorem (CGMT).\n\nThe specific goals of the project were.\n\n1. To derive exact formulas for the mean-square error, probability of error, etc., of generalized LASSO algorithms in the regime of arbitrary noise.\n\nThe project was successful in doing so and the answer was given in terms of geometrical quantities that naturally arise, since as the Gaussian width of the descent cone of the objective function, as well as various Moreau envelopes.\n\n2. Going beyond the Gaussian measurement matrix ensemble.\n\nBeing based on Gaussian comparison lemmas, our results apply directly to Gaussian measurement matrices. One goal was to extend the results beyond Gaussian measurement matrices. This was successfully done for complex Gaussian measurement matrices, as well as for nonlinear measurements. For example, in the latter case, the performance of one-bit-compressed sensing was exactly analyzed. The results were also somewhat extended to quadratic Gaussian measurements (through the development of certain upper bounds), which has applications in covariance estimation, phase retrieval, and graphical LASSO. One fruitful extension was the demonstration of a universality result which demonstrated that all the asymptotic results hold when the measurement matrix with iid Gaussian entries is replaced with one with iid sub-Gaussian entries.\n\n3. Cost functions beyond least-squares.\n\nThe generalized LASSO algorithm considers a least-squares cost, along with a structure-inducing regularizer. The least-squares cost is most natural when the noise is additive and Gaussian. However, for other noise distributions, say bounded noise, or sparse noise, other cost functions are more appropriate. The project was able to generalize these results to a very wide class of optimization problems which, in the statistics literature, are referred to as M-estimators.\n\nSpecific application areas where the project obtained strikingly new results include phase retrieval, massive MIMO signal detection, compressed sensing with quantized measurements, blind data transmission, and structured covariance estimation.\n\n\t\t\t\t\tLast Modified: 03/19/2018\n\n\t\t\t\t\tSubmitted by: Babak Hassibi"
 }
}