{
 "awd_id": "1350901",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Filling the Gaps in Domain-Specific Functional-Based Solutions for High-Performance Execution",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2014-06-01",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 521201.0,
 "awd_amount": 553182.0,
 "awd_min_amd_letter_date": "2014-02-04",
 "awd_max_amd_letter_date": "2019-08-13",
 "awd_abstract_narration": "Physicists, meteorologists, and other users of high-performance computing want to write large and complicated models and systems that execute correctly and quickly. These models and systems are allowing us to understand our world better, and explore solutions to hard problems. Unfortunately, to make efficient use of computing resources, a non-computing specialist needs to, for all intents and purposes, become a proficient computer scientist. The fundamental problem is that a non-computing specialist is not necessarily aware of available options for optimizing their model. Domain Specific Languages (DSLs) are one possible solution to this knowledge gap, because DSLs can encode the necessary knowledge required to map programs onto high-performance targets, at the same time to provide a customized environment in the parlance of the domain specialist. By utilizing a DSL, the non-specialist does not need to worry about performance at the outset; it is the responsibility of the DSL compiler to map the model onto the high-performance target. This research will produce new DSL techniques and technologies that will substantially lower the cost of developing such high-performance DSLs, as well as improve the performance opportunities offered to users.\r\n\r\nOver the five-year span of the project, two specific high-performance parallel platforms (GPGPUs and FPGAs) will be made more accessible. A resource-aware DSL will allow a broad range of parallel programming idioms to be directly expressed. This resource-aware DSL will also work in combination with existing DSLs. With the use of rewriting technologies, the resource-aware DSL will act as an expressive common medium for exploring design tradeoffs before, during and after the commitment to specific implementation technologies. A set of custom compiler plugins will allow translation into existing development environments and tools for GPGPUs and FPGAs. As programming continues to migrate from being the task of the general programmer into the hands of domain specialists, the broader impact of this research is lowering the performance-related knowledge burden on the domain specialist, and more generally, supporting the use of specialists by pushing the state-of-the-art in DSL design and implementation. The principal intellectual merit of this research is the challenge of creating the generic DSL that supports post-hoc and resource-aware on-the-fly refactoring on user models. This new DSL will inform future high-performance DSL compilers, and open opportunities for customized DSLs and the domain specialist working together to find implementations that execute correctly and quickly.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Gill",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew Gill",
   "pi_email_addr": "andygill@ku.edu",
   "nsf_id": "000516934",
   "pi_start_date": "2014-02-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Kansas Center for Research Inc",
  "inst_street_address": "2385 IRVING HILL RD",
  "inst_street_address_2": "",
  "inst_city_name": "LAWRENCE",
  "inst_state_code": "KS",
  "inst_state_name": "Kansas",
  "inst_phone_num": "7858643441",
  "inst_zip_code": "660457563",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "KS01",
  "org_lgl_bus_name": "UNIVERSITY OF KANSAS CENTER FOR RESEARCH INC",
  "org_prnt_uei_num": "SSUJB3GSH8A5",
  "org_uei_num": "SSUJB3GSH8A5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Kansas Center for Research Inc",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "KS",
  "perf_st_name": "Kansas",
  "perf_zip_code": "660457568",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "KS01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7943",
   "pgm_ref_txt": "PROGRAMMING LANGUAGES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0119",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001920DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 216005.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 118499.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 106980.0
  },
  {
   "fund_oblg_fiscal_yr": 2019,
   "fund_oblg_amt": 111698.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-97d6486b-7fff-79fc-d352-e38935f5f5d0\"> </span></p>\n<p dir=\"ltr\"><span>Physicists, meteorologists, and other users of high-performance computing want to write large and complicated models and systems that execute correctly and quickly. Compute-expensive machine learning models are the backbone of the recent improvements in artificial intelligence. These models and systems are allowing us to understand our world better and explore solutions to hard problems.</span></p>\n<p dir=\"ltr\"><span>To make efficient use of computing resources, historically a non-computing specialist needs to, for all intents and purposes, become a proficient computer scientist. The fundamental problem is that a non-computing specialist is not necessarily aware of available options for optimizing their model. Domain Specific Languages (DSLs) are one possible solution to this knowledge gap, because DSLs can encode the necessary knowledge required to map programs onto high-performance targets, at the same time providing a customized environment in the parlance of the domain specialist. In machine learning, PyTorch and TensorFlow are examples of such DSLs.</span></p>\n<p dir=\"ltr\"><span>This project investigated the underlying mechanisms for generating such DSLs for high-performance computing. We used the functional programming language Haskell as a tool to build implementations, but the ideas are completely general and reusable. Specifically, we developed a general framework called the Remote Monad, and realized it using both publicly available libraries, and as a well-documented design pattern. DSLs built around the remote monad allow capture of code fragments, as well as remote execution on acceleration platforms such as GPUs. We built several research examples, and systematically investigated the challenges of mapping in the presence of locally constrained resources, a key problem in high-performance computing.</span></p>\n<p dir=\"ltr\"><span>The ideas generated in this project, and related initiatives, have been adopted internally by industry, and used by such companies as Meta and Google. As compute demands continue to increase, we propose that using the Remote Monad design pattern provides a robust and principled way of giving access to solutions in the high-performance space.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/14/2022<br>\n\t\t\t\t\tModified by: Andrew&nbsp;Gill</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nPhysicists, meteorologists, and other users of high-performance computing want to write large and complicated models and systems that execute correctly and quickly. Compute-expensive machine learning models are the backbone of the recent improvements in artificial intelligence. These models and systems are allowing us to understand our world better and explore solutions to hard problems.\nTo make efficient use of computing resources, historically a non-computing specialist needs to, for all intents and purposes, become a proficient computer scientist. The fundamental problem is that a non-computing specialist is not necessarily aware of available options for optimizing their model. Domain Specific Languages (DSLs) are one possible solution to this knowledge gap, because DSLs can encode the necessary knowledge required to map programs onto high-performance targets, at the same time providing a customized environment in the parlance of the domain specialist. In machine learning, PyTorch and TensorFlow are examples of such DSLs.\nThis project investigated the underlying mechanisms for generating such DSLs for high-performance computing. We used the functional programming language Haskell as a tool to build implementations, but the ideas are completely general and reusable. Specifically, we developed a general framework called the Remote Monad, and realized it using both publicly available libraries, and as a well-documented design pattern. DSLs built around the remote monad allow capture of code fragments, as well as remote execution on acceleration platforms such as GPUs. We built several research examples, and systematically investigated the challenges of mapping in the presence of locally constrained resources, a key problem in high-performance computing.\nThe ideas generated in this project, and related initiatives, have been adopted internally by industry, and used by such companies as Meta and Google. As compute demands continue to increase, we propose that using the Remote Monad design pattern provides a robust and principled way of giving access to solutions in the high-performance space.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/14/2022\n\n\t\t\t\t\tSubmitted by: Andrew Gill"
 }
}