{
 "awd_id": "1422987",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Cognitive models of the acquisition of vowels in context",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 240000.0,
 "awd_amount": 240000.0,
 "awd_min_amd_letter_date": "2014-08-18",
 "awd_max_amd_letter_date": "2015-06-15",
 "awd_abstract_narration": "Models of infant language acquisition allow researchers to test hypotheses about how infants learn their native language from speech.  This project focuses on how infants determine which sounds are meaningful in their language and concentrates on co-articulation, a process in which sounds are produced differently depending on qualities of neighboring sounds. Co-articulation is closely tied to the historical creation, change, and loss of sound patterns in language.  As a result, the models developed in this project further our understanding of both infant language acquisition and historical language change. Knowing how different aspects of the learning process interact can also help pinpoint deficits that might underlie developmental language disorders. Finally, building models of language acquisition that can work on speech data can potentially help create speech recognition technology that adapts robustly to novel languages, accents, and domains of discourse in the same way that human learners do.\r\n \r\nA series of unsupervised phonetic learning models are created to examine how learners take into account co-articulation.  Whereas previous models of phonetic learning have categorized each sound independently, these models begin with a set of constraints that capture possible dependences between sounds, formalized using a Markov random field. They then learn from the data which constraints characterize a particular language.  Recordings of child-directed speech from the CHILDES database are annotated through forced alignment to serve as test corpora for comparing the newly developed models with previous models. The project yields a rigorous evaluation of where existing models fall short, a new framework for accounting for phonetic variability, and corpora to support the development and evaluation of future phonetic learning models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Micha",
   "pi_last_name": "Elsner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Micha Elsner",
   "pi_email_addr": "elsner.14@osu.edu",
   "nsf_id": "000635045",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "Department of Linguistics",
  "perf_city_name": "Columbus",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 77123.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 162877.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-71c8ac46-7fff-633d-26b3-65ae7886a4ca\"> </span></p>\n<p dir=\"ltr\"><span>Although all linguists believe that babies learn to talk by listening to their parents, it is still not clear exactly how the learning process works. One challenging aspect of language learning is that some differences are meaningful (the different vowels in the words \"bit\" and \"bat\" change the identity of the word), whereas others are not (you can say the word \"bit\" faster or slower, but it is still the same word). Researchers used to believe that babies could pick out meaningful differences in two ways: first, where a difference was meaningful, the sounds would form two distinct categories (one for \"bit\" and one for \"bat\") with a gap between them, while meaningless variation would lie on a continuum. Second, even if this was not the case in the original data, it would be possible to \"normalize\" the data by subtracting out the influences of factors like speech rate--- listeners could mentally \"slow down\" the fast speech so that it corresponded to the slow speech, and then check if there was still a difference. Our results cast doubt on this explanation of how learning works, but we offer some other possibilities that may work better.</span></p>\n<p dir=\"ltr\"><span>We study these problems primarily by conducting computer simulations of proposed learning mechanisms. If the mechanism works, the computer will be able to learn the correct sound categories--- otherwise, it will learn too many. This is exactly what we found when testing a standard categorization model on samples of American English and of Japanese. The model learns correctly from careful speech recorded under controlled conditions, but does not learn from real speech.</span></p>\n<p dir=\"ltr\"><span>In one line of followup studies, we looked carefully at Japanese vowels, which fall into \"long\" and \"short\" categories. We looked at recordings of parents to understand the data that babies learn from, and at recordings of toddlers to see what they learn. The parents' speech does not have a clear gap between the long and short categories. Moreover, trying to normalize the data to compensate for effects like how quickly the speaker talks, or what word they are saying, does not make a gap appear. We show mathematically that some previous theories of normalization cannot work for realistic speech, because they assume incorrectly that both types of vowel will be equally likely in any context. On the other hand, we show that, by the age of two, toddlers have learned which words have long versus short vowels. It is clear that the previous theory does not account for their successful learning. We propose a new theory of how learning might work: by looking at how the proportion of longer versus shorter vowels varies as a function of context, babies might notice that there are two categories without needing to find a gap.</span></p>\n<p dir=\"ltr\"><span>In another line of inquiry, we explore more powerful computer learning mechanisms called \"neural networks\". Neural networks can learn complex interactions between many different dimensions of a signal, and so could potentially learn to integrate evidence for the identity of a speech sound with evidence about its context, how fast the speaker is talking, and other mediating factors. This makes them highly effective in speech recognition programs. However, speech recognizers learn from audio recordings which have been \"labeled\" with human-written transcripts; babies do not have access to transcripts of the words they are trying to learn. We showed that trying to memorize and repeat back the audio recording was enough to force the network to learn some meaningful properties of speech, like whether it was listening to a vowel or a consonant. But so far, our results still fall short of human performance.</span></p>\n<p dir=\"ltr\"><span>Finally, we looked at a particular source of differences between particular speakers: social context. Speakers may sound different than one another because of where they live, their cultural background, and other factors based on who they are. Previous research has found that four and five-year-olds are sensitive to these differences between speakers; we find that similar modeling tools explain how 14-month-olds decide which adult speakers to attend to when learning how to pronounce new words.</span></p>\n<p dir=\"ltr\"><span>Along the way, our project made other contributions. We annotated a large database of American English mothers speaking to their babies, using speech recognition to label the sound categories in the recordings. This will help us to understand the kinds of variations babies hear when listening to natural speech. We gave a variety of talks to professional and non-professional audiences, including high school and college students, to explain our findings and discuss how computer simulations can help us to understand language learning. Several graduate students participated in the project, including three women (who are underrepresented in computational science fields). All these students gained experience in writing computer programs, analyzing the results and linking their findings to experimental research on how babies learn.</span></p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/01/2019<br>\n\t\t\t\t\tModified by: Micha&nbsp;Elsner</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nAlthough all linguists believe that babies learn to talk by listening to their parents, it is still not clear exactly how the learning process works. One challenging aspect of language learning is that some differences are meaningful (the different vowels in the words \"bit\" and \"bat\" change the identity of the word), whereas others are not (you can say the word \"bit\" faster or slower, but it is still the same word). Researchers used to believe that babies could pick out meaningful differences in two ways: first, where a difference was meaningful, the sounds would form two distinct categories (one for \"bit\" and one for \"bat\") with a gap between them, while meaningless variation would lie on a continuum. Second, even if this was not the case in the original data, it would be possible to \"normalize\" the data by subtracting out the influences of factors like speech rate--- listeners could mentally \"slow down\" the fast speech so that it corresponded to the slow speech, and then check if there was still a difference. Our results cast doubt on this explanation of how learning works, but we offer some other possibilities that may work better.\nWe study these problems primarily by conducting computer simulations of proposed learning mechanisms. If the mechanism works, the computer will be able to learn the correct sound categories--- otherwise, it will learn too many. This is exactly what we found when testing a standard categorization model on samples of American English and of Japanese. The model learns correctly from careful speech recorded under controlled conditions, but does not learn from real speech.\nIn one line of followup studies, we looked carefully at Japanese vowels, which fall into \"long\" and \"short\" categories. We looked at recordings of parents to understand the data that babies learn from, and at recordings of toddlers to see what they learn. The parents' speech does not have a clear gap between the long and short categories. Moreover, trying to normalize the data to compensate for effects like how quickly the speaker talks, or what word they are saying, does not make a gap appear. We show mathematically that some previous theories of normalization cannot work for realistic speech, because they assume incorrectly that both types of vowel will be equally likely in any context. On the other hand, we show that, by the age of two, toddlers have learned which words have long versus short vowels. It is clear that the previous theory does not account for their successful learning. We propose a new theory of how learning might work: by looking at how the proportion of longer versus shorter vowels varies as a function of context, babies might notice that there are two categories without needing to find a gap.\nIn another line of inquiry, we explore more powerful computer learning mechanisms called \"neural networks\". Neural networks can learn complex interactions between many different dimensions of a signal, and so could potentially learn to integrate evidence for the identity of a speech sound with evidence about its context, how fast the speaker is talking, and other mediating factors. This makes them highly effective in speech recognition programs. However, speech recognizers learn from audio recordings which have been \"labeled\" with human-written transcripts; babies do not have access to transcripts of the words they are trying to learn. We showed that trying to memorize and repeat back the audio recording was enough to force the network to learn some meaningful properties of speech, like whether it was listening to a vowel or a consonant. But so far, our results still fall short of human performance.\nFinally, we looked at a particular source of differences between particular speakers: social context. Speakers may sound different than one another because of where they live, their cultural background, and other factors based on who they are. Previous research has found that four and five-year-olds are sensitive to these differences between speakers; we find that similar modeling tools explain how 14-month-olds decide which adult speakers to attend to when learning how to pronounce new words.\nAlong the way, our project made other contributions. We annotated a large database of American English mothers speaking to their babies, using speech recognition to label the sound categories in the recordings. This will help us to understand the kinds of variations babies hear when listening to natural speech. We gave a variety of talks to professional and non-professional audiences, including high school and college students, to explain our findings and discuss how computer simulations can help us to understand language learning. Several graduate students participated in the project, including three women (who are underrepresented in computational science fields). All these students gained experience in writing computer programs, analyzing the results and linking their findings to experimental research on how babies learn.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/01/2019\n\n\t\t\t\t\tSubmitted by: Micha Elsner"
 }
}