{
 "awd_id": "1414931",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Teaching Computers to Follow Verbal Instructions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Weng-keen Wong",
 "awd_eff_date": "2013-07-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 514356.0,
 "awd_amount": 514356.0,
 "awd_min_amd_letter_date": "2014-04-09",
 "awd_max_amd_letter_date": "2015-09-17",
 "awd_abstract_narration": "The goal of this research is to develop techniques that will permit a computer or robot to learn from examples to carry out multipart tasks specified in natural language on behalf of a user.  It will study each of these components in isolation, but a significant focus will be on integrating them into a coherent system.  The project will also leverage this technology to provide an entry point to educate non- or pre-computer science students about the capabilities and utility of computers as tools.\r\n\r\nOur approach uses three main subcomponents, each of which requires innovative research to solve its portion of the overall problem.  In addition, the integrated architecture is a novel contribution of this work.  The three components are (1) recognizing intention from observed behavior using extensions of inverse reinforcement learning, (2) translating instructions to task specifications using novel techniques in the area of natural language processing, and (3) creating generalized task specifications to match user intentions using probabilistic methods for creating and managing abstractions.\r\n\r\nThe goal of the work is develop technology for an improved ability for human users to interact with intelligent agents, the incorporation of novel AI research insights and activities into education and outreach activities, and the development of resources for the AI educator community.  In addition to permitting intelligent agents to be developed and trained in the future for a broad range of complex application domains, the interactive agents that we will develop will be used for outreach and student learning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Littman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Littman",
   "pi_email_addr": "mlittman@cs.brown.edu",
   "nsf_id": "000210482",
   "pi_start_date": "2014-04-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029121929",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 514356.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 8.5px Helvetica; color: #424242} -->\n<p class=\"p1\">The goal of this project was to develop techniques that will permit a computer or robot to learn from examples to carry out multipart tasks specified in natural language (English) on behalf of a user. Our approach used three main subcomponents: (1) recognizing intention from observed behavior using extensions of inverse reinforcement learning (IRL), (2) translating instructions to task specifications using novel techniques in the area of natural language processing (NLP), and (3) creating generalized task specifications to match user intentions using probabilistic methods for creating and managing task abstractions (TA). These subcomponents are closely integrated via a probabilistic model that permits evidence from training examples to be combined with evidence from linguistic annotations, across multiple tasks, to form appropriate task abstractions, linguistic models, and goal representations. The learning system uses a shared representation that is based on our previously developed object-oriented Markov decision process (MDP) representation, and has been tested on several simulated and physical robot domains.</p>\n<p class=\"p1\">Our novel IRL algorithm allows arbitrary mappings from state features to rewards to be learned. It uses a modular design so that any new regression algorithm can be integrated into the system. The probabilistic model permits the use of different linguistic models as well as different forms of feedback from the human user. We also developed new option discovery methods for object-oriented MDP domains based on previous techniques that were developed for discrete state domains. These new methods enable abstraction and knowledge transfer in more complex domains. The framework also supports transfer of task learning across different domains with different action spaces, including transfer from simulated environments to a physical mobile robot.</p>\n<p class=\"p1\">As part of the project, we created an algorithmic decision-making library we call the Brown/UMBC Reinforcement-Learning and Planning Library (or BURLAP). Our software was we built &nbsp;on top of BURLAP, but the BURLAP system itself has since been used by a variety of projects and different universities and also in advanced classes on machine learning and artificial intelligence.</p>\n<p class=\"p1\">Our work has been presented in several workshop and conference papers, as well as a AAAI student abstract and an IJCAI doctoral consortium presentation. The work has also been presented in keynote and other invited presentations by the PIs at a variety of top research venues.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/05/2016<br>\n\t\t\t\t\tModified by: Michael&nbsp;L&nbsp;Littman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to develop techniques that will permit a computer or robot to learn from examples to carry out multipart tasks specified in natural language (English) on behalf of a user. Our approach used three main subcomponents: (1) recognizing intention from observed behavior using extensions of inverse reinforcement learning (IRL), (2) translating instructions to task specifications using novel techniques in the area of natural language processing (NLP), and (3) creating generalized task specifications to match user intentions using probabilistic methods for creating and managing task abstractions (TA). These subcomponents are closely integrated via a probabilistic model that permits evidence from training examples to be combined with evidence from linguistic annotations, across multiple tasks, to form appropriate task abstractions, linguistic models, and goal representations. The learning system uses a shared representation that is based on our previously developed object-oriented Markov decision process (MDP) representation, and has been tested on several simulated and physical robot domains.\nOur novel IRL algorithm allows arbitrary mappings from state features to rewards to be learned. It uses a modular design so that any new regression algorithm can be integrated into the system. The probabilistic model permits the use of different linguistic models as well as different forms of feedback from the human user. We also developed new option discovery methods for object-oriented MDP domains based on previous techniques that were developed for discrete state domains. These new methods enable abstraction and knowledge transfer in more complex domains. The framework also supports transfer of task learning across different domains with different action spaces, including transfer from simulated environments to a physical mobile robot.\nAs part of the project, we created an algorithmic decision-making library we call the Brown/UMBC Reinforcement-Learning and Planning Library (or BURLAP). Our software was we built  on top of BURLAP, but the BURLAP system itself has since been used by a variety of projects and different universities and also in advanced classes on machine learning and artificial intelligence.\nOur work has been presented in several workshop and conference papers, as well as a AAAI student abstract and an IJCAI doctoral consortium presentation. The work has also been presented in keynote and other invited presentations by the PIs at a variety of top research venues.\n\n\t\t\t\t\tLast Modified: 12/05/2016\n\n\t\t\t\t\tSubmitted by: Michael L Littman"
 }
}