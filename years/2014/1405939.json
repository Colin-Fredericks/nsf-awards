{
 "awd_id": "1405939",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "II-New: A Cluster of Nodes with 32 Cores and 256-GB Memory to Enable Many-Core Systems Research and Education",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 286300.0,
 "awd_amount": 286300.0,
 "awd_min_amd_letter_date": "2014-07-31",
 "awd_max_amd_letter_date": "2014-07-31",
 "awd_abstract_narration": "Research and education in many-core computing systems are of importance to the NSF CRI program as well as to the research community. This project targets performance, energy consumption, and scalability of many-core systems, which are important for the computer industry. The team is committed to releasing the research artifacts of the project as open-source software to be used by the research community as well.  This project will benefit graduate student research and help educational activities in undergraduate and graduate curricula. The project will support outreach activities sponsored by various centers at Purdue University via the involvement of the team in Purdue Computing Research Institute's High Performance Computing workshops, for example.\r\n\r\nThis infrastructure will support research and education efforts in multiple areas: computer architecture, compilers, high-performance cloud computing, and run-times for managed languages. Computer architects will explore optimizations for performance, programmability and power of many-core architectures, on-chip networks, and disk optimizations. Compiler writers will explore shared memory optimizations and their scalability targeting shared-memory applications for distributed memory machines, and techniques to transform seemingly irregular memory access patterns into regular and parallel computations and memory accesses. Run-time researchers will pursue parallel garbage collection of large garbage-collected heaps and associated scalability issues. High performance computing researchers will explore the performance overhead of virtualization and cloud computing for cluster workloads, along with mechanisms for reducing overhead.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Terani",
   "pi_last_name": "Vijaykumar",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Terani N Vijaykumar",
   "pi_email_addr": "vijay@ecn.purdue.edu",
   "nsf_id": "000337724",
   "pi_start_date": "2014-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Antony",
   "pi_last_name": "Hosking",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Antony L Hosking",
   "pi_email_addr": "hosking@cs.umass.edu",
   "nsf_id": "000107257",
   "pi_start_date": "2014-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vijay",
   "pi_last_name": "Pai",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Vijay S Pai",
   "pi_email_addr": "vpai@purdue.edu",
   "nsf_id": "000184842",
   "pi_start_date": "2014-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mithuna",
   "pi_last_name": "Thottethodi",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Mithuna S Thottethodi",
   "pi_email_addr": "mithuna@purdue.edu",
   "nsf_id": "000253970",
   "pi_start_date": "2014-07-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Milind",
   "pi_last_name": "Kulkarni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Milind Kulkarni",
   "pi_email_addr": "milind@purdue.edu",
   "nsf_id": "000549148",
   "pi_start_date": "2014-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072017",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 286300.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>This project includes</span>&nbsp;research and education efforts in computer architecture, compilers, run-times for managed languages, and distributed systems. The architecture research explore optimizations for performance, programmability and power of multicores, many-core architectures, and datacenters. The compiler writers explore techniques to transform seemingly irregular memory access patterns into regular and parallel computations and memory accesses. The run-time researchers pursue parallel garbage collection of large garbage-collected heaps and associated scalability issues. Specific efforts and their significant results are:&nbsp;</p>\n<ul>\n<li>&nbsp;unscaling of clock frequency to tackle slowing of Dennard's Scaling; 15% throughput improvement in many-core systems where voltage scaling has stopped (exceeding previously published \"dark silicon performance limit\"),</li>\n<li>exploiting value locality for soft-error tolerance; 75% soft-error coverage at 10% performance and 25% power overheads whereas redundancy-based schemes incur 80% power overhead,</li>\n<li>a&nbsp; novel 3-D cache architecture to reduce on-chip tag overhead while converting 3-D bandwidth advantage into performance; for under 1-MB of on-chip overhead, our&nbsp; 256-MB 3-D &nbsp;DRAM cache performs 15% better than the best previous design with similar on-chip tag.</li>\n<li>a novel cost-effective distributed system architecture for causal consistency; reduce the &nbsp;cost of a causally-consistent geo-replicated data store by 28-37% via partial replication while achieving the same performance as full replication,&nbsp;</li>\n<li>power and performance optimization of MapReduce via stratified sampling; improve average MapReduce perrformance by 40% while maintaining per-key error within 1%,</li>\n<li>optimizing datacenter power by exploiting latency tail in online data intensive applications; reduce datacenter energy by&nbsp;15% and 40% at 90% and 30% datacenter loading,</li>\n<li>a novel processing-near-memory (PNM) architectures for Big Data Machine learning;&nbsp;<span>improves performance and energy over GPGPU and a \"sea of simple MIMD cores\", respectively,&nbsp; by 145% and 20% and 37% and 34%&nbsp;</span><span>&nbsp;when all the three architectures have the same number of cores, on-die memory, and die-stacked bandwidth.&nbsp;</span></li>\n<li>addressing message buffer management and flow control for RDMA in datacenters;&nbsp;Our RDMA architecture either reduces buffer memory by three orders of magnitude under little programmer effort or achieves same buffer memory at much less programmer burden,</li>\n<li><span><span>implementing nested transactions for Java; our XJ prototype achieves good performance,&nbsp;</span><br /></span></li>\n<li><span>multicore scaling for garbage collection;&nbsp;<span>our&nbsp; implementation achieves scalable perofrmance,</span></span></li>\n<li><span><span><span>development of the machine-checked proof for a real-time concurrent collector, allowing parallelized execution of the proof script,</span><br /></span></span></li>\n<li><span><span><span><span>scalable global routing for HPC; achieves s<span>calable, global, Optimal-bandwidth via application-specific routing,&nbsp;</span></span><br /></span></span></span></li>\n<li><span><span><span>optimizing off-chip traffic of convolutional neural networks (CNNs) via&nbsp; a novel tiling strategy;&nbsp;<span>provably-optimal tiling for CNNs using a given on-chip cache capacity (2-10x fewer off-chip misses), and&nbsp;</span><br /></span></span></span></li>\n<li><span><span><span><span><span>new compiler optimizations of irregular applications led to performance improvements of up to 10x on data mining applications, and 70% for tree traversal applications like compiler passes.&nbsp;</span><br /></span></span></span></span></li>\n</ul>\n<p><span>This infrastructure has supported &nbsp;the &nbsp;research of more than eight graduate students who are being trained in one or more of computer architecture, compilers, distributed systems, and runtime systems via the above-mentioned efforts.&nbsp;&nbsp;<span>As part of their 'senior design project', an undergraduate team of four developed a DNN (deep neural network) based software infrastructure to automatically track student attendance in classrooms. The analysis of the large dataset for training and evaluation was facilitated by the CRI infrastructure. We expect continued participation of undergraduate students in this activity over the next few semesters.&nbsp;</span></span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/06/2017<br>\n\t\t\t\t\tModified by: T.&nbsp;N&nbsp;Vijaykumar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project includes research and education efforts in computer architecture, compilers, run-times for managed languages, and distributed systems. The architecture research explore optimizations for performance, programmability and power of multicores, many-core architectures, and datacenters. The compiler writers explore techniques to transform seemingly irregular memory access patterns into regular and parallel computations and memory accesses. The run-time researchers pursue parallel garbage collection of large garbage-collected heaps and associated scalability issues. Specific efforts and their significant results are: \n\n unscaling of clock frequency to tackle slowing of Dennard's Scaling; 15% throughput improvement in many-core systems where voltage scaling has stopped (exceeding previously published \"dark silicon performance limit\"),\nexploiting value locality for soft-error tolerance; 75% soft-error coverage at 10% performance and 25% power overheads whereas redundancy-based schemes incur 80% power overhead,\na  novel 3-D cache architecture to reduce on-chip tag overhead while converting 3-D bandwidth advantage into performance; for under 1-MB of on-chip overhead, our  256-MB 3-D  DRAM cache performs 15% better than the best previous design with similar on-chip tag.\na novel cost-effective distributed system architecture for causal consistency; reduce the  cost of a causally-consistent geo-replicated data store by 28-37% via partial replication while achieving the same performance as full replication, \npower and performance optimization of MapReduce via stratified sampling; improve average MapReduce perrformance by 40% while maintaining per-key error within 1%,\noptimizing datacenter power by exploiting latency tail in online data intensive applications; reduce datacenter energy by 15% and 40% at 90% and 30% datacenter loading,\na novel processing-near-memory (PNM) architectures for Big Data Machine learning; improves performance and energy over GPGPU and a \"sea of simple MIMD cores\", respectively,  by 145% and 20% and 37% and 34%  when all the three architectures have the same number of cores, on-die memory, and die-stacked bandwidth. \naddressing message buffer management and flow control for RDMA in datacenters; Our RDMA architecture either reduces buffer memory by three orders of magnitude under little programmer effort or achieves same buffer memory at much less programmer burden,\nimplementing nested transactions for Java; our XJ prototype achieves good performance, \n\nmulticore scaling for garbage collection; our  implementation achieves scalable perofrmance,\ndevelopment of the machine-checked proof for a real-time concurrent collector, allowing parallelized execution of the proof script,\n\nscalable global routing for HPC; achieves scalable, global, Optimal-bandwidth via application-specific routing, \n\noptimizing off-chip traffic of convolutional neural networks (CNNs) via  a novel tiling strategy; provably-optimal tiling for CNNs using a given on-chip cache capacity (2-10x fewer off-chip misses), and \n\nnew compiler optimizations of irregular applications led to performance improvements of up to 10x on data mining applications, and 70% for tree traversal applications like compiler passes. \n\n\n\nThis infrastructure has supported  the  research of more than eight graduate students who are being trained in one or more of computer architecture, compilers, distributed systems, and runtime systems via the above-mentioned efforts.  As part of their 'senior design project', an undergraduate team of four developed a DNN (deep neural network) based software infrastructure to automatically track student attendance in classrooms. The analysis of the large dataset for training and evaluation was facilitated by the CRI infrastructure. We expect continued participation of undergraduate students in this activity over the next few semesters. \n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/06/2017\n\n\t\t\t\t\tSubmitted by: T. N Vijaykumar"
 }
}