{
 "awd_id": "1420897",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Dynamic Attractor Computing: A Novel Computational Approach Applied Towards Temporal Pattern and Speech Recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2014-10-01",
 "awd_exp_date": "2017-09-30",
 "tot_intn_awd_amt": 399698.0,
 "awd_amount": 399698.0,
 "awd_min_amd_letter_date": "2014-08-18",
 "awd_max_amd_letter_date": "2014-08-18",
 "awd_abstract_narration": "Harnessing the brain's computational strategies has been a long sought objective of computational neuroscience and machine learning. One reason this goal has remained elusive is that most neurocomputational frameworks have not effectively captured a fundamental computational feature of the brain: the ability to seamlessly encode, represent, and processes temporal information. The current project seeks to address this shortcoming by using the neural dynamics inherent to recurrent neural networks to generate temporal patterns and process temporal information. \r\n\r\nThe ability to generate the fine motor patterns necessary to play the piano or parse the complex temporal structure of speech, are but two examples of the human brain's sophisticated ability to generate and process complex temporal patterns. Notably, both these examples also illustrate an additional feature of the brain's computational abilities: \"temporal warping.\" We can play the same musical piece at different speeds, or understand speech spoken at slow or fast rates. The mechanisms underlying the brain's ability to process temporal information in a flexible and temporally invariant fashion are a key focus of the current proposal. Recent theoretical and experimental studies have favored the view that the brain does not have sampling rates, time bins or explicit delay lines; but rather encodes time and the temporal features of stimuli through the internal dynamics of recurrent neural networks. The computational potential of these recurrent neural network models, however, has been limited for two reasons: 1) while it is well established that the recurrent connections of neural circuits are plastic, it has proven challenging to incorporate plasticity into simulated recurrent neural network models; 2) the dynamic regimes with the most computational potential are precisely those that also exhibit chaos--voiding much of their computational potential because the dynamics is not reproducible across trials. Building on a novel framework, this project tunes the weights of the recurrent connections in a manner that \"tames\" the chaotic dynamics of recurrent networks. The approach creates locally stable trajectories (dynamic attractors) which provide a novel and potentially powerful computational approach that can elegantly encode temporal information, and retain internal memories of recent events. Of particular relevance to the current project is to demonstrate that these networks can produce families of similar neural trajectories that flow at different speeds, thus allowing the network to generate the same complex motor pattern at different speeds. This project will also determine if the principles of dynamic attractors and time warping can be applied in the domain of sensory processing, using speech recognition as a test bed for the brain's ability to discriminate complex spatiotemporal stimuli.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dean",
   "pi_last_name": "Buonomano",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Dean V Buonomano",
   "pi_email_addr": "dbuono@ucla.edu",
   "nsf_id": "000109468",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "695 Charles Young Dr S",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951763",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 399698.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The human brain effortlessly performs very complex computations, such as recognizing and producing speech. Speech is composed of spatiotemporal patterns&mdash;changing patterns of frequency (pitch) over time&mdash;and one characteristic of human pattern recognition is the ability to recognize the same word or song played at different speeds&mdash;a phenomenon referred to as temporal scaling. The research preformed in the current project demonstrates how recurrent neural network models can perform complex computations such as speech recognition and temporal scaling. In these models simple units representing neurons are connected to each other in a recurrent fashion, and these connections are defined by a certain synaptic weight (the strength of a connection: the ability of one unit to influence the other). The research in this project describes a way to adjust to the weights of the units in these recurrent neural networks in a manner that the network can perform a complex computation: recognize spoken digits, classify them, and report the output by &ldquo;handwriting&rdquo; these digits. Thus the same network performs a sensory and motor task (see Figure).</p>\n<p>A key concept in the underlying research is that brain is an inherently dynamical organ, and as such information is not stored in static patterns, such as the memory stored on a computer hard drive, but in time-changing patterns of neural activity (&ldquo;neural trajectories&rdquo;). The notion is that the &ldquo;code&rdquo; or representation of a spoken word is not in the static levels of activity of a given population of neurons, but in the temporal pattern of activity&mdash;if we think of the activity pattern of neurons as represent a point in 3D space, information is not stored by the location of a point but by the path it takes&mdash;e.g., the different trajectories a downhill skier can take would represent different words). And the skier can take the same trajectory at different speeds, but it is the same trajectory. Similarly, our research suggests that words heard at different speeds activate the same pattern of neurons but this pattern unfolds at different speeds.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/17/2017<br>\n\t\t\t\t\tModified by: Dean&nbsp;V&nbsp;Buonomano</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1420897/1420897_10334447_1510946741208_Fig1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1420897/1420897_10334447_1510946741208_Fig1--rgov-800width.jpg\" title=\"Trained RNNs perform a sensorimotor spoken-to-handwritten digit transcription task.\"><img src=\"/por/images/Reports/POR/2017/1420897/1420897_10334447_1510946741208_Fig1--rgov-66x44.jpg\" alt=\"Trained RNNs perform a sensorimotor spoken-to-handwritten digit transcription task.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Trained RNNs perform a sensorimotor spoken-to-handwritten digit transcription task.</div>\n<div class=\"imageCredit\">Goudar & Buonomano</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dean&nbsp;V&nbsp;Buonomano</div>\n<div class=\"imageTitle\">Trained RNNs perform a sensorimotor spoken-to-handwritten digit transcription task.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe human brain effortlessly performs very complex computations, such as recognizing and producing speech. Speech is composed of spatiotemporal patterns&mdash;changing patterns of frequency (pitch) over time&mdash;and one characteristic of human pattern recognition is the ability to recognize the same word or song played at different speeds&mdash;a phenomenon referred to as temporal scaling. The research preformed in the current project demonstrates how recurrent neural network models can perform complex computations such as speech recognition and temporal scaling. In these models simple units representing neurons are connected to each other in a recurrent fashion, and these connections are defined by a certain synaptic weight (the strength of a connection: the ability of one unit to influence the other). The research in this project describes a way to adjust to the weights of the units in these recurrent neural networks in a manner that the network can perform a complex computation: recognize spoken digits, classify them, and report the output by \"handwriting\" these digits. Thus the same network performs a sensory and motor task (see Figure).\n\nA key concept in the underlying research is that brain is an inherently dynamical organ, and as such information is not stored in static patterns, such as the memory stored on a computer hard drive, but in time-changing patterns of neural activity (\"neural trajectories\"). The notion is that the \"code\" or representation of a spoken word is not in the static levels of activity of a given population of neurons, but in the temporal pattern of activity&mdash;if we think of the activity pattern of neurons as represent a point in 3D space, information is not stored by the location of a point but by the path it takes&mdash;e.g., the different trajectories a downhill skier can take would represent different words). And the skier can take the same trajectory at different speeds, but it is the same trajectory. Similarly, our research suggests that words heard at different speeds activate the same pattern of neurons but this pattern unfolds at different speeds.\n\n\t\t\t\t\tLast Modified: 11/17/2017\n\n\t\t\t\t\tSubmitted by: Dean V Buonomano"
 }
}