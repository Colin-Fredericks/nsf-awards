{
 "awd_id": "1450933",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Studying Emotional Responses of Children with Autism in Interaction with Facially Expressive Social Robots",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 80000.0,
 "awd_amount": 80000.0,
 "awd_min_amd_letter_date": "2014-08-22",
 "awd_max_amd_letter_date": "2014-08-22",
 "awd_abstract_narration": "Recent research suggests that children with autism exhibit certain positive social behaviors when interacting with robots when compared to their peers that do not interact with robots. These investigations suggest that interaction with robots may be a promising approach for rehabilitation of children with autism. Despite the positive signs and observations reported in the literature, research on using social robots for autism therapy is in its infancy and more fundamental and exploratory studies should be carried out before one can study the effectiveness and clinical impact of robots for autism therapy.\r\n\r\nThis project explores several research questions on emotional and behavioral response of children with autism when interacting with a facially expressive robot.  Representative questions include: (1) Do children with autism recognize facial expressions shown by an expressive robot similarly to typically developed (TD) children? (2) Should the robot use body gesture and movement in conjunction with facial expression to better convey emotion to children with autism? (3) How do physiological responses of Autistic and TD children vary in interaction with a human versus a robot? To address these questions, the research team plans to recruit a group of children diagnosed with High Functioning Autism and TD children to interact with a facially expressive robot. The sessions are video recorded and analyzed using state-of-the-art automatic computer vision algorithms for facial expression recognition. Children's physiological and emotional responses are also acquired using Electro Dermal Activity (EDA) sensors. The EDA signals are used to create a computational affective model to describe and compare the arousal (excitement) level of the two groups of children in interaction with the robot.  Results from this study can be used in designing robot-assisted therapy for human mental and social disabilities such as designing interventions for children suffering from autism, depression, or attention disorder. In addition, the affective model can impact the measurement of emotional conditions in experimental non-interactive protocols. Data generated in this project are a rich source for further analysis by other researchers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mohammad",
   "pi_last_name": "Mahoor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mohammad Mahoor",
   "pi_email_addr": "mmahoor@du.edu",
   "nsf_id": "000511471",
   "pi_start_date": "2014-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Denver",
  "inst_street_address": "2199 S UNIVERSITY BLVD RM 222",
  "inst_street_address_2": "",
  "inst_city_name": "DENVER",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3038712000",
  "inst_zip_code": "802104711",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "CO01",
  "org_lgl_bus_name": "UNIVERSITY OF DENVER",
  "org_prnt_uei_num": "WCUGNQQ8DZU1",
  "org_uei_num": "WCUGNQQ8DZU1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Denver",
  "perf_str_addr": "2390 S York Street",
  "perf_city_name": "Denver",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "802105345",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "CO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 80000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Children with Autism Spectrum Disorder (ASD) experience limited abilities in recognizing non-verbal elements of social interactions such as facial expressions of emotions. They also show deficiencies in imitating facial expressions in social situations. In this grant, we focused on studying the ability of children with ASD in recognizing facial expressions and imitating the expressions particularly in interaction with a social humanoid robot (we used a robot b/c children with ASD can socially communicate with a robot than a human).&nbsp; We designed and developed three studies, first to evaluate the ability of children with ASD in recognizing facial expressions that are presented to them with different methods (i.e. robot versus video), and second to determine the effect of various methods on the facial expression imitation performance of children with ASD using Reinforcement Learning (RL).</p>\n<p>In the first study, we compared the facial expression recognition ability of children with ASD with Typically Developing (TD) children using a rear-projected social robot. Overall, the results did not show a significant difference between the performance of the ASD and groups in expression recognition. The study revealed the significant effect of increasing the expression intensity level on the expression recognition accuracy. The study also revealed both groups perform significantly worse in recognizing fear and disgust expressions.</p>\n<p>The second study focused on the effect of context on the facial expression recognition ability of children with ASD compared to their TD peers. The result of this study showed a higher general performance of TD children compared to the ASD group. Within the TD group, the fear expression and in the ASD group the sadness expression were recognized with the lowest accuracy compared to the average accuracy of other expressions. The result of this study did not show any difference between groups; however, we found that there is a significant effect of different background categories in both groups. It means, we found a significantly higher recognition accuracy for the negative backgrounds compared to positive backgrounds in 20% intensity for the fear and sadness expressions.</p>\n<p>In the third study, we designed an active learning method using the RL algorithm to identify and adapt based on the individual differences in expression imitation in response to different conditions. We implemented the RL to first, identify the effective imitation method based on an individual&rsquo;s performance and preference; and second, to make an online adaptation and adjustment based on the effective method for each individual. The result of this study showed that the active learning method could successfully identify and adjust the session based on the participant&rsquo;s strength and preference. The results also showed that each participant responded differently to each method in general and for each expression.&nbsp;</p>\n<p>Another objective of the grant was to develop an automated computational method and study its efficacy for emotion classification in children using electrodermal activity (EDA) signals. We used the time-frequency analysis of raw EDAs using the complex Morlet (C-Morlet) wavelet function. We used a publically available dataset (created by a research group at Georgia Tech) that includes a set of multimodal recordings of social and communicative behavior as well as EDA recordings of 100 children younger than 30 months old. The dataset was annotated by two experts to extract the time sequence corresponding to three main emotions including &ldquo;Joy&rdquo;, &ldquo;Boredom&rdquo;, and &ldquo;Acceptance&rdquo;. The annotation process was performed considering the synchronicity between the children&rsquo;s facial expressions and the EDA time sequences. Various experiments were conducted on the annotated EDA signals to classify emotions using a support vector machine (SVM) classifier. Our experimental results show that emotion can be recognized using the EDA signals, thought the recognition accuracy may vary from subject to subject (between 48% to 86%).&nbsp; In addition, some emotions are more difficult to recognize (e.g. \"Acceptance\") than other emotions using the EDA signals.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/01/2018<br>\n\t\t\t\t\tModified by: Mohammad&nbsp;Mahoor</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nChildren with Autism Spectrum Disorder (ASD) experience limited abilities in recognizing non-verbal elements of social interactions such as facial expressions of emotions. They also show deficiencies in imitating facial expressions in social situations. In this grant, we focused on studying the ability of children with ASD in recognizing facial expressions and imitating the expressions particularly in interaction with a social humanoid robot (we used a robot b/c children with ASD can socially communicate with a robot than a human).  We designed and developed three studies, first to evaluate the ability of children with ASD in recognizing facial expressions that are presented to them with different methods (i.e. robot versus video), and second to determine the effect of various methods on the facial expression imitation performance of children with ASD using Reinforcement Learning (RL).\n\nIn the first study, we compared the facial expression recognition ability of children with ASD with Typically Developing (TD) children using a rear-projected social robot. Overall, the results did not show a significant difference between the performance of the ASD and groups in expression recognition. The study revealed the significant effect of increasing the expression intensity level on the expression recognition accuracy. The study also revealed both groups perform significantly worse in recognizing fear and disgust expressions.\n\nThe second study focused on the effect of context on the facial expression recognition ability of children with ASD compared to their TD peers. The result of this study showed a higher general performance of TD children compared to the ASD group. Within the TD group, the fear expression and in the ASD group the sadness expression were recognized with the lowest accuracy compared to the average accuracy of other expressions. The result of this study did not show any difference between groups; however, we found that there is a significant effect of different background categories in both groups. It means, we found a significantly higher recognition accuracy for the negative backgrounds compared to positive backgrounds in 20% intensity for the fear and sadness expressions.\n\nIn the third study, we designed an active learning method using the RL algorithm to identify and adapt based on the individual differences in expression imitation in response to different conditions. We implemented the RL to first, identify the effective imitation method based on an individual?s performance and preference; and second, to make an online adaptation and adjustment based on the effective method for each individual. The result of this study showed that the active learning method could successfully identify and adjust the session based on the participant?s strength and preference. The results also showed that each participant responded differently to each method in general and for each expression. \n\nAnother objective of the grant was to develop an automated computational method and study its efficacy for emotion classification in children using electrodermal activity (EDA) signals. We used the time-frequency analysis of raw EDAs using the complex Morlet (C-Morlet) wavelet function. We used a publically available dataset (created by a research group at Georgia Tech) that includes a set of multimodal recordings of social and communicative behavior as well as EDA recordings of 100 children younger than 30 months old. The dataset was annotated by two experts to extract the time sequence corresponding to three main emotions including \"Joy\", \"Boredom\", and \"Acceptance\". The annotation process was performed considering the synchronicity between the children?s facial expressions and the EDA time sequences. Various experiments were conducted on the annotated EDA signals to classify emotions using a support vector machine (SVM) classifier. Our experimental results show that emotion can be recognized using the EDA signals, thought the recognition accuracy may vary from subject to subject (between 48% to 86%).  In addition, some emotions are more difficult to recognize (e.g. \"Acceptance\") than other emotions using the EDA signals. \n\n\t\t\t\t\tLast Modified: 12/01/2018\n\n\t\t\t\t\tSubmitted by: Mohammad Mahoor"
 }
}