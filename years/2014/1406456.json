{
 "awd_id": "1406456",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Variable Selection via Measurement Error Modeling",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2014-07-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2014-06-10",
 "awd_max_amd_letter_date": "2018-07-03",
 "awd_abstract_narration": "Technological advances make it possible to collect and store enormous amounts of data.  The implications for how businesses run (online retailing, precision manufacturing), how science is conducted (environmental science, climate monitoring and modeling, astrophysics), and how governments operate (health care delivery, public safety, homeland security) are comparably enormous.  However, for many particular uses of massive data sets, not all of the available information is relevant; and a key first step in many big-data explorations is the identification of the most relevant subset of information required to address the particular question at hand. For example, when studying certain diseases, it is essential to first identify the most relevant risk factors and precursors. The more information that is available, the more difficult it is to identify the most relevant subset for a particular purpose, akin to the problem of finding a needle in a haystack.  Just as a threshing machine separates the wheat from the chaff, the research in this project will develop statistical methods that separate the relevant information (the wheat) from that information that is not relevant (the chaff), thereby enabling more focused and productive analyses of large data sets.\r\n\r\nMore specifically, the research in this project will develop methods for identifying the subset of information that is most relevant when the data are used to derive a regression/prediction model or algorithm. In this case the problem of separating the wheat from the chaff is the often-studied problem of variable selection. This project will develop a new approach to variable selection that differs conceptually from existing approaches and promises to offer new insights as well as new methodologies. The new approach is based on the intuitive and universally relevant idea that a non-informative variable can be contaminated with noise without a subsequent loss of predictive power; whereas any amount of contamination to an informative predictor necessarily entails a loss of predictive power. Starting from the noise-contamination idea of variable informativeness, the project shows how the theory, methods, and algorithms from the field of measurement error modeling can be used to develop new methods of variable selection applicable across the full spectrum of model- and algorithmic-based prediction methods. Instances of the general strategy will be studied and refined for several particular prediction methods such as: nonparametric regression (based on splines, or kernels, etc.); classification/regression trees; dimension reduction methods (principle components, partial least squares, SIR, etc.); bagged or model-averaged predictors of any type; and ridge regression.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Leonard",
   "pi_last_name": "Stefanski",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Leonard A Stefanski",
   "pi_email_addr": "stefansk@ncsu.edu",
   "nsf_id": "000117049",
   "pi_start_date": "2014-06-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "2701 Sullivan Drive",
  "perf_city_name": "Raleigh",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276958203",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><tt>Finding relationships in data is at the heart of scientific       discovery. Hardly a day goes by when there is not a report in the media about a statistical relationship of interest. For       example, coffee (caffeine) has been linked to anxiety, insomnia,       high blood pressure, etc. An education researcher might be       interested in the relationship between school performance and a       number of factors such as attendance, class size, socioeconomic       status, and so on. Online advertisers want to know the       relationship of advertisement click-through rates and features of       the advertisement (location, style, color, etc.) and features of       the potential customer (age, sex, education, income, etc.).&nbsp; In       studies of the genetic determinants of disease there may be one,       or a few, or several hundreds of thousands of genes related to       a particular disorder. In all of these examples, and especially in       the last one, the statistical challenge lies in finding which of many (hundrds, thousands, millions) possible features are       linked to the phenomenon under study.&nbsp; It is often the case that       in large data sets, separating the useful information from that       which is not useful is the key step in advancing science. In the       important and common case of the machine learning technique regression modeling, the problem of       separating the useful predictive information from that which is       not useful is known as variable selection.&nbsp; <br /> <br /> The objective of this research was to develop and study a novel       approach for the identification of important predictive variables       from the often large set of potential predictive variables in the       context of regression modeling.&nbsp; The key idea is that if a       variable is not useful for prediction, then contaminating it with       noise does not further lessen its predictive value; whereas       contamination with noise of a useful predictive variable does       lessen its predictive value. Thus contamination by noise followed       by an assessment of a variable's predictive value can discriminate       between useful and non-useful predictive variables. </tt><tt><tt>The         noise-addition approach has the advantage of being         model-independent in the sense that it works the same way         regardless of the type of regression model being fit to the         data. <br /> </tt><br /> The primary outcome of this research is a proof-of-concept that       the noise-addition method for identifying useful predictors works       well and is competitive with other more conventional methods of       variable selection.&nbsp; This was established first, and main ideas       developed, by developing the approach for variable selection in       nonparametric classification. The notion of a measurement error       model selection likelihood was defined and developed and then       applied to nonparametric classification resulting in a new       kernel-based classifier with LASSO-like shrinkage and       variable-selection properties. A second major milestone of the       research was adaptation of the approach in kernel regression       resulting in the measurement error kernel regression operator       (MEKRO) that has the same form as the Nadaraya?Watson kernel       estimator, but optimizes a measurement error model selection       likelihood to estimate the kernel bandwidths. Evidence of the       adaptability and generality of noise-addition strategy is provided       by two secondary research methodologies developed from the core       project ideas. These are a method for automatic structure recovery       in additive regression models, and a method for nonparametric       screening of predictor variables.&nbsp; <br /> <br /> The manner in which a problem is conceived and framed influences       the solutions found. For the last 20 years, parameter shrinkage       and penalization (via the LASSO and L1 and related penalties) has       been the dominant mode of framing variable selection problems and       thus also of deriving solutions. The major intellectual merits of       the completed project stem from the benefits derived by thinking       about variable selection not as parameter shrinkage, but rather       via the noise-addition strategy described above. The       noise-addition approach provides researchers a new way of thinking       about variable selection problems, which in turn will lead to new       and effective variable selection methods for very general       prediction methods.&nbsp; <br /> <br /> The completed project provides researchers (applied, theoretical,       and methodological), including graduate students in statistics, a       new way to think about variable informativeness, and also the       basic tools and techniques for developing noise-addition-based       methods of variable selection tailored for particular       applications. The project contributed to education, workforce       development, and peer mentoring through the collaboration of       senior and junior researchers. Key benefits included the       development of research skills and experience, and lecturing and       communication skills.</tt></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/27/2019<br>\n\t\t\t\t\tModified by: Leonard&nbsp;A&nbsp;Stefanski</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nFinding relationships in data is at the heart of scientific       discovery. Hardly a day goes by when there is not a report in the media about a statistical relationship of interest. For       example, coffee (caffeine) has been linked to anxiety, insomnia,       high blood pressure, etc. An education researcher might be       interested in the relationship between school performance and a       number of factors such as attendance, class size, socioeconomic       status, and so on. Online advertisers want to know the       relationship of advertisement click-through rates and features of       the advertisement (location, style, color, etc.) and features of       the potential customer (age, sex, education, income, etc.).  In       studies of the genetic determinants of disease there may be one,       or a few, or several hundreds of thousands of genes related to       a particular disorder. In all of these examples, and especially in       the last one, the statistical challenge lies in finding which of many (hundrds, thousands, millions) possible features are       linked to the phenomenon under study.  It is often the case that       in large data sets, separating the useful information from that       which is not useful is the key step in advancing science. In the       important and common case of the machine learning technique regression modeling, the problem of       separating the useful predictive information from that which is       not useful is known as variable selection.  \n \n The objective of this research was to develop and study a novel       approach for the identification of important predictive variables       from the often large set of potential predictive variables in the       context of regression modeling.  The key idea is that if a       variable is not useful for prediction, then contaminating it with       noise does not further lessen its predictive value; whereas       contamination with noise of a useful predictive variable does       lessen its predictive value. Thus contamination by noise followed       by an assessment of a variable's predictive value can discriminate       between useful and non-useful predictive variables. The         noise-addition approach has the advantage of being         model-independent in the sense that it works the same way         regardless of the type of regression model being fit to the         data. \n \n The primary outcome of this research is a proof-of-concept that       the noise-addition method for identifying useful predictors works       well and is competitive with other more conventional methods of       variable selection.  This was established first, and main ideas       developed, by developing the approach for variable selection in       nonparametric classification. The notion of a measurement error       model selection likelihood was defined and developed and then       applied to nonparametric classification resulting in a new       kernel-based classifier with LASSO-like shrinkage and       variable-selection properties. A second major milestone of the       research was adaptation of the approach in kernel regression       resulting in the measurement error kernel regression operator       (MEKRO) that has the same form as the Nadaraya?Watson kernel       estimator, but optimizes a measurement error model selection       likelihood to estimate the kernel bandwidths. Evidence of the       adaptability and generality of noise-addition strategy is provided       by two secondary research methodologies developed from the core       project ideas. These are a method for automatic structure recovery       in additive regression models, and a method for nonparametric       screening of predictor variables.  \n \n The manner in which a problem is conceived and framed influences       the solutions found. For the last 20 years, parameter shrinkage       and penalization (via the LASSO and L1 and related penalties) has       been the dominant mode of framing variable selection problems and       thus also of deriving solutions. The major intellectual merits of       the completed project stem from the benefits derived by thinking       about variable selection not as parameter shrinkage, but rather       via the noise-addition strategy described above. The       noise-addition approach provides researchers a new way of thinking       about variable selection problems, which in turn will lead to new       and effective variable selection methods for very general       prediction methods.  \n \n The completed project provides researchers (applied, theoretical,       and methodological), including graduate students in statistics, a       new way to think about variable informativeness, and also the       basic tools and techniques for developing noise-addition-based       methods of variable selection tailored for particular       applications. The project contributed to education, workforce       development, and peer mentoring through the collaboration of       senior and junior researchers. Key benefits included the       development of research skills and experience, and lecturing and       communication skills.\n\n\t\t\t\t\tLast Modified: 11/27/2019\n\n\t\t\t\t\tSubmitted by: Leonard A Stefanski"
 }
}