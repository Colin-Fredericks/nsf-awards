{
 "awd_id": "1436574",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "A Methodology for Reliable Risk Assessment with Error-prone Electronic Medical Records Using Optimal Design of Experiments Concepts",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 399999.0,
 "awd_amount": 399999.0,
 "awd_min_amd_letter_date": "2014-07-14",
 "awd_max_amd_letter_date": "2014-07-14",
 "awd_abstract_narration": "Enormous healthcare resources are devoted to compiling electronic medical record (EMR) databases that are increasingly integrated and rich in patient population and that offer potential for identifying disease risk factors via statistical analyses to predict the disease risk as a function of various factors (e.g., clinical and demographic) for that patient. Unfortunately, the disease event data may have high miscoding error rates, due to the fact that clerical personnel with limited training are employed to enter their codes. For example, in one EMR database of patients with cardiac workup, after reviewing a random sample of cases recorded as sudden cardiac arrest events, the error rate was found to be 75 percent. In order to take such errors into account and avoid developing unreliable risk assessment models, it is imperative that a doctor perform chart reviews to validate a sample of cases and determine whether the events were true events. However, the number of chart reviews is limited due to the high cost of doctors' time. The objective of this research is to develop a methodology for judiciously and efficiently selecting validation cases for maximum information content, which will allow reliable disease risk assessment even with highly error-prone EMR data. The anticipated benefits to the health and well-being of society are substantial, as this research will allow the enormous untapped potential of large EMR databases to be more fully utilized for discovering new disease risk factors. It is also anticipated that this research can be extended to other big-data application domains for extracting reliable information from large quantities of data that are of questionable quality.\r\n\r\nLarge electronic medical record (EMR) databases offer potential for developing clinical hypotheses and identifying disease risk associations by fitting statistical models that predict the likelihood that a patient develops a particular condition as a function of various predictor variables (e.g., clinical, phenotypical, and demographic data) for that patient. Although the predictor variable data are often recorded reliably, the event data may have high error rates due to ICD-9 disease miscoding. To avoid developing unreliable risk assessment models, previous research used random validation sampling to estimate error probabilities for correcting biases in logistic regression models fit to the entire data, which is both inefficient and unreliable with high error rates. In contrast, this research will develop a validation sampling and reliable risk assessment (VSRRA) methodology for judiciously designing a validation sample. The intellectual underpinning is the observed analogy between VSRRA and traditional design of experiments (DOE), whereby validating the response for one error-prone case in VSRRA corresponds to conducting one experimental run in DOE. In light of this analogy, this research will develop (i) suitable VSRRA design criteria based on the Fisher information matrix for the model parameters and Bayesian counterparts such as posterior and preposterior parameter covariance matrices, applicable to a broad class of generalized linear models commonly used in medical risk studies; (ii) heuristic and more exact hybrid algorithms for selecting the validation sample to optimize the design criteria; (iii) multistage, sequential versions of the VSRRA sampling strategies that refine the designs based on information that is learned along the way, as new cases are validated; and (iv) methods that determine whether and how the full set of unvalidated data can be reliably included, along with the validated data, in the final model fitting. A fundamental tenet of data analysis is that carefully designed experimental studies produce far more reliable statistical conclusions than observational studies. Likewise, it is anticipated that the DOE-based VSRRA methodology will allow far more reliable disease risk assessment and hypotheses generation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Apley",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel W Apley",
   "pi_email_addr": "apley@northwestern.edu",
   "nsf_id": "000329132",
   "pi_start_date": "2014-07-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sanjay",
   "pi_last_name": "Mehrotra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sanjay Mehrotra",
   "pi_email_addr": "mehrotra@northwestern.edu",
   "nsf_id": "000303258",
   "pi_start_date": "2014-07-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University at Chicago",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "KG76WYENL5K1",
  "org_uei_num": "KG76WYENL5K1"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "TECH 2145 Sheridan Rd, C150",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602080834",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "178700",
   "pgm_ele_name": "SERVICE ENTERPRISE SYSTEMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "076E",
   "pgm_ref_txt": "SERVICE ENTERPRISE SYSTEMS"
  },
  {
   "pgm_ref_code": "078E",
   "pgm_ref_txt": "ENTERPRISE DESIGN & LOGISTICS"
  },
  {
   "pgm_ref_code": "8023",
   "pgm_ref_txt": "Health Care Enterprise Systems"
  },
  {
   "pgm_ref_code": "9147",
   "pgm_ref_txt": "GENERIC TECHNOL FOR MANUFACTURING CELLS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 399999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp; &nbsp; Enormous healthcare resources are devoted to compiling electronic medical record (EMR) databases. Because they are becoming increasingly integrated and comprehensive across patient populations, they offer tremendous potential for developing clinical hypotheses and identifying disease risk associations by fitting statistical models that predict the likelihood that a patient develops a particular condition, as a function of an observed set of predictor variables (e.g., clinical, phenotypical, and demographic data) for that patient. Although the predictor variable data are often recorded reliably, the event data may have a high percentage of errors (miscoding), due to the fact that clerical personnel with limited training are employed to enter the ICD-9 codes for the diseases. For example, in a testbed database of 23,041 cases of patients with cardiac workup, when reviewing a random sample of 20 cases recorded as sudden cardiac arrest events, it was discovered that only 5 of these were true events, which constitutes a 75% error rate. In order to take such errors into account and avoid developing unreliable risk assessment models, it is imperative that a doctor perform chart reviews to validate a sample of cases and determine whether the events are true events. However, the number of chart reviews is limited due to the high cost of doctors&rsquo; time. The main goal of this research was to develop a methodology for judiciously and efficiently selecting validation cases for maximum information content, which allows reliable disease risk assessment even with highly error-prone EMR data. The benefits to the health and well-being of society are substantial, as this research allows the enormous untapped potential of large EMR databases to be more fully utilized for discovering new disease risk factors.&nbsp;</p>\n<p>&nbsp; &nbsp; Previous research had used random validation sampling to estimate error probabilities for correcting biases in logistic regression models fit to the entire unvalidated data, which is both inefficient and unreliable when the error rates are high. In contrast, this research developed a validation sampling and reliable risk assessment (VSRRA) methodology for judiciously designing a validation sample. The intellectual underpinning of the new approach is an analogy between VSRRA and traditional design of experiments (DOE), whereby validating the response for one error-prone case in VSRRA corresponds to conducting one experimental run in DOE. In light of this analogy, this research has created a DOE-based VSRRA methodology by developing (i) effective VSRRA design criteria based on the Fisher information matrix for the model parameters, which are applicable to a broad class of generalized linear models commonly used in medical risk studies; (ii) heuristic and more exact hybrid algorithms for selecting the validation sample to optimize the design criteria; and (iii) robust VSRRA algorithms that are model agnostic in the sense that no prior knowledge of the predictive relationship is needed when selecting the validation sample.</p>\n<p>&nbsp; &nbsp; This research has also extended these concepts to more general situations in which the goal is to select a designed sample from an existing database of predictor variables, to which a predictive statistical model will be fit, which has broad societal impact both within and beyond the healthcare field. One specific application targeted in this research is controlled trial studies, which are ubiquitously used to investigate the effect of a medical treatment that is dependent on a set of patient covariates. Existing controlled study design approaches have relied primarily on randomized patient sampling and allocation to treatment and control group. However, when covariate data for a large cohort group of patients have already been collected and are available in a database, this research has shown that judiciously designed treatment/control sampling provide far more accurate estimates of the covariate-dependent effects of the treatment. In particular, this research has developed a new methodology, referred to as designed sampling from databases (DSD), that uses DOE concepts to optimally select the samples. When applied to controlled trial studies, the DSD approach selects the patients for the treatment and control samples upfront, based on their covariate values, in a manner that optimizes the information content in the data and results in the most accurate estimation of the treatment effects. The many other DSD application domains include marketing and manufacturing process improvement. Two female students supported by this grant have successfully completed their PhDs, and a third female student is near completion of her PhD.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/20/2018<br>\n\t\t\t\t\tModified by: Daniel&nbsp;W&nbsp;Apley</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n    Enormous healthcare resources are devoted to compiling electronic medical record (EMR) databases. Because they are becoming increasingly integrated and comprehensive across patient populations, they offer tremendous potential for developing clinical hypotheses and identifying disease risk associations by fitting statistical models that predict the likelihood that a patient develops a particular condition, as a function of an observed set of predictor variables (e.g., clinical, phenotypical, and demographic data) for that patient. Although the predictor variable data are often recorded reliably, the event data may have a high percentage of errors (miscoding), due to the fact that clerical personnel with limited training are employed to enter the ICD-9 codes for the diseases. For example, in a testbed database of 23,041 cases of patients with cardiac workup, when reviewing a random sample of 20 cases recorded as sudden cardiac arrest events, it was discovered that only 5 of these were true events, which constitutes a 75% error rate. In order to take such errors into account and avoid developing unreliable risk assessment models, it is imperative that a doctor perform chart reviews to validate a sample of cases and determine whether the events are true events. However, the number of chart reviews is limited due to the high cost of doctors? time. The main goal of this research was to develop a methodology for judiciously and efficiently selecting validation cases for maximum information content, which allows reliable disease risk assessment even with highly error-prone EMR data. The benefits to the health and well-being of society are substantial, as this research allows the enormous untapped potential of large EMR databases to be more fully utilized for discovering new disease risk factors. \n\n    Previous research had used random validation sampling to estimate error probabilities for correcting biases in logistic regression models fit to the entire unvalidated data, which is both inefficient and unreliable when the error rates are high. In contrast, this research developed a validation sampling and reliable risk assessment (VSRRA) methodology for judiciously designing a validation sample. The intellectual underpinning of the new approach is an analogy between VSRRA and traditional design of experiments (DOE), whereby validating the response for one error-prone case in VSRRA corresponds to conducting one experimental run in DOE. In light of this analogy, this research has created a DOE-based VSRRA methodology by developing (i) effective VSRRA design criteria based on the Fisher information matrix for the model parameters, which are applicable to a broad class of generalized linear models commonly used in medical risk studies; (ii) heuristic and more exact hybrid algorithms for selecting the validation sample to optimize the design criteria; and (iii) robust VSRRA algorithms that are model agnostic in the sense that no prior knowledge of the predictive relationship is needed when selecting the validation sample.\n\n    This research has also extended these concepts to more general situations in which the goal is to select a designed sample from an existing database of predictor variables, to which a predictive statistical model will be fit, which has broad societal impact both within and beyond the healthcare field. One specific application targeted in this research is controlled trial studies, which are ubiquitously used to investigate the effect of a medical treatment that is dependent on a set of patient covariates. Existing controlled study design approaches have relied primarily on randomized patient sampling and allocation to treatment and control group. However, when covariate data for a large cohort group of patients have already been collected and are available in a database, this research has shown that judiciously designed treatment/control sampling provide far more accurate estimates of the covariate-dependent effects of the treatment. In particular, this research has developed a new methodology, referred to as designed sampling from databases (DSD), that uses DOE concepts to optimally select the samples. When applied to controlled trial studies, the DSD approach selects the patients for the treatment and control samples upfront, based on their covariate values, in a manner that optimizes the information content in the data and results in the most accurate estimation of the treatment effects. The many other DSD application domains include marketing and manufacturing process improvement. Two female students supported by this grant have successfully completed their PhDs, and a third female student is near completion of her PhD.\n\n \n\n\t\t\t\t\tLast Modified: 12/20/2018\n\n\t\t\t\t\tSubmitted by: Daniel W Apley"
 }
}