{
 "awd_id": "1407820",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Better efficiency, better forecasting, better accuracy: A new light on the dependence structure in high frequency data",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 123851.0,
 "awd_amount": 123851.0,
 "awd_min_amd_letter_date": "2014-08-10",
 "awd_max_amd_letter_date": "2014-08-10",
 "awd_abstract_narration": "Recent years have seen an explosion in the availability and size of data in many areas of endeavor; the phenomenon is often referred to as big data. This project is concerned with a particular form of such data, namely high frequency data (HFD), where series of observations can see new data to arrive in fractions of milliseconds. HFD occurs in medicine, in finance and economics, in certain recordings relating to the environment, and perhaps in other areas. Research is often concerned with how to turn this data into knowledge, and this is where the current project will help. Specifically, the project has discovered a new way to look at the dependence relationships between the parameters governing the state of the HFD system. The new dependence structure permits the borrowing of information from adjacent time periods, and also from other series if one has a panel of data. The consequences of this new approach are being explored by the project. The research produces transformational improvements in the statistical handling of high frequency data. \r\n\r\nThe new way to look at dependence involves the representation of series of ordinary integrals with the help of stochastic integrals. This permits the use of high frequency regression techniques to connect the information in adjacent time intervals. It is achieved without altering current models. This has far-reaching consequences, leading to more efficient estimators, better prediction, and, in terms of accuracy, a more systematic treatment of the estimation of standard errors. Model selection will also be greatly facilitated. The methodology does not depend on either time or panel size being large; neither does it depend on assumptions such as stationarity of the data series. All the new dependence relationships can be consistently estimated from high frequency data inside the relevant time periods. Efficiency gains are at the very least close to 50%, and thus existing efficiency bounds will become irrelevant. It is expected that this approach will form a new paradigm for high frequency data. In addition to developing a general theory, the project is concerned with applications to financial data. Applied quantities of interest include realized daily volatility, correlations, leverage effect, volatility risk, fraction of jumps, and so on. We also work on applications to risk management, forecasting, and portfolio management. More precise estimators, with improved standard errors, will be useful in all these areas of finance. The results are of interest to main-street investors, regulators and policymakers, and the results are entirely in the public domain. The dependence structure also has application in other areas of research that have high frequency data, including medicine, neural science, and turbulence.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lan",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lan Zhang",
   "pi_email_addr": "lanzhang@uic.edu",
   "nsf_id": "000313767",
   "pi_start_date": "2014-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Chicago",
  "inst_street_address": "809 S MARSHFIELD AVE M/C 551",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3129962862",
  "inst_zip_code": "606124305",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "IL07",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "",
  "org_uei_num": "W8XEAJDKMXH3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Chicago",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606077124",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "IL07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 123851.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Background: Recent years have seen an explosion in the availability  of high frequency financial data. The data is usually called  tick-by-tick data: every quote and every transaction is recorded. The  data arrive at high frequency: for some stocks, there are more than a  hundred thousand transactions per day. However, this vast amount of  rapidly arriving data is not free of noise and its information content  is often masked. An important question which faces us, as a society, is  how to turn this data into knowledge and uncover its informational  architecture. This project has made progress in this direction.</p>\n<p>Big data often lead to more uncertainty, which makes it hard to  reveal the underlying structure. In the first part of this project  (\"Assessment of Uncertainty in High Frequency Data: The Observed  Asymptotic Variance&rdquo;), the PIs has made contributions on three fronts:</p>\n<p>1) the PIs have discovered a new way to look at the dependence  relationships governing the latent structures in the data. Each latent  element is estimated from high frequency data series. Then the  dependence structure between latent elements in adjacent time periods is  established. The new structure will permit the \"borrowing\" of  information from adjacent time periods, and also from other data series  if one has a panel of data. The methodology does not depend on either  time or panel size being large, neither does it depend on mathematical  assumptions such as stable structure. All the new dependence  relationships can be consistently estimated from high frequency data  inside the relevant time periods. Efficiency gains are at the very least  close to 50%, and thus existing efficiency bounds may become  irrelevant. It is expected that this approach will form a new paradigm  for high frequency data.</p>\n<p>2) the PIs have developed a more systematic treatment of the  uncertainty measure, called &ldquo;observed AVAR&rdquo;. This uncertainty measure is  not restricted by any specific econometric model, and can be  implemented daily or intra-day. The new measure is off the shelf, and  can be applied to various economic and financial quantities.</p>\n<p>3) One application of this project produces an intra-day real-time  risk measure of the market volatility. Recent ten years have witnessed  many episodes of risk on and risk off. It is during the period of highly  volatile market that old-time or in-house measures all break down and  investors lose oversight on how to evaluate risk. Based on the real-time  equity market behavior our approach gives a quantitative evaluation on  market greed and fear, which complements the Market Volatility Index  created by the Chicago Board Options Exchange. The latter is about  market expectations for future volatility.</p>\n<p>In another part of the project, the PIs analyzed behavior of  pre-averaged data. Data averaging is a practice that combines the data  that arrives in a nearby window, for example, within the same minute.  While dealing with intra-day data, this is often the first step that  many researchers and practitioners do, since it reduces the noise and  total data size. The PIs documented the situations when this data  reduction could cause problems, in terms of under-reporting the risk in  market data and obscuring the information in the original trade times.  The paper \"Between data cleaning and inference: Pre-averaging and robust  estimators of the efficient price\" shows how to conduct data reduction  in an appropriate way.</p>\n<p>In a third part of the project, the PIs reported how to assess market  quantity when data arrives in an irregular fashion. The randomness in  how information arrives intra-day is often overlooked in the literature.  In &ldquo;The Algebra of Two Scales Estimation - High Frequency Estimation  that is Robust to Sampling Times&rdquo;, the PIs found that ignoring the  irregular arrival times of the information leads to further  understatement of the market risk. The PIs proceeds to device a  bias-corrected measure of market risk. This new measure is particularly  useful when information arrives in a time that is endogenous to the  system, as in the financial market.</p>\n<p>Broader Impacts: The data are mostly financial, and interest focuses  on quantities like realized daily volatility, correlations, leverage  effect, volatility risk, fraction of jumps, and so on. We also work on  applications to risk management, forecasting, and portfolio management.  More precise estimators, with more precise standard errors, will be  useful in all these areas of finance. In particular, as we have seen in  the recent financial crisis, markets can change abruptly. Better  measurements of market behavior will make early detection possible when  market conditions change, greatly aiding the management of risk. The  results will be of interest to investors, regulators and policymakers.  The dependence structure also has use in other areas of research that  use high frequency data, including neural science, and turbulence.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/15/2018<br>\n\t\t\t\t\tModified by: Lan&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nBackground: Recent years have seen an explosion in the availability  of high frequency financial data. The data is usually called  tick-by-tick data: every quote and every transaction is recorded. The  data arrive at high frequency: for some stocks, there are more than a  hundred thousand transactions per day. However, this vast amount of  rapidly arriving data is not free of noise and its information content  is often masked. An important question which faces us, as a society, is  how to turn this data into knowledge and uncover its informational  architecture. This project has made progress in this direction.\n\nBig data often lead to more uncertainty, which makes it hard to  reveal the underlying structure. In the first part of this project  (\"Assessment of Uncertainty in High Frequency Data: The Observed  Asymptotic Variance\"), the PIs has made contributions on three fronts:\n\n1) the PIs have discovered a new way to look at the dependence  relationships governing the latent structures in the data. Each latent  element is estimated from high frequency data series. Then the  dependence structure between latent elements in adjacent time periods is  established. The new structure will permit the \"borrowing\" of  information from adjacent time periods, and also from other data series  if one has a panel of data. The methodology does not depend on either  time or panel size being large, neither does it depend on mathematical  assumptions such as stable structure. All the new dependence  relationships can be consistently estimated from high frequency data  inside the relevant time periods. Efficiency gains are at the very least  close to 50%, and thus existing efficiency bounds may become  irrelevant. It is expected that this approach will form a new paradigm  for high frequency data.\n\n2) the PIs have developed a more systematic treatment of the  uncertainty measure, called \"observed AVAR\". This uncertainty measure is  not restricted by any specific econometric model, and can be  implemented daily or intra-day. The new measure is off the shelf, and  can be applied to various economic and financial quantities.\n\n3) One application of this project produces an intra-day real-time  risk measure of the market volatility. Recent ten years have witnessed  many episodes of risk on and risk off. It is during the period of highly  volatile market that old-time or in-house measures all break down and  investors lose oversight on how to evaluate risk. Based on the real-time  equity market behavior our approach gives a quantitative evaluation on  market greed and fear, which complements the Market Volatility Index  created by the Chicago Board Options Exchange. The latter is about  market expectations for future volatility.\n\nIn another part of the project, the PIs analyzed behavior of  pre-averaged data. Data averaging is a practice that combines the data  that arrives in a nearby window, for example, within the same minute.  While dealing with intra-day data, this is often the first step that  many researchers and practitioners do, since it reduces the noise and  total data size. The PIs documented the situations when this data  reduction could cause problems, in terms of under-reporting the risk in  market data and obscuring the information in the original trade times.  The paper \"Between data cleaning and inference: Pre-averaging and robust  estimators of the efficient price\" shows how to conduct data reduction  in an appropriate way.\n\nIn a third part of the project, the PIs reported how to assess market  quantity when data arrives in an irregular fashion. The randomness in  how information arrives intra-day is often overlooked in the literature.  In \"The Algebra of Two Scales Estimation - High Frequency Estimation  that is Robust to Sampling Times\", the PIs found that ignoring the  irregular arrival times of the information leads to further  understatement of the market risk. The PIs proceeds to device a  bias-corrected measure of market risk. This new measure is particularly  useful when information arrives in a time that is endogenous to the  system, as in the financial market.\n\nBroader Impacts: The data are mostly financial, and interest focuses  on quantities like realized daily volatility, correlations, leverage  effect, volatility risk, fraction of jumps, and so on. We also work on  applications to risk management, forecasting, and portfolio management.  More precise estimators, with more precise standard errors, will be  useful in all these areas of finance. In particular, as we have seen in  the recent financial crisis, markets can change abruptly. Better  measurements of market behavior will make early detection possible when  market conditions change, greatly aiding the management of risk. The  results will be of interest to investors, regulators and policymakers.  The dependence structure also has use in other areas of research that  use high frequency data, including neural science, and turbulence.\n\n \n\n\t\t\t\t\tLast Modified: 11/15/2018\n\n\t\t\t\t\tSubmitted by: Lan Zhang"
 }
}