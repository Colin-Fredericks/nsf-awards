{
 "awd_id": "1429316",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Acquisition of an Adaptive Data Cluster for Data-intensive Applications in Science and Engineering",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2014-08-29",
 "awd_max_amd_letter_date": "2015-12-08",
 "awd_abstract_narration": "This project, acquiring an adaptive multi-petabyte scalable storage cluster for high-end applications, aims to service multiple research areas such as: genomics, bioinformatics, computer security, digital ethnography, environmental modeling, and computer science. The flexible storage consists of an integrated multi-petabyte disk system with integrated compute capabilities and a scalable tape archive system for data and storage intensive applications. The system allows I/O-limited calculations to be performed directly on the storage nodes (unlike the traditional storage clusters) and can also act as a distributed file system with massive bandwidth to allow CPU-limited calculations to benefit from existing cluster computational resources (unlike typical Hadoop clusters). The instrument enables \r\n-\tLooking deeper in modeling genomes, hyper-extractive economies and phenotyping\r\n-\tBringing greater data capacity tied to computational resourcing,\r\n-\tAttacking grand challenges (e.g., forecasting responses of ecological systems to natural and anthropogenic global and regional change).\r\n-\tDeveloping new algorithms in high-throughput phenotyping, computer security, and genome annotation (hence enabling a new science with the hybrid platform).\r\nThus, the storage cluster constitutes a seminal component for a critical need, a campus-based facility for data immersive computing, not only at the institution, but for the entire state, since the institution currently does not have a central facility with capability for 'big data' high-end computing. \r\n\r\nAs new algorithms are developed for better modeling cyber interactions that can lead to increase the financial and network infrastructure's ability, resiliency, and resistance, multidisciplinary impacts on cybersecurity research are likely to be felt. It can also contribute to train a new generation of researchers in tools and techniques for data-intensive computing and ease their migration to XSEDE when their research needs exceed the local resources. It can contribute to protect the environment when developing better models of the interaction of water, ecology, and economic factors. Moreover, it can enhance and integrate educational efforts at the K-12, undergraduate, and graduate levels in bioinformatics  (e.g., preparation of educational materials, impacting the K-12 and STEM education such as 'It's a BLAST' and GROW workshops for female high school students).  Ultimately, it allows access to community colleges and non-PhD granting institutions to extend big data through and EPSCoR state, enabling many researchers and educators.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Andresen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Andresen",
   "pi_email_addr": "dan@k-state.edu",
   "nsf_id": "000406107",
   "pi_start_date": "2014-08-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Susan",
   "pi_last_name": "Brown",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Susan J Brown",
   "pi_email_addr": "sjbrown@ksu.edu",
   "nsf_id": "000115406",
   "pi_start_date": "2014-08-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Hsu",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "William H Hsu",
   "pi_email_addr": "bhsu@ksu.edu",
   "nsf_id": "000260913",
   "pi_start_date": "2015-12-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Doina",
   "pi_last_name": "Caragea",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Doina Caragea",
   "pi_email_addr": "dcaragea@ksu.edu",
   "nsf_id": "000391976",
   "pi_start_date": "2014-08-29",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Xinming",
   "pi_last_name": "Ou",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xinming Ou",
   "pi_email_addr": "xou@usf.edu",
   "nsf_id": "000189047",
   "pi_start_date": "2014-08-29",
   "pi_end_date": "2015-12-08"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jesse",
   "pi_last_name": "Poland",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Jesse A Poland",
   "pi_email_addr": "jpoland@ksu.edu",
   "nsf_id": "000585994",
   "pi_start_date": "2014-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Kansas State University",
  "inst_street_address": "1601 VATTIER STREET",
  "inst_street_address_2": "103 FAIRCHILD HALL",
  "inst_city_name": "MANHATTAN",
  "inst_state_code": "KS",
  "inst_state_name": "Kansas",
  "inst_phone_num": "7855326804",
  "inst_zip_code": "665062504",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "KS01",
  "org_lgl_bus_name": "KANSAS STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "CFMMM5JM7HJ9"
 },
 "perf_inst": {
  "perf_inst_name": "Kansas State University",
  "perf_str_addr": "2 Fairchild Hall",
  "perf_city_name": "Manhattan",
  "perf_st_code": "KS",
  "perf_st_name": "Kansas",
  "perf_zip_code": "665061103",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "KS01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project has received NSF support for the purchase of an adaptive multi-petabyte scalable storage cluster for high-end applications in science and engineering, which will target research and research training. The flexible storage cluster is an integrated multi-petabyte disk system with integrated compute capabilities for storage intensive applications. Unlike traditional storage clusters, the system will allow I/O-limited calculations to be performed directly on the storage nodes; unlike typical Hadoop clusters, the system can also act as a distributed filesystem with massive bandwidth to allow CPU-limited calculations to benefit from our existing cluster computational resources.</p>\n<p>The goal of this project was to dramatically expand existing research activities in a number of disciplines and multidisciplinary areas, while tightly integrating with the KSU Library to provide scalable curation, archiving, and outreach. The proposed system on the KSU campus provides faculty and students with a critical resource to expand research training, will foster the development of the next generation of research opportunities and exploration in several science and engineering departments, provide an &ldquo;on ramp&rdquo; to XSEDE, and will promote partnerships on a common infrastructure between researchers and educators both internally and externally to KSU.</p>\n<p>Our principle activities have included full system operating status (with the purchase of 24 Dell R730xd servers and accompanying network infrastructure) using the Ceph filesystem, and the development of techniques to allow an updated Hadoop to run on the storage nodes themselves (eliminating the need for separate servers). We added in 2016 two additional fileservers (funded through other sources).&nbsp;</p>\n<p>We have made a concerted effort to help our researchers at Kansas State University to use our Beocat cluster to do bigger science.&nbsp; This includes providing them with a much larger file system using a balanced structure to provide fast access to files, a full backup system, and including long term archival storage. We have also implemented a data transfer node (DTN) based on materials from ESNet, and are implementing a region-wide science DMZ based on the Pacific Research Platform model.</p>\n<p>The system has been foundational for research and outreach. It has supported hundreds of students in big data, bioinformatics, and AI courses, and been a focal point for over 2,500 visitors touring the data center in the last year alone.</p>\n<p>Our researchers have used Beocat for projects like</p>\n<p>&ldquo;Beocat has contributed to &hellip; a collaboration with Jianhan Chen (UMass) to <strong>understand the molecular mechanisms of protein aggregation diseases like Alzheimer's</strong>.&rdquo;</p>\n<p>&ldquo;Beocat and its staff allowed me to advance the genome assembling of winter wheat cultivar &lsquo;Jagger&rsquo;, and building novel bioinformatics pipelines to <strong>predict wheat performance </strong>in the field.&rdquo;</p>\n<p>&ldquo;Through Beocat resources and the support of Beocat staff, my group has recently begun to &hellip; <strong>design therapeutic peptides for lung cancer immunotherapy</strong>.&rdquo;</p>\n<p>&ldquo;Beocat's computational power accelerated our research by enabling several timely implementations of calculation-intensive time-series simulations and visualizations of <strong>building-to-grid performance in Houston, TX</strong>.&rdquo;</p>\n<p>&ldquo;[Beocat] provided us instrumental approaches to use massive storage that we required to <strong>simulate the atmospheric interaction process through meso-scale downscaling dynamics</strong>.&rdquo;</p>\n<p>They also talk about its impact, such as:</p>\n<p>&ldquo;[Beocat] allowed for <strong>interstate collaboration</strong>, and for better utilization of our research code.&rdquo;</p>\n<p>&ldquo;Beocat has also been an important resource for allowing <strong>undergraduates and graduate students</strong> in my lab to perform computational research.&rdquo;</p>\n<p>&ldquo;Beocat offers <strong>an incredible opportunity</strong> for our undergraduate research students to utilize a world class computing resource.&rdquo;</p>\n<p>&ldquo;<strong>Beocat is extremely useful to help handle such large data sets</strong> in term of storage capacity and parallel computing. By using Beocat, time for processing data can be shortened from months to days.&rdquo;</p>\n<p>&ldquo;Once I have explored and learned about Beocat at K-state, my life has changed.<strong> Beocat has become my friend</strong> when it comes to computation.&rdquo;</p>\n<p>In total, we initially moved over 300 TB of data including over 900 user accounts from our previous gluster filesystem to the new ceph filesystem. The data transition took about 4 weeks. During only 5 days of those 4 weeks were users unable to use Beocat due to this data copying. We have also transitioned our virtual server infrastructure to use Ceph for storage.</p>\n<p>Now, at the end of our grant period, that 300TB of storage has grown to nearly 900TB and 1.4M files, which, with replication, has the system at over 80% utilization. The overall size of the system has grown dramatically as well, from 2,500 to 7,800 cores, and from 600 to 900 users. We have also maintained an uptime of 98%, with most of our downtime attributable to teething issues with the cooling in our new data center.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2017<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Andresen</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1429316/1429316_10339534_1511379784036_IMG_3050--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1429316/1429316_10339534_1511379784036_IMG_3050--rgov-800width.jpg\" title=\"Disk drives.\"><img src=\"/por/images/Reports/POR/2017/1429316/1429316_10339534_1511379784036_IMG_3050--rgov-66x44.jpg\" alt=\"Disk drives.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Some of the storage comprising the funded system in action.</div>\n<div class=\"imageCredit\">Daniel Andresen</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Andresen</div>\n<div class=\"imageTitle\">Disk drives.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project has received NSF support for the purchase of an adaptive multi-petabyte scalable storage cluster for high-end applications in science and engineering, which will target research and research training. The flexible storage cluster is an integrated multi-petabyte disk system with integrated compute capabilities for storage intensive applications. Unlike traditional storage clusters, the system will allow I/O-limited calculations to be performed directly on the storage nodes; unlike typical Hadoop clusters, the system can also act as a distributed filesystem with massive bandwidth to allow CPU-limited calculations to benefit from our existing cluster computational resources.\n\nThe goal of this project was to dramatically expand existing research activities in a number of disciplines and multidisciplinary areas, while tightly integrating with the KSU Library to provide scalable curation, archiving, and outreach. The proposed system on the KSU campus provides faculty and students with a critical resource to expand research training, will foster the development of the next generation of research opportunities and exploration in several science and engineering departments, provide an \"on ramp\" to XSEDE, and will promote partnerships on a common infrastructure between researchers and educators both internally and externally to KSU.\n\nOur principle activities have included full system operating status (with the purchase of 24 Dell R730xd servers and accompanying network infrastructure) using the Ceph filesystem, and the development of techniques to allow an updated Hadoop to run on the storage nodes themselves (eliminating the need for separate servers). We added in 2016 two additional fileservers (funded through other sources). \n\nWe have made a concerted effort to help our researchers at Kansas State University to use our Beocat cluster to do bigger science.  This includes providing them with a much larger file system using a balanced structure to provide fast access to files, a full backup system, and including long term archival storage. We have also implemented a data transfer node (DTN) based on materials from ESNet, and are implementing a region-wide science DMZ based on the Pacific Research Platform model.\n\nThe system has been foundational for research and outreach. It has supported hundreds of students in big data, bioinformatics, and AI courses, and been a focal point for over 2,500 visitors touring the data center in the last year alone.\n\nOur researchers have used Beocat for projects like\n\n\"Beocat has contributed to &hellip; a collaboration with Jianhan Chen (UMass) to understand the molecular mechanisms of protein aggregation diseases like Alzheimer's.\"\n\n\"Beocat and its staff allowed me to advance the genome assembling of winter wheat cultivar ?Jagger?, and building novel bioinformatics pipelines to predict wheat performance in the field.\"\n\n\"Through Beocat resources and the support of Beocat staff, my group has recently begun to &hellip; design therapeutic peptides for lung cancer immunotherapy.\"\n\n\"Beocat's computational power accelerated our research by enabling several timely implementations of calculation-intensive time-series simulations and visualizations of building-to-grid performance in Houston, TX.\"\n\n\"[Beocat] provided us instrumental approaches to use massive storage that we required to simulate the atmospheric interaction process through meso-scale downscaling dynamics.\"\n\nThey also talk about its impact, such as:\n\n\"[Beocat] allowed for interstate collaboration, and for better utilization of our research code.\"\n\n\"Beocat has also been an important resource for allowing undergraduates and graduate students in my lab to perform computational research.\"\n\n\"Beocat offers an incredible opportunity for our undergraduate research students to utilize a world class computing resource.\"\n\n\"Beocat is extremely useful to help handle such large data sets in term of storage capacity and parallel computing. By using Beocat, time for processing data can be shortened from months to days.\"\n\n\"Once I have explored and learned about Beocat at K-state, my life has changed. Beocat has become my friend when it comes to computation.\"\n\nIn total, we initially moved over 300 TB of data including over 900 user accounts from our previous gluster filesystem to the new ceph filesystem. The data transition took about 4 weeks. During only 5 days of those 4 weeks were users unable to use Beocat due to this data copying. We have also transitioned our virtual server infrastructure to use Ceph for storage.\n\nNow, at the end of our grant period, that 300TB of storage has grown to nearly 900TB and 1.4M files, which, with replication, has the system at over 80% utilization. The overall size of the system has grown dramatically as well, from 2,500 to 7,800 cores, and from 600 to 900 users. We have also maintained an uptime of 98%, with most of our downtime attributable to teething issues with the cooling in our new data center. \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/22/2017\n\n\t\t\t\t\tSubmitted by: Daniel Andresen"
 }
}