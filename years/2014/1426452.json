{
 "awd_id": "1426452",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Jointly Learning Language and Affordances",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 337792.0,
 "awd_amount": 347622.0,
 "awd_min_amd_letter_date": "2014-08-06",
 "awd_max_amd_letter_date": "2016-08-10",
 "awd_abstract_narration": "The investigators of this project envision a world where robots surround us, in our homes, in our hospitals, and in our factories, helping people by delivering medicine, preparing food, and assembling objects. Achieving this vision requires robots to communicate with people about their needs, and then plan their activities to help meet those needs.  Previous research has addressed these two problems separately, leading to technical solutions that do not work reliably in real-world situations, and to difficulties in human-robot communication.  To solve these problems, we are developing the Physically-Grounded Language with Affordances (PGLA) framework and concentrate our research into two thrusts: 1) enable a robot to observe a patient, then answer a nurse's questions about the patient's activity, and 2) enable a robot to respond to natural language requests in a collaborative cooking task and in a manufacturing setting. We will release our open-source data sets and code, which will have impact in other technical areas beyond robotics, such as computer vision and machine learning.  The results of our proposed research will find direct applications in industries such as manufacturing and assistive robotics.\r\n\r\nThis project takes a probabilistic approach to jointly learn to recognize affordances in the environment and predict associated natural language requests and descriptions.  Since the affordance map is grounded to perceptual data, our robots will learn to robustly manipulate objects in the physical world, respond to natural language commands, and describe their experiences using words.  Our learning approach enables the robot to infer cross-model knowledge from large data sets of people carrying out activities paired with natural language descriptions of the activities, leveraging the strength of each modality to inform the others. Our novel learning algorithms will integrate and learn from multi-domain databases such as the semantic web, visual scenes, and a novel activity database paired with natural language descriptions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stefanie",
   "pi_last_name": "Tellex",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stefanie Tellex",
   "pi_email_addr": "stefie10@cs.brown.edu",
   "nsf_id": "000651585",
   "pi_start_date": "2014-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "CS Dept, 115 Waterman Street",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 337792.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 9830.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focused on methods for learning object affordances and mappings between words and affordances.&nbsp; We created new models for language understanding that maps between words and abstract reward functions given at different levels of abstraction.&nbsp; &nbsp; We also developed new models for perception that takes into account the direction of light rays as well as their intensity (light fields), which allows the robot to reason about reflections and transparent objects.&nbsp; This technology also allows the robot to very precisely manipulate objects.&nbsp; &nbsp;</p>\n<p>The intellectual merit of this project was new models for robotic perception and language understanding.&nbsp; These models enabled the robot to better understand human language and better collaborate with people.&nbsp; Specifically we developed 1) a model for inference in POMDPs with large observation spaces 2) a model for reasoning in MDPs using abstraction and 3) a new model for perception using light fields that allows the robot to precisely manipulate objects and reason about reflections and transparency.</p>\n<p>The broader impact of this project was 1) the training of Ph.D. and undergraduate students 2) a collaboration with the Providence Career and Technical Academy to host summer interns and 3) a graduate course called Topics in Collaborative Robotics.&nbsp; &nbsp;This impact enabled us to share project outcomes with the public as well as by training students.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/15/2018<br>\n\t\t\t\t\tModified by: Stefanie&nbsp;Tellex</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focused on methods for learning object affordances and mappings between words and affordances.  We created new models for language understanding that maps between words and abstract reward functions given at different levels of abstraction.    We also developed new models for perception that takes into account the direction of light rays as well as their intensity (light fields), which allows the robot to reason about reflections and transparent objects.  This technology also allows the robot to very precisely manipulate objects.   \n\nThe intellectual merit of this project was new models for robotic perception and language understanding.  These models enabled the robot to better understand human language and better collaborate with people.  Specifically we developed 1) a model for inference in POMDPs with large observation spaces 2) a model for reasoning in MDPs using abstraction and 3) a new model for perception using light fields that allows the robot to precisely manipulate objects and reason about reflections and transparency.\n\nThe broader impact of this project was 1) the training of Ph.D. and undergraduate students 2) a collaboration with the Providence Career and Technical Academy to host summer interns and 3) a graduate course called Topics in Collaborative Robotics.   This impact enabled us to share project outcomes with the public as well as by training students. \n\n \n\n\t\t\t\t\tLast Modified: 02/15/2018\n\n\t\t\t\t\tSubmitted by: Stefanie Tellex"
 }
}