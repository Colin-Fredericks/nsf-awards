{
 "awd_id": "1427872",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI Development: Enabling Research in Natural Communication with Virtual Tutors, Therapists, and Robotic Companions",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 1300000.0,
 "awd_amount": 1348000.0,
 "awd_min_amd_letter_date": "2014-08-20",
 "awd_max_amd_letter_date": "2017-05-11",
 "awd_abstract_narration": "This project, developing SocioBot-SDS (SocioBot-Spoken Dialog System), an instrument in the form of a robotic character with an emotional response, is expected to advance research involving next generation human-machine interactions. The robotic instrument will be used for therapeutic and educational purposes. Its development will specifically contribute to the research area of perceived speech and visual behaviors. The components of the instrument integrate a unique level of programmability and robustness to the display of human- and non-human-like emotive gestures with conventional orientation control through an articulated neck. The work is expected to accelerate research and development of social robots that can accurately model the dynamics of face-to-face communication with a sensitive and effective human tutor, clinician, or caregiver to a degree unachievable with current instrumentation. The robotic agent builds on advances in computer vision, spoken dialogue systems, character animation and effective computing to conduct dialogues that establish rapport with users producing rich, emotive facial gestures synchronized with prosodic speech generations in response to users' speech and emotions. The instrument represents a new level of integration of emotive capabilities that enable researchers to study socially emotive/robots/agents that can understand spoken language and show emotions and interact, speak, and communicate effectively with people in a natural way (as humans do).\r\n\r\nThe instrument provides an exciting platform for research and training supporting cross-discipline technology. Since the research community would have access to the instrument, research in how to optimize communication among people and avatars might be accelerated through the perception of speech patterns and visual behaviors of those interacting. The instrument represents a new level of integration of emotive capabilities and serves as a platform for designing a new generation of more immersive and effective intelligent tutoring and therapy systems, and robot-assisted therapeutic treatments for human disabilities that include infants at risk for sensory, attention and language delays, as well as adults with mental disabilities.  From an educational perspective, the proposed activities will enhance inter-disciplinary education by involving students at all levels, during and beyond the development of the instrument. The resulting instrument will be freely distributed for researchers to investigate robotic behaviors that lead to immersive and effective applications across a variety of task domains such as teaching students to read, tutoring students in science, conducting speech and language therapy sessions, or providing companionship to elderly individuals in their homes. Moreover, the activities initiated by this development enhance interdisciplinary education, involving students at the undergraduate and graduate levels, during and beyond the development of the instrument.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mohammad",
   "pi_last_name": "Mahoor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mohammad Mahoor",
   "pi_email_addr": "mmahoor@du.edu",
   "nsf_id": "000511471",
   "pi_start_date": "2014-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Wayne",
   "pi_last_name": "Ward",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Wayne H Ward",
   "pi_email_addr": "wayne.ward@colorado.edu",
   "nsf_id": "000264998",
   "pi_start_date": "2014-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ronald",
   "pi_last_name": "Cole",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Ronald A Cole",
   "pi_email_addr": "rcole@boulderlearning.com",
   "nsf_id": "000163558",
   "pi_start_date": "2014-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Juan",
   "pi_last_name": "Wachs",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Juan P Wachs",
   "pi_email_addr": "jpwachs@purdue.edu",
   "nsf_id": "000557038",
   "pi_start_date": "2014-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Denver",
  "inst_street_address": "2199 S UNIVERSITY BLVD RM 222",
  "inst_street_address_2": "",
  "inst_city_name": "DENVER",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3038712000",
  "inst_zip_code": "802104711",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "CO01",
  "org_lgl_bus_name": "UNIVERSITY OF DENVER",
  "org_prnt_uei_num": "WCUGNQQ8DZU1",
  "org_uei_num": "WCUGNQQ8DZU1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Denver",
  "perf_str_addr": "2390 S York Street",
  "perf_city_name": "Denver",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "802105345",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "CO01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 1300000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This&nbsp; MRI-Development project focused on developing SocioBot-SDS (SocioBot-Spoken Dialog System), an instrument in the form of a robotic character with an emotional response for research involving human-machine interactions. The end result of the project is a life-like robotic head that is based on facial animation projected onto a translucent mask capable of showing accurate facial expressions and visual speech. The instrument represents a new level of integration of emotive capabilities that enable researchers to study socially emotive/robots/agents that can understand spoken language and show emotions and interact, speak, and communicate effectively with people in a natural way (as humans do).&nbsp;&nbsp;The instrument has a neck system with three degrees of freedom that can be programmed to show different head gestures. Using this instrument, research on human-robot-interaction with a life-like character has been enabled. The instrument can be programmed so users can have a conversation with the robotic agent. Researchers can study the effect of gaze perception, facial expressions, and lips movements in communication with a life-like robot. The instrument has been used in various researches on using therapeutic robotic agents for people with mental disabilities or maladies such as Autism, Depression, and Dementia.&nbsp;</p>\n<p>The intellectual merits of the project center on the developed hardware and software integrated into the form of an instrument for research on human-robot-interaction with a life-like agent for different applications.</p>\n<p>Broader impacts center on promoting the cross-disciplinary research idea of using social and human-like robotic agents for science tutoring and education as well as assisting people with disabilities such as Autism and Dementia. Furthermore, the instrument has provided the infrastructure allowing graduate and undergraduate students and different research groups on conducting experiments related to HRI and disseminating findings through publications.</p>\n<p>A major impact of this project is patented technology and also producing multiple peer-reviewed conference and journal papers.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/01/2020<br>\n\t\t\t\t\tModified by: Mohammad&nbsp;Mahoor</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis  MRI-Development project focused on developing SocioBot-SDS (SocioBot-Spoken Dialog System), an instrument in the form of a robotic character with an emotional response for research involving human-machine interactions. The end result of the project is a life-like robotic head that is based on facial animation projected onto a translucent mask capable of showing accurate facial expressions and visual speech. The instrument represents a new level of integration of emotive capabilities that enable researchers to study socially emotive/robots/agents that can understand spoken language and show emotions and interact, speak, and communicate effectively with people in a natural way (as humans do).  The instrument has a neck system with three degrees of freedom that can be programmed to show different head gestures. Using this instrument, research on human-robot-interaction with a life-like character has been enabled. The instrument can be programmed so users can have a conversation with the robotic agent. Researchers can study the effect of gaze perception, facial expressions, and lips movements in communication with a life-like robot. The instrument has been used in various researches on using therapeutic robotic agents for people with mental disabilities or maladies such as Autism, Depression, and Dementia. \n\nThe intellectual merits of the project center on the developed hardware and software integrated into the form of an instrument for research on human-robot-interaction with a life-like agent for different applications.\n\nBroader impacts center on promoting the cross-disciplinary research idea of using social and human-like robotic agents for science tutoring and education as well as assisting people with disabilities such as Autism and Dementia. Furthermore, the instrument has provided the infrastructure allowing graduate and undergraduate students and different research groups on conducting experiments related to HRI and disseminating findings through publications.\n\nA major impact of this project is patented technology and also producing multiple peer-reviewed conference and journal papers. \n\n\t\t\t\t\tLast Modified: 01/01/2020\n\n\t\t\t\t\tSubmitted by: Mohammad Mahoor"
 }
}