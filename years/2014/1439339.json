{
 "awd_id": "1439339",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Using Somatosensory Speech And Non-Speech Categories To Test The Brain's General Principles Of Perceptual Learning",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Kurt Thoroughman",
 "awd_eff_date": "2014-09-15",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 271602.0,
 "awd_amount": 271602.0,
 "awd_min_amd_letter_date": "2014-08-14",
 "awd_max_amd_letter_date": "2014-08-14",
 "awd_abstract_narration": "The human brain displays astonishing adaptation to novel types of sensory information. An example of such adaptation is deaf-blind individuals who learned to perceive spoken language through their sense of touch, by placing a hand on the face and throat of someone producing speech. This example tells us that the somatosensory system can carry out speech perception, which is normally thought to be in the domain of hearing. Drs. Maximilian Riesenhuber of Georgetown University and Lynne E. Bernstein of George Washington University along with their multidisciplinary team will use advanced functional magnetic resonance brain imaging (fMRI) and electroencephalography (EEG) to investigate the neural mechanisms underlying the learning of artificial categories and speech categories by the somatosensory system. In their research they are using a novel transducer to present high-dimensional  stimuli to the forearm of participants who are trained on artificial or speech categories. The team is addressing whether perceptual learning of artificial categories of somatosensory patterns follows principles known to govern auditory and visual category learning. For their second aim, the researchers are training participants to recognize spoken words that are transformed into patterns of vibration. The speech stimuli are designed to address questions about cross-sensory learning and the linking of speech categories across hearing and vision. Before and following training, fMRI and EEG measures are being applied to determine where and when in the brain newly learned categories are represented. This project is pushing the frontiers of knowledge about the brain's plasticity for learning novel somatosensory categories, including showing for the first time the neural bases for speech learning through the sense of touch.\r\n\r\nUnderstanding the general principles of sensory processing in the brain, and in particular the commonalities and differences in the underlying neural mechanisms across sensory modalities, is of great interest for practical applications such as the design of neuroprostheses for hearing and/or vision disorders. For example, patients who have auditory or visual sensory system damage may benefit from devices that substitute vibrotactile stimuli for information no longer available through their damaged sensory systems. Vibrotactile stimuli can be combined with visual or auditory stimuli to improve speech perception in noisy situations such as the cockpit of a plane. The fMRI and EEG data from this project along with detailed records kept during training of participants will be made available to the research community. The brain measures obtained before and after training will be valuable for cost-effective testing of new hypotheses about brain plasticity and learning. Research results will be broadly disseminated through publications and conference presentations. The research project will also be leveraged extensively to train the next generation of scientists, at the graduate and undergraduate level, with a particular focus on underrepresented minorities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lynne",
   "pi_last_name": "Bernstein",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Lynne E Bernstein",
   "pi_email_addr": "lbernste@gwu.edu",
   "nsf_id": "000575826",
   "pi_start_date": "2014-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Washington University",
  "inst_street_address": "1918 F ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2029940728",
  "inst_zip_code": "200520042",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "GEORGE WASHINGTON UNIVERSITY (THE)",
  "org_prnt_uei_num": "",
  "org_uei_num": "ECR5E2LU5BL6"
 },
 "perf_inst": {
  "perf_inst_name": "George Washington University",
  "perf_str_addr": "550 Rome Hall,",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200520058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "169900",
   "pgm_ele_name": "Cognitive Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1699",
   "pgm_ref_txt": "COGNEURO"
  },
  {
   "pgm_ref_code": "7298",
   "pgm_ref_txt": "COLLABORATIVE RESEARCH"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 271602.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The human brain has the potential to learn novel stimuli. Remarkable examples of people who became deaf and blind early in life and then learned speech and language through touching the talker's face and neck confirm that the somatosensory system is capable of learning. There were efforts throughout the twentieth century to exploit the somatosensory system's plasticity and learning to convey speech through touch to people with profound hearing loss. Mechanical vibrotactile devices were developed and tested. While research demonstrated some success with those devices, there was limited understanding of how the brain supported learning vibrotactile speech stimuli. Prior to this project, the researchers on this project, Lynne E. Bernstein, PI, Silvio P. Eberhardt, and Edward T. Auer at George Washington University (GWU), had carried out several behavioral studies and an early brain imaging study on vibrotactile speech perception. But they sought to extend their understanding of the underlying neural basis for perceptual learning of vibrotactile categories through the sense of touch. They collaborated with researchers at Georgetown University, led by Dr. Maximillian Riesenhuber (collaborative award BCS-149338), to investigate in depth the ability of normal-hearing adults to learn to perceive vibrotactile speech and non-speech stimulus categories.</p>\n<p>The scientific work on the project first required the GWU researchers to engineer a multichannel vibrotactile device that was compatible with functional magnetic resonance brain imaging (fMRI) and with recording of brain potentials with electroencephalography (EEG). For this project, the researchers designed, realized, and tested a multi-channel vibrotactile device for use in their and in the collaborators' lab. The device incorporated an array of 14 piezoelectric bimorph stimulator wafers and was driven by custom hardware and software. A software system was developed to generate stimuli for experiments. When the stimuli were speech, a custom vocoder system was used to transform acoustic signals to vibrotactile ones.</p>\n<p>The second part of the project had the goal to investigate the neural basis for non-speech vibrotactile category learning. Category learning requires being able to recognize that some stimuli though different among themselves nevertheless belong to a single category, and other differing stimuli belong to another categories. Category learning has been studied extensively with visual and auditory stimuli but to a much lesser extent with somatosensory ones. Human participants with normal hearing and vision were trained on a two-category learning task with stimuli that changed gradually from one category to the next. Brain imaging (fMRI) showed that vibrotactile categorization involved somatosensory brain areas that first represented the physical stimuli and then represented the categories.</p>\n<p>The third part of the project had the goal to investigate category learning of speech stimuli. Normal-hearing and -sighted adults were trained with vibrotactile speech stimuli that varied in terms of their consonants in a vowel-consonant-vowel format. Multiple training sessions were given until all of the participants had demonstrated learning. Before and after training, fMRI scanning was carried out to compare responses to analogous auditory and vibrotactile speech. After training, EEG recording was carried out. The fMRI scans revealed that vibrotactile speech stimuli were processed by both somatosensory and auditory brain areas. While fMRI reveals detailed spatial information, EEG's can record temporal processing details. EEG results demonstrated the somatosensory system's responsiveness on a millisecond basis to the vibrotactile speech stimuli as they unfolded over time. The EEG recordings showed that the somatosensory system responded to category differences among spoken consonants. The auditory system was also responsive to vibrotactile consonant categories. However, the auditory speech stimuli were processed faster and in more detail by the auditory system, consistent with that system's lifelong experience with auditory speech.</p>\n<p>The broader impact of the project extends to multiple areas. The project provides encouragement for future development of sensory substitution devices for impaired sensory-perceptual systems. As people age, speech perception in noisy environments becomes more difficult, and vibrotactile stimuli may support detecting and tracking the voices of partners in conversation. Perceptual learning is typically faster with multisensory stimuli. For example, vibrotactile signals could help second language learners to detect and attend to unfamiliar speech stimulus cues in their new language. The hardware and software that were developed for this project may assist the research community in testing additional hypotheses about speech and non-speech vibrotactile stimuli.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/12/2019<br>\n\t\t\t\t\tModified by: Lynne&nbsp;E&nbsp;Bernstein</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe human brain has the potential to learn novel stimuli. Remarkable examples of people who became deaf and blind early in life and then learned speech and language through touching the talker's face and neck confirm that the somatosensory system is capable of learning. There were efforts throughout the twentieth century to exploit the somatosensory system's plasticity and learning to convey speech through touch to people with profound hearing loss. Mechanical vibrotactile devices were developed and tested. While research demonstrated some success with those devices, there was limited understanding of how the brain supported learning vibrotactile speech stimuli. Prior to this project, the researchers on this project, Lynne E. Bernstein, PI, Silvio P. Eberhardt, and Edward T. Auer at George Washington University (GWU), had carried out several behavioral studies and an early brain imaging study on vibrotactile speech perception. But they sought to extend their understanding of the underlying neural basis for perceptual learning of vibrotactile categories through the sense of touch. They collaborated with researchers at Georgetown University, led by Dr. Maximillian Riesenhuber (collaborative award BCS-149338), to investigate in depth the ability of normal-hearing adults to learn to perceive vibrotactile speech and non-speech stimulus categories.\n\nThe scientific work on the project first required the GWU researchers to engineer a multichannel vibrotactile device that was compatible with functional magnetic resonance brain imaging (fMRI) and with recording of brain potentials with electroencephalography (EEG). For this project, the researchers designed, realized, and tested a multi-channel vibrotactile device for use in their and in the collaborators' lab. The device incorporated an array of 14 piezoelectric bimorph stimulator wafers and was driven by custom hardware and software. A software system was developed to generate stimuli for experiments. When the stimuli were speech, a custom vocoder system was used to transform acoustic signals to vibrotactile ones.\n\nThe second part of the project had the goal to investigate the neural basis for non-speech vibrotactile category learning. Category learning requires being able to recognize that some stimuli though different among themselves nevertheless belong to a single category, and other differing stimuli belong to another categories. Category learning has been studied extensively with visual and auditory stimuli but to a much lesser extent with somatosensory ones. Human participants with normal hearing and vision were trained on a two-category learning task with stimuli that changed gradually from one category to the next. Brain imaging (fMRI) showed that vibrotactile categorization involved somatosensory brain areas that first represented the physical stimuli and then represented the categories.\n\nThe third part of the project had the goal to investigate category learning of speech stimuli. Normal-hearing and -sighted adults were trained with vibrotactile speech stimuli that varied in terms of their consonants in a vowel-consonant-vowel format. Multiple training sessions were given until all of the participants had demonstrated learning. Before and after training, fMRI scanning was carried out to compare responses to analogous auditory and vibrotactile speech. After training, EEG recording was carried out. The fMRI scans revealed that vibrotactile speech stimuli were processed by both somatosensory and auditory brain areas. While fMRI reveals detailed spatial information, EEG's can record temporal processing details. EEG results demonstrated the somatosensory system's responsiveness on a millisecond basis to the vibrotactile speech stimuli as they unfolded over time. The EEG recordings showed that the somatosensory system responded to category differences among spoken consonants. The auditory system was also responsive to vibrotactile consonant categories. However, the auditory speech stimuli were processed faster and in more detail by the auditory system, consistent with that system's lifelong experience with auditory speech.\n\nThe broader impact of the project extends to multiple areas. The project provides encouragement for future development of sensory substitution devices for impaired sensory-perceptual systems. As people age, speech perception in noisy environments becomes more difficult, and vibrotactile stimuli may support detecting and tracking the voices of partners in conversation. Perceptual learning is typically faster with multisensory stimuli. For example, vibrotactile signals could help second language learners to detect and attend to unfamiliar speech stimulus cues in their new language. The hardware and software that were developed for this project may assist the research community in testing additional hypotheses about speech and non-speech vibrotactile stimuli.\n\n \n\n\t\t\t\t\tLast Modified: 12/12/2019\n\n\t\t\t\t\tSubmitted by: Lynne E Bernstein"
 }
}