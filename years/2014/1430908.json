{
 "awd_id": "1430908",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:  Three Dimensional Headphone Audio for Music, Gaming, Entertainment and Telepresence",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032928772",
 "po_email": "patherto@nsf.gov",
 "po_sign_block_name": "Peter Atherton",
 "awd_eff_date": "2014-10-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 739899.0,
 "awd_amount": 1429878.0,
 "awd_min_amd_letter_date": "2014-09-04",
 "awd_max_amd_letter_date": "2018-08-03",
 "awd_abstract_narration": "The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase II project extends to wherever headphones are used to listen to sound. The first commercial markets targeted by the technology being developed are gaming, virtual reality and consumer entertainment. These are areas in which the US has strong market and technology leads. By the end of the project, the company intends to have developed software that can be licensed to major players in these areas, reaching tens of millions of users. Beyond these markets, there are many applications for spatial audio in human-computer interfaces to deliver spatial information along with the intended semantic message. Several niche markets also exist, such as data presentation/exploration via sonification and specifically designed auditory interfaces for vision-impaired users. Overall it is expected that the proposed R&D work will advance the state of the art in science and technology and will be of substantial value to society as a whole due to high usability, fidelity, naturalness, and portability of the developed spatial audio solutions.\r\n\r\nThis Small Business Innovation Research (SBIR) Phase II project seeks to develop highly realistic and computationally efficient software for synthesis of personalized spatial audio, for applications that include virtual reality, gaming, and prostheses for the blind. Current algorithms are either perceptually unsatisfactory or have very high computational load. Approximate modeling of sound propagation, reverberation, and diffusion, along with tradeoffs between complexity and quality, will be explored using perceptual distortion metrics combined with the skills of professional listeners. The software will be optimized for use on mobile, console and embedded platforms. Additionally, a method to personalize the software to individual listeners, which was previously developed and tested in laboratory conditions, will be further refined and ruggedized for use in realistic environments. The expected outcomes will include a high-quality and efficient audio rendering library, a portable personalization apparatus, and several demonstrations that highlight the capabilities of the technology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adam",
   "pi_last_name": "O'Donovan",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Adam E O'Donovan",
   "pi_email_addr": "adam.o@visisonics.com",
   "nsf_id": "000566170",
   "pi_start_date": "2014-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "VisiSonics Corporation",
  "inst_street_address": "6800 Koandah Gardens",
  "inst_street_address_2": "",
  "inst_city_name": "Highland",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3013322507",
  "inst_zip_code": "207779797",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MD03",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": null
 },
 "perf_inst": {
  "perf_inst_name": "VisiSonics Corporation",
  "perf_str_addr": "387 Technology Drive; #2107",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207420001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "082E",
   "pgm_ref_txt": "MFG MACHINES & METROLOGY"
  },
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "165E",
   "pgm_ref_txt": "SBIR Phase IIB"
  },
  {
   "pgm_ref_code": "169E",
   "pgm_ref_txt": "SBIR Tech Enhan Partner (TECP)"
  },
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "8032",
   "pgm_ref_txt": "Software Services and Applications"
  },
  {
   "pgm_ref_code": "8035",
   "pgm_ref_txt": "Hardware Devices"
  },
  {
   "pgm_ref_code": "8240",
   "pgm_ref_txt": "SBIR/STTR CAP"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 739899.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 165979.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 516000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;VisiSonics is a UMD spin-out, built on a decade of R&amp;D. Our initial IP was funded by over $6M of grants to the university, including by NSF. The founders developed the IP and an understanding on how perceptually accurate audio can be reproduced and captured. The resulting technology was exclusively licensed from university, with the right to sublicense. The company has translated the research results into several licensable products during the course of the SBIR project.</p>\n<p>The software developed helps immersion in VR and AR by making the rendered audio as spatially accurate as being at the scene. By licensing our physics-based software engine, game and movie creators place sound objects at the correct locations in the virtual world. Mobile telephone and headphone makers license our IP to create more immersive sound using emerging MPEG spatial audio standards. Licensing our hardware/algorithms allows capture/playback of 3D audio from mobile devices. End users can personalize the experience to their body/ear shape via in-app upgrades achieving a level of realism that is as good as being there. Based on these products, the company has entered into several licenses.&nbsp;</p>\n<p>Emerging standards based on object audio and ambisonics, are part of the released MPEG-H standard, while the MPEG-I standard being developed supports personalization of audio to the end user. While these support VR/AR/MR, they also enable immersive audio to be provided for games, streaming experiences for movies and music. Our engine and personalization technologies are designed to provide perceptually superior audio experiences at any given processor budget on mobile, PC, VR/AR headsets and headphones. Based on this project, we have a clear path to continued improvements and easily delivered personalization, promising to further surprise and delight.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/17/2019<br>\n\t\t\t\t\tModified by: Adam&nbsp;E&nbsp;O'donovan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n VisiSonics is a UMD spin-out, built on a decade of R&amp;D. Our initial IP was funded by over $6M of grants to the university, including by NSF. The founders developed the IP and an understanding on how perceptually accurate audio can be reproduced and captured. The resulting technology was exclusively licensed from university, with the right to sublicense. The company has translated the research results into several licensable products during the course of the SBIR project.\n\nThe software developed helps immersion in VR and AR by making the rendered audio as spatially accurate as being at the scene. By licensing our physics-based software engine, game and movie creators place sound objects at the correct locations in the virtual world. Mobile telephone and headphone makers license our IP to create more immersive sound using emerging MPEG spatial audio standards. Licensing our hardware/algorithms allows capture/playback of 3D audio from mobile devices. End users can personalize the experience to their body/ear shape via in-app upgrades achieving a level of realism that is as good as being there. Based on these products, the company has entered into several licenses. \n\nEmerging standards based on object audio and ambisonics, are part of the released MPEG-H standard, while the MPEG-I standard being developed supports personalization of audio to the end user. While these support VR/AR/MR, they also enable immersive audio to be provided for games, streaming experiences for movies and music. Our engine and personalization technologies are designed to provide perceptually superior audio experiences at any given processor budget on mobile, PC, VR/AR headsets and headphones. Based on this project, we have a clear path to continued improvements and easily delivered personalization, promising to further surprise and delight.\n\n\t\t\t\t\tLast Modified: 02/17/2019\n\n\t\t\t\t\tSubmitted by: Adam E O'donovan"
 }
}