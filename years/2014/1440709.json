{
 "awd_id": "1440709",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SI2-SSE: Petascale Enzo: Software Infrastructure Development and Community Engagement",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032928235",
 "po_email": "bmihaila@nsf.gov",
 "po_sign_block_name": "Bogdan Mihaila",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2014-08-13",
 "awd_max_amd_letter_date": "2014-08-13",
 "awd_abstract_narration": "The purpose of this project is to develop an astrophysics and cosmology software application \"Enzo-P\", built on the highly scalable parallel adaptive mesh refinement (AMR) software framework \"Cello\" that is being developed concurrently.  The Enzo-P application will be capable of running extreme scale numerical simulations to investigate frontier questions in star formation, molecular cloud turbulence, interstellar medium dynamics, galaxy formation, intergalctic medium, formation of the first stars and galaxies, galaxy clusters, and cosmic reionization.  This new software will empower the current large and diverse Enzo user/developer community to take full advantage of current and future high performance computer (HPC) systems. The Cello AMR framework can be used independently of Enzo-P, thus enabling researchers in other diverse scientific fields to develop AMR applications capable of running on \"Petascale-and-beyond\" HPC platforms. \r\n\r\nThe novel approach used for Cello is to implement a \"forest-of-octree\" AMR scheme using the Charm++ parallel programming system.  Octree-based AMR has been shown to be among the highest scaling AMR approaches, with demonstrated scaling to over 200K CPU cores.  The Charm++ object-oriented parallel programming language supports data-driven asynchronous execution, is inherently latency-tolerant and automatically overlaps computation with communication, and provides support for developing Exascale applications, including in-memory distributed checkpointing and sophisticated dynamic load balancing schemes.  Enzo-P development will be directed by the vibrant Enzo open development community, who will migrate Enzo's self-gravity, cosmology, chemistry and cooling, MHD, and radiation hydrodynamics  capabilities to use the Cello scalable AMR framework.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Norman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Norman",
   "pi_email_addr": "mlnorman@ucsd.edu",
   "nsf_id": "000235680",
   "pi_start_date": "2014-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1206",
   "pgm_ref_txt": "THEORETICAL & COMPUTATIONAL ASTROPHYSICS"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8005",
   "pgm_ref_txt": "Scientific Software Elements"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Computer simulation is fundamental to progress in many fields of science and engineering. To quote Richard Hamming, \"The purpose of computing is insight, not numbers.\" Nowhere is this more true than in astronomy and cosmology, where the object of interest is inaccessible to direct laboratory exploration. Complex numerical simulations of astrophysical phenomena such as star forming clouds and supernova explosions take the place of laboratory experiments. </span></p>\n<p><span>The goal of this project is to develop a software system for astrophysical and cosmological simulations capable of exploiting today's and tomorrow's most powerful supercomputers. This requires combining new parallel programming approaches with new, more scalable numerical algorithms that can run efficiently on millions of computer processors simultaneously. </span></p>\n<p><span>Enter Enzo-P, a re-implementation of the popular community software Enzo for petascale (one quadrillion floating point operations per second) computers and beyond. The Enzo code is an open-source software application used by hundreds of researchers around the world to simulate astrophysical systems as diverse as binary stars, planet formation, star and galaxy formation, and the large scale structure of the universe. Part of Enzo's power is the use of a variable resolution grid which is adaptive in space and time to capture complex features that develop from the physical processes modeled. This capability has been applied to study the formation of the first generation of stars and galaxies in the universe. However Enzo was implemented in such a way that limits its execution to no more than a few thousand processors, far fewer than today's most power supercomputers possess.&nbsp;</span></p>\n<p><span><span>Enzo-P leapfrogs this limitation by implementing Enzo's suite of physics solvers atop an entirely new, extreme scale AMR framework called Cello, developed concurrently with Enzo-P. Cello is based on the highly scalable array-of-octrees spatial decomposition pioneered by other researchers. However here we use the parallel object framework Charm++ to manage the parallel processing aspects. This approach hides the hardware-specific details from the application programmer, and improves portability. </span></span></p>\n<p><span><span><span>In this project we have built the Enzo-P/Cello software and demonstrated its near-perfect scaling to over 1/4 million processors on the Blue Waters Sustained Petascale supercomputer at the University of Illinois. This is a 20-fold increase over the largest AMR simulation ever performed with Enzo. We have also conducted outreach to the Enzo community to engage them as users and developers of Enzo-P. Together, we are poised to transition the entire Enzo user community to a new software platform so they may pursue tomorrow's grand challenge problems in astrophysics and cosmology. </span><strong></strong><em></em>&nbsp;</span></span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/04/2018<br>\n\t\t\t\t\tModified by: Michael&nbsp;L&nbsp;Norman</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969636840_AMRcosmology--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969636840_AMRcosmology--rgov-800width.jpg\" title=\"AMR cosmology in Enzo-P\"><img src=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969636840_AMRcosmology--rgov-66x44.jpg\" alt=\"AMR cosmology in Enzo-P\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Application of Enzo-P/Cello to cosmological structure formation.</div>\n<div class=\"imageCredit\">J. Bordner & M. Norman, UCSD</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Norman</div>\n<div class=\"imageTitle\">AMR cosmology in Enzo-P</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969536031_EnzovsEnzo-P--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969536031_EnzovsEnzo-P--rgov-800width.jpg\" title=\"Enzo-P versus Enzo\"><img src=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969536031_EnzovsEnzo-P--rgov-66x44.jpg\" alt=\"Enzo-P versus Enzo\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Comparison of approaches to adaptive mesh refinement (AMR) in Enzo-P compared to Enzo.</div>\n<div class=\"imageCredit\">J. Bordner & M. Norman, UCSD</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Norman</div>\n<div class=\"imageTitle\">Enzo-P versus Enzo</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969413098_Scientificquestions--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969413098_Scientificquestions--rgov-800width.jpg\" title=\"Scientific questions in astrophysics\"><img src=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969413098_Scientificquestions--rgov-66x44.jpg\" alt=\"Scientific questions in astrophysics\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Scientific questions in astrophysics and cosmology cover a vast range of spatial scales.</div>\n<div class=\"imageCredit\">J. Bordner and M. Norman, UCSD</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Norman</div>\n<div class=\"imageTitle\">Scientific questions in astrophysics</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969810161_AlphabetSoup--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969810161_AlphabetSoup--rgov-800width.jpg\" title=\"Interacting blast waves\"><img src=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543969810161_AlphabetSoup--rgov-66x44.jpg\" alt=\"Interacting blast waves\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A large array of interacting blastwaves, simulated with Enzo-P/Cello AMR on more than 1/4 million processor cores on NCSA Blue Waters.</div>\n<div class=\"imageCredit\">J. Bordner & M. Norman</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Norman</div>\n<div class=\"imageTitle\">Interacting blast waves</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543970211421_Scaling--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543970211421_Scaling--rgov-800width.jpg\" title=\"Parallel scaling of Enzo-P/Cello\"><img src=\"/por/images/Reports/POR/2018/1440709/1440709_10332091_1543970211421_Scaling--rgov-66x44.jpg\" alt=\"Parallel scaling of Enzo-P/Cello\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Parallel scaling data of Enzo-P/Cello on an AMR hydrodynamics test problem involving interacting blast waves. Ideal weak scaling is a horizontal line in this diagram.</div>\n<div class=\"imageCredit\">J. Bordner & M. Norman, UCSD</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Michael&nbsp;L&nbsp;Norman</div>\n<div class=\"imageTitle\">Parallel scaling of Enzo-P/Cello</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nComputer simulation is fundamental to progress in many fields of science and engineering. To quote Richard Hamming, \"The purpose of computing is insight, not numbers.\" Nowhere is this more true than in astronomy and cosmology, where the object of interest is inaccessible to direct laboratory exploration. Complex numerical simulations of astrophysical phenomena such as star forming clouds and supernova explosions take the place of laboratory experiments. \n\nThe goal of this project is to develop a software system for astrophysical and cosmological simulations capable of exploiting today's and tomorrow's most powerful supercomputers. This requires combining new parallel programming approaches with new, more scalable numerical algorithms that can run efficiently on millions of computer processors simultaneously. \n\nEnter Enzo-P, a re-implementation of the popular community software Enzo for petascale (one quadrillion floating point operations per second) computers and beyond. The Enzo code is an open-source software application used by hundreds of researchers around the world to simulate astrophysical systems as diverse as binary stars, planet formation, star and galaxy formation, and the large scale structure of the universe. Part of Enzo's power is the use of a variable resolution grid which is adaptive in space and time to capture complex features that develop from the physical processes modeled. This capability has been applied to study the formation of the first generation of stars and galaxies in the universe. However Enzo was implemented in such a way that limits its execution to no more than a few thousand processors, far fewer than today's most power supercomputers possess. \n\nEnzo-P leapfrogs this limitation by implementing Enzo's suite of physics solvers atop an entirely new, extreme scale AMR framework called Cello, developed concurrently with Enzo-P. Cello is based on the highly scalable array-of-octrees spatial decomposition pioneered by other researchers. However here we use the parallel object framework Charm++ to manage the parallel processing aspects. This approach hides the hardware-specific details from the application programmer, and improves portability. \n\nIn this project we have built the Enzo-P/Cello software and demonstrated its near-perfect scaling to over 1/4 million processors on the Blue Waters Sustained Petascale supercomputer at the University of Illinois. This is a 20-fold increase over the largest AMR simulation ever performed with Enzo. We have also conducted outreach to the Enzo community to engage them as users and developers of Enzo-P. Together, we are poised to transition the entire Enzo user community to a new software platform so they may pursue tomorrow's grand challenge problems in astrophysics and cosmology.  \n\n\t\t\t\t\tLast Modified: 12/04/2018\n\n\t\t\t\t\tSubmitted by: Michael L Norman"
 }
}