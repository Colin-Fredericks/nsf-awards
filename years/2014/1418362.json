{
 "awd_id": "1418362",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "\"Big-Data\" Asymptotics: Theory and Large-Scale Experiments",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Christopher Stark",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 700594.0,
 "awd_amount": 700594.0,
 "awd_min_amd_letter_date": "2014-08-06",
 "awd_max_amd_letter_date": "2014-08-06",
 "awd_abstract_narration": "Large datasets are becoming increasingly available and important in science and technology. This project will develop new tools for dealing with large datasets, as well as a new understanding of some of the fascinating phenomena that emerge with high-dimensional data. This project will develop methods for recovering signals (vectors and matrices) from highly undersampled measurements (also known as compressed sensing and matrix completion), and for recovering low-rank matrices from noisy and under sampled measurements, and tools for robustly fitting predictive models when the number of predictor variables is very large. All of these tools have broad domains of applicability -- basically wherever big data are being gathered, researchers will want to use such tools. Phenomena that will be explored include the phase transitions that some algorithms undergo, going abruptly from successful recovery to failure, as the amount of undersampling and/or contamination of the data increases, and the fact that fundamental formulas of classical statistics, such as the Fisher information formula for variance of the maximum-likelihood estimator, no longer apply in high-dimensional statistics. We expect to develop quantitatively precise explanations of these phenomena.  Our quantitative explanations will help engineers and scientists plan experiments and make reliable inferences from large datasets.\r\n\r\nSeveral classical problems in multivariate data analysis develop a new character when the number of variables p and the number of observations n are both large. These problems include estimation in the linear model, robust estimation in the linear model, sparsity-penalized estimation in the linear model, and estimation of covariance matrices obeying a factor model. In particular, work in a range of fields shows that if certain underlying features of the problem are random (such as iid Gaussian predictor variables, or uniformly distributed eigenvectors), then various surprises occur in the limit when p and n tend to infinity in a fixed proportion.  These surprises include sharp phase transitions in the success/failure of recovering the object of interest and extra gaussian noise beyond that caused by the measurements which have no classical counterpart in the p fixed n tending to infinity case. In the proposed work, we will use both massive computational experiments and precise theoretical calculations, to predict and verify such surprising phenomena in the large n, large p setting. Our techniques involve (on the computing side) new tools for design and execution of computational experiments involving many processors in cloud-based configurations, as well as (on the theoretical side) analysis tools for understanding phase transitions in compressed sensing, both in exact reconstruction from noiseless measurements and in asymptotic mean-squared error of convex optimization reconstructions as well as asymptotic mean-squared error of non-convex optimizations. In the study of low-rank models for matrices, we will develop and adapt recent advances in random matrix theory, such as recent progress in the so-called spiked covariance model.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Donoho",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "David L Donoho",
   "pi_email_addr": "donoho@stat.stanford.edu",
   "nsf_id": "000367778",
   "pi_start_date": "2014-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Iain",
   "pi_last_name": "Johnstone",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Iain M Johnstone",
   "pi_email_addr": "imj@stanford.edu",
   "nsf_id": "000155839",
   "pi_start_date": "2014-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "CA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  },
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8084",
   "pgm_ref_txt": "CDS&E"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 700594.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>OUTCOMES -- 1</strong><strong>418362</strong><strong>&nbsp;</strong></p>\n<p>&nbsp;Workers on this project made notable contributionsin several areas relevant to the scientific analysis of Big Data.</p>\n<p>&nbsp;</p>\n<p>(1)&nbsp;&nbsp;<em>Running large scale computational experiments.</em>Our personnel developed ClusterJob, a system making it painless to scale computational experiments on modern compute clusters to massive scales. ClusterJob is heavily used on Stanford?s Sherlock cluster and has been used to run more than 1 Million compute jobs during the term of this project. ClusterJob is freely available software and has been used not only on Stanford?s cluster but also on Google Cloud Platform as well. As a side effect of this effort, we developed and taught a popular master?s level course at Stanford called ?Massive Computational Experiments, Painlessly.?&nbsp;</p>\n<p>(2)&nbsp;&nbsp;<em>Study of modern deep networks.</em>Many of the compute jobs we ran recently were used to run modern large-scale deepnets. Our general area of interest was to determine the scaling laws for generalization error as a function of training sample size, so our personnel ran many large-scale experiments aimed at measuring error-rate performance as a function of sample size. As a side effect of this effort, we developed and taught a popular PhD level course at Stanford called ?Theories of Deep Learning? with more than 100 regular PhD student attendees.</p>\n<p>(3)&nbsp;&nbsp;<em>Mathematical Analysis under Big Data Asymptotics.</em>&nbsp;&nbsp;We developed theory of modern statistical procedures under the assumption that there are large numbers of variables as well as large numbers of observations on each variable. We made decisive progress in three important areas:</p>\n<p>(a)&nbsp;&nbsp;<em>Optimal thresholding in estimation of high-dimensional data matrices by SVD.</em>Truncated singular value decomposition approximations are ubiquitous in science and technology, but until now the standard tool for deciding where to threshold has been heuristic based on the so-called scree plot. The scree plot approach has tens of thousands of uses in the literature. We introduced a new principled and fully theoretically understood method that is provably optimal in a very strong sense even in finite samples. Importantly the method is optimal even when the noise is not white noise. Implications are expected wherever the scree plot is currently used, which is to say in nearly all fields science-wide. Convenient software implementations in Matlab and R are to be made publicly available.</p>\n<p>(b)&nbsp;&nbsp;<em>P-values for perfect linear separation.</em>Often in binary classification in small samples we notice that a perfect linear separator exists, namely a linear combination of the variables that perfectly separates the two classes. We derive exact finite samples expressions for this probability using modern high-dimensional combinatorial geometry, and develop a range of asymptotic approximations. We also show by massive computational experiments that the formulas apply accurately to many well-known data matrices used in statistics and machine learning.</p>\n<p>(c)&nbsp;&nbsp;<em>Optimal shrinkage of eigenvalues under log condition number loss in the Spiked Covariance Model.</em>We show that log-condition number loss is important for a variety of new and interesting problems including multi-task linear classification and multi-use covariance estimation under privacy constraints. We derive the exact asymptotically optimal shrinkage of eigenvalues for these problems. Unlike earlier problem where optimal shrinkage procedures were found, in the current project, the optimal procedure is nonseparable but can still be efficiently applied. Impacts are described in Empirical Finance and in Computational Biology.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/05/2019<br>\n\t\t\t\t\tModified by: David&nbsp;L&nbsp;Donoho</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOUTCOMES -- 1418362 \n\n Workers on this project made notable contributionsin several areas relevant to the scientific analysis of Big Data.\n\n \n\n(1)  Running large scale computational experiments.Our personnel developed ClusterJob, a system making it painless to scale computational experiments on modern compute clusters to massive scales. ClusterJob is heavily used on Stanford?s Sherlock cluster and has been used to run more than 1 Million compute jobs during the term of this project. ClusterJob is freely available software and has been used not only on Stanford?s cluster but also on Google Cloud Platform as well. As a side effect of this effort, we developed and taught a popular master?s level course at Stanford called ?Massive Computational Experiments, Painlessly.? \n\n(2)  Study of modern deep networks.Many of the compute jobs we ran recently were used to run modern large-scale deepnets. Our general area of interest was to determine the scaling laws for generalization error as a function of training sample size, so our personnel ran many large-scale experiments aimed at measuring error-rate performance as a function of sample size. As a side effect of this effort, we developed and taught a popular PhD level course at Stanford called ?Theories of Deep Learning? with more than 100 regular PhD student attendees.\n\n(3)  Mathematical Analysis under Big Data Asymptotics.  We developed theory of modern statistical procedures under the assumption that there are large numbers of variables as well as large numbers of observations on each variable. We made decisive progress in three important areas:\n\n(a)  Optimal thresholding in estimation of high-dimensional data matrices by SVD.Truncated singular value decomposition approximations are ubiquitous in science and technology, but until now the standard tool for deciding where to threshold has been heuristic based on the so-called scree plot. The scree plot approach has tens of thousands of uses in the literature. We introduced a new principled and fully theoretically understood method that is provably optimal in a very strong sense even in finite samples. Importantly the method is optimal even when the noise is not white noise. Implications are expected wherever the scree plot is currently used, which is to say in nearly all fields science-wide. Convenient software implementations in Matlab and R are to be made publicly available.\n\n(b)  P-values for perfect linear separation.Often in binary classification in small samples we notice that a perfect linear separator exists, namely a linear combination of the variables that perfectly separates the two classes. We derive exact finite samples expressions for this probability using modern high-dimensional combinatorial geometry, and develop a range of asymptotic approximations. We also show by massive computational experiments that the formulas apply accurately to many well-known data matrices used in statistics and machine learning.\n\n(c)  Optimal shrinkage of eigenvalues under log condition number loss in the Spiked Covariance Model.We show that log-condition number loss is important for a variety of new and interesting problems including multi-task linear classification and multi-use covariance estimation under privacy constraints. We derive the exact asymptotically optimal shrinkage of eigenvalues for these problems. Unlike earlier problem where optimal shrinkage procedures were found, in the current project, the optimal procedure is nonseparable but can still be efficiently applied. Impacts are described in Empirical Finance and in Computational Biology.\n\n \n\n\t\t\t\t\tLast Modified: 02/05/2019\n\n\t\t\t\t\tSubmitted by: David L Donoho"
 }
}