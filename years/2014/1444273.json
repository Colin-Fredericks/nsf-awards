{
 "awd_id": "1444273",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "BRAIN EAGER: A Proposed New Principle of Brain Organization",
 "cfda_num": "47.049",
 "org_code": "03010000",
 "po_phone": "7032924666",
 "po_email": "kblagoev@nsf.gov",
 "po_sign_block_name": "Krastan Blagoev",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 299994.0,
 "awd_amount": 299994.0,
 "awd_min_amd_letter_date": "2014-08-18",
 "awd_max_amd_letter_date": "2014-09-02",
 "awd_abstract_narration": "Most brain areas (like visual or motor cortex) are believed to have a map in which neuron location is related to the type of information that neuron processes. For example, in the visual cortex, neurons process information about a part of the visual world that is closely related to their map location in the brain. The idea explored in this project is that some important brain areas have an antimap, rather than a map, in which the type of information conveyed by a neuron is not at all related to its location in the antimap. Instead, information is spread out over the antimap in a way that makes it possible to get all of the available information from a small collection of any neurons in the antimap, as long as a critical number of neurons is selected. The conceptual basis for this antimap idea comes from a new field of mathematics and computer science called \"compressed sensing\", and compressed sensing places strict limits on the possible ways brain areas can communicate if an antimap is to be formed. A goal of the project, then, is to discover if the evolutionarily ancient brain areas noted above conform to these limits. The reason for the brain to use antimaps is that they provide information in a format that can be used to collect arbitrary pieces of information into a single \"object\" through learning. The goal of this EAGER project is to explore the new idea about how information is represented in four evolutionarily ancient brain areas: hippocampus, cerebellum, olfactory cortex, and basal ganglia, present in all vertebrates. To achieve this goal the PI will search the literature on anatomical and physiological characteristics of inputs to these four areas (and sub-parts of them), and will compile quantitative neuroanatomical data that will permit a comparison of what is observed with what the idea predicts. The outcome will determine the extent to which this new idea is tenable and lead to understanding its possible implications for computations in the various brain areas. \r\n\r\nThis work is, by its nature, interdisciplinary as it relies on ideas from neurobiology, mathematics and computer science, and on the methods of theoretical physics. If the idea of antimaps is correct, all vertebrate (and perhaps invertebrate) brains use them to solve a wide variety of computational problems. Furthermore, knowing how information is represented in the brain is fundamental to understanding how the brain works, and if this new idea is correct it will play an essential role in achieving the goals of the BRAIN initiative.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "PHY",
 "org_div_long_name": "Division Of Physics",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Stevens",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Charles F Stevens",
   "pi_email_addr": "stevens@salk.edu",
   "nsf_id": "000439064",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Dr 0126",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930126",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "116500",
   "pgm_ele_name": "ADVANCES IN BIO INFORMATICS"
  },
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "724600",
   "pgm_ele_name": "PHYSICS OF LIVING SYSTEMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1253",
   "pgm_ref_txt": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ref_code": "7237",
   "pgm_ref_txt": "NANO NON-SOLIC SCI & ENG AWD"
  },
  {
   "pgm_ref_code": "7246",
   "pgm_ref_txt": "PHYSICS OF LIVING SYSTEMS"
  },
  {
   "pgm_ref_code": "7275",
   "pgm_ref_txt": "CROSS-EF ACTIVITIES"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8091",
   "pgm_ref_txt": "BRAIN Initiative Res Support"
  },
  {
   "pgm_ref_code": "9183",
   "pgm_ref_txt": "GENERAL FOUNDATIONS OF BIOTECHNOLOGY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 299994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The problem I wanted to solve is the following: How does the brain deal with complex stimuli that require a description in a high-dimensional space? For example, human faces are generally similar, but we can learn to recognize any of the 7 billion faces in the world. We do not know the dimensionality of the face space, bt we do know that it is at least 1000-dimensional.</p>\n<p>A simpler problem of this same sort is illustrated by fly olfaction. The fruit fly has about 50 different olfactory receptor types, so each odor must be represented by a combinatorial code (the firing rates of 50 different olfactory receptor neurons). This was the first problem I solved. I will first describe the solution to this problem, second relate this solution to the larger picture, and third explain how the solution can have a larger impact by translating the fly's olfactory recognition algorithm into an essential computer algorithm.</p>\n<p>(1) The 50 receptor types in the fly's nose send information to&nbsp;the antennal lobe (AL) where each odorant&nbsp;receptor types is represented. Each receptor type sends olfactory information over 50 types of projection neurons to a second structure, called the mushroom body, whose main input neurons are 2000 Kenyon&nbsp;cells (KCs). Each KC samples at random 6 of the 50 projection neurons and combines the information. A winner-take-all circuit uses inhibition to silence all KCs except 100 of the 2000 KCs. The 100 KCs that are permitted to fire spikes are a 'tag' for each odor. These tags have two properties: (1) The tag is based on all of the information the fly has about each odor, and (2) the tags for two randomly selected odors&nbsp;are non-overlapping (use different KCs).</p>\n<p>This tag is used to learn an odor by changing the synaptic strengths of KC inputs onto mushroom bod output neurons. A fly can learn to approach or avoid any odor depending whether it was reworded (food) or punished (electric shock) in the presence of the odor. A fly can learn any odor it smells, and can recognize that odor in a fraction of a second.</p>\n<p>The input to the KCs of olfactory information comes from a population of AL projection neurons, which gets their input directly from the fly's odorant receptors neurons in &nbsp;the fly's nose. I have found that the projection neuron population has the same mean rate for every odor. Further, the distribution of the population firing rates is described by an exponential function, so most of the population is firing at low rates, but a small fraction are at the highest rates.Thus, the tags for two different odors tend not to use the same KCs because it is unlikely that the fasted firing KCs are used for different odors.</p>\n<p>In summary: The scheme is odor -&gt; tag -&gt; odor learning. The tag is used to change the strength of synapses from KCs to output neurons in the lobes of the mushroom bodies which is the where odor learning &nbsp;occurs. Because tags are non-overlapping, the tags, and thus the odors, are not mixed up.</p>\n<p>(2) Is this way of attaching tags to arbitrary points in a high dimensional space unique to insects? Mammals also have brain regions that can generate tags for complex stimuli. Neurons that respond to faces are found in the inferior temporal cortex, and these 'face' cells probably project to the hippocamppus where learning commonly occurs. Doris Tsao recently provided me with data for about 100 face cells recorded from monkeys &nbsp;presented with 2000 different human face images. On analysis of tthese data, I found that the population of face cells has the same mean firing rate for all 2000 faces, and that the distribution of rates is, like the fly olfactory system, an exponential function. This parallel suggests that the fly olfactory system and the inferior temporal cortex are using the same learning scheme.</p>\n<p>(3) We noticed that the fly odor algorithm has some similarities to the way Google dose a similarity search (\"Show me the 50 pictures of elephants that are most like my elephant picture.\"). It differs, however, in three specific ways. We wondered how well the Google similarity search algorithm would work if it was modified to work the fly's way. We discovered that the fly's way was never worse that the Google way, and that it was usually better (Science, 358:793,2017). We believe this is the first example of directly translating a brain algorithm into a computer algorithm. If this works for other examples, this sort of translation could have considerable impact on computer science.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/14/2017<br>\n\t\t\t\t\tModified by: Charles&nbsp;F&nbsp;Stevens</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe problem I wanted to solve is the following: How does the brain deal with complex stimuli that require a description in a high-dimensional space? For example, human faces are generally similar, but we can learn to recognize any of the 7 billion faces in the world. We do not know the dimensionality of the face space, bt we do know that it is at least 1000-dimensional.\n\nA simpler problem of this same sort is illustrated by fly olfaction. The fruit fly has about 50 different olfactory receptor types, so each odor must be represented by a combinatorial code (the firing rates of 50 different olfactory receptor neurons). This was the first problem I solved. I will first describe the solution to this problem, second relate this solution to the larger picture, and third explain how the solution can have a larger impact by translating the fly's olfactory recognition algorithm into an essential computer algorithm.\n\n(1) The 50 receptor types in the fly's nose send information to the antennal lobe (AL) where each odorant receptor types is represented. Each receptor type sends olfactory information over 50 types of projection neurons to a second structure, called the mushroom body, whose main input neurons are 2000 Kenyon cells (KCs). Each KC samples at random 6 of the 50 projection neurons and combines the information. A winner-take-all circuit uses inhibition to silence all KCs except 100 of the 2000 KCs. The 100 KCs that are permitted to fire spikes are a 'tag' for each odor. These tags have two properties: (1) The tag is based on all of the information the fly has about each odor, and (2) the tags for two randomly selected odors are non-overlapping (use different KCs).\n\nThis tag is used to learn an odor by changing the synaptic strengths of KC inputs onto mushroom bod output neurons. A fly can learn to approach or avoid any odor depending whether it was reworded (food) or punished (electric shock) in the presence of the odor. A fly can learn any odor it smells, and can recognize that odor in a fraction of a second.\n\nThe input to the KCs of olfactory information comes from a population of AL projection neurons, which gets their input directly from the fly's odorant receptors neurons in  the fly's nose. I have found that the projection neuron population has the same mean rate for every odor. Further, the distribution of the population firing rates is described by an exponential function, so most of the population is firing at low rates, but a small fraction are at the highest rates.Thus, the tags for two different odors tend not to use the same KCs because it is unlikely that the fasted firing KCs are used for different odors.\n\nIn summary: The scheme is odor -&gt; tag -&gt; odor learning. The tag is used to change the strength of synapses from KCs to output neurons in the lobes of the mushroom bodies which is the where odor learning  occurs. Because tags are non-overlapping, the tags, and thus the odors, are not mixed up.\n\n(2) Is this way of attaching tags to arbitrary points in a high dimensional space unique to insects? Mammals also have brain regions that can generate tags for complex stimuli. Neurons that respond to faces are found in the inferior temporal cortex, and these 'face' cells probably project to the hippocamppus where learning commonly occurs. Doris Tsao recently provided me with data for about 100 face cells recorded from monkeys  presented with 2000 different human face images. On analysis of tthese data, I found that the population of face cells has the same mean firing rate for all 2000 faces, and that the distribution of rates is, like the fly olfactory system, an exponential function. This parallel suggests that the fly olfactory system and the inferior temporal cortex are using the same learning scheme.\n\n(3) We noticed that the fly odor algorithm has some similarities to the way Google dose a similarity search (\"Show me the 50 pictures of elephants that are most like my elephant picture.\"). It differs, however, in three specific ways. We wondered how well the Google similarity search algorithm would work if it was modified to work the fly's way. We discovered that the fly's way was never worse that the Google way, and that it was usually better (Science, 358:793,2017). We believe this is the first example of directly translating a brain algorithm into a computer algorithm. If this works for other examples, this sort of translation could have considerable impact on computer science.\n\n\t\t\t\t\tLast Modified: 11/14/2017\n\n\t\t\t\t\tSubmitted by: Charles F Stevens"
 }
}