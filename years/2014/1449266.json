{
 "awd_id": "1449266",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Wireless Sensing of Speech Kinematics and Acoustics for Remediation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 149994.0,
 "awd_amount": 149994.0,
 "awd_min_amd_letter_date": "2014-07-03",
 "awd_max_amd_letter_date": "2014-07-03",
 "awd_abstract_narration": "Speech is a complex and intricately timed task that requires the coordination of numerous muscle groups and physiological systems.  While most children acquire speech with relative ease, it is one of the most complex patterned movements accomplished by humans and thus susceptible to impairment.  Approximately 2% of Americans have imprecise speech either due to mislearning during development (articulation disorder) or as a result of neuromotor conditions such as stroke, brain injury, Parkinson's disease, cerebral palsy, etc.  An equally sizeable group of Americans have difficulty with English pronunciation because it is their second language.  Both of these user groups would benefit from tools that provide explicit feedback on speech production clarity.  Traditional speech remediation relies on viewing a trained clinician's accurate articulation and repeated practice with visual feedback via a mirror.  While these interventions are effective for readily viewable speech sounds (visemes such as /b/p/m/), they are largely unsuccessful for sounds produced inside the mouth.  The tongue is the primary articulator for these obstructed sounds and its movements are difficult to capture.  Thus, clinicians use diagrams and other low-tech means (such as placing edible substances on the palate or physically manipulating the oral articulators) to show clients where to place their tongue.  While sophisticated research tools exist for measuring and tracking tongue movements during speech, they are prohibitively expensive, obtrusive, and impractical for clinical and/or home use.  The PIs' goal in this exploratory project, which represents a collaboration across two institutions, is to lay the groundwork for a Lingual-Kinematic and Acoustic sensor technology (LinKa) that is lightweight, low-cost, wireless and easy to deploy both clinically and at home for speech remediation.\r\n\r\nPI Ghovanloo's lab has developed a low-cost, wireless, and wearable magnetic sensing system, known as the Tongue Drive System (TDS).  An array of electromagnetic sensors embedded within a headset detects the position of a small magnet that is adhered to the tongue.  Clinical trials have demonstrated the feasibility of using the TDS for computer access and wheelchair control by sensing tongue movements in up to 6 discrete locations within the oral cavity.  This research will leverage the sensing capabilities of the TDS system and PI Patel's expertise in spoken interaction technologies for individuals with speech impairment, as well as Co-PI Fu's work on machine learning and multimodal data fusion, to develop a prototype clinically viable tool for enhancing speech clarity by coupling lingual-kinematic and acoustic data.  To this end, the team will extend the TDS to track tongue movements during running speech, which are quick, compacted within a small area of the oral cavity, and often overlap for several phonemes, so the challenge will be to accurately classify movements for different sound classes.  To complement this effort, pattern recognition of sensor spatiotemporal dynamics will be embedded into an interactive game to offer a motivating, personalized context for speech motor (re)learning by enabling audiovisual biofeedback, which is critical for speech modification.  To benchmark the feasibility of the approach, the system will be evaluated on six individuals with neuromotor speech impairment and six healthy age-matched controls.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rupal",
   "pi_last_name": "Patel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rupal Patel",
   "pi_email_addr": "rupal@vocaliD.ai",
   "nsf_id": "000671566",
   "pi_start_date": "2014-07-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yun",
   "pi_last_name": "Fu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yun Fu",
   "pi_email_addr": "y.fu@neu.edu",
   "nsf_id": "000560082",
   "pi_start_date": "2014-07-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave.",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 149994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"DataField11pt\"><strong>Significance:</strong> Nearly 4.1 million Americans suffer from some form of speech motor impairment.&nbsp; Speech delays or disorders are estimated to occur in 10% of preschool and school age children. In adults, speech impairments are notable in 30-40% of those with traumatic brain injury (TBI), 25% of stroke survivors, and 60% of those with degenerative diseases such as Parkinson?s disease. In addition to obvious consequences on communicative success, speech impairments have been shown to increase risk for psychiatric disorders as well as result in social and behavioral problems in children and adversely impact psychosocial factors and quality of life in adults.</p>\n<p class=\"DataField11pt\">Unlike other physical disabilities where the affected structure is both visible and accessible for manipulation, speech impairments can persist despite intervention. Resistance to motor (re)learning is due in part to the duration and frequency of intervention (typically one 30?60 min. session/week) as well as the lack of reliable, objective tools to monitor progress. Once a patient leaves the treatment room, if motor (re)learning has not generalized, incorrect motor programs continue to be practiced during the remaining 8-10 talking hours/day. Technologies that can offer online corrective biofeedback have the potential to significantly advance current clinical practice and thereby mitigate health care costs.&nbsp;</p>\n<p><strong>Intellectual merit.</strong> This interdisciplinary collaborative project was aimed at developing a low-cost, multimodal sensor system to detect and improve speech precision via biofeedback. The LinKA (Lingual Kinematic and Acoustic) system captures speech movements using an array of twenty-four 3-axial magnetometers that track movements of a magnet affixed to the tongue, a forward-facing camera used to monitor lip gestures, and a pair of microphones that record speech acoustics. These signals are synchronized and displayed to a user on a graphical interface for real-time speech biofeedback. Throughout the lifecycle of this project, the design and accuracy of the software and hardware were informed by user studies. Continued efforts to improve the usability of the system and to develop an intuitive interface for end users and clinicians are warranted.</p>\n<p class=\"DataField11pt\"><strong>Broader implications.</strong> In addition to providing critical new tool for speech remediation, this work has had scientific and educational impact on the fields of speech science, signal processing and automatic speech recognition. The collaborative and interdisciplinary nature of the project team provided opportunities for students and faculty to learn across disciplinary boundaries.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/05/2018<br>\n\t\t\t\t\tModified by: Rupal&nbsp;Patel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Significance: Nearly 4.1 million Americans suffer from some form of speech motor impairment.  Speech delays or disorders are estimated to occur in 10% of preschool and school age children. In adults, speech impairments are notable in 30-40% of those with traumatic brain injury (TBI), 25% of stroke survivors, and 60% of those with degenerative diseases such as Parkinson?s disease. In addition to obvious consequences on communicative success, speech impairments have been shown to increase risk for psychiatric disorders as well as result in social and behavioral problems in children and adversely impact psychosocial factors and quality of life in adults.\nUnlike other physical disabilities where the affected structure is both visible and accessible for manipulation, speech impairments can persist despite intervention. Resistance to motor (re)learning is due in part to the duration and frequency of intervention (typically one 30?60 min. session/week) as well as the lack of reliable, objective tools to monitor progress. Once a patient leaves the treatment room, if motor (re)learning has not generalized, incorrect motor programs continue to be practiced during the remaining 8-10 talking hours/day. Technologies that can offer online corrective biofeedback have the potential to significantly advance current clinical practice and thereby mitigate health care costs. \n\nIntellectual merit. This interdisciplinary collaborative project was aimed at developing a low-cost, multimodal sensor system to detect and improve speech precision via biofeedback. The LinKA (Lingual Kinematic and Acoustic) system captures speech movements using an array of twenty-four 3-axial magnetometers that track movements of a magnet affixed to the tongue, a forward-facing camera used to monitor lip gestures, and a pair of microphones that record speech acoustics. These signals are synchronized and displayed to a user on a graphical interface for real-time speech biofeedback. Throughout the lifecycle of this project, the design and accuracy of the software and hardware were informed by user studies. Continued efforts to improve the usability of the system and to develop an intuitive interface for end users and clinicians are warranted.\nBroader implications. In addition to providing critical new tool for speech remediation, this work has had scientific and educational impact on the fields of speech science, signal processing and automatic speech recognition. The collaborative and interdisciplinary nature of the project team provided opportunities for students and faculty to learn across disciplinary boundaries.\n\n \n\n\t\t\t\t\tLast Modified: 12/05/2018\n\n\t\t\t\t\tSubmitted by: Rupal Patel"
 }
}