{
 "awd_id": "1405822",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CI-New: Collaborative Research: Federated Data Set Infrastructure for Recognition Problems in Computer Vision",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2014-10-01",
 "awd_exp_date": "2019-09-30",
 "tot_intn_awd_amt": 299751.0,
 "awd_amount": 299751.0,
 "awd_min_amd_letter_date": "2014-08-26",
 "awd_max_amd_letter_date": "2014-08-26",
 "awd_abstract_narration": "Broad access to image and video datasets has been responsible for much of the progress in computer vision recognition problems over the last decade.  These common benchmarks have played a leading role in transforming recognition research from a black art into an experimental science.  Progress, however, has stagnated; although datasets continue to grow, they are developed and annotated in isolation: e.g., a collection of sporting activities, a set of objects in images, etc. These isolated datasets suffer from task and domain-specific bias, and knowledge transfer across them is extremely limited.  This project is investigating and establishing a prototype architecture that federates across various recognition problems and modalities, by establishing a common namespace for entities, events and annotations across the datasets.  The project is also establishing a web-portal for the prototype federated dataset architecture and linking two existing recognition datasets into the prototype architecture. The resulting federated structure is truly greater than the sum of its parts, and can support new research that was not previously possible for the computer vision community and other related fields.\r\n\r\nAs a first test scenario for this federated architecture, this project is investigating and constructing a new federated dataset of images and video annotated with various forms of associated text.  Image and video content annotations span both the spatial and temporal dimensions while textual annotations reflecting depicted content range from complete free-form natural language descriptions, to more targeted phrases and referring expressions, to individual keyword lists.  This dataset is being constructed to promote and enhance collaboration efforts between the vision and language communities by providing a new multi-modal annotated dataset with associated research competitions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tamara",
   "pi_last_name": "Berg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tamara Berg",
   "pi_email_addr": "tlberg@cs.unc.edu",
   "nsf_id": "000519059",
   "pi_start_date": "2014-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "UNC Chapel Hill",
  "perf_str_addr": "Computer Science Department",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 299751.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The goal of this grant was to build a rich new collection of datasets for the computer vision and natural language processing communities to help drive research in multi-modal understanding and applications. To enhance the usefulness of these datasets they were federated, with common sets of images and videos and a common semantic space, allowing for meaningful translation and cross-pollination between each dataset.&nbsp;</span></p>\n<p><span>In particular, the grant resulted in <span>two datasets of natural language annotations augmenting the MS COCO image set -- the Visual Madlibs Dataset, a fill-in-the blanks question-answering dataset, and the Referring Expression Dataset, a crowd-sourced dataset of natural language expressions referring to objects in natural scenes, collected in a two-player game. In addition, several datasets were collected on video clips from 6 popular TV series. The resulting TVQA dataset contains 152.5k question-answer pairs from 21.8k video clips, spanning over 460 hours of video for driving research in multi-modal question-answering. Questions were designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. In later work, several augmentations were added to the TVQA datset, including object-level bounding boxes (TVQA+), localized video moments and natural language queries for moment retrieval tasks (TVR), and caption descriptions paired with video moments (TVC). All code and datasets from this work have been released publicly and are helping to drive research into better multi-modal understanding.</span></span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/09/2020<br>\n\t\t\t\t\tModified by: Tamara&nbsp;Berg</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this grant was to build a rich new collection of datasets for the computer vision and natural language processing communities to help drive research in multi-modal understanding and applications. To enhance the usefulness of these datasets they were federated, with common sets of images and videos and a common semantic space, allowing for meaningful translation and cross-pollination between each dataset. \n\nIn particular, the grant resulted in two datasets of natural language annotations augmenting the MS COCO image set -- the Visual Madlibs Dataset, a fill-in-the blanks question-answering dataset, and the Referring Expression Dataset, a crowd-sourced dataset of natural language expressions referring to objects in natural scenes, collected in a two-player game. In addition, several datasets were collected on video clips from 6 popular TV series. The resulting TVQA dataset contains 152.5k question-answer pairs from 21.8k video clips, spanning over 460 hours of video for driving research in multi-modal question-answering. Questions were designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. In later work, several augmentations were added to the TVQA datset, including object-level bounding boxes (TVQA+), localized video moments and natural language queries for moment retrieval tasks (TVR), and caption descriptions paired with video moments (TVC). All code and datasets from this work have been released publicly and are helping to drive research into better multi-modal understanding.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 06/09/2020\n\n\t\t\t\t\tSubmitted by: Tamara Berg"
 }
}