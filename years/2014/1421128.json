{
 "awd_id": "1421128",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small: Symbolic computation with sparsity, error checking and error correction",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balasubramanian Kalyanasundaram",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 469905.0,
 "awd_amount": 469905.0,
 "awd_min_amd_letter_date": "2014-06-06",
 "awd_max_amd_letter_date": "2014-06-06",
 "awd_abstract_narration": "A hallmark of symbolic computation is that the outputs are exact. Digital error correcting decoding produces exact outputs from inputs in which some entries are incorrect, and minimizes the required redundancy.  Symbolic polynomial interpolation algorithms take advantage of sparsity.  Kaltofen proposes to create algorithms that can interpolate sparse uni- and multivariate polynomials and rational functions from evaluations where some of the values are incorrect.  A goal is to minimize the amount of oversampling that is necessary to locate and correct those faulty evaluations.  Hybrid symbolic-numeric computation accepts approximate scalar entries in the inputs, which can be imprecise because they come from a floating point computation or a physical measurement.  Sparse multivariate polynomial interpolation has been adapted to such data, for purpose of constructing sparse models for the observed measurements, and Kaltofen proposes to create hybrid symbolic-numeric versions of our interpolation algorithms that can correct outlier errors. In addition, Kaltofen proposes to construct easily verifiable certificates for complex non-linear problems, such as certificates that a real multivariate polynomial is unbounded or that a symmetric real matrix is positive definite.  Both problems are important for global non-linear optimization.  Lastly, Kaltofen proposes to apply the matrix generalization of the Berlekamp/Massey algorithm and the multidimensional generalization by Shojiro Sakata to recurrences with polynomial coefficients, such as the sequence of the factorials and the binomial coefficients.  He will also study how to correct errors in the linear generated arrays.\r\n\r\nKaltofen's proposed research combines hybrid symbolic-numeric computation with digital error-correcting decoding for purpose of removing outliers in sparse model synthesis, which constitutes a brand-new approach for ``cleaning-up'' errors in data sets. Certificates that prove that computed minima are global minima permit the use of unproven algorithmic heuristics in the optimization methods, especially algorithms with floating point arithmetic whose stability is not analyzed, and greatly broaden what can be placed in publishable software:  the programs do not give a false output.  Lastly, recurrences are fundamental tools in symbolic computation algorithms.  Kaltofen is making the developed software for the algorithms freely available.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Erich",
   "pi_last_name": "Kaltofen",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Erich L Kaltofen",
   "pi_email_addr": "kaltofen@eos.ncsu.edu",
   "nsf_id": "000367970",
   "pi_start_date": "2014-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276958205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7933",
   "pgm_ref_txt": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 469905.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div>Merit:</div>\n<div>When synthesizing a compressed, that is, concise model for a data set of observation points, one can choose different building blocks for the model.&nbsp; The project focused on orthogonal functions as the basis of representation.&nbsp; Kaltofen and his postdoctoral scholar and collaborators have shown how to compute a function that fits the data and which is a linear sum of Chebyshev polynomials (of all 4 kinds) from twice as many points as polynomials in the sum. In other words, a t-sparse model is constructed from 2 times t observations.&nbsp; In addition, they give algorithms that correct errors in the observations, that is, detect function points are incorrect outliers.&nbsp; One of the results shows that for sparse models constructed from points and values that are real numbers, one can in theory correct errors by oversampling at as many points as there are errors, which is the error rate for the classical algebraic Reed-Solomon code.&nbsp; However, for sparse models that fit points and values which are complex numbers, very few errors can lead to multiple models that fit the points except with those few errors at possibly different locations.&nbsp; Kaltofen and his collaborators have addressed the problem by list-decoding, meaning by computing (in polynomial-time) a set of models that fit.&nbsp; However, the correctable error rate is still much smaller than in the real number case.&nbsp; That list-decoding sparse interpolation algorithm is the only algorithm of polynomial-time complexity known in the real number case, which then returns a single real model.</div>\n<div>Kaltofen and his PhD student and two outside collaborators have deployed algebraic error correcting decoding to solving systems of linear equations by interpolation, again when some of the scalar linear systems, which are input and which are assumed to be projections by evaluation of the parameter in the parametric system, are erroneous.&nbsp; In turn, the algorithms by Kaltofen et al.&nbsp; can be used in two special cases to decode beyond the information-theoretical maximal transmission error rate (the classical Reed-Solomon limit): 1. when the recovered polynomials form the solution of parametric linear system; 2. when the errors occur in clusters (``bursts''): they have designed a so-called interleaved Reed-Solomon decoder that can corrects burst errors with a higher error rate.&nbsp; In comparison to the interleaved codes from the literature, the errors need not be uniformly random.</div>\n<div>Kaltofen and Jean-Guillaume Dumas at the University of Grenoble have designed certificates for high complexity computational tasks that are executed in the cloud.&nbsp; The certificates are tested by a verifier in low complexity and verify that the cloud has faithfully performed the computation.&nbsp; Kaltofen et al. focus on problems from symbolic computation, for example, the computation of the determinant of a very large but sparse matrix over a finite field or the solution of a system of polynomial equations in many variables.&nbsp; Their certificates for the determinants of a sparse matrix can be tested by the verifier in essentially linear time in the input size, which is optimal within logarithmic time factors.&nbsp; Their certificates for polynomial systems are based on the Goldwasser-Kalai-Rothblum (GKR) interactive protocol for delegated computing.&nbsp; GKR provide proof-of-work for any algorithm in the complexity class NC.&nbsp; Kaltofen has generalized the GKR protocol to any algorithm in the complexity class polynomial-time-uniform-NC.&nbsp; The latter class relaxes the complexity required for the representation of the exponential-size and polynomial-depth circuit, which constitutes the algorithm.&nbsp; GKR require that the circuit structure can be computed by algorithms of linear space complexity, which Kaltofen has generalized to polynomial-time complexity.&nbsp; Note that linear space complexity is always polynomial-time but not the reverse.</div>\n<div>Broader impact:</div>\n<div>A PhD student who was a full-time research assistant on the project for seven semesters is an African American male.&nbsp; The PhD student participated in all major conferences in the symbolic computation discipline and presented one peer-reviewed paper at the 2017 International Symposium on Symbolic and Algebraic Computation.&nbsp; The PhD student also participated in all of the annually occurring Conferences for African-American Researchers in the Mathematical Sciences (CARMS), and won the Angela E.&nbsp; Grant Poster Award for Best Algorithm in 2016 and 2018.&nbsp; The PhD student is tutoring African-American high school students in mathematics.&nbsp; He is hired by their parents specifically as a role model for his pupils.</div>\n<div>There were 2 postdoctoral scholars funded by the project, one of who is now employed by Google.</div>\n<div>Kaltofen co-organized the Major Program in Computer Algebra at the Fields Institute in Toronto in Fall 2015 and spent 6 weeks (October 2015 through first half of November 2015) there together with his PhD student.&nbsp; He was the principal investigator of the NSF portion of the funding for the program.</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/08/2018<br>\n\t\t\t\t\tModified by: Erich&nbsp;L&nbsp;Kaltofen</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nMerit:\nWhen synthesizing a compressed, that is, concise model for a data set of observation points, one can choose different building blocks for the model.  The project focused on orthogonal functions as the basis of representation.  Kaltofen and his postdoctoral scholar and collaborators have shown how to compute a function that fits the data and which is a linear sum of Chebyshev polynomials (of all 4 kinds) from twice as many points as polynomials in the sum. In other words, a t-sparse model is constructed from 2 times t observations.  In addition, they give algorithms that correct errors in the observations, that is, detect function points are incorrect outliers.  One of the results shows that for sparse models constructed from points and values that are real numbers, one can in theory correct errors by oversampling at as many points as there are errors, which is the error rate for the classical algebraic Reed-Solomon code.  However, for sparse models that fit points and values which are complex numbers, very few errors can lead to multiple models that fit the points except with those few errors at possibly different locations.  Kaltofen and his collaborators have addressed the problem by list-decoding, meaning by computing (in polynomial-time) a set of models that fit.  However, the correctable error rate is still much smaller than in the real number case.  That list-decoding sparse interpolation algorithm is the only algorithm of polynomial-time complexity known in the real number case, which then returns a single real model.\nKaltofen and his PhD student and two outside collaborators have deployed algebraic error correcting decoding to solving systems of linear equations by interpolation, again when some of the scalar linear systems, which are input and which are assumed to be projections by evaluation of the parameter in the parametric system, are erroneous.  In turn, the algorithms by Kaltofen et al.  can be used in two special cases to decode beyond the information-theoretical maximal transmission error rate (the classical Reed-Solomon limit): 1. when the recovered polynomials form the solution of parametric linear system; 2. when the errors occur in clusters (``bursts''): they have designed a so-called interleaved Reed-Solomon decoder that can corrects burst errors with a higher error rate.  In comparison to the interleaved codes from the literature, the errors need not be uniformly random.\nKaltofen and Jean-Guillaume Dumas at the University of Grenoble have designed certificates for high complexity computational tasks that are executed in the cloud.  The certificates are tested by a verifier in low complexity and verify that the cloud has faithfully performed the computation.  Kaltofen et al. focus on problems from symbolic computation, for example, the computation of the determinant of a very large but sparse matrix over a finite field or the solution of a system of polynomial equations in many variables.  Their certificates for the determinants of a sparse matrix can be tested by the verifier in essentially linear time in the input size, which is optimal within logarithmic time factors.  Their certificates for polynomial systems are based on the Goldwasser-Kalai-Rothblum (GKR) interactive protocol for delegated computing.  GKR provide proof-of-work for any algorithm in the complexity class NC.  Kaltofen has generalized the GKR protocol to any algorithm in the complexity class polynomial-time-uniform-NC.  The latter class relaxes the complexity required for the representation of the exponential-size and polynomial-depth circuit, which constitutes the algorithm.  GKR require that the circuit structure can be computed by algorithms of linear space complexity, which Kaltofen has generalized to polynomial-time complexity.  Note that linear space complexity is always polynomial-time but not the reverse.\nBroader impact:\nA PhD student who was a full-time research assistant on the project for seven semesters is an African American male.  The PhD student participated in all major conferences in the symbolic computation discipline and presented one peer-reviewed paper at the 2017 International Symposium on Symbolic and Algebraic Computation.  The PhD student also participated in all of the annually occurring Conferences for African-American Researchers in the Mathematical Sciences (CARMS), and won the Angela E.  Grant Poster Award for Best Algorithm in 2016 and 2018.  The PhD student is tutoring African-American high school students in mathematics.  He is hired by their parents specifically as a role model for his pupils.\nThere were 2 postdoctoral scholars funded by the project, one of who is now employed by Google.\nKaltofen co-organized the Major Program in Computer Algebra at the Fields Institute in Toronto in Fall 2015 and spent 6 weeks (October 2015 through first half of November 2015) there together with his PhD student.  He was the principal investigator of the NSF portion of the funding for the program.\n\n \n\n\t\t\t\t\tLast Modified: 12/08/2018\n\n\t\t\t\t\tSubmitted by: Erich L Kaltofen"
 }
}