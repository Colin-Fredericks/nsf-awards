{
 "awd_id": "1460149",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CRCNS:  Adaptive perceptual-motor feedback for the analysis of complex scenes",
 "cfda_num": "47.074",
 "org_code": "08090200",
 "po_phone": "7032924845",
 "po_email": "sraghava@nsf.gov",
 "po_sign_block_name": "Sridhar Raghavachari",
 "awd_eff_date": "2014-08-05",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 694905.0,
 "awd_amount": 694905.0,
 "awd_min_amd_letter_date": "2014-09-09",
 "awd_max_amd_letter_date": "2014-09-09",
 "awd_abstract_narration": "The broad goal of this project is to understand the processes that support perception and action in complex settings.  The research focuses on spatial perception and navigation in the echolocating bat, an auditory specialist that produces high frequency sonar calls and listens to echo returns to determine the location of objects in its environment.  The echolocating bat modifies its sonar calls in response to echo information from targets (insect prey) and obstacles, and quantitative analyses of this animal?s adaptive vocal behavior will be used to infer its perception of a changing environment.  The biological component of this research combines behavioral and neurophysiological experiments to gain insight to how sensory information from complex scenes is coded and used to guide behaviors.  Analysis of behavioral and neural data will be coordinated with modeling efforts and the development of a robotic spatial navigation system.  Together, the biological and engineering arms of this research project will generate new knowledge that contributes to a deeper understanding of perception and action in complex, natural environments.  Students and postdocs working on this project will learn to translate knowledge and methodologies across biology and engineering, ranging from ethology and neurobiology to computational modeling and robotic demonstrations. These individuals will be poised to make major contributions that impact both basic science and future technology, enabling breakthroughs that cannot be achieved through work solely within traditional disciplines.  This research project will contribute to a rich library of multimedia materials that will be made available to educators and scientists working in both the private and public sectors: http://www.bsos.umd.edu/psyc/batlab/movies.html.  Collectively, this research has wide-ranging impact for neurobiology, interdisciplinary research training, neuroscience techniques, robotics, and the design of assistive devices.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "IOS",
 "org_div_long_name": "Division Of Integrative Organismal Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cynthia",
   "pi_last_name": "Moss",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Cynthia F Moss",
   "pi_email_addr": "cynthia.moss@jhu.edu",
   "nsf_id": "000233746",
   "pi_start_date": "2014-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N. Charles Street",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "771300",
   "pgm_ele_name": "Activation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1096",
   "pgm_ref_txt": "NEURAL SYSTEMS"
  },
  {
   "pgm_ref_code": "7327",
   "pgm_ref_txt": "CRCNS"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 694905.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The sensory world of an animal is noisy, complex and dynamic. &nbsp;From a barrage of stimuli, an animal must detect, sort, group and track biologically relevant signals to communicate with conspecifics, seek food, engage in courtship, avoid predators and navigate in space. &nbsp;Parsing, integrating and organizing complex acoustic stimuli to support such behaviors are tasks of auditory scene analysis, the perceptual organization of sound, which must be coordinated with motor behaviors to enable successful orientation and navigation in the environment. A great deal of research on auditory scene analysis has employed simplified acoustic stimuli in artificial tasks. &nbsp;Furthermore, movement, which would typically influence the listener&rsquo;s perception of a natural soundscape, is often excluded from studies of scene analysis. &nbsp;To bridge this gap, we adopted a neuroethological approach to the problem of auditory scene analysis. &nbsp;Specifically, we integrated behavioral, neurobiological, and computational studies of scene analysis in the echolocating bat, an animal that can negotiate a complex auditory world in complete darkness. &nbsp;</p>\n<p><br /> For the echolocating bat, the analysis of auditory scenes builds upon its active production of sounds that reflect from objects in the environment. &nbsp;The bat adaptively adjusts the features of its sonar vocalizations in response to information obtained from echo returns. &nbsp;Therefore, the bat&rsquo;s behavioral control of its sonar vocalizations provides a window into its perceptual world. Importantly, the bat adjusts the direction and duration of its calls to probe information from different locations in space, and this vocal-motor control provides a measure of the animal&rsquo;s acoustic gaze. &nbsp;Our studies of auditory scene analysis focused on the perceptual and motor systems that enable an animal to select out and track objects in the complex natural scene.</p>\n<p>&nbsp;</p>\n<p>We analyzed adaptive sonar behavior and neural recordings from bats engaged in target selection and tracking tasks, and used the data to develop mathematical representations and neural architectures for the analysis of natural scenes. &nbsp;Within this framework, we investigated the role of attention and planning in the representation of auditory objects in the environment.&nbsp; We discovered that bats adjust the timing and spectro-temporal features of their sonar sounds to discriminate target features, to localize and track objects in clutter, and to avoid interference from the calls and echoes of other bats. &nbsp;</p>\n<p>&nbsp;</p>\n<p>Our project included measurement of head and ear movements of bats engaged in a sonar target localization and tracking task.&nbsp; Here, we discovered that bats move their ears further apart as the distance to a target decreases.&nbsp; This finding suggests that the bat enhances cues for sonar localization accuracy by increasing differences in arrival time and intensity of echoes arriving at the two ears. &nbsp;We also discovered that the bat waggles its head as it tracks a target, which would serve to further increase differences between echoes arriving at the two ears and create acoustic parallax cues to gather information about a target&rsquo;s 3D position in space.</p>\n<p>&nbsp;</p>\n<p>In this project, we considered the challenge the bat faces in assigning echoes to the calls that produced them. &nbsp;If echoes from an echolocation call are received after the next call is broadcast, a situation that is likely to occur in cluttered environments, these &ldquo;aliased&rdquo; echoes appear as phantom objects.&nbsp; Little is known about how bats cope with this situation. In a model, we demonstrate a novel strategy with an artificial sonar system to manage aliasing in cases where a single target is actively being tracked.&nbsp; This engineered system reacts to aliased echoes by changing the time between calls to move the aliased echoes away from echoes returning from the tracked target.</p>\n<p>&nbsp;</p>\n<p>Our project yielded important discoveries about neural coding of stimulus space in freely behaving animals. &nbsp;Using wireless multichannel neural recordings, synchronized with high-speed stereo video and audio data, and an echo model, which computes the animal&rsquo;s instantaneous stimulus space, we demonstrated 3D echo-evoked receptive fields of neurons in the midbrain of free-flying bats engaged in naturalistic echolocation tasks. A key finding of this project is that the bat&rsquo;s active sonar inspection of objects dramatically sharpens range tuning of sensory neurons. &nbsp;Importantly, because animals were flying and actively engaged in a navigation task, we uncovered strong dynamism in neural activity that could not be measured in traditional experiments &ndash; a dramatic result which highlights the importance of studying neural systems in the context of species-specific, natural behaviors.</p>\n<p><br /> Finally, our project has provided an excellent interdisciplinary training platform for undergraduate/graduate students and postdoctoral researchers in the fields of biology, psychology, neuroscience, computer science and engineering, who have acquired new skills and perspectives through participation in this research. &nbsp;We have also created multimedia teaching tools that are used by instructors around the world. &nbsp;In addition, our research has wide-ranging impact for general advances in neuroscience techniques, robotics, and the design of assistive medical devices.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2016<br>\n\t\t\t\t\tModified by: Cynthia&nbsp;F&nbsp;Moss</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe sensory world of an animal is noisy, complex and dynamic.  From a barrage of stimuli, an animal must detect, sort, group and track biologically relevant signals to communicate with conspecifics, seek food, engage in courtship, avoid predators and navigate in space.  Parsing, integrating and organizing complex acoustic stimuli to support such behaviors are tasks of auditory scene analysis, the perceptual organization of sound, which must be coordinated with motor behaviors to enable successful orientation and navigation in the environment. A great deal of research on auditory scene analysis has employed simplified acoustic stimuli in artificial tasks.  Furthermore, movement, which would typically influence the listener?s perception of a natural soundscape, is often excluded from studies of scene analysis.  To bridge this gap, we adopted a neuroethological approach to the problem of auditory scene analysis.  Specifically, we integrated behavioral, neurobiological, and computational studies of scene analysis in the echolocating bat, an animal that can negotiate a complex auditory world in complete darkness.  \n\n\n For the echolocating bat, the analysis of auditory scenes builds upon its active production of sounds that reflect from objects in the environment.  The bat adaptively adjusts the features of its sonar vocalizations in response to information obtained from echo returns.  Therefore, the bat?s behavioral control of its sonar vocalizations provides a window into its perceptual world. Importantly, the bat adjusts the direction and duration of its calls to probe information from different locations in space, and this vocal-motor control provides a measure of the animal?s acoustic gaze.  Our studies of auditory scene analysis focused on the perceptual and motor systems that enable an animal to select out and track objects in the complex natural scene.\n\n \n\nWe analyzed adaptive sonar behavior and neural recordings from bats engaged in target selection and tracking tasks, and used the data to develop mathematical representations and neural architectures for the analysis of natural scenes.  Within this framework, we investigated the role of attention and planning in the representation of auditory objects in the environment.  We discovered that bats adjust the timing and spectro-temporal features of their sonar sounds to discriminate target features, to localize and track objects in clutter, and to avoid interference from the calls and echoes of other bats.  \n\n \n\nOur project included measurement of head and ear movements of bats engaged in a sonar target localization and tracking task.  Here, we discovered that bats move their ears further apart as the distance to a target decreases.  This finding suggests that the bat enhances cues for sonar localization accuracy by increasing differences in arrival time and intensity of echoes arriving at the two ears.  We also discovered that the bat waggles its head as it tracks a target, which would serve to further increase differences between echoes arriving at the two ears and create acoustic parallax cues to gather information about a target?s 3D position in space.\n\n \n\nIn this project, we considered the challenge the bat faces in assigning echoes to the calls that produced them.  If echoes from an echolocation call are received after the next call is broadcast, a situation that is likely to occur in cluttered environments, these \"aliased\" echoes appear as phantom objects.  Little is known about how bats cope with this situation. In a model, we demonstrate a novel strategy with an artificial sonar system to manage aliasing in cases where a single target is actively being tracked.  This engineered system reacts to aliased echoes by changing the time between calls to move the aliased echoes away from echoes returning from the tracked target.\n\n \n\nOur project yielded important discoveries about neural coding of stimulus space in freely behaving animals.  Using wireless multichannel neural recordings, synchronized with high-speed stereo video and audio data, and an echo model, which computes the animal?s instantaneous stimulus space, we demonstrated 3D echo-evoked receptive fields of neurons in the midbrain of free-flying bats engaged in naturalistic echolocation tasks. A key finding of this project is that the bat?s active sonar inspection of objects dramatically sharpens range tuning of sensory neurons.  Importantly, because animals were flying and actively engaged in a navigation task, we uncovered strong dynamism in neural activity that could not be measured in traditional experiments &ndash; a dramatic result which highlights the importance of studying neural systems in the context of species-specific, natural behaviors.\n\n\n Finally, our project has provided an excellent interdisciplinary training platform for undergraduate/graduate students and postdoctoral researchers in the fields of biology, psychology, neuroscience, computer science and engineering, who have acquired new skills and perspectives through participation in this research.  We have also created multimedia teaching tools that are used by instructors around the world.  In addition, our research has wide-ranging impact for general advances in neuroscience techniques, robotics, and the design of assistive medical devices.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/29/2016\n\n\t\t\t\t\tSubmitted by: Cynthia F Moss"
 }
}