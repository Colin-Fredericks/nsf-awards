{
 "awd_id": "1433228",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Small: Collaborative Research: Mobile Gesture Interaction for Kids: Sensing, Recognition, and Error Recovery",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2013-08-15",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 197100.0,
 "awd_amount": 213100.0,
 "awd_min_amd_letter_date": "2014-03-28",
 "awd_max_amd_letter_date": "2016-05-10",
 "awd_abstract_narration": "Though many children use mobile applications to support their learning and entertainment, the devices and underlying interactions were not designed specifically for children. The goal of this project is to make touch and gesture interactions more accessible and user-friendly to young users. This research will yield new understanding about the appropriate and successful ways to sense, recognize, and recover from errors in touch-based interactions with children. The approach involves studying children interacting with mobile applications that gather data on their touch and gesture interactions, as well as conducting design sessions with children to elicit their preferences for mobile device interactions and error feedback and recovery strategies. This approach will result in design guidelines for those creating applications and tools for young users. The proposed research contributes towards the evolution of alternative interaction technologies such as touch and gesture, and the understanding of child-computer interaction with new gesture-based technologies. \r\n\r\nBroader impacts: The broader impacts of this project lie in contributions towards the evolution of alternative interaction technologies such as touch and gesture, and the understanding of child-computer interaction with new gesture-based technologies. This work also will develop and validate an approach for investigating such interaction issues and designing improvements for them that can be used in future work with other populations such as older individuals or those with varying physical abilities.  The grant will support two female young investigators, and will fund research experiences to benefit computer science students attending Bowie State University, a minority serving institution.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lisa",
   "pi_last_name": "Anthony",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lisa Anthony",
   "pi_email_addr": "lanthony@cise.ufl.edu",
   "nsf_id": "000610857",
   "pi_start_date": "2014-03-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Florida",
  "inst_street_address": "1523 UNION RD RM 207",
  "inst_street_address_2": "",
  "inst_city_name": "GAINESVILLE",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "3523923516",
  "inst_zip_code": "326111941",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "FL03",
  "org_lgl_bus_name": "UNIVERSITY OF FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NNFQH1JAPEP3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Florida",
  "perf_str_addr": "",
  "perf_city_name": "Gainesville",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "326112002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "FL03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 197100.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With 2.1 billion touch screen devices sold through 2017, there little doubt that smart phones and tablets are an integral part of life in the 21<sup>st</sup> century. Though designed for adults, mobile devices are commonly used by children who either own their own device or have access to one. For example, mobile devices are now the most popular gaming device for children, ages 2 to 17. As human computer interaction researchers, our interest is in exploring usage patterns of children and understanding how software can be designed to improve the accessibility and success of young children using touch screen devices. This project was initiated to investigate the ways in which young children use mobile devices, and to provide mobile application and interaction designers with empirical findings that can be extended in their work.</p>\n<p>In total over five years, we conducted user studies with over 60 adults and 120 children and collected over 46,000 touch events and 26,000 gestures. The first study conducted was designed to compare between adults (ages 18-33) and children (ages 5-17) using touch and gesture interaction on simple apps focusing on isolated interactions. The second study involved the use of an app that used contextualized interactions and allowed for further examination of children&rsquo;s interactions. Follow-up studies tested how well our previous findings on small-screen smartphones would extend to larger screen devices such as tablets and tabletop computers.</p>\n<p>Though mobile device platforms respond quickly to users&rsquo; touches, prior work has shown that users sometimes experience unintended behaviors. One of our key research results indicate that a reliable subset of touch events are actually intended for a target no longer onscreen (e.g., due to the child user perhaps not noticing the intended target has already been activated). We refer to these touches as <em>holdovers:</em> meaning touches that are located within the vicinity of the preceding target (Figure 1). As users of mobile devices ourselves, we have experienced <em>holdovers</em> in real tasks: touchscreen devices may be slow to register a touch and so we touch again intentionally in the same location, but the interface has already moved on, causing unintentional effects. Though experienced by users of all ages, we have found that holdovers are most prevalent among young users and occur with very small targets. We have also found that holdovers are distributed equally over the entire screen, suggesting that handedness and occlusion are not contributing factors. This key finding is an example of an interaction challenge and our findings can be useful for touchscreen application developers aiming to reduce unintended interactions.</p>\n<p>Another key result is that children&rsquo;s gestures on touchscreens are recognized much less accurately than those of adults. In fact, the gestures of the youngest children are recognized the most poorly. Poor recognition can prevent children from successfully interacting with the application. Machine recognition of gestures belongs to an area of technology development called &ldquo;pattern recognition.&rdquo; But because of children&rsquo;s still developing motor skills, their writing and gesturing abilities are inconsistent and unpredictable (Figure 3). This means looking for patterns in their gestures to help the machine algorithms do the recognition is a significant challenge. This project has dug in depth into children&rsquo;s gesturing patterns and identified some behaviors that could be filtered out to improve recognition. This key finding is an example of a technical challenge, and our findings can be useful for recognition experts aiming to improve recognition for children&rsquo;s gestures.</p>\n<p>The research results of these activities have been shared with the public though journal publications, conference presentations, and events including locally, nationally, and internationally. The design interactions in the published papers codify the empirical findings into actionable interface and interaction design strategies. These strategies can improve the accessibility and success of young children using touch screen devices.</p>\n<p>The broader impacts of this project at the University of Florida include the training and mentorship of thirteen students, including three black students (1 graduate male, 1 graduate female, and 1 undergraduate female) and a total of 7 female students (4 undergraduate and 3 graduate students), with exposure to research methods and training, including IRB applications, human computer interaction research, child computer interaction, and mobile application development. The students also participated in professional development activities including attending conference and presenting research findings. Students involved in the project are now in various career stages including one former undergraduate student who is now pursuing a PhD degree, and five software developers for large tech companies focused on mobile applications or other areas.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/15/2018<br>\n\t\t\t\t\tModified by: Lisa&nbsp;Anthony</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516027305366_Slide1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516027305366_Slide1--rgov-800width.jpg\" title=\"Left image shows hand touching a small blue target on a mobile touchscreen; right image shows the hand still touching the same spot on the screen although the blue target has changed position.\"><img src=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516027305366_Slide1--rgov-66x44.jpg\" alt=\"Left image shows hand touching a small blue target on a mobile touchscreen; right image shows the hand still touching the same spot on the screen although the blue target has changed position.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1. Attempt to touch target located in (a) after the interface has moved the target to new location (b).</div>\n<div class=\"imageCredit\">Quincy Brown</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Lisa&nbsp;Anthony</div>\n<div class=\"imageTitle\">Left image shows hand touching a small blue target on a mobile touchscreen; right image shows the hand still touching the same spot on the screen although the blue target has changed position.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516027837429_Slide2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516027837429_Slide2--rgov-800width.jpg\" title=\"Left image shows distribution of touch events around the target (black outline) in red squares for child touches and blue triangles for adult touches; right image shows a new target (black outline) with the distribution of touches showing some holdovers for the previous target position\"><img src=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516027837429_Slide2--rgov-66x44.jpg\" alt=\"Left image shows distribution of touch events around the target (black outline) in red squares for child touches and blue triangles for adult touches; right image shows a new target (black outline) with the distribution of touches showing some holdovers for the previous target position\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2. Holdover patterns from target (a) to (b).</div>\n<div class=\"imageCredit\">Anthony et al, ITS 2012</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Lisa&nbsp;Anthony</div>\n<div class=\"imageTitle\">Left image shows distribution of touch events around the target (black outline) in red squares for child touches and blue triangles for adult touches; right image shows a new target (black outline) with the distribution of touches showing some holdovers for the previous target position</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516028114800_Slide3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516028114800_Slide3--rgov-800width.jpg\" title=\"Left image shows samples from our children&rsquo;s gesture dataset, including letters, numbers, shapes, and symbols; right images shows standard forms of the gestures the children were writing.\"><img src=\"/por/images/Reports/POR/2018/1433228/1433228_10208596_1516028114800_Slide3--rgov-66x44.jpg\" alt=\"Left image shows samples from our children&rsquo;s gesture dataset, including letters, numbers, shapes, and symbols; right images shows standard forms of the gestures the children were writing.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 3. Children?s gesture patterns are heavily affected by their writing ability (a); intended gestures are shown in (b).</div>\n<div class=\"imageCredit\">Shaw et al, ICMI 2017</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Lisa&nbsp;Anthony</div>\n<div class=\"imageTitle\">Left image shows samples from our children?s gesture dataset, including letters, numbers, shapes, and symbols; right images shows standard forms of the gestures the children were writing.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWith 2.1 billion touch screen devices sold through 2017, there little doubt that smart phones and tablets are an integral part of life in the 21st century. Though designed for adults, mobile devices are commonly used by children who either own their own device or have access to one. For example, mobile devices are now the most popular gaming device for children, ages 2 to 17. As human computer interaction researchers, our interest is in exploring usage patterns of children and understanding how software can be designed to improve the accessibility and success of young children using touch screen devices. This project was initiated to investigate the ways in which young children use mobile devices, and to provide mobile application and interaction designers with empirical findings that can be extended in their work.\n\nIn total over five years, we conducted user studies with over 60 adults and 120 children and collected over 46,000 touch events and 26,000 gestures. The first study conducted was designed to compare between adults (ages 18-33) and children (ages 5-17) using touch and gesture interaction on simple apps focusing on isolated interactions. The second study involved the use of an app that used contextualized interactions and allowed for further examination of children?s interactions. Follow-up studies tested how well our previous findings on small-screen smartphones would extend to larger screen devices such as tablets and tabletop computers.\n\nThough mobile device platforms respond quickly to users? touches, prior work has shown that users sometimes experience unintended behaviors. One of our key research results indicate that a reliable subset of touch events are actually intended for a target no longer onscreen (e.g., due to the child user perhaps not noticing the intended target has already been activated). We refer to these touches as holdovers: meaning touches that are located within the vicinity of the preceding target (Figure 1). As users of mobile devices ourselves, we have experienced holdovers in real tasks: touchscreen devices may be slow to register a touch and so we touch again intentionally in the same location, but the interface has already moved on, causing unintentional effects. Though experienced by users of all ages, we have found that holdovers are most prevalent among young users and occur with very small targets. We have also found that holdovers are distributed equally over the entire screen, suggesting that handedness and occlusion are not contributing factors. This key finding is an example of an interaction challenge and our findings can be useful for touchscreen application developers aiming to reduce unintended interactions.\n\nAnother key result is that children?s gestures on touchscreens are recognized much less accurately than those of adults. In fact, the gestures of the youngest children are recognized the most poorly. Poor recognition can prevent children from successfully interacting with the application. Machine recognition of gestures belongs to an area of technology development called \"pattern recognition.\" But because of children?s still developing motor skills, their writing and gesturing abilities are inconsistent and unpredictable (Figure 3). This means looking for patterns in their gestures to help the machine algorithms do the recognition is a significant challenge. This project has dug in depth into children?s gesturing patterns and identified some behaviors that could be filtered out to improve recognition. This key finding is an example of a technical challenge, and our findings can be useful for recognition experts aiming to improve recognition for children?s gestures.\n\nThe research results of these activities have been shared with the public though journal publications, conference presentations, and events including locally, nationally, and internationally. The design interactions in the published papers codify the empirical findings into actionable interface and interaction design strategies. These strategies can improve the accessibility and success of young children using touch screen devices.\n\nThe broader impacts of this project at the University of Florida include the training and mentorship of thirteen students, including three black students (1 graduate male, 1 graduate female, and 1 undergraduate female) and a total of 7 female students (4 undergraduate and 3 graduate students), with exposure to research methods and training, including IRB applications, human computer interaction research, child computer interaction, and mobile application development. The students also participated in professional development activities including attending conference and presenting research findings. Students involved in the project are now in various career stages including one former undergraduate student who is now pursuing a PhD degree, and five software developers for large tech companies focused on mobile applications or other areas.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/15/2018\n\n\t\t\t\t\tSubmitted by: Lisa Anthony"
 }
}