{
 "awd_id": "1409823",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Social Learning in Mixed Human-Robot Groups for People with Disabilities",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2022-08-31",
 "tot_intn_awd_amt": 1062043.0,
 "awd_amount": 1094043.0,
 "awd_min_amd_letter_date": "2014-08-18",
 "awd_max_amd_letter_date": "2021-08-20",
 "awd_abstract_narration": "Assistive robots promise to improve the lives of many people with disabilities in the near future.  But whether due to traumatic spinal cord injury, early onset multiple sclerosis, or the common effects of advancing age, the variety of physical and mental disabilities, and the different psychological reaction of each individual to them, make it impossible to program one-size-fits-all behaviors for assistive robots.  To achieve its full potential, the assistive robot must learn to match the type and degree of assistance offered to the disability level and preferences of the user, as well as to the user's environment and the level of trust between the user and the robot.  Thus, training the robot to fit the individual user is essential - but requiring all users to train all aspects of robot behavior is unrealistic. In this collaborative project involving faculty at two institutions, the PIs argue that a possible solution may derive from the observation that whenever a user needs to train a robot for a new behavior, it is likely that there are other users with similar disabilities, preferences and environments who might also benefit from this behavior.  The PIs will develop techniques which enable the learning of behaviors in human+robot pairs, the identification of possible beneficiaries of the new behaviors, and the transfer of these behaviors to these beneficiaries (where transferring a behavior from one human+robot pair to another might involve the transfer of code and data for the robot and/or the transfer of skills to the human user).  This research will demonstrate how mixed human+robot interaction can alter the relationship between users and their environment, while also rendering physical interaction between robot and human safer and more efficient.  The work will have broad national impact because of the expected rapid growth in coming years of the elderly segment of the population.\r\n\r\nThe PIs will pursue four thrusts to achieve their vision.  They will design adaptive algorithms and controllers (e.g., for sliding-scale robot autonomy) which allow a robot to be an effective facilitator of user interaction with novel environments during activities of daily living (ADLs).  They will develop models of human+robot trust in the context of assistive robot technology, and examine the effect of trust on the user experience.  They will implement social agents through which the community of users with a specific disability via their social networks can help in the creation and adoption of new solutions for ADL tasks.  And they will validate the ability of human+robot exchanges to increase functionality and performance of ADLs for disabled individuals.  The research will build on recent advances in robot control, psychological models of social learning, and models of social networks, as well as machine learning techniques of collaborative filtering and recommendation.  Project outcomes will include the creation of social agents that can interact on behalf of the user, discover learning opportunities, and actively participate in the transfer of learning.  The work will contribute to our understanding of how users can partner, both individually and collectively, with assistive robots, and will answer open questions relating to the interoperability and intelligibility of knowledge developed in one learning system to another.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aman",
   "pi_last_name": "Behal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aman Behal",
   "pi_email_addr": "abehal@mail.ucf.edu",
   "nsf_id": "000493898",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Hancock",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Peter A Hancock",
   "pi_email_addr": "phancock@pegasus.cc.ucf.edu",
   "nsf_id": "000313114",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ladislau",
   "pi_last_name": "Boloni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ladislau Boloni",
   "pi_email_addr": "lboloni@cs.ucf.edu",
   "nsf_id": "000292644",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "The University of Central Florida Board of Trustees",
  "inst_street_address": "4000 CENTRAL FLORIDA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "ORLANDO",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "4078230387",
  "inst_zip_code": "328168005",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "FL10",
  "org_lgl_bus_name": "THE UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RD7MXJV7DKT9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Central Florida",
  "perf_str_addr": "",
  "perf_city_name": "Orlando",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "328263252",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "FL10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 724622.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 345421.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project contributed to the topic of assistive robotics from three different points of view.&nbsp;</p>\n<p><br />PI Behal's team considered the design of algorithms and control strategies to promote efficient planning and performance of activities of daily living by assistive robots. The team developed algorithms that allow the automated grasping of objects necessary for a wheelchair-bound user. These algorithms had been implemented both on a wheelchair-mounted MANUS robotic arm and a Baxter robot. To consider the particularities of a home environment, the robot algorithms had to consider the presence of cluttered, as well as the requirement that the robot applies a minimal grasping force to avoid deformation of objects being handled by the robotic gripper. In assistive robotics applications, the user is often part of the environment and might be a participant in the robot tasks, such as in the case of hair and facial grooming or feeding. The team also performed research studies about the appropriate design of a user interface that takes into consideration the capabilities and preferences of disabled users.&nbsp;</p>\n<p>Co-PI Hancock's team considered the problems of assistive robotics from a psychology and human factors perspective. The technical abilities of a robot are largely irrelevant if the users do not develop a trust relationship with the robot. The studies found that the shape of the robot and its distance from the user influences the trust the user feels in the robot and whether the user anthropomorphizes it.&nbsp;</p>\n<p>Co-PI B?l?ni's team focused on the artificial intelligence and machine learning aspects of the control of assistive robots. The behavior expected from an assistive robot is often difficult to describe in formal terms, as the actions must match the preferences of the user. Thus, the work centered on learning from demonstration models that capture how the users want a particular task to be executed. It was found, however, that it is often difficult to collect demonstrations safely in the home environment of a disabled person. To solve this problem, the team developed a virtual, gamified environment in which demonstrations can be performed safely and the environment can recover from failed demonstrations such as dropped objects. The use of a trained deep neural network allowed the translation of the robot behaviors learned in the virtual environment to the physical world. A follow-up paper extended this work to a system where the entire end-to-end robot controller was learned from demonstration, with a visual input being translated into a robot control signal.&nbsp;&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/28/2023<br>\n\t\t\t\t\tModified by: Ladislau&nbsp;Boloni</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674923726133_Robot-2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674923726133_Robot-2--rgov-800width.jpg\" title=\"Robot instructed by natural language statements\"><img src=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674923726133_Robot-2--rgov-66x44.jpg\" alt=\"Robot instructed by natural language statements\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The robot was instructed by natural language to pick up the red bowl. The deep learning-based visual servoing controller was trained by demonstration, and it is robust to physical disruptions.</div>\n<div class=\"imageCredit\">University of Central Florida</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Ladislau&nbsp;Boloni</div>\n<div class=\"imageTitle\">Robot instructed by natural language statements</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674923881174_Robot-3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674923881174_Robot-3--rgov-800width.jpg\" title=\"Robot adjusts to the movement of the human\"><img src=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674923881174_Robot-3--rgov-66x44.jpg\" alt=\"Robot adjusts to the movement of the human\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The assistive robot is tracking the movement of the face of the human and accurately adjusts it position to help the disabled person in the performance of activities of daily living.</div>\n<div class=\"imageCredit\">University of Central Florida</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Ladislau&nbsp;Boloni</div>\n<div class=\"imageTitle\">Robot adjusts to the movement of the human</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674924060569_Robot-1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674924060569_Robot-1--rgov-800width.jpg\" title=\"From virtual demonstration to physical robot\"><img src=\"/por/images/Reports/POR/2023/1409823/1409823_10334196_1674924060569_Robot-1--rgov-66x44.jpg\" alt=\"From virtual demonstration to physical robot\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The desired behavior for the assistive robot had been entirely demonstrated in a safe, virtual environment. The resulting behavior was then successfully transferred to a physical robot.</div>\n<div class=\"imageCredit\">University of Central Florida</div>\n<div class=\"imageSubmitted\">Ladislau&nbsp;Boloni</div>\n<div class=\"imageTitle\">From virtual demonstration to physical robot</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project contributed to the topic of assistive robotics from three different points of view. \n\n\nPI Behal's team considered the design of algorithms and control strategies to promote efficient planning and performance of activities of daily living by assistive robots. The team developed algorithms that allow the automated grasping of objects necessary for a wheelchair-bound user. These algorithms had been implemented both on a wheelchair-mounted MANUS robotic arm and a Baxter robot. To consider the particularities of a home environment, the robot algorithms had to consider the presence of cluttered, as well as the requirement that the robot applies a minimal grasping force to avoid deformation of objects being handled by the robotic gripper. In assistive robotics applications, the user is often part of the environment and might be a participant in the robot tasks, such as in the case of hair and facial grooming or feeding. The team also performed research studies about the appropriate design of a user interface that takes into consideration the capabilities and preferences of disabled users. \n\nCo-PI Hancock's team considered the problems of assistive robotics from a psychology and human factors perspective. The technical abilities of a robot are largely irrelevant if the users do not develop a trust relationship with the robot. The studies found that the shape of the robot and its distance from the user influences the trust the user feels in the robot and whether the user anthropomorphizes it. \n\nCo-PI B?l?ni's team focused on the artificial intelligence and machine learning aspects of the control of assistive robots. The behavior expected from an assistive robot is often difficult to describe in formal terms, as the actions must match the preferences of the user. Thus, the work centered on learning from demonstration models that capture how the users want a particular task to be executed. It was found, however, that it is often difficult to collect demonstrations safely in the home environment of a disabled person. To solve this problem, the team developed a virtual, gamified environment in which demonstrations can be performed safely and the environment can recover from failed demonstrations such as dropped objects. The use of a trained deep neural network allowed the translation of the robot behaviors learned in the virtual environment to the physical world. A follow-up paper extended this work to a system where the entire end-to-end robot controller was learned from demonstration, with a visual input being translated into a robot control signal.  \n\n\t\t\t\t\tLast Modified: 01/28/2023\n\n\t\t\t\t\tSubmitted by: Ladislau Boloni"
 }
}