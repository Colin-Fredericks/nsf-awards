{
 "awd_id": "1351089",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: New Abstractions for Sensitive Data Management in Modern Operating Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Indrajit Ray",
 "awd_eff_date": "2014-02-01",
 "awd_exp_date": "2020-01-31",
 "tot_intn_awd_amt": 499999.0,
 "awd_amount": 499999.0,
 "awd_min_amd_letter_date": "2014-01-30",
 "awd_max_amd_letter_date": "2018-03-23",
 "awd_abstract_narration": "The evolution of data storage in modern operating systems (OSes) brings challenges for fine-grained data protection. While traditional OSes offer simple, relatively low-level data abstractions -- files and directories -- modern operating systems, including Android and iOS, embed much higher-level abstractions, such as relational databases and object-relational models. The new abstractions complicate file structures and access patterns, greatly challenging existing protection systems, such as encrypted file systems, deniable file systems, antiviruses, and anomaly detectors, which, fallen behind the times, continue to operate at file level.\r\n\r\nIn this project, we are investigating new data protection abstractions that are better attuned to modern OSes. One example is a logical data object (LDO). An LDO corresponds to an application-specific resource -- such as an email, a document, or a bank account -- and includes all the data related to it, no matter how or where it is persisted (e.g., rows in databases, objects in object-relational models, files in the file system, etc.). Protection systems use LDOs to acquire rich semantics about the data to refine their effectiveness. Using LDOs, we are building HideIt, a fine-grained object hiding system that lets users select, through the familiar UIs of their unmodified applications, arbitrary objects -- such as individual emails, documents, bank accounts -- and hide or unhide them. By creating new, convenient protection abstractions, and teaching students and the broader community about them, we hope to promote a responsible approach to data management, in which users manage their data carefully, minimizing its exposure to attacks.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Roxana",
   "pi_last_name": "Geambasu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Roxana Geambasu",
   "pi_email_addr": "roxana@cs.columbia.edu",
   "nsf_id": "000602293",
   "pi_start_date": "2014-01-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 110659.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 97154.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 99721.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 102359.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 90106.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>Intellectual Merit:</p>\n<p><span>My project&nbsp;</span>revisits decades' old protection abstractions from operating systems (OSes), which have become unfit for emerging data-driven workloads. Traditional protection units -- such as files, directories, or database tables -- fail to support the data access and sharing patterns common in ML ecosystems.&nbsp; This leads some companies to adopt either too loose, wide-access data policies (e.g., all engineers and processes within the company get access to all user data), or too restrictive, siloing policies (e.g., the data from service X is beyond reach for any engineer or process outside X's scope).&nbsp; Neither extreme is preferable: the former can resultin wide exposure to hackers or snooping employees; the latter can disable potentiallyvaluable uses of the data.</p>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 5\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>I hypothesize that the most appropriate protection abstraction for ML is not files/directories/tables, but rather&nbsp;</span><em>ML models</em><span>, and particularly </span><em>feature models</em><span>. Most ML predictive models are not built directly on the raw data, but on features that encode, aggregate, and otherwise transform the raw data to make learning on it more efficient. These features, such as embeddings, covariates, and various statistical aggregates, are often shared across teams in big companies through company-wide feature stores. I thus developed the notion of a </span><em>private feature model</em><span>, a new abstraction for protected data access and sharing for ML ecosystems. A private feature model is one that is </span><span>learned incrementally </span><span>over historical data, made </span><span>differentially private (DP)&nbsp;</span><span>to bound leakage of the data through its parameters, and made </span><span>broadly accessible </span><span>within the company so engineers can use it to improve their service, ideally in lieu of accessing, and thus exposing, historical raw data.</span></p>\n<p><span>My team and I developed, evaluated, and published </span><em>Pyramid </em>(<a title=\"Pyramid SP\" href=\"https://roxanageambasu.github.io/publications/oakland2017pyramid.pdf\">paper</a>,&nbsp;<a title=\"Pyramid\" href=\"https://roxanageambasu.github.io/publications/spmagazine2018pyramid.pdf\">paper</a>)<span>, a system that implements a special case of this abstraction based on a feature model called </span><em>count featurization</em><span>. Count featurization, a frequently used method for scaling ML to very large datasets, replaces the features of an example with the probability of its label, conditioned on feature values. For instance, for a movie rating, </span><span>userId </span><span>becomes </span><span>P</span><span>(</span><span>rating</span><span>|</span><span>userId</span><span>)</span><span>. Because the new features are low dimension, predictive models trained on them require much less data to fit. Pyramid constructs count tables to compute these probabilities over historical data and stores them with DP in company-wide feature stores. From there, they can be accessed by all engineers and incorporated as part of predictive models to reduce the amount and timeframe of the raw data to which the models need access. Using multiple workloads, including a production workload from Microsoft, we show that predictive models need access to 2-3 orders of magnitude less raw data when training with DP counts versus when training without the counts, while sacrificing 2-4% in accuracy. This reduces exposure of raw data&nbsp;</span><span>by a corresponding 2-3 orders of magnitude. </span></p>\n<p><span>My team and I then enhanced Pyramid to support arbitrary feature models, including embeddings and latent variable models. Doing so raises substantial technical challenges because each feature model exposes a different privacy-accuracy tradeoff when made DP, complicating the design of a uniform abstraction and system. I addressed these challenges using a combination of new DP theory extensions and systems techniques for resource allocation to manage the privacy resource judiciously and enforce both a user-level global privacy guarantee and accuracy service-level agreements for all private feature models (<a title=\"Sage\" href=\"https://roxanageambasu.github.io/publications/sosp2019sage-with-appendix.pdf\">paper</a>).</span></p>\n<p><span>Beyond ML workloads, I have also worked on new protection abstractions for mobile workloads (<a title=\"Pebbles\" href=\"https://roxanageambasu.github.io/publications/osdi2014pebbles.pdf\">paper</a>,&nbsp;<a title=\"CleanOS\" href=\"https://roxanageambasu.github.io/publications/osdi2012cleanos.pdf\">paper</a>). A similar mismatch between traditional protection abstractions and the needs of emerging workloads occurs in mobile devices. While traditional OSes provide low-level storage abstractions -- files and directories -- mobile OSes embed higher-level abstractions, such as relational databases or object-</span>relational models. Despite the change in abstraction, many crucial protection systems, such as encryption or deniable systems, continue to operate at file level, which renders them ineffective since files are both too fine- and too coarse-grained for effective protection. My team and I introduced a new and more meaningful abstraction for protecting data in mobile OSes: <em>logical data objects</em>. These correspond to user-relevant objects, such as emails, documents, and bank accounts. I developed <em>CleanOS, </em>a modified Android that provides logical data objects as abstractions to let protection tools operate at a level more relevant to the user, such as encrypting, hiding, or properly deleting emails, documents, or bank accounts along with all pieces of related data.</p>\n<div class=\"page\" title=\"Page 6\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>Broader Impacts:</p>\n<p>My work provides critical missing tools needed by data-driven systems to safeguard the increasing amounts of sensitive information they collect.&nbsp; I have disseminated my insights widely by: making code available; giving numerous research and mentoring talks; organizing learning workshops for underrepresented groups; giving interviews to media outlets to introduce the public to emerging privacy threats and solutions.&nbsp; Finally, I added to the Columbia University curriculum three major courses on the design and implementation of large-scale distributed and private systems (<a title=\"DS1\" href=\"https://columbia.github.io/ds1-class/\">link</a>, <a title=\"DS2\" href=\"https://columbia.github.io/ds2-class/\">link</a>, <a title=\"Private Systems\" href=\"https://columbia.github.io/private-systems-class/\">link</a>).</p>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/01/2020<br>\n\t\t\t\t\tModified by: Roxana&nbsp;Geambasu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nIntellectual Merit:\n\nMy project revisits decades' old protection abstractions from operating systems (OSes), which have become unfit for emerging data-driven workloads. Traditional protection units -- such as files, directories, or database tables -- fail to support the data access and sharing patterns common in ML ecosystems.  This leads some companies to adopt either too loose, wide-access data policies (e.g., all engineers and processes within the company get access to all user data), or too restrictive, siloing policies (e.g., the data from service X is beyond reach for any engineer or process outside X's scope).  Neither extreme is preferable: the former can resultin wide exposure to hackers or snooping employees; the latter can disable potentiallyvaluable uses of the data.\n\n\n\n\n\n\n\nI hypothesize that the most appropriate protection abstraction for ML is not files/directories/tables, but rather ML models, and particularly feature models. Most ML predictive models are not built directly on the raw data, but on features that encode, aggregate, and otherwise transform the raw data to make learning on it more efficient. These features, such as embeddings, covariates, and various statistical aggregates, are often shared across teams in big companies through company-wide feature stores. I thus developed the notion of a private feature model, a new abstraction for protected data access and sharing for ML ecosystems. A private feature model is one that is learned incrementally over historical data, made differentially private (DP) to bound leakage of the data through its parameters, and made broadly accessible within the company so engineers can use it to improve their service, ideally in lieu of accessing, and thus exposing, historical raw data.\n\nMy team and I developed, evaluated, and published Pyramid (paper, paper), a system that implements a special case of this abstraction based on a feature model called count featurization. Count featurization, a frequently used method for scaling ML to very large datasets, replaces the features of an example with the probability of its label, conditioned on feature values. For instance, for a movie rating, userId becomes P(rating|userId). Because the new features are low dimension, predictive models trained on them require much less data to fit. Pyramid constructs count tables to compute these probabilities over historical data and stores them with DP in company-wide feature stores. From there, they can be accessed by all engineers and incorporated as part of predictive models to reduce the amount and timeframe of the raw data to which the models need access. Using multiple workloads, including a production workload from Microsoft, we show that predictive models need access to 2-3 orders of magnitude less raw data when training with DP counts versus when training without the counts, while sacrificing 2-4% in accuracy. This reduces exposure of raw data by a corresponding 2-3 orders of magnitude. \n\nMy team and I then enhanced Pyramid to support arbitrary feature models, including embeddings and latent variable models. Doing so raises substantial technical challenges because each feature model exposes a different privacy-accuracy tradeoff when made DP, complicating the design of a uniform abstraction and system. I addressed these challenges using a combination of new DP theory extensions and systems techniques for resource allocation to manage the privacy resource judiciously and enforce both a user-level global privacy guarantee and accuracy service-level agreements for all private feature models (paper).\n\nBeyond ML workloads, I have also worked on new protection abstractions for mobile workloads (paper, paper). A similar mismatch between traditional protection abstractions and the needs of emerging workloads occurs in mobile devices. While traditional OSes provide low-level storage abstractions -- files and directories -- mobile OSes embed higher-level abstractions, such as relational databases or object-relational models. Despite the change in abstraction, many crucial protection systems, such as encryption or deniable systems, continue to operate at file level, which renders them ineffective since files are both too fine- and too coarse-grained for effective protection. My team and I introduced a new and more meaningful abstraction for protecting data in mobile OSes: logical data objects. These correspond to user-relevant objects, such as emails, documents, and bank accounts. I developed CleanOS, a modified Android that provides logical data objects as abstractions to let protection tools operate at a level more relevant to the user, such as encrypting, hiding, or properly deleting emails, documents, or bank accounts along with all pieces of related data.\n\n\n\n\nBroader Impacts:\n\nMy work provides critical missing tools needed by data-driven systems to safeguard the increasing amounts of sensitive information they collect.  I have disseminated my insights widely by: making code available; giving numerous research and mentoring talks; organizing learning workshops for underrepresented groups; giving interviews to media outlets to introduce the public to emerging privacy threats and solutions.  Finally, I added to the Columbia University curriculum three major courses on the design and implementation of large-scale distributed and private systems (link, link, link).\n\n\n\n\n\n\n\n\t\t\t\t\tLast Modified: 08/01/2020\n\n\t\t\t\t\tSubmitted by: Roxana Geambasu"
 }
}