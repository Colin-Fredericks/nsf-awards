{
 "awd_id": "1419196",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Making Big and Complex Data Easier to Assemble and Analyze in Distributed CI Environments: Expanding on Metagenomics Challenges Defined by CAMERA",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922247",
 "po_email": "rchadduc@nsf.gov",
 "po_sign_block_name": "Robert Chadduck",
 "awd_eff_date": "2014-02-15",
 "awd_exp_date": "2016-01-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2014-02-18",
 "awd_max_amd_letter_date": "2014-02-18",
 "awd_abstract_narration": "The Community Cyberinfrastructure for Advanced Microbial Ecology Research and Analysis (CAMERA, http://camera.calit2.net/) is a semantically enabled database and distributed computational infrastructure that provides a single system for depositing, locating, analyzing, visualizing, and sharing microbial biology data. With the rapid advance of newer DNA sequencing methods, so called Next Generation Sequencing (NGS) technologies, such as Illumina HiSeq and MiSeq, it is becoming increasingly difficult for researchers using sequencing data to meet the computing requirements for large-scale NGS datasets with existing methods. In response to these aspects of the BIG DATA challenge, the CAMERA team is developing new bioinformatics algorithms, high performance computing solutions, visualization interfaces, and data resources to specifically address the NGS data analysis challenges. Here, the group proposes a crosscutting methodology for analyzing NGS data that marries innovative bioinformatics algorithms and workflows with leading edge computational methods for managing large scale distributed computing. The integration of XSEDE resources for BIG DATA analysis will provide the scale and specification necessary to drive the development of this system. This project will be conducted over two years. Year one will be focused on the refinement of core CAMERA CI (e.g. Panfish) and the continued development of core NGS workflows/algorithms. Specifically, CAMERA CI will be extended to take full advantage of two new NSF XSEDE resources to be commissioned in early 2015 (Wrangler at TACC & Comet at SDSC).  Year 2 will be focused on the production integration of Wrangler and Comet and the subsequent deployment of the NGS workflows (via CAMERA CI) to the entire CAMERA community.   These new software tools and pipelined processes facilitate the processing and analyze very large-scale metagenomic data on the scale of tens of GB per sample and provide comprehensive and unique functions such as 16S analysis[7], taxonomy binning[8], assembly, rRNA finding, clustering, filtering, function and pathway annotation, and visualization]. These next generation tools enable orders of magnitude faster computational process, more comprehensive analysis, integrated data output, and novel ways to investigate complex data, once made to operate in extensible HPC cloud environments. The Broader Impact is viewed as that currently, manual operations are necessary to complete analysis with these tools due to the complexity of the process and the large number of software tools involved. The goal of this project is to develop a series of fully integrated and easy-to-use analysis workflows encapsulating these tools. These new workflows of software tools will significantly improve NGS data analysis for researchers who use metagenomics as an investigative tool, researchers who are now impeded by challenges with regard to managing and analyzing BIG DATA.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Ellisman",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Mark H Ellisman",
   "pi_email_addr": "mark@alex.ucsd.edu",
   "nsf_id": "000457536",
   "pi_start_date": "2014-02-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Scientific data is growing at an explosive rate.&nbsp; The volume of information, in both public domain databases and private research projects, is stressing the current capacity of high-performance computational systems. The major goal of the project was to provide an intelligent meta-submission technology for optimizing the throughput of computational scientific workflows. In particular, we aimed to provide a mechanism to execute workflows simultaneously across local clusters, XSEDE, and commercial cloud resources, allowing for the more efficient mapping of algorithmic requirements to the scale or unique configuration of heterogeneous distributed resources that also lie within different administrative domains and utilize different execution models and software.&nbsp; Specifically, we refined and commissioned into production use a software tool, Panfish, to address this need.</p>\n<p>Using XSEDE compute resources (or any external compute resource) typically requires the following three operations to be performed: upload data, run workflow, and copy back results.&nbsp; Panfish (semi-)automatically coordinates these operations across multiple cluster resources. Furthermore, Panfish has been expressly designed to enable job submission in a process similar to invocation of jobs on a single local cluster running Sun Grid Engine (SGE) - with the added advantage that those jobs can optionally be sent to multiple XSEDE resources.&nbsp;</p>\n<p>Using Panfish, we are able to submit jobs from a single workflow across multiple and heterogeneous cluster computing systems, including local cluster, XSEDE, and commercial cloud resources.&nbsp;&nbsp; To date, Panfish has coordinated the <em><span style=\"text-decoration: underline;\">simultaneous</span></em> use of up to nine heterogeneous and geographically distributed cluster resources.</p>\n<p>In completing this project, we have endeavored to fulfill the following merit review criteria.&nbsp; By increasing the throughput of computationally intensive algorithms, Panfish fulfills the &ldquo;Intellectual Merit&rdquo; criteria by providing a means to accelerate the rate of scientific discovery and therefore the rate at which knowledge is advanced.&nbsp; The &ldquo;Broader Impact&rdquo; of Panfish is that it allows XSEDE (in particular the COMET, Stampede, and Gordon resources), as well as local and commercial cloud resources, to be used with greater coordination and efficiency than could be run individually.&nbsp; Specifically, Panfish provides a mechanism whereby all disparate XSEDE resources, which span physical and administrative domains, can be simultaneously employed towards a common scientific question.&nbsp;</p>\n<p>Panfish is available as an open source software from:&nbsp; https://github.com/CRBS/Panfish</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/23/2016<br>\n\t\t\t\t\tModified by: Mark&nbsp;H&nbsp;Ellisman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nScientific data is growing at an explosive rate.  The volume of information, in both public domain databases and private research projects, is stressing the current capacity of high-performance computational systems. The major goal of the project was to provide an intelligent meta-submission technology for optimizing the throughput of computational scientific workflows. In particular, we aimed to provide a mechanism to execute workflows simultaneously across local clusters, XSEDE, and commercial cloud resources, allowing for the more efficient mapping of algorithmic requirements to the scale or unique configuration of heterogeneous distributed resources that also lie within different administrative domains and utilize different execution models and software.  Specifically, we refined and commissioned into production use a software tool, Panfish, to address this need.\n\nUsing XSEDE compute resources (or any external compute resource) typically requires the following three operations to be performed: upload data, run workflow, and copy back results.  Panfish (semi-)automatically coordinates these operations across multiple cluster resources. Furthermore, Panfish has been expressly designed to enable job submission in a process similar to invocation of jobs on a single local cluster running Sun Grid Engine (SGE) - with the added advantage that those jobs can optionally be sent to multiple XSEDE resources. \n\nUsing Panfish, we are able to submit jobs from a single workflow across multiple and heterogeneous cluster computing systems, including local cluster, XSEDE, and commercial cloud resources.   To date, Panfish has coordinated the simultaneous use of up to nine heterogeneous and geographically distributed cluster resources.\n\nIn completing this project, we have endeavored to fulfill the following merit review criteria.  By increasing the throughput of computationally intensive algorithms, Panfish fulfills the \"Intellectual Merit\" criteria by providing a means to accelerate the rate of scientific discovery and therefore the rate at which knowledge is advanced.  The \"Broader Impact\" of Panfish is that it allows XSEDE (in particular the COMET, Stampede, and Gordon resources), as well as local and commercial cloud resources, to be used with greater coordination and efficiency than could be run individually.  Specifically, Panfish provides a mechanism whereby all disparate XSEDE resources, which span physical and administrative domains, can be simultaneously employed towards a common scientific question. \n\nPanfish is available as an open source software from:  https://github.com/CRBS/Panfish\n\n\t\t\t\t\tLast Modified: 03/23/2016\n\n\t\t\t\t\tSubmitted by: Mark H Ellisman"
 }
}