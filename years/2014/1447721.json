{
 "awd_id": "1447721",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: DKA: Collaborative Research: Theory and Algorithms for Parallel Probabilistic Inference with Big Data, via Big Model, in Realistic Distributed Computing Environments",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2014-08-25",
 "awd_max_amd_letter_date": "2014-08-25",
 "awd_abstract_narration": "This project develops a new framework that enables machine learning (ML) systems to automatically comprehend and mine massive and complex data via parallel Bayesian inference on large computer clusters. The research has a profound impact on the practice and direction of Big Learning. The developed technologies have a catalytic effect on both ML research and applications: ML scientists are able to rapidly experiment on novel, cutting-edge ML models with minimal programming effort, unhindered by the limitations of single machines. Researchers from other fields, like biology and social sciences, are able to run contemporary advanced ML methods that transcend the capabilities of simple models, yielding new scientific insights on data whose size would otherwise be daunting. Data scientists at small start-ups are able to conduct ML analytics with complex models, putting their capabilities on par with huge companies possessing dedicated engineering and infrastructure teams. Students and beginners are able to witness distributed ML in action with just a few lines of code, driving ML education to new heights. \r\n\r\nTechnically, this research focuses on scaling up and parallelizing Bayesian machine learning, which provides a powerful, elegant and theoretically justified framework for modeling a wide variety of datasets.  The research team develops a suite of complementary distributed inference algorithms for hierarchical Bayesian models, which cover most commonly used Bayesian ML methods. The project focuses on combining speed and scalability with theoretical guarantees that allow us to assess the accuracy of the resulting methods, and allow practitioners to make trade-offs between speed and accuracy. Rather than focus on a few disconnected models, the project develops techniques applicable to a broad spectrum of hierarchical Bayesian models, resulting in a toolkit of building blocks that can be combined as needed for arbitrary probabilistic models - be they parametric or nonparametric, discriminative or generative. This is in contrast to much existing work on parallel inference, which tends to focus on parallelization in a specific model and cannot be easily extended. The project provides a solid algorithmic foundation for learning on Big Data with powerful models. The research contributes to democratizing advanced and large-scale ML methods for broad applications, by offering the user and developer community a library of general-purpose parallelizable algorithms for working on diverse problems using computer clusters and the cloud, bridging the gap between practical needs from data and basic research in ML.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sinead",
   "pi_last_name": "Williamson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sinead Williamson",
   "pi_email_addr": "sinead.williamson@mccombs.utexas.edu",
   "nsf_id": "000653788",
   "pi_start_date": "2014-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "101 E. 27th St. Suite 5.300",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121532",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern machine learning techniques provide flexible modeling solutions for real-world data. In particular, Bayesian machine learning techniques allow us to honestly evaluate uncertainty and incorporate this uncertainty in our decision-making processes. However, it is often challenging to scale these solutions to large, modern datasets such as networks, text or multimedia data, without introducing harmful approximations.</p>\n<p>The goal of this project was to develop Bayesian inference algorithms that are designed for large data scenarios, where the data must be partitioned across multiple machines. Our focus was on the family of Bayesian nonparametric models. Bayesian nonparametric models provide a flexible framework for modeling data where the underlying complexity is unknown; by assuming infinite complexity a priori, Bayesian nonparametric models learn a posterior distribution that concentrates on the appropriate model complexity for the data. Unfortunately, this flexibility comes at a heavy computational cost &ndash; Bayesian nonparametric models must explore an infinite-dimensional state-space, and typically involve long-range dependencies between observations that make parallelization difficult.&nbsp;</p>\n<p>We explored two different approaches to scaling up inference in (nonparametric) Bayesian models. First, we exploited the underlying mathematical structure of models such as the Dirichlet process and the Indian buffet process, to obtain distributed MCMC algorithms that will provably converge to the true posterior distribution, allowing us to scale up inference while maintaining &ldquo;gold standard&rdquo; asymptotic performance. Second, we explored the use of well-motivated approximations, which allowed us to break existing dependencies in models such as the Gaussian process. Through mindful use of such approximations, we are able to parallelize computation while minimizing the error introduced.</p>\n<p>We expect that the resulting algorithms will encourage broader use of Bayesian nonparametric algorithms in a Big Data context, allowing practitioners to easily access the flexible modeling and uncertainty quantification that these methods provide.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/12/2021<br>\n\t\t\t\t\tModified by: Sinead&nbsp;Williamson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern machine learning techniques provide flexible modeling solutions for real-world data. In particular, Bayesian machine learning techniques allow us to honestly evaluate uncertainty and incorporate this uncertainty in our decision-making processes. However, it is often challenging to scale these solutions to large, modern datasets such as networks, text or multimedia data, without introducing harmful approximations.\n\nThe goal of this project was to develop Bayesian inference algorithms that are designed for large data scenarios, where the data must be partitioned across multiple machines. Our focus was on the family of Bayesian nonparametric models. Bayesian nonparametric models provide a flexible framework for modeling data where the underlying complexity is unknown; by assuming infinite complexity a priori, Bayesian nonparametric models learn a posterior distribution that concentrates on the appropriate model complexity for the data. Unfortunately, this flexibility comes at a heavy computational cost &ndash; Bayesian nonparametric models must explore an infinite-dimensional state-space, and typically involve long-range dependencies between observations that make parallelization difficult. \n\nWe explored two different approaches to scaling up inference in (nonparametric) Bayesian models. First, we exploited the underlying mathematical structure of models such as the Dirichlet process and the Indian buffet process, to obtain distributed MCMC algorithms that will provably converge to the true posterior distribution, allowing us to scale up inference while maintaining \"gold standard\" asymptotic performance. Second, we explored the use of well-motivated approximations, which allowed us to break existing dependencies in models such as the Gaussian process. Through mindful use of such approximations, we are able to parallelize computation while minimizing the error introduced.\n\nWe expect that the resulting algorithms will encourage broader use of Bayesian nonparametric algorithms in a Big Data context, allowing practitioners to easily access the flexible modeling and uncertainty quantification that these methods provide.\n\n \n\n\t\t\t\t\tLast Modified: 03/12/2021\n\n\t\t\t\t\tSubmitted by: Sinead Williamson"
 }
}