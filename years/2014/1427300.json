{
 "awd_id": "1427300",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "NRI: Representing and Anticipating Actions in Human-Robot Collaborative Assembly Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2020-07-31",
 "tot_intn_awd_amt": 849999.0,
 "awd_amount": 849999.0,
 "awd_min_amd_letter_date": "2014-08-06",
 "awd_max_amd_letter_date": "2019-07-08",
 "awd_abstract_narration": "For robots to effectively collaborate with humans on a variety of tasks, they must go beyond responding to human actions to anticipating them.   This is especially true in the domain of collaborative assembly tasks where the robot assists a human worker by providing tools or parts as required.  In this project, a method for formally specifying collaborative assembly tasks is being developed that allows a robot both to understand the action of the human as well as to determine which action the robot has to perform and when. \r\n\r\nThis project is making fundamental advances to enable task specification to be compiled or converted into a grammar-like description of the human activity.  Given this description, a probabilistic inference method that integrates sensory information to analyze the actions of a human and predict which actions the human will take and when. The system learns the necessary perceptual detection information for human actions will be learned from small amounts of training examples of individual actions.  By integrating these perceptual observations within a structured representation of the task derived from the specification, the robot can make effective predictions about the timing of human action and can thus anticipate when it will need to provide assistance and of what type.  Given these probabilistic predictions the robot makes a plan of action that optimizes a collaboration measure such as how idle time of the human or overall task completion time.  The broader impact of this project is along two dimensions.  The first is within the small to medium enterprise manufacturing and assembly industry.  Successful development of the technologies described is critical for human-robot collaboration in a variety of structured tasks in these domains.  Second, the ability to successfully anticipate human behavior is essential for the general integration of assistive robotics into society.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Irfan",
   "pi_last_name": "Essa",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Irfan A Essa",
   "pi_email_addr": "irfan@cc.gatech.edu",
   "nsf_id": "000307835",
   "pi_start_date": "2016-05-05",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Aaron",
   "pi_last_name": "Bobick",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Aaron F Bobick",
   "pi_email_addr": "afb@wustl.edu",
   "nsf_id": "000122613",
   "pi_start_date": "2014-08-06",
   "pi_end_date": "2016-05-05"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Irfan",
   "pi_last_name": "Essa",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Irfan A Essa",
   "pi_email_addr": "irfan@cc.gatech.edu",
   "nsf_id": "000307835",
   "pi_start_date": "2014-08-06",
   "pi_end_date": "2016-05-05"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Henrik",
   "pi_last_name": "Christensen",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Henrik I Christensen",
   "pi_email_addr": "hichristensen@ucsd.edu",
   "nsf_id": "000348956",
   "pi_start_date": "2014-08-06",
   "pi_end_date": "2019-07-08"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Stilman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Stilman",
   "pi_email_addr": "mstilman@cc.gatech.edu",
   "nsf_id": "000509038",
   "pi_start_date": "2014-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Ave NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 549993.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 300006.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-5216b40b-7fff-eb69-d2c5-a91b5e44bd9a\"> </span></p>\n<p dir=\"ltr\">The primary focus of this multiyear research effort is to combine work on (1) grammar-based interpretation of human action in assembly, with (2) formal specification of robot assembly tasks. Use these approaches and related methods that measure and model human actions for both specifications of humans actions and assessment of human actions during tasks performed. The primary exploration in this area seeks to understand how humans can perform certain tasks, how can these tasks be assessed in terms of how they were performed, and how to explore these tasks robotic embodiments.&nbsp;</p>\n<p dir=\"ltr\"><span>The specification of an assembly task, or more broadly a complex task, with many subsets of actions, aimed to do a specific thing (assemble something, fix something, etc.) is used to derive both robot control and perception knowledge needed to interpret human actions.&nbsp;</span></p>\n<p dir=\"ltr\"><span>During the course of this project, we applied these approaches to a variety of domains, ranging from simple assembly tasks to exploring complex movements like people moving, to even relatively complex tasks that require many phases and objects/actions, like cooking and surgeries. Our work builds on approaches for modeling actions and movement, predicting, anticipating, and recognizing activities, to assessing the performance of these actions. We explored domains aimed at getting robots to complete tasks, driven by our models, recognizing and categorizing tasks, to even doing interactive training of complex tasks.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Some specific outcomes of this project include.</span></p>\n<ol>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We introduced a method that allows modeling of multi-path branching in a probabilistic manner for the task of a collaborative robot-human assembly task. By maintaining densities over multiple branch possibilities, the robot can act in a way that does not require it to commit to only one particular branch belief. By encoding the task structure, the robot can continually integrate new information and propagate it forward and backward in time, always improving its perception of the human&rsquo;s past, current, and future states.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We developed a planning domain representation is developed, which in concert with a task modeling framework can be used to reduce the work involved in plan creation for complex manufacturing tasks. This provides the opportunity to relieve some burden from the robot user, and to improve the reliability of those plans. For the scope of this work, we focused on the basic functionality of robotic arms in a manufacturing assembly setting.&nbsp;</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We introduced an automated framework for task completion and assessment. W</span>e studied the average performance of different types of motions features across all applicable Objective structured assessment of technical skill (OSATS) criteria for suturing and knot tying tasks. This allowed us to develop a state-of-the-art surgical assessment automated system that using video analysis coupled with accelerometer data showed best results using the OSATS criteria for surgical training. This effort was also explored for commercialization under the NSF I-Corps program.</p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We explored the use of context derived from the environment along with the visual cues to automatically summarize videos of extended activities. While this work was focused on sports videos, we are now exploring methods to employ this approach for robot acitivites.</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>We extended some of our work to the new domain of video assessment of human motions, we collected a dataset from YouTube of a variety of Dances and Gymnastic Competitions.&nbsp;</span></p>\n</li>\n</ol>\n<p dir=\"ltr\"><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p>\n<p dir=\"ltr\"><strong>Intellectual Merit</strong><span>: Much of the past research on human activity recognition has focused on correct classification from video. This project extends those efforts and explores developing a robot that leverages vision to function in a human-inhabited world. We showed that while labels are helpful, structural understanding is necessary to interact with the environment and the people who inhabit it. Additionally, using an abstract description of tasks in support of perceptual understanding serves as a unifying step in unifying the representations of activity within robotics. We demonstrated these principles in the context of getting robots to complete tasks, driven by our models, recognizing and categorizing tasks, to even doing interactive training of complex tasks. </span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p>\n<p dir=\"ltr\"><strong>Broader Impact</strong><span>: We build our proposed work on the ideals of the </span><span>Roadmap for US Robotics- From Internet to Robotics</span><span>, robotics and especially robots that operate in human environments have the potential of being a key driver of US economic growth of the 21</span><span>st </span><span>century. Successful development of the proposed capabilities would be a critical technology for human-robot collaboration in a variety of structured tasks including manufacturing. Our project considered realistic domains and we even took our work to real world, exploring commercialization with the NSF I-Corps program of our surgical training system.&nbsp; Our team is part of the larger Interdisciplinary Institute for Robotics at GA Tech (IRIM) and we hosted visits from local Atlanta K-12 schools with primarily under-represented minority students. Numerous demonstration events held during the year include National Robotics Week and we showcased our work to young students to showcase the value of STEM education. </span><span><span> </span></span><span><span> </span></span><span><span> </span></span></p>\n<p dir=\"ltr\"><span><span> </span></span><span><span> </span></span><span><span> </span></span></p>\n<p dir=\"ltr\"><span><span> </span></span><span><span> </span></span></p>\n<p><br /><br /></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/16/2021<br>\n\t\t\t\t\tModified by: Irfan&nbsp;A&nbsp;Essa</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThe primary focus of this multiyear research effort is to combine work on (1) grammar-based interpretation of human action in assembly, with (2) formal specification of robot assembly tasks. Use these approaches and related methods that measure and model human actions for both specifications of humans actions and assessment of human actions during tasks performed. The primary exploration in this area seeks to understand how humans can perform certain tasks, how can these tasks be assessed in terms of how they were performed, and how to explore these tasks robotic embodiments. \nThe specification of an assembly task, or more broadly a complex task, with many subsets of actions, aimed to do a specific thing (assemble something, fix something, etc.) is used to derive both robot control and perception knowledge needed to interpret human actions. \nDuring the course of this project, we applied these approaches to a variety of domains, ranging from simple assembly tasks to exploring complex movements like people moving, to even relatively complex tasks that require many phases and objects/actions, like cooking and surgeries. Our work builds on approaches for modeling actions and movement, predicting, anticipating, and recognizing activities, to assessing the performance of these actions. We explored domains aimed at getting robots to complete tasks, driven by our models, recognizing and categorizing tasks, to even doing interactive training of complex tasks. \nSome specific outcomes of this project include.\n\n\nWe introduced a method that allows modeling of multi-path branching in a probabilistic manner for the task of a collaborative robot-human assembly task. By maintaining densities over multiple branch possibilities, the robot can act in a way that does not require it to commit to only one particular branch belief. By encoding the task structure, the robot can continually integrate new information and propagate it forward and backward in time, always improving its perception of the human\u2019s past, current, and future states.\n\n\nWe developed a planning domain representation is developed, which in concert with a task modeling framework can be used to reduce the work involved in plan creation for complex manufacturing tasks. This provides the opportunity to relieve some burden from the robot user, and to improve the reliability of those plans. For the scope of this work, we focused on the basic functionality of robotic arms in a manufacturing assembly setting. \n\n\nWe introduced an automated framework for task completion and assessment. We studied the average performance of different types of motions features across all applicable Objective structured assessment of technical skill (OSATS) criteria for suturing and knot tying tasks. This allowed us to develop a state-of-the-art surgical assessment automated system that using video analysis coupled with accelerometer data showed best results using the OSATS criteria for surgical training. This effort was also explored for commercialization under the NSF I-Corps program.\n\n\nWe explored the use of context derived from the environment along with the visual cues to automatically summarize videos of extended activities. While this work was focused on sports videos, we are now exploring methods to employ this approach for robot acitivites.\n\n\nWe extended some of our work to the new domain of video assessment of human motions, we collected a dataset from YouTube of a variety of Dances and Gymnastic Competitions. \n\n\n     \nIntellectual Merit: Much of the past research on human activity recognition has focused on correct classification from video. This project extends those efforts and explores developing a robot that leverages vision to function in a human-inhabited world. We showed that while labels are helpful, structural understanding is necessary to interact with the environment and the people who inhabit it. Additionally, using an abstract description of tasks in support of perceptual understanding serves as a unifying step in unifying the representations of activity within robotics. We demonstrated these principles in the context of getting robots to complete tasks, driven by our models, recognizing and categorizing tasks, to even doing interactive training of complex tasks.    \nBroader Impact: We build our proposed work on the ideals of the Roadmap for US Robotics- From Internet to Robotics, robotics and especially robots that operate in human environments have the potential of being a key driver of US economic growth of the 21st century. Successful development of the proposed capabilities would be a critical technology for human-robot collaboration in a variety of structured tasks including manufacturing. Our project considered realistic domains and we even took our work to real world, exploring commercialization with the NSF I-Corps program of our surgical training system.  Our team is part of the larger Interdisciplinary Institute for Robotics at GA Tech (IRIM) and we hosted visits from local Atlanta K-12 schools with primarily under-represented minority students. Numerous demonstration events held during the year include National Robotics Week and we showcased our work to young students to showcase the value of STEM education.    \n   \n  \n\n\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/16/2021\n\n\t\t\t\t\tSubmitted by: Irfan A Essa"
 }
}