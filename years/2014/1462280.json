{
 "awd_id": "1462280",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CHS: Medium: Collaborative Research: Immediate Feedback to Support Learning American Sign Language through Multisensory Recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-08-20",
 "awd_exp_date": "2020-08-31",
 "tot_intn_awd_amt": 537997.0,
 "awd_amount": 537997.0,
 "awd_min_amd_letter_date": "2014-09-15",
 "awd_max_amd_letter_date": "2014-09-15",
 "awd_abstract_narration": "American Sign Language (ASL) is a primary means of communication for 500,000 people in the United States and a distinct language from English, conveyed through hands, facial expressions, and body movements.  Studies indicate that deaf children of deaf parents read better than deaf children of hearing parents, mainly due to better communication when both children and parents are deaf.  However, more than 80% of children who are deaf or hard of hearing are born to hearing parents.  It is challenging for parents, teachers, and other people in the life of a deaf child to learn ASL rapidly enough to support the visual language acquisition of the child.  Technology that can automatically recognize aspects of ASL signing and provide instant feedback to these students of ASL would give them a time-flexible way to practice and improve their signing skills.  The goal of this project, which involves an interdisciplinary team of researchers at three colleges within the City University of New York (CUNY) with expertise in computer vision, human-computer interaction, and Deaf and Hard of Hearing education, is to discover the most effective underlying technologies, user-interface design, and pedagogical use for an interactive tool to provide such immediate, automatic feedback for students of ASL.\r\n\r\nMost prior work on ASL recognition has focused on identifying a small set of simple signs performed, but current technology is not sufficiently accurate on continuous signing of sentences with an unrestricted vocabulary.  The PIs will develop technologies to fundamentally advance ASL partial recognition, that is to identify linguistic/performance attributes of ASL without necessarily identifying the entire sequence of signs, and automatically determine if a performance is fluent or contains errors.  The research  will include five thrusts: (1) based on ASL linguistics and pedagogy, to identify a set of observable attributes indicating ASL fluency; (2) to discover new technologies for automatic detection of the ASL fluency attributes through fusion of multimodality (facial expression, hand gesture, and body pose) and multisensory information (RGB and Depth videos); (3) to collect and annotate a dataset of RGBD videos of ASL, performed at varied levels of fluency, by students and native signers; (4) to develop an interactive ASL learning tool that provides ASL students immediate feedback about whether their signing is fluent or not; and (5) to evaluate the robustness of the new algorithms and the effectiveness of the ASL learning tool, including its educational benefits.  The work will lead to advances in computer vision technologies for human behavior perception, to new understanding of user-interface design with ASL video, and to a revolutionary and cost-effective educational tool to assist ASL learners achieve fluency, using recognition technologies that are robust and accurate in the near-term.  Project outcomes will include a dataset of videos at varied fluency levels, which will be valuable for future ASL linguists or instructors, students learning ASL, and computer vision researchers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matt",
   "pi_last_name": "Huenerfauth",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matt Huenerfauth",
   "pi_email_addr": "matt.huenerfauth@rit.edu",
   "nsf_id": "000220138",
   "pi_start_date": "2014-09-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rochester Institute of Tech",
  "inst_street_address": "1 LOMB MEMORIAL DR",
  "inst_street_address_2": "",
  "inst_city_name": "ROCHESTER",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5854757987",
  "inst_zip_code": "146235603",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "NY25",
  "org_lgl_bus_name": "ROCHESTER INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "J6TWTRKC1X14"
 },
 "perf_inst": {
  "perf_inst_name": "Rochester Institute of Tech",
  "perf_str_addr": "",
  "perf_city_name": "Rochester",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "146235603",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "NY25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 537997.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This interdisciplinary research project was led by computer-vision researchers from the City College of New York (CCNY), computing accessibility researchers from Rochester Institute of Technology (RIT), and education researchers from Hunter College (HC). The team investigated intelligent technologies for automatically analyzing videos of students who are learning American Sign Language (ASL) to provide them with useful feedback about their signing, to support their learning.&nbsp; In this work, we identified a set of observable attributes indicating ASL fluency based on ASL linguistics and pedagogy and then investigated new technologies for automatic detection of these ASL fluency attributes from video. To identify fluency attributes of ASL from video, we integrated multimodality (facial expressions, hand gestures, and body movements) from multisensory information, i.e. red-green-blue (RGB) and depth (D) videos. To provide data in support this research, we collected and annotated a dataset of RGBD videos of ASL, performed at varied levels of fluency, by students and native signers. We developed an interactive ASL learning tool to provide ASL students immediate feedback about whether their signing may contain errors. Finally, we evaluated the robustness of the new algorithms and the effectiveness of the ASL learning tool, including its educational benefits. While this technology is not a replacement for the detailed feedback that an ASL instructor can provide, it can support students in rehearsing their ASL signing skills outside of a classroom.</p>\n<p>&nbsp;</p>\n<p>While most prior work on ASL recognition had focused on identifying a small set of simple signs performed, current technology is not sufficiently accurate on continuous signing of sentences with an unrestricted vocabulary.&nbsp; For this reason, this research project has investigated technologies to fundamentally advance <em>ASL partial-recognition</em>, i.e., to identify linguistic or performance attributes of ASL (without necessarily identifying the entire sequence of signs) and automatically determine if a performance is fluent or contains errors made by ASL students. This research has led to new advances in computer vision technologies for human behavior perception; new understanding of user-interface design with ASL video; and knowledge of how feedback software can assist ASL learners. In addition, this project has produced a new RGBD video dataset of ASL, which has been disseminated to the research community, to support future research on ASL and computer-vision technologies.</p>\n<p>&nbsp;</p>\n<p>This research project also has significant broader impacts. American Sign Language (ASL) is a primary means of communication for 500,000 people in the US, and a distinct language from English, conveyed through hands, facial expressions, and body movements. &nbsp;Studies indicate that deaf children of deaf parents have better language outcomes that deaf children of hearing parents, mainly due to full and frequent access to communication. However, more than 80% of children who are deaf or hard of hearing are born to hearing parents. It is challenging for parents, teachers, and other people in the life of a deaf child to learn ASL rapidly enough to support the visual language acquisition of the child. Technology that can automatically recognize aspects of ASL signing can provide instant feedback to ASL learners, giving them a time-flexible way to practice and improve their signing skills.</p>\n<p>&nbsp;</p>\n<p>Through the close collaborations among the multidisciplinary fields, this project has provided a platform to bring together young researchers from different disciplines (i.e., computer vision and artificial intelligence, assistive technology and human-computer interaction, ASL education) under one unified and exciting research program. The collaboration between CCNY, RIT, and Hunter College increases cross-campus education opportunities for our students by introducing them to this research project.</p>\n<p>&nbsp;</p>\n<p>In summary, with the support of this project and the REU program, a total of 17 PhD students, 14 masters students, and 22 undergraduate students participated the project. Students have gained interdisciplinary knowledge and unique research experience in computer vision, artificial intelligence, human-computer interaction, and assistive technologies, as well as theoretical model development and real problem solving skills. The research experiences have prepared them for their advanced degree study and future careers. Supported in part by this project, we have published and presented a total of 63 papers and presentations in conferences and journals.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/13/2020<br>\n\t\t\t\t\tModified by: Matt&nbsp;Huenerfauth</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis interdisciplinary research project was led by computer-vision researchers from the City College of New York (CCNY), computing accessibility researchers from Rochester Institute of Technology (RIT), and education researchers from Hunter College (HC). The team investigated intelligent technologies for automatically analyzing videos of students who are learning American Sign Language (ASL) to provide them with useful feedback about their signing, to support their learning.  In this work, we identified a set of observable attributes indicating ASL fluency based on ASL linguistics and pedagogy and then investigated new technologies for automatic detection of these ASL fluency attributes from video. To identify fluency attributes of ASL from video, we integrated multimodality (facial expressions, hand gestures, and body movements) from multisensory information, i.e. red-green-blue (RGB) and depth (D) videos. To provide data in support this research, we collected and annotated a dataset of RGBD videos of ASL, performed at varied levels of fluency, by students and native signers. We developed an interactive ASL learning tool to provide ASL students immediate feedback about whether their signing may contain errors. Finally, we evaluated the robustness of the new algorithms and the effectiveness of the ASL learning tool, including its educational benefits. While this technology is not a replacement for the detailed feedback that an ASL instructor can provide, it can support students in rehearsing their ASL signing skills outside of a classroom.\n\n \n\nWhile most prior work on ASL recognition had focused on identifying a small set of simple signs performed, current technology is not sufficiently accurate on continuous signing of sentences with an unrestricted vocabulary.  For this reason, this research project has investigated technologies to fundamentally advance ASL partial-recognition, i.e., to identify linguistic or performance attributes of ASL (without necessarily identifying the entire sequence of signs) and automatically determine if a performance is fluent or contains errors made by ASL students. This research has led to new advances in computer vision technologies for human behavior perception; new understanding of user-interface design with ASL video; and knowledge of how feedback software can assist ASL learners. In addition, this project has produced a new RGBD video dataset of ASL, which has been disseminated to the research community, to support future research on ASL and computer-vision technologies.\n\n \n\nThis research project also has significant broader impacts. American Sign Language (ASL) is a primary means of communication for 500,000 people in the US, and a distinct language from English, conveyed through hands, facial expressions, and body movements.  Studies indicate that deaf children of deaf parents have better language outcomes that deaf children of hearing parents, mainly due to full and frequent access to communication. However, more than 80% of children who are deaf or hard of hearing are born to hearing parents. It is challenging for parents, teachers, and other people in the life of a deaf child to learn ASL rapidly enough to support the visual language acquisition of the child. Technology that can automatically recognize aspects of ASL signing can provide instant feedback to ASL learners, giving them a time-flexible way to practice and improve their signing skills.\n\n \n\nThrough the close collaborations among the multidisciplinary fields, this project has provided a platform to bring together young researchers from different disciplines (i.e., computer vision and artificial intelligence, assistive technology and human-computer interaction, ASL education) under one unified and exciting research program. The collaboration between CCNY, RIT, and Hunter College increases cross-campus education opportunities for our students by introducing them to this research project.\n\n \n\nIn summary, with the support of this project and the REU program, a total of 17 PhD students, 14 masters students, and 22 undergraduate students participated the project. Students have gained interdisciplinary knowledge and unique research experience in computer vision, artificial intelligence, human-computer interaction, and assistive technologies, as well as theoretical model development and real problem solving skills. The research experiences have prepared them for their advanced degree study and future careers. Supported in part by this project, we have published and presented a total of 63 papers and presentations in conferences and journals.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/13/2020\n\n\t\t\t\t\tSubmitted by: Matt Huenerfauth"
 }
}