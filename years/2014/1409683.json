{
 "awd_id": "1409683",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Write A Classifier: Learning Fine-Grained Visual Classifiers from Text and Images",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2014-06-15",
 "awd_exp_date": "2021-05-31",
 "tot_intn_awd_amt": 499988.0,
 "awd_amount": 509588.0,
 "awd_min_amd_letter_date": "2014-06-16",
 "awd_max_amd_letter_date": "2020-07-15",
 "awd_abstract_narration": "This project develops the learning strategy using textual narrative and images makes the learning effective without a huge number of images that a typical visual learning algorithm would need to learn the class boundaries.  The research team investigates computational models for joint learning of visual concepts from images and textual descriptions of fine-grained categories, for example, discriminating between bird species.  The research activities have broader impact in three fields: computer vision, natural language processing, and machine learning. There is a huge need to develop algorithms to automatically understand the content of images and videos, with numerous potential applications in web searches, image and video archival and retrieval, surveillance applications, robot navigation and others. There are various applications for developing an intelligent system that can use narrative to define and recognize categories.\r\n\r\nThis project addresses two research questions:  First, given a visual corpus and a textual corpus about a specific domain, how to jointly and effectively learn visual concepts? Second, given these two modalities how to facilitate learning novel visual concepts using only pure textual descriptions of novel categories in the domain? The research team approaches the problem on three integrated fronts: Learning, Natural Language Processing (NLP), and Computer Vision. On the learning front, the project investigates and develops algorithms suitable for learning and predicting visual classifiers with side textual information. On the NLP front, the project aims to develop novel methods for learning global and local discriminative category-level attributes and their values from text, with feedback from human computation and visual signal. The project investigates supervised and unsupervised methods for detecting visual text, and learning methods for deep language understanding to build such rich domain models from the noisy visual text. On the Vision front, the project addresses the tasks of detection and classification with side textual information. The project investigates models for the shape and appearance of a general category that can specialize to different subordinates, in a way that allows interpreting information from text within a proper geometric context, and handle variability in viewpoints and articulation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ahmed",
   "pi_last_name": "Elgammal",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Ahmed M Elgammal",
   "pi_email_addr": "elgammal@cs.rutgers.edu",
   "nsf_id": "000189001",
   "pi_start_date": "2014-06-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "617 Bowser Road",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548072",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 166600.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 333388.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 9600.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Humans has the ability to imagine novel visual concepts when reading text descriptions. For example, we can imagine how a certain exotic bird might look like, mainly from reading a couple of sentences describing it. This ability capitalizes on our prior knowledge about object categories and their relationships. This project investigated building computational models that links text descriptions and images in specific domain in order to be able to extrapolate and recognize new categories mainly based on text. <br /><br />This project investigated the problem on three integrated fronts: Learning, Natural Language Processing (NLP), and Computer Vision. On the learning front, the project investigated and developed algorithms suitable for learning and predicting visual classifiers with side textual information. On the NLP front, the project studied and developed novel methods for learning global and local discriminative category-level attributes and their values from text, with feedback from human computation and visual signal. On the Vision front, the project addressed the tasks of detection and classification with side textual information. The project investigated models for the shape and appearance of a general category that can specialize to different subordinates, in a way that allows interpreting information from text within a proper geometric context, and handle variability in viewpoints and articulation.<br /><br />The project resulted in state-of-the-art approaches for zero-shot learning from noisy text description. The project also resulted in developing state-of-the-art models for understanding visual relationships in images. Finally, the project resulted in developing methods for bi-directional image-to-test and text-to-image translation, which facilitate generating images from text as well as generating captions from images.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/07/2022<br>\n\t\t\t\t\tModified by: Ahmed&nbsp;M&nbsp;Elgammal</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2022/1409683/1409683_10310783_1641577184053_ScreenShot2022-01-07at12.26.57PM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1409683/1409683_10310783_1641577184053_ScreenShot2022-01-07at12.26.57PM--rgov-800width.jpg\" title=\"A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts\"><img src=\"/por/images/Reports/POR/2022/1409683/1409683_10310783_1641577184053_ScreenShot2022-01-07at12.26.57PM--rgov-66x44.jpg\" alt=\"A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We leverages GANs to visually imagine the objects given noisy Wikipedia articles. With hallucinated features, a supervised classifier is trained to predict image\ufffds label.</div>\n<div class=\"imageCredit\">Ahmed Elgammal - rutgers</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Ahmed&nbsp;M&nbsp;Elgammal</div>\n<div class=\"imageTitle\">A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2022/1409683/1409683_10310783_1641577295229_ScreenShot2022-01-07at11.34.33AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2022/1409683/1409683_10310783_1641577295229_ScreenShot2022-01-07at11.34.33AM--rgov-800width.jpg\" title=\"Visual Relationship Understanding\"><img src=\"/por/images/Reports/POR/2022/1409683/1409683_10310783_1641577295229_ScreenShot2022-01-07at11.34.33AM--rgov-66x44.jpg\" alt=\"Visual Relationship Understanding\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our model recognizes a wide range of relation ship triples. Even if they are not always matching the ground truth they are frequently correct or at least reasonable as the ground truth is not complete.</div>\n<div class=\"imageCredit\">Ahmed Elgammal - rutgers</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Ahmed&nbsp;M&nbsp;Elgammal</div>\n<div class=\"imageTitle\">Visual Relationship Understanding</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nHumans has the ability to imagine novel visual concepts when reading text descriptions. For example, we can imagine how a certain exotic bird might look like, mainly from reading a couple of sentences describing it. This ability capitalizes on our prior knowledge about object categories and their relationships. This project investigated building computational models that links text descriptions and images in specific domain in order to be able to extrapolate and recognize new categories mainly based on text. \n\nThis project investigated the problem on three integrated fronts: Learning, Natural Language Processing (NLP), and Computer Vision. On the learning front, the project investigated and developed algorithms suitable for learning and predicting visual classifiers with side textual information. On the NLP front, the project studied and developed novel methods for learning global and local discriminative category-level attributes and their values from text, with feedback from human computation and visual signal. On the Vision front, the project addressed the tasks of detection and classification with side textual information. The project investigated models for the shape and appearance of a general category that can specialize to different subordinates, in a way that allows interpreting information from text within a proper geometric context, and handle variability in viewpoints and articulation.\n\nThe project resulted in state-of-the-art approaches for zero-shot learning from noisy text description. The project also resulted in developing state-of-the-art models for understanding visual relationships in images. Finally, the project resulted in developing methods for bi-directional image-to-test and text-to-image translation, which facilitate generating images from text as well as generating captions from images.\n\n\t\t\t\t\tLast Modified: 01/07/2022\n\n\t\t\t\t\tSubmitted by: Ahmed M Elgammal"
 }
}