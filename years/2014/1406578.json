{
 "awd_id": "1406578",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CHS: Medium: Design Tools for Physical Computing Objects",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 1123577.0,
 "awd_amount": 1123577.0,
 "awd_min_amd_letter_date": "2014-07-28",
 "awd_max_amd_letter_date": "2020-04-29",
 "awd_abstract_narration": "As computers decline in cost and interactive computation moves off of the desktop, computation can assume a variety of physical forms. At present, the creation of new computer interfaces is limited to already-available formats, such as smartphones, and even then is constrained to relatively simplistic computer interfaces. Computing in the future will push beyond desktops, laptops, tablets and smartphones into objects that fit into every part of our lives. But for this to happen, design tools are needed that can rapidly adapt computing to different physical forms and task needs. This project will study toolkits for the development of interactive physical objects. The project will increase national competitiveness in the invention and development of new uses of computation, and make possible rich and diverse usability research with physical computing objects in the wild rather than in the lab. Creating physical computing objects will allow a greater proportion of society to use computers in ways that fit their work requirements in fundamentally new ways. The rapid design and creation of such devices will open new markets for consumer computing.\r\n\r\nThese ends are achieved through three interrelated efforts: (1) A pluggable toolkit for creating sensor/actuator systems that form the computational basis for such objects. This toolkit will be unique in that the components when plugged together not only form an initial prototype of the device but self-reveal their physical and computational characteristics to automatically support the other design tools. A prototype constructed with this toolkit will inherently contain sufficient information to drive its own fabrication. By plugging together the prototype our tools will automatically know how to perform a custom fabrication of the electronics at a size that can be readily deployed. (2) Development of physical form via 3D printing such that the physical shape, sensors and actuators integrate with the software and electronics so as to easily prototype a complete physical/computational object. One goal is to suppress the challenges of mechanical design so that designers can focus on shape and usability. (3) Interactive machine learning techniques will be created to easily develop the mapping between human activities (as detected by sensors) and their recognition in software. It is known that putting humans in the training loop for machine learning changes the way training sets are built. Algorithms will be developed that both adapt to and exploit this behavior. This project will produce new knowledge of the concepts that are most difficult for designers when creating physical computational objects.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Dan",
   "pi_last_name": "Olsen",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Dan R Olsen",
   "pi_email_addr": "olsen@cs.byu.edu",
   "nsf_id": "000458943",
   "pi_start_date": "2014-07-28",
   "pi_end_date": "2015-07-08"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Jones",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Jones",
   "pi_email_addr": "jones@cs.byu.edu",
   "nsf_id": "000353002",
   "pi_start_date": "2015-07-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Jones",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Jones",
   "pi_email_addr": "jones@cs.byu.edu",
   "nsf_id": "000353002",
   "pi_start_date": "2014-07-28",
   "pi_end_date": "2015-07-08"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Seppi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin Seppi",
   "pi_email_addr": "kseppi@byu.edu",
   "nsf_id": "000470369",
   "pi_start_date": "2014-07-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brigham Young University",
  "inst_street_address": "A-153 ASB",
  "inst_street_address_2": "",
  "inst_city_name": "PROVO",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8014223360",
  "inst_zip_code": "846021128",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "UT03",
  "org_lgl_bus_name": "BRIGHAM YOUNG UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JWSYC7RUMJD1"
 },
 "perf_inst": {
  "perf_inst_name": "Brigham Young University",
  "perf_str_addr": "",
  "perf_city_name": "Provo",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "846021231",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "UT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 272606.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 278059.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 283619.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 289293.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-5c2843d5-7fff-40f5-4d74-b14c03f2aa6e\">&nbsp;</span></p>\n<p dir=\"ltr\"><span>This project explored design tools for what we call \"physicomps.\"&nbsp; A physicomp is a physical object containing computation.&nbsp; Unlike other computational devices, such as laptops and smartphones, physicomps do not contain screens and are typically not housed in a rectangular box.&nbsp; We organized our project into three areas: shape, electronics and machine learning.&nbsp;&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>Shape</span></p>\n<p dir=\"ltr\"><span>PHysicomp shapes are often complicated and it can be difficult to place interactive elements, called &ldquo;widgets&rdquo;--think buttons, sliders or knobs, on those shapes.&nbsp; We invented design tools for physicomps that create a \"what you see is what you get\" process in which the designer adds interactivity to a shape by dragging and dropping widgets onto the object surface in a 3d modeling tool.&nbsp; This tool automatically manages important details to make it easy for novices to quickly and accurately place widgets.&nbsp; Once the widgets are placed, the tool does the rest of the work to make that widget work in that location.&nbsp; This includes cutting holes in objects to allow access to buttons or knobs and creating mounting brackets to hold the widget exactly where placed.&nbsp; The tool also calculates a path for wiring inside the object.&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>We also created a \"what you </span><span>sculpt</span><span> is what you get\" tool for physicomps in which the designer works with clay to create the shape and embeds 3D printed widgets in the clay.&nbsp; Once the shape is refined and the widgets placed, the shape is scanned, processed by the tool and ready for printing.&nbsp; The designer then snaps in the actual widgets exactly where intended on the printed shape.&nbsp; Working with clay allows the designer to feel the shape rather than imaging the shape on a computer screen before printing.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Electronics</span></p>\n<p dir=\"ltr\"><span>A physicomp includes both shape and computing.&nbsp; We fabricated a circuit system in which a designer can snap electronics onto a cable and program them while ignoring details about how the circuit actually works.&nbsp; One challenge in programming widgets in physicomps is knowing which widget is which.&nbsp; If a system contains 10 different widgets, then the programmer needs to know the name of each of those widgets in code and keep track of where that widget is in the physical computing device.&nbsp; We invented a select-to-edit system in which the programmer shines a laser on the widget to select that widget for programming.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Machine Learning</span></p>\n<p dir=\"ltr\"><span>Finally, some physicomps have sensors that record data about the world around them.&nbsp; Finding important events in that data is an important part of interacting with the world.&nbsp; Machine learning can be used to recognize events in data, but machine learning algorithms require labeled examples as input during the training phase.&nbsp; It is difficult for most people to identify and label events in raw sensor data (which is just a list of numbers).&nbsp; We found that video recorded during data collection is helpful but not always required when people label events in sensor data.&nbsp; It seems that when using the video and data together, people quickly learn to spot events in the data and then can label those events more efficiently and more accurately in the data alone compared to the video alone.&nbsp; A key factor here is that a person can see a range of time at once in a graph of sensor data but can only see one instant in time in a frame of video. &nbsp; However, the role of data and video vary with the kind of event being labeled and how often it occurs in that datastream.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Education, Training and Broader Impacts</span></p>\n<p dir=\"ltr\"><span>The project not only resulted in inventions and insights but also involved students in meaningful roles.&nbsp; A total of&nbsp; 24 students participated in the project of which 9 were graduate students, 15 were undergraduate students and 7 were from historically underrepresented groups.&nbsp; Students learned research skills, attended research conferences and in some cases presented their work to the broader research community.&nbsp; These experiences prepared students to contribute in their careers.&nbsp;&nbsp;</span></p>\n<p dir=\"ltr\"><span>The project had broader impacts beyond research.&nbsp; The most significant impact occurred in the early stages of the COVID-19 pandemic.&nbsp; Because personal protective equipment was in short supply for frontline medical workers, we used our 3D prototyping expertise and equipment to redesign and fabricate personal protective equipment (PPE) for nurses at two local hospitals.&nbsp; We printed and delivered more than 100 modified faceshield holders that went into immediate service in spring of 2020.&nbsp;</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/12/2021<br>\n\t\t\t\t\tModified by: Kevin&nbsp;Seppi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nThis project explored design tools for what we call \"physicomps.\"  A physicomp is a physical object containing computation.  Unlike other computational devices, such as laptops and smartphones, physicomps do not contain screens and are typically not housed in a rectangular box.  We organized our project into three areas: shape, electronics and machine learning.   \nShape\nPHysicomp shapes are often complicated and it can be difficult to place interactive elements, called \"widgets\"--think buttons, sliders or knobs, on those shapes.  We invented design tools for physicomps that create a \"what you see is what you get\" process in which the designer adds interactivity to a shape by dragging and dropping widgets onto the object surface in a 3d modeling tool.  This tool automatically manages important details to make it easy for novices to quickly and accurately place widgets.  Once the widgets are placed, the tool does the rest of the work to make that widget work in that location.  This includes cutting holes in objects to allow access to buttons or knobs and creating mounting brackets to hold the widget exactly where placed.  The tool also calculates a path for wiring inside the object.  \nWe also created a \"what you sculpt is what you get\" tool for physicomps in which the designer works with clay to create the shape and embeds 3D printed widgets in the clay.  Once the shape is refined and the widgets placed, the shape is scanned, processed by the tool and ready for printing.  The designer then snaps in the actual widgets exactly where intended on the printed shape.  Working with clay allows the designer to feel the shape rather than imaging the shape on a computer screen before printing. \nElectronics\nA physicomp includes both shape and computing.  We fabricated a circuit system in which a designer can snap electronics onto a cable and program them while ignoring details about how the circuit actually works.  One challenge in programming widgets in physicomps is knowing which widget is which.  If a system contains 10 different widgets, then the programmer needs to know the name of each of those widgets in code and keep track of where that widget is in the physical computing device.  We invented a select-to-edit system in which the programmer shines a laser on the widget to select that widget for programming. \nMachine Learning\nFinally, some physicomps have sensors that record data about the world around them.  Finding important events in that data is an important part of interacting with the world.  Machine learning can be used to recognize events in data, but machine learning algorithms require labeled examples as input during the training phase.  It is difficult for most people to identify and label events in raw sensor data (which is just a list of numbers).  We found that video recorded during data collection is helpful but not always required when people label events in sensor data.  It seems that when using the video and data together, people quickly learn to spot events in the data and then can label those events more efficiently and more accurately in the data alone compared to the video alone.  A key factor here is that a person can see a range of time at once in a graph of sensor data but can only see one instant in time in a frame of video.   However, the role of data and video vary with the kind of event being labeled and how often it occurs in that datastream. \nEducation, Training and Broader Impacts\nThe project not only resulted in inventions and insights but also involved students in meaningful roles.  A total of  24 students participated in the project of which 9 were graduate students, 15 were undergraduate students and 7 were from historically underrepresented groups.  Students learned research skills, attended research conferences and in some cases presented their work to the broader research community.  These experiences prepared students to contribute in their careers.  \nThe project had broader impacts beyond research.  The most significant impact occurred in the early stages of the COVID-19 pandemic.  Because personal protective equipment was in short supply for frontline medical workers, we used our 3D prototyping expertise and equipment to redesign and fabricate personal protective equipment (PPE) for nurses at two local hospitals.  We printed and delivered more than 100 modified faceshield holders that went into immediate service in spring of 2020. \n\n \n\n\t\t\t\t\tLast Modified: 10/12/2021\n\n\t\t\t\t\tSubmitted by: Kevin Seppi"
 }
}