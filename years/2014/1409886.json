{
 "awd_id": "1409886",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Models of Handshape Articulatory Phonology for Recognition and Analysis of American Sign Language",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2014-06-01",
 "awd_exp_date": "2019-05-31",
 "tot_intn_awd_amt": 275546.0,
 "awd_amount": 275546.0,
 "awd_min_amd_letter_date": "2014-06-09",
 "awd_max_amd_letter_date": "2014-06-09",
 "awd_abstract_narration": "Sign languages are the primary means of communication for millions of Deaf people in the world, including about 350,000-500,000 American Sign Language (ASL) users in the US.  While the hearing population has benefited from advances in speech technologies such as speech recognition and spoken web search, much less progress has been made for sign language interfaces.  Advances depend on improved technology for analyzing sign language from video.  In addition, the linguistics of sign language is less well-understood than that of spoken language.  This project addresses both of these needs, with an interdisciplinary approach that will contribute to research in linguistics, language processing, computer vision, and machine learning.  Applications of the work include better access to ASL social media video archives, interactive recognition and search applications for Deaf individuals, and ASL-English interpretation assistance.\r\n\r\nThis project focuses on handshape in ASL, in particular on one constrained but very practical component:  fingerspelling, or the spelling out of a word as a sequence of handshapes and trajectories between them.  Fingerspelling comprises up to 35% of ASL, depending on the context, and includes 72% of ASL handshapes, making it an excellent testing ground.  The project addresses gaps in existing work by focusing on handshape in various conditions, including fast, highly coarticulated signing.  The main project activities include development of (1) robust automatic detection and recognition of fingerspelled words using new handshape models, including segmental and \"multi-segmental\" graphical models of ASL phonological features; (2) techniques for generalizing across signers, styles, and recording conditions; (3) improved phonetics and phonology of handshape, in particular contributing to an articulatory phonology of sign; and (4) publicly released multi-speaker, multi-style fingerspelling data and associated semi-automatic annotation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Diane",
   "pi_last_name": "Brentari",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Diane Brentari",
   "pi_email_addr": "dbrentari@uchicago.edu",
   "nsf_id": "000211416",
   "pi_start_date": "2014-06-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jason",
   "pi_last_name": "Riggle",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jason Riggle",
   "pi_email_addr": "jriggle@uchicago.edu",
   "nsf_id": "000490084",
   "pi_start_date": "2014-06-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "Linguistics Department - University of Chicago",
  "perf_str_addr": "1010 East 59th Street",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606371512",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 275546.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This aims of this project are: (i) to develop a tool for the automatic recognition of fingerspelling in American Sign Language (ASL) from videos, and (ii) to better understanding of the linguistic role and properties of fingerspelling within ASL. &nbsp;The long-term goal is for computational innovation and analysis to help sign language users in their everyday lives.</p>\n<p>Fingerspelling is a part of ASL in which words are spelled out letter by letter, using the English alphabet, where each letter is represented by a distinct handshape. Fingerspelled words are typically names, technical words, or words borrowed from another language. Fingerspelling can also be used for emphasis. Recognizing fingerspelling has great practical importance because fingerspelled words are often some of the most important content words. At the same time, &nbsp;recognizing fingerspelling in video is very challenging &nbsp;problem for machine learning because it involves quick, highly co-articulated motions of the fingers and hands.</p>\n<p>This project addresses the challenges of fingerspelling recognition with a combination of innovative data collection techniques and new research ideas in computer vision, sequence modeling, machine learning, and linguistics. The project has focused on two data sets of ASL fingerspelling: a pre-existing smaller studio data set, consisting of four native signers producing individual fingerspelled words from a list in a controlled lab setting; and a larger data set of fingerspelling ?in the wild?, collected and annotated as part of the project from naturally occurring videos harvested from the internet.</p>\n<p>The main outcomes of the project can be divided into: (i) collection and annotation of the new data set, (ii) contributions to the main task of ASL fingerspelling recognition from video, which include contributions to the related research areas of computer vision and machine learning, (iii) contributions to the linguistics, and (iv) broader impact on other fields and on education. Each of these is described further below.</p>\n<p><strong>Data collection and annotation </strong></p>\n<p>This project has produced the largest fingerspelling video data set to date, and the only one including a large proportion of spontaneous ASL produced by signers for signers. We have found that human annotators can label such data with very high agreement, but the data is very challenging for automatic recognizers. We have developed an approach for crowdsourcing the annotation, which allows us to collect data much more efficiently. Using this data set we have trained automatic recognizers to identify the handshapes of rapid fingerspelling. Our initial recognizers and the data we have collected will be made publicly available to enable further study and improved recognizers.</p>\n<p><strong>Language models, computer vision and artificial intelligence</strong></p>\n<p>The project has produced and studied a variety of models that advance the state of the art of fingerspelling recognition from video. These include models for recognizing fingerspelling by a known signer (that the model was trained for), or an unknown signer for which a small amount of adaptation data is available. We have developed end-to-end neural models that do not rely on frame-by-frame</p>\n<p>annotation or a predefined list of known words. Since sign language training data is scarce, we have also developed ways of leveraging unannotated hand images to improve recognition. This project has produced some methods that are more broadly applicable to computer vision tasks and other artificial intelligence problems. In particular, we have developed methods for handling image blur and low resolution, using very deep neural networks, and for learning from unlabeled sequential data.</p>\n<p><strong>Linguistic Analyses</strong></p>\n<p>The project has produced new linguistic understanding of fingerspelling. One outcome is an articulatory model of handshape, which parameterizes handshape and allows for direct measures of handshape similarity. Our analysis has also found that fingerspelling functions similarly to codeswitching, where signers choose the fingerspelled word from English or the ASL sign according to the particular linguistic topic (politics, research, etc.) and context (who the participants in the conversation are). Finally, we have studied how fingerspelled forms sometimes become new signs. The new data set will provide a large corpus to work a broad range of linguistic issues in sign languages. These insights will inform both future linguistic research and future work on automatically detecting fingerspelling within ASL video.</p>\n<p><strong>Broader Impact</strong></p>\n<p>The linguistic results are likely to have an impact on ASL teaching and interpreter training. Ultimately, the project will lead to improvements in interaction between Deaf and hearing individuals, as well as on increased usability and searchability of Deaf community video media.</p>\n<p>Several graduate and undergraduate students have been trained on this project, many of whom have been from groups that are typically under-represented in the fields of science and technology. The project has also had broader impact through the PIs? courses, which have used some of the materials produced by the project and have involved a broad range of students. These courses include those on the computational field and in the discipline of linguistics.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/14/2019<br>\n\t\t\t\t\tModified by: Diane&nbsp;Brentari</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis aims of this project are: (i) to develop a tool for the automatic recognition of fingerspelling in American Sign Language (ASL) from videos, and (ii) to better understanding of the linguistic role and properties of fingerspelling within ASL.  The long-term goal is for computational innovation and analysis to help sign language users in their everyday lives.\n\nFingerspelling is a part of ASL in which words are spelled out letter by letter, using the English alphabet, where each letter is represented by a distinct handshape. Fingerspelled words are typically names, technical words, or words borrowed from another language. Fingerspelling can also be used for emphasis. Recognizing fingerspelling has great practical importance because fingerspelled words are often some of the most important content words. At the same time,  recognizing fingerspelling in video is very challenging  problem for machine learning because it involves quick, highly co-articulated motions of the fingers and hands.\n\nThis project addresses the challenges of fingerspelling recognition with a combination of innovative data collection techniques and new research ideas in computer vision, sequence modeling, machine learning, and linguistics. The project has focused on two data sets of ASL fingerspelling: a pre-existing smaller studio data set, consisting of four native signers producing individual fingerspelled words from a list in a controlled lab setting; and a larger data set of fingerspelling ?in the wild?, collected and annotated as part of the project from naturally occurring videos harvested from the internet.\n\nThe main outcomes of the project can be divided into: (i) collection and annotation of the new data set, (ii) contributions to the main task of ASL fingerspelling recognition from video, which include contributions to the related research areas of computer vision and machine learning, (iii) contributions to the linguistics, and (iv) broader impact on other fields and on education. Each of these is described further below.\n\nData collection and annotation \n\nThis project has produced the largest fingerspelling video data set to date, and the only one including a large proportion of spontaneous ASL produced by signers for signers. We have found that human annotators can label such data with very high agreement, but the data is very challenging for automatic recognizers. We have developed an approach for crowdsourcing the annotation, which allows us to collect data much more efficiently. Using this data set we have trained automatic recognizers to identify the handshapes of rapid fingerspelling. Our initial recognizers and the data we have collected will be made publicly available to enable further study and improved recognizers.\n\nLanguage models, computer vision and artificial intelligence\n\nThe project has produced and studied a variety of models that advance the state of the art of fingerspelling recognition from video. These include models for recognizing fingerspelling by a known signer (that the model was trained for), or an unknown signer for which a small amount of adaptation data is available. We have developed end-to-end neural models that do not rely on frame-by-frame\n\nannotation or a predefined list of known words. Since sign language training data is scarce, we have also developed ways of leveraging unannotated hand images to improve recognition. This project has produced some methods that are more broadly applicable to computer vision tasks and other artificial intelligence problems. In particular, we have developed methods for handling image blur and low resolution, using very deep neural networks, and for learning from unlabeled sequential data.\n\nLinguistic Analyses\n\nThe project has produced new linguistic understanding of fingerspelling. One outcome is an articulatory model of handshape, which parameterizes handshape and allows for direct measures of handshape similarity. Our analysis has also found that fingerspelling functions similarly to codeswitching, where signers choose the fingerspelled word from English or the ASL sign according to the particular linguistic topic (politics, research, etc.) and context (who the participants in the conversation are). Finally, we have studied how fingerspelled forms sometimes become new signs. The new data set will provide a large corpus to work a broad range of linguistic issues in sign languages. These insights will inform both future linguistic research and future work on automatically detecting fingerspelling within ASL video.\n\nBroader Impact\n\nThe linguistic results are likely to have an impact on ASL teaching and interpreter training. Ultimately, the project will lead to improvements in interaction between Deaf and hearing individuals, as well as on increased usability and searchability of Deaf community video media.\n\nSeveral graduate and undergraduate students have been trained on this project, many of whom have been from groups that are typically under-represented in the fields of science and technology. The project has also had broader impact through the PIs? courses, which have used some of the materials produced by the project and have involved a broad range of students. These courses include those on the computational field and in the discipline of linguistics.\n\n\t\t\t\t\tLast Modified: 07/14/2019\n\n\t\t\t\t\tSubmitted by: Diane Brentari"
 }
}