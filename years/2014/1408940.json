{
 "awd_id": "1408940",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: Automatic Locality Management for Dynamically Scheduled Parallelism",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2014-06-01",
 "awd_exp_date": "2020-05-31",
 "tot_intn_awd_amt": 962951.0,
 "awd_amount": 962951.0,
 "awd_min_amd_letter_date": "2014-06-03",
 "awd_max_amd_letter_date": "2014-06-03",
 "awd_abstract_narration": "Title: Automatic Locality Management for Dynamically Scheduled Parallelism\r\n\r\nToday's multicore and manycore computers provide increasing amounts of computational power in the form of parallel processing coupled with a complex memory organization with many levels of hierarchy and orders of magnitude difference in cost between accessing different levels. When software exhibits spatial and temporal locality, meaning that it reads and writes memory addresses that are close to one another in relatively small time span, it is able to primarily access data in fast caches, rather than in slow main memory, and deliver good sequential and parallel performance. Unfortunately, with software written in high-level managed programming languages it is difficult to ensure or to predict the amount of spatial and temporal locality, due to the lack of low-level programmer control and the complexities of and interactions between the specific hardware platform and the thread scheduler and the memory manager. This project explores techniques for automatic management of locality in high-level managed programming languages executing on parallel computers with sophisticated memory hierarchies. Using the theoretical models, efficient algorithms, and practical implementations being developed in the project, programmers are able to reason about the expected locality of their programs independent of the target hardware, while a runtime system, including thread scheduler and memory manager, maps the program onto specific hardware to achieve the established performance bounds.\r\n\r\nIn particular, this project addresses the problem of automatically managing locality via the runtime system of a high-level garbage-collected parallel functional programming language. A comprehensive approach that considers scheduling, memory allocation, and memory reclamation together is used, allowing the thread scheduler to influence the memory manager and vice versa. A key insight of this research program is to view the allocated data of a program as a hierarchical collection of task- and scheduler-mapped heaps. This view guides the theoretical cost model that enables a programmer to reason about locality at a high-level, the efficient algorithms that control when to create and to garbage collect a heap with provable bounds, and the practical implementation that delivers automatic locality management in a parallel functional programming language. The intellectual merits are advances in understanding the interaction of thread scheduling and memory management with locality on modern parallel hardware, the development of high-level, machine-independent cost model, and a synthesis of programming languages, algorithmic theory, and system design to address the challenges of automatic locality management. The broader impacts are improvements in software quality and programmer productivity, the creation of a parallel functional programming language usable in both education and research, and the integration of results into courses and outreach activities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Umut",
   "pi_last_name": "Acar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Umut Acar",
   "pi_email_addr": "umut@cs.cmu.edu",
   "nsf_id": "000636864",
   "pi_start_date": "2014-06-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Guy",
   "pi_last_name": "Blelloch",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Guy E Blelloch",
   "pi_email_addr": "guyb@cs.cmu.edu",
   "nsf_id": "000196851",
   "pi_start_date": "2014-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7943",
   "pgm_ref_txt": "PROGRAMMING LANGUAGES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 962951.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-8dcfa962-7fff-67c8-feaf-a9b904ae0c2e\">\n<p dir=\"ltr\"><span>The major goal of this project has been to develop techniques for automatically managing and exploiting spatial and temporal locality within the run-time system of high-level managed programming languages executing on parallel computers with sophisticated memory hierarchies (SNAPL'15).&nbsp; The project has addressed the problem in the context of a high-level garbage-collected parallel functional programming language using a comprehensive approach that considers scheduling, memory allocation, and memory reclamation together, allowing the thread scheduler to influence the memory manager and vice versa.</span></p>\n<p dir=\"ltr\"><span>In our primary line of research, we have explored hierarchical garbage collection techniques.&nbsp; The hierarchical garbage collector creates a sub-heap for each created parallel task; these sub-heaps are organized in a tree that mirrors the parent-child relationship between tasks and when a child joins with its parent, its sub-heap is merged into the parent's sub-heap.&nbsp; Only the sub-heaps that correspond to leaves of the tree are active (running tasks are performing allocations).&nbsp; This work has been the main focus of Ph.D. students Ram Raghunathan and Sam Westrick at Carnegie Mellon University.&nbsp; Stefan Muller, another PhD student at Carnegie Mellon, has also contributed.&nbsp; In our initial work (ICFP'16), we focused on pure (mutationless) computations, which naturally maintain the invariant that data in child sub-heaps can reference data in ancestor sub-heaps.&nbsp; This invariant remains valid under load balancing actions by the scheduler without needing to promote or copy data in memory.&nbsp; This property turns out to be key to efficient and scalable parallelism.&nbsp;</span></p>\n<p dir=\"ltr\"><span>Because data in parent sub-heaps cannot reference data in descendent sub-heaps, any sub-tree of sub-heaps can be independently collected; an important special case is that leaves of the tree can be collected without any coordination or synchronization.&nbsp; Noting that this invariant can be violated in the presence of mutation (crucial for the efficient implementation of imperative algorithms), we turned our attention to the important problem of supporting mutation.&nbsp; We first developed a promotion scheme to maintain the invariants required by the hierarchical garbage collector (PPoPP'18).&nbsp; When a task attempts to update an object in an ancestor's heap with a reference to an object in a descendent's heap, we require the descendent object be promoted to the ancestor's sub-heap, effectively creating a copy of the descendent object as an ancestor object, and the promoted copy is used to perform the update.&nbsp; We demonstrated that our approach has a number of useful properties and performs well.&nbsp; We have also investigated a weaker *disentanglement* invariant (POPL'20).&nbsp; At a high-level, the disentanglement property ensures that a task cannot access memory allocated by a concurrently executing task.&nbsp; Importantly, disentanglement is satisfied by all determinacy-race-free nested-parallel programs (and, consequently, by many real parallel algorithms).&nbsp; But, disentanglement is weaker than race-freedom and allows certain kinds of races (for example, concurrently executing threads can communicate via objects allocated by shared ancestor tasks).&nbsp; We modified the hierarchical garbage collection scheme to take advantage of the disentanglement property and empirically demonstrate that it is efficient and scales well.</span></p>\n<p dir=\"ltr\"><span>An important aspect of our work has been to develop practical implementations of these memory management techniques.&nbsp; To that end, we have developed MaPLe (MPL), a fork of the MLton Standard ML compiler that supports parallel programs and hierarchical memory management, developed in a public GitHub repository (https://github.com/MPLLang/mpl). Throughout the project, we have made infrastructure improvements to the compiler and runtime system to facilitate current and future research.&nbsp; We have periodically updated the MaPLe compiler source code with respect to the upstream MLton source code, gaining general improvements and bug fixes, and we have also backported some infrastructure changes from the MaPLe compiler source code.&nbsp; The empirical evaluations described above were conducted using the MaPLe compiler.&nbsp; The MaPLe compiler has proven robust enough to be used in the CMU undergraduate algorithms course, with as many as 500 students per year taking the course developed by PIs Acar and Blelloch.</span></p>\n<p dir=\"ltr\"><span>In a second line of research, we have studied algorithms for scheduling of parallel computations that mix traditional compute-intensive parallelism with interactive tasks, where responsiveness to external requests (from a user or the network) is important and latency is an issue. This line of research has led to the PhD thesis of Stefan Muller at Carnegie Mellon University.&nbsp; We have extended work-stealing scheduling to support latency-hiding (SPAA'16), developed a graph-based cost model for cooperative and competitive threading (PLDI'17), integrated a notion of fairness and developed a fairly prompt scheduling principle that bounds the time by which a low-priority task can be delayed by a high-priority task (ICFP'19), developed the Adaptive Priority Scheduling (APS) algorithm that provides provably efficient response time for tasks at all priority levels (SPAA'20). and developed a type system and cost model for a language with implicit parallelism in the form of prioritized futures and mutable state in the form of references that statically prevents priority inversions (PLDI'20).</span></p>\n</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/26/2020<br>\n\t\t\t\t\tModified by: Umut&nbsp;Acar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThe major goal of this project has been to develop techniques for automatically managing and exploiting spatial and temporal locality within the run-time system of high-level managed programming languages executing on parallel computers with sophisticated memory hierarchies (SNAPL'15).  The project has addressed the problem in the context of a high-level garbage-collected parallel functional programming language using a comprehensive approach that considers scheduling, memory allocation, and memory reclamation together, allowing the thread scheduler to influence the memory manager and vice versa.\nIn our primary line of research, we have explored hierarchical garbage collection techniques.  The hierarchical garbage collector creates a sub-heap for each created parallel task; these sub-heaps are organized in a tree that mirrors the parent-child relationship between tasks and when a child joins with its parent, its sub-heap is merged into the parent's sub-heap.  Only the sub-heaps that correspond to leaves of the tree are active (running tasks are performing allocations).  This work has been the main focus of Ph.D. students Ram Raghunathan and Sam Westrick at Carnegie Mellon University.  Stefan Muller, another PhD student at Carnegie Mellon, has also contributed.  In our initial work (ICFP'16), we focused on pure (mutationless) computations, which naturally maintain the invariant that data in child sub-heaps can reference data in ancestor sub-heaps.  This invariant remains valid under load balancing actions by the scheduler without needing to promote or copy data in memory.  This property turns out to be key to efficient and scalable parallelism. \nBecause data in parent sub-heaps cannot reference data in descendent sub-heaps, any sub-tree of sub-heaps can be independently collected; an important special case is that leaves of the tree can be collected without any coordination or synchronization.  Noting that this invariant can be violated in the presence of mutation (crucial for the efficient implementation of imperative algorithms), we turned our attention to the important problem of supporting mutation.  We first developed a promotion scheme to maintain the invariants required by the hierarchical garbage collector (PPoPP'18).  When a task attempts to update an object in an ancestor's heap with a reference to an object in a descendent's heap, we require the descendent object be promoted to the ancestor's sub-heap, effectively creating a copy of the descendent object as an ancestor object, and the promoted copy is used to perform the update.  We demonstrated that our approach has a number of useful properties and performs well.  We have also investigated a weaker *disentanglement* invariant (POPL'20).  At a high-level, the disentanglement property ensures that a task cannot access memory allocated by a concurrently executing task.  Importantly, disentanglement is satisfied by all determinacy-race-free nested-parallel programs (and, consequently, by many real parallel algorithms).  But, disentanglement is weaker than race-freedom and allows certain kinds of races (for example, concurrently executing threads can communicate via objects allocated by shared ancestor tasks).  We modified the hierarchical garbage collection scheme to take advantage of the disentanglement property and empirically demonstrate that it is efficient and scales well.\nAn important aspect of our work has been to develop practical implementations of these memory management techniques.  To that end, we have developed MaPLe (MPL), a fork of the MLton Standard ML compiler that supports parallel programs and hierarchical memory management, developed in a public GitHub repository (https://github.com/MPLLang/mpl). Throughout the project, we have made infrastructure improvements to the compiler and runtime system to facilitate current and future research.  We have periodically updated the MaPLe compiler source code with respect to the upstream MLton source code, gaining general improvements and bug fixes, and we have also backported some infrastructure changes from the MaPLe compiler source code.  The empirical evaluations described above were conducted using the MaPLe compiler.  The MaPLe compiler has proven robust enough to be used in the CMU undergraduate algorithms course, with as many as 500 students per year taking the course developed by PIs Acar and Blelloch.\nIn a second line of research, we have studied algorithms for scheduling of parallel computations that mix traditional compute-intensive parallelism with interactive tasks, where responsiveness to external requests (from a user or the network) is important and latency is an issue. This line of research has led to the PhD thesis of Stefan Muller at Carnegie Mellon University.  We have extended work-stealing scheduling to support latency-hiding (SPAA'16), developed a graph-based cost model for cooperative and competitive threading (PLDI'17), integrated a notion of fairness and developed a fairly prompt scheduling principle that bounds the time by which a low-priority task can be delayed by a high-priority task (ICFP'19), developed the Adaptive Priority Scheduling (APS) algorithm that provides provably efficient response time for tasks at all priority levels (SPAA'20). and developed a type system and cost model for a language with implicit parallelism in the form of prioritized futures and mutable state in the form of references that statically prevents priority inversions (PLDI'20).\n\n\n \n\n\t\t\t\t\tLast Modified: 07/26/2020\n\n\t\t\t\t\tSubmitted by: Umut Acar"
 }
}