{
 "awd_id": "1431856",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Research in Student Peer Review: A Cooperative Web-Services Approach",
 "cfda_num": "47.076",
 "org_code": "11040200",
 "po_phone": "7032922635",
 "po_email": "mferrara@nsf.gov",
 "po_sign_block_name": "Mike Ferrara",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2019-08-31",
 "tot_intn_awd_amt": 208589.0,
 "awd_amount": 208589.0,
 "awd_min_amd_letter_date": "2014-08-05",
 "awd_max_amd_letter_date": "2014-08-05",
 "awd_abstract_narration": "Hundreds of thousands of students have used online peer review applications to review their classmates' work. While learning gains from peer review have been documented repeatedly, current systems do not always produce accurate scores and often give inadequate guidance to students about what constitutes a good review, resulting in haphazard feedback. This research addresses these issues with a common set of web services that can be used by any peer review system, as well as new visualizations that identify students' strengths and weaknesses, and gauge improvement over time.\r\n\r\nThis project differs from previous research that typically involves a single peer review system. It will develop a set of web services that will be usable by any peer review system in the same way that Google Maps is available to any website that wants to display location data. This common implementation will allow the project team to gather data from large numbers of students in a wide variety of contexts, thereby giving us the statistical power to produce more convincing, highly generalizable results\r\n\r\nPeer review during the writing process is an example of formative assessment, feedback that is received while the recipient still has a chance to improve his/her work. Several studies have found that the students who benefit most from formative assessment are those who typically underperform as measured by exams and standardized tests. Formative assessment tends to level the playing field for underrepresented minorities by allowing these students to receive input from their peers when they are not stressed about how their grade is being affected.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Tinapple",
   "pi_mid_init": "a",
   "pi_sufx_name": "",
   "pi_full_name": "David a Tinapple",
   "pi_email_addr": "david.tinapple@asu.edu",
   "nsf_id": "000605422",
   "pi_start_date": "2014-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852816011",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "153600",
   "pgm_ele_name": "S-STEM-Schlr Sci Tech Eng&Math"
  },
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8209",
   "pgm_ref_txt": "Improv Undergrad STEM Ed(IUSE)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0414",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001415DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "13XX",
   "app_name": "H-1B FUND, EHR, NSF",
   "app_symb_id": "045176",
   "fund_code": "1300XXXXDB",
   "fund_name": "H-1B FUND, EDU, NSF",
   "fund_symb_id": "045176"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 208589.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project was conceived of by a group of researchers interested in \"peer-review,\" also called \"peer-evaluation,\". Peer-review is the practice of having students give and receive useful and critical feedback to and from each other as part of their course work. It's been extensively studied over many decades, shown to be a valuable teaching tool, and is implemented in a wide variety of styles. However, it is seldom used in digital learning environments, despite the flexibility and new forms of peer review that are made possible by a controlled networked environment. As researchers and developers, we wondered why it's so underutilized. We hypothesized that the reason could be that peer review is more complex that traditional evaluation, even without computers, and that implementing peer review in virtual environments is actually more difficult rather than less, because there are no natural constraints to limit the complexity.</p>\n<p>Our approach was to identify and create a set of modules, or \"web services\" that could be easily plugged in to many learning management systems, and that cover the essential aspects of peer review, allowing sophisticated peer review systems to be quickly built on top of existing digital teaching tools.</p>\n<p>Our first challenge was to identify what the essential components of peer review are by evaluating many systems and tools that implement it and looking form commonalities. We also had to decide what the right scale was for our web services. Too big, and the web service pushes users into a specific style or type of peer review that they may not desire. Too small, and the web services would not provide enough utility and wouldn&rsquo;t be worth the effort of integrating.</p>\n<p>This first phase of the project involved an analysis of existing peer review tools. Some tools refer to \"peer-grading\", where students are given other students work to grade, using a familiar grading rubric. In these types of systems, the student grader is often anonymous, and sometimes only asked click on what they believe is the appropriate grade. Other tools refer to the practice as \"peer-critique\", where students are not anonymous, and are asked to write constructive feedback.</p>\n<p>Right away we have systems that approach peer review quite differently. We found other similar dichotomies and differences in approach that were just as dramatic as the split between (critique/grade), and (anonymous/named). Some systems pair up or group peer reviewers to review each other. Some try to ensure that peer review is not reciprocal (high/low reciprocity). Some have peer reviewers evaluate one work at a time, while others assign work in clusters and ask them to compare works (clusteredness). Some clustered systems allow for rating systems to be used by reviewers, and other systems prefer to use ranking and thus produce very different kinds of data. And some systems are designed for massive online classes with thousands of students, and others are geared toward augmenting traditional classrooms with tens of students.</p>\n<p>We realized that the \"design space\" of peer review is much bigger and more varied than expected, and that it may be this fact that makes it difficult to implement in real-world use. There are so many approaches, that no single \"one size fits all\" tool has been created that can encompass them all. This analysis was critical in deciding what services to build. We created a set of web services which you can find at https://www.peerlogic.org/ and are being actively used in peer review tools.</p>\n<p>Possibly the greatest outcome of this project is that we created a much more broad and detailed map of the design space for peer review tools. In classifying the design approaches, we came to understand the particular qualities and affordances associated with these different approaches and their impact on different kinds and scales of teaching and learning. This allowed us to focus work in particular areas.</p>\n<p>This project increased the resolution of our picture of what makes peer review work but also why it works, and is leading to a much greater ability to transition the practice to online and digital environments. More importantly, it's leading to increased interest and adoption among teachers and administrators, as we are now able to design for particular needs, and to discuss individual design parameters with stakeholders in a way that makes the benefits of peer review easier to understand, envision, and promote. Peer review in the classroom is underutilized, and as teaching becomes increasingly remote, and courses grow in scale, peer review can become more impactful and more important.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/26/2020<br>\n\t\t\t\t\tModified by: David&nbsp;A&nbsp;Tinapple</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project was conceived of by a group of researchers interested in \"peer-review,\" also called \"peer-evaluation,\". Peer-review is the practice of having students give and receive useful and critical feedback to and from each other as part of their course work. It's been extensively studied over many decades, shown to be a valuable teaching tool, and is implemented in a wide variety of styles. However, it is seldom used in digital learning environments, despite the flexibility and new forms of peer review that are made possible by a controlled networked environment. As researchers and developers, we wondered why it's so underutilized. We hypothesized that the reason could be that peer review is more complex that traditional evaluation, even without computers, and that implementing peer review in virtual environments is actually more difficult rather than less, because there are no natural constraints to limit the complexity.\n\nOur approach was to identify and create a set of modules, or \"web services\" that could be easily plugged in to many learning management systems, and that cover the essential aspects of peer review, allowing sophisticated peer review systems to be quickly built on top of existing digital teaching tools.\n\nOur first challenge was to identify what the essential components of peer review are by evaluating many systems and tools that implement it and looking form commonalities. We also had to decide what the right scale was for our web services. Too big, and the web service pushes users into a specific style or type of peer review that they may not desire. Too small, and the web services would not provide enough utility and wouldn\u2019t be worth the effort of integrating.\n\nThis first phase of the project involved an analysis of existing peer review tools. Some tools refer to \"peer-grading\", where students are given other students work to grade, using a familiar grading rubric. In these types of systems, the student grader is often anonymous, and sometimes only asked click on what they believe is the appropriate grade. Other tools refer to the practice as \"peer-critique\", where students are not anonymous, and are asked to write constructive feedback.\n\nRight away we have systems that approach peer review quite differently. We found other similar dichotomies and differences in approach that were just as dramatic as the split between (critique/grade), and (anonymous/named). Some systems pair up or group peer reviewers to review each other. Some try to ensure that peer review is not reciprocal (high/low reciprocity). Some have peer reviewers evaluate one work at a time, while others assign work in clusters and ask them to compare works (clusteredness). Some clustered systems allow for rating systems to be used by reviewers, and other systems prefer to use ranking and thus produce very different kinds of data. And some systems are designed for massive online classes with thousands of students, and others are geared toward augmenting traditional classrooms with tens of students.\n\nWe realized that the \"design space\" of peer review is much bigger and more varied than expected, and that it may be this fact that makes it difficult to implement in real-world use. There are so many approaches, that no single \"one size fits all\" tool has been created that can encompass them all. This analysis was critical in deciding what services to build. We created a set of web services which you can find at https://www.peerlogic.org/ and are being actively used in peer review tools.\n\nPossibly the greatest outcome of this project is that we created a much more broad and detailed map of the design space for peer review tools. In classifying the design approaches, we came to understand the particular qualities and affordances associated with these different approaches and their impact on different kinds and scales of teaching and learning. This allowed us to focus work in particular areas.\n\nThis project increased the resolution of our picture of what makes peer review work but also why it works, and is leading to a much greater ability to transition the practice to online and digital environments. More importantly, it's leading to increased interest and adoption among teachers and administrators, as we are now able to design for particular needs, and to discuss individual design parameters with stakeholders in a way that makes the benefits of peer review easier to understand, envision, and promote. Peer review in the classroom is underutilized, and as teaching becomes increasingly remote, and courses grow in scale, peer review can become more impactful and more important.\n\n \n\n\t\t\t\t\tLast Modified: 08/26/2020\n\n\t\t\t\t\tSubmitted by: David A Tinapple"
 }
}