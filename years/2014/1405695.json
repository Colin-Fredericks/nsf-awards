{
 "awd_id": "1405695",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "II-EN:  Collaborative Research:  Large-Scale FPGA-Centric Cluster with Direct and Programmable Communication",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 349946.0,
 "awd_amount": 349946.0,
 "awd_min_amd_letter_date": "2014-07-24",
 "awd_max_amd_letter_date": "2014-07-24",
 "awd_abstract_narration": "Together with theory and experimentation, computer simulation now constitutes the third pillar of scientific inquiry, enabling researchers to build and test models of complex phenomena that either cannot be, or would be prohibitively expensive to be, replicated in the laboratory.  Applications range from the practical, such as designing more efficient aircraft and effective drugs, to basic research in understanding the molecular basis of diseases such as Alzheimer?s.  Yet computing capability is currently only a small fraction of what is needed:  detailed biological simulations are limited to small numbers of macro-molecules; additional factors of millions are needed to simulate cells and far more than that for larger structures.  The overall goal of this work is to give the Scientific Computing user community the capability to conduct transformative research via scalable, cost-effective, high-performance, general-purpose systems built from off-the-shelf components.  The particular objective is to build a compute cluster and related infrastructure that facilitates research that advances such computer systems.  The unifying technical mechanism to be explored is the integration of communication and computation in accelerator-centric clusters with direct and programmable interconnects.\r\n\r\nThree fundamental issues limiting performance are computational efficiency, power density, and communication latency.  All of these issues are being addressed through increased heterogeneity, but the last in particular by integrating communication into the accelerator.  This integration enables direct and programmable communication among compute components.  Direct links enable the bypassing of CPU, network interface, and even device memory.  Programmable communication enables data transfers to proceed with high efficiency even under substantial loads.  The proposed infrastructure is a large-scale FPGA-centric cluster with direct and programmable communication (DPC).  This server class is referred to as Novo-G#, where # is a place holder for DPC, because this award will target enhancing and leveraging Novo-G, the reconfigurable supercomputer at the University of Florida.  The infrastructure will consist of the physical hardware, but also software and configurations to be developed to enhance both general usability and the enabled research projects.  Another aspect of this infrastructure, as with the Novo-G, is the community of collaborators who will contribute tools, applications, evaluation, and feedback.  Currently, a number of internal and external collaborators have been identified but there are more potential research projects?in diverse areas of applications, architecture, and systems that will be enabled by the proposed infrastructure.\r\n\r\nThe broader impact of the enabled research is to advance the capability of scientific computing.  The technical broader impact of the proposed infrastructure is to provide a system testbed for transformative research in a variety of areas in Computer Science and Engineering including programmable network components, processor/network interfaces especially for accelerators, FPGA-based systems, applications in Reconfigurable Computing, architecture of clusters with direct and programmable communication, and libraries and tools to support such clusters. The community of researchers using the infrastructure will consist of the PIs and their collaborators, but also the members of the broader community who commit to contributing to the infrastructure.  The infrastructure will provide a platform to develop novel components for education and outreach. \r\n\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Martin",
   "pi_last_name": "Herbordt",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Martin C Herbordt",
   "pi_email_addr": "herbordt@bu.edu",
   "nsf_id": "000112548",
   "pi_start_date": "2014-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "8 St. Mary's St.",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151300",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 349946.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Together with theory and experiment, computer simulation constitutes the third pillar of scientific inquiry, enabling researchers to build and test models of complex phenomena that either cannot be, or would be prohibitively expensive to be, replicated in the laboratory.&nbsp; Applications range from the practical, such as designing more efficient aircraft and effective drugs, to basic research in understanding the molecular basis of diseases such as Alzheimer&rsquo;s.&nbsp; Yet computing capability is currently only a small fraction of what is needed:&nbsp; additional factors of millions are needed to simulate cells, and far more than that for larger structures.&nbsp; The overall goal of this work is to give the Scientific Computing user community the capability to conduct transformative research via scalable, cost-effective, high-performance, general-purpose systems built from off-the-shelf components.&nbsp;</p>\n<p>The problem underlying this work is that the computer industry has reached a technological inflection point where we can no longer rely on improvements in the manufacturing process, as we have for the last 50 years, to advance computational capability.&nbsp; Three fundamental issues now limiting performance are computational efficiency, power density, and communication latency.&nbsp; All of these issues can benefit through increased heterogeneity, but the last in particular by integrating communication into the computational accelerators.&nbsp; At the time this project began, these types of systems were just beginning to be explored, and then only in extreme niche areas such as low-latency trading.</p>\n<p>The particular objective was to build a compute cluster, the Novo-G#, and related hardware and software infrastructure, that would facilitate research that advances such computer systems and so facilitate their broad application in scientific computing.&nbsp; The unifying technical mechanism to be explored was the integration of communication and computation in accelerator-centric clusters with direct and programmable interconnects.&nbsp; Direct links enable the bypassing of CPU, network interface, and even device memory.&nbsp; Programmable communication enables data transfers to proceed with high efficiency even under substantial loads.&nbsp; &nbsp;Integrating the two enables computations to be offloaded into the communication fabric, increasing compute capacity, helping to balance the workload, and substantially decreasing the traffic in the network.</p>\n<p>The technical broader impact has been to drive the movement of Reconfigurable Computing (RC) into large scalable systems, especially in their role as the devices where communication and computation are co-located.&nbsp; Lessons learned from the development and construction of the Novo-G# system have greatly influenced our understanding and will contribute to the architecture and design of high-density, high-connectivity clusters of the future. Analysis of the Novo-G# simulation model has helped to identify and mitigate potential bottlenecks in the design and performance of Novo-G# and other scalable RC systems. Performance measurements through hardware experiments and predictions through simulation have demonstrated the viability of FPGA-centric interconnected clusters for HPC.</p>\n<p>Since this project began, RC used in this manner has become a crucial part of cloud infrastructure with millions of such nodes deployed. Work in RC in the cloud has extended up the stack into operating systems and middleware with a commercial effort underway, based on this project, to provide such support. With HPC the penetration is nearly as exciting with RC projected to be a critical part of future generation High End Computing.</p>\n<p>The direct broader impacts include the following.&nbsp; At the primary institutions, more than a dozen students worked directly on the project and many more have worked as volunteers.&nbsp; These included several from under-represented groups, including two women PhD students and a female undergraduate who was inspired to pursue a PhD.&nbsp; Although the infrastructure was intended for use by the primary institutions, it was also used by at least 15 others across government, academia, and industry.</p>\n<p>The intellectual merit is first and foremost in the proposed technology itself, which has now been widely adopted.&nbsp; The actual construction of the infrastructure resulted in a number of findings, both related directly to communication and routing, but also to software tools.&nbsp; The test cases and other spin-offs, for example, in machine learning and molecular modeling, resulted in many more.&nbsp; Overall, more than 50 refereed publications and several theses and dissertations have so far come out of this project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/02/2018<br>\n\t\t\t\t\tModified by: Martin&nbsp;C&nbsp;Herbordt</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nTogether with theory and experiment, computer simulation constitutes the third pillar of scientific inquiry, enabling researchers to build and test models of complex phenomena that either cannot be, or would be prohibitively expensive to be, replicated in the laboratory.  Applications range from the practical, such as designing more efficient aircraft and effective drugs, to basic research in understanding the molecular basis of diseases such as Alzheimer?s.  Yet computing capability is currently only a small fraction of what is needed:  additional factors of millions are needed to simulate cells, and far more than that for larger structures.  The overall goal of this work is to give the Scientific Computing user community the capability to conduct transformative research via scalable, cost-effective, high-performance, general-purpose systems built from off-the-shelf components. \n\nThe problem underlying this work is that the computer industry has reached a technological inflection point where we can no longer rely on improvements in the manufacturing process, as we have for the last 50 years, to advance computational capability.  Three fundamental issues now limiting performance are computational efficiency, power density, and communication latency.  All of these issues can benefit through increased heterogeneity, but the last in particular by integrating communication into the computational accelerators.  At the time this project began, these types of systems were just beginning to be explored, and then only in extreme niche areas such as low-latency trading.\n\nThe particular objective was to build a compute cluster, the Novo-G#, and related hardware and software infrastructure, that would facilitate research that advances such computer systems and so facilitate their broad application in scientific computing.  The unifying technical mechanism to be explored was the integration of communication and computation in accelerator-centric clusters with direct and programmable interconnects.  Direct links enable the bypassing of CPU, network interface, and even device memory.  Programmable communication enables data transfers to proceed with high efficiency even under substantial loads.   Integrating the two enables computations to be offloaded into the communication fabric, increasing compute capacity, helping to balance the workload, and substantially decreasing the traffic in the network.\n\nThe technical broader impact has been to drive the movement of Reconfigurable Computing (RC) into large scalable systems, especially in their role as the devices where communication and computation are co-located.  Lessons learned from the development and construction of the Novo-G# system have greatly influenced our understanding and will contribute to the architecture and design of high-density, high-connectivity clusters of the future. Analysis of the Novo-G# simulation model has helped to identify and mitigate potential bottlenecks in the design and performance of Novo-G# and other scalable RC systems. Performance measurements through hardware experiments and predictions through simulation have demonstrated the viability of FPGA-centric interconnected clusters for HPC.\n\nSince this project began, RC used in this manner has become a crucial part of cloud infrastructure with millions of such nodes deployed. Work in RC in the cloud has extended up the stack into operating systems and middleware with a commercial effort underway, based on this project, to provide such support. With HPC the penetration is nearly as exciting with RC projected to be a critical part of future generation High End Computing.\n\nThe direct broader impacts include the following.  At the primary institutions, more than a dozen students worked directly on the project and many more have worked as volunteers.  These included several from under-represented groups, including two women PhD students and a female undergraduate who was inspired to pursue a PhD.  Although the infrastructure was intended for use by the primary institutions, it was also used by at least 15 others across government, academia, and industry.\n\nThe intellectual merit is first and foremost in the proposed technology itself, which has now been widely adopted.  The actual construction of the infrastructure resulted in a number of findings, both related directly to communication and routing, but also to software tools.  The test cases and other spin-offs, for example, in machine learning and molecular modeling, resulted in many more.  Overall, more than 50 refereed publications and several theses and dissertations have so far come out of this project.\n\n\t\t\t\t\tLast Modified: 11/02/2018\n\n\t\t\t\t\tSubmitted by: Martin C Herbordt"
 }
}