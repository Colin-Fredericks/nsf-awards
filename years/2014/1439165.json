{
 "awd_id": "1439165",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FULL: CCA: Collaborative Research: SPARTA: a Stream-based Processor And Run-Time Architecture",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Yuanyuan Yang",
 "awd_eff_date": "2014-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 271000.0,
 "awd_amount": 311558.0,
 "awd_min_amd_letter_date": "2014-07-24",
 "awd_max_amd_letter_date": "2017-07-24",
 "awd_abstract_narration": "Computer systems have undergone a fundamental transformation recently, from single&#8208;core processors to devices with increasingly higher core counts within a single chip. The semi&#8208;conductor industry now faces the infamous power and utilization walls, that is, physical constraints such as levels of power and energy consumption, but also reliability of the various components, must be taken into account not only during the chip fabrication process, but also when generating machine code and during program execution. To meet these challenges, heterogeneity in design, both at the architecture and technology levels, will be the prevailing approach for energy efficient computing as specialized cores, accelerators, and graphical processing units (GPUs) can eliminate the energy overheads of general&#8208;purpose homogeneous cores. However, with future technological challenges pointing in the direction of on&#8208;chip heterogeneity, and because of the traditional difficulty of parallel programming, it becomes imperative to produce new system software stacks that can take advantage of the heterogeneous hardware. \r\n\r\nThis project proposes to rethink the whole hardware&#8208;software interface, by researching novel ways to design many&#8208;core chip architectures and weaving heterogeneous components together and binding them by a fast and energy efficient on&#8208;chip interconnection network. On top of it will lay a system software layer to efficiently drive applications and map them onto the best suited components of the chip. Both the hardware and software layer are encompassed by a novel execution model, which describes how to orchestrate the various parts of a program in the most efficient way (be it with respect to power and energy, performance, or reliability). To achieve these goals, the development of a new model of computation called SPARTA (Stream-based Processor And RunTime Architecture) is proposed. The proposed model combines a new runtime and compiler technology with a hierarchical heterogeneous many&#8208;core chip and features hardware mechanisms for stream&#8208;based fine&#8208;grain program execution models to be reflected in different new software/hardware systems. Many issues are be envisioned, including programmability, scalability, performance evaluation, and power efficiency. Specifically, the goal is to identify the major challenges and obstacles toward an efficient exploitation of parallelism and scalability. To do so, traditional approaches will be re-evaluated by studying a collection of representative programs. A vertical design methodology is then proposed to effectively address the above challenges through the SPARTA approach and its implementation. In particular, the proposed cross-layer methodology consists of (a) a programming/execution model that will combine the Codelet model (leveraging our past research in dataflow models and extensions) with generalized streams: the Streaming Codelets, (b) an architecture model that will efficiently support the Streaming Codelets in heterogeneous hardware, and (c) a system software Stack that will be capable of effectively mapping Streaming Codelets to the proposed architecture. Finally, a qualitative and quantitative study of SPARTA will be performed via selected benchmarks and a consolidated methodology based on experimentation and analysis. The holistic cross-layer design methodology spanning the hardware/software stack and the reliability techniques developed from this research will significantly impact next generation multi&#8208;core and System&#8208;on&#8208;Chip (SoC) architectures with improvements in energy efficiency, programmability, performance and robustness.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jean-Luc",
   "pi_last_name": "Gaudiot",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jean-Luc Gaudiot",
   "pi_email_addr": "gaudiot@uci.edu",
   "nsf_id": "000465406",
   "pi_start_date": "2014-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "4424 Engineering Hall",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926973425",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 271000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 40558.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">This project was a collaborative between University of California Irvine (Gaudiot), University of Deleware (Gao), and the University of Arizona (Louri).&nbsp; In this project, we developed a new model of computation called SPARTA (Stream-based Processor And Run Time Architecture) to address the many programmability challenges.&nbsp; The proposed model combines a new runtime and compiler technologies with a hierarchical heterogeneous many-core chip and features hardware mechanisms for stream-based fine-grain program execution models to be reflected in different new software/hardware systems.&nbsp; Many issues were tackled, including programmability, scalability, performance evaluation, and power efficiency. This included major challenges and obstacles found toward an efficient exploitation of parallelism and scalability.&nbsp; To do so, we have reexamined traditional approaches by a study of a collection of representative programs. We have identified a vertical design methodology to effectively address these challenges through our SPARTA approach and its implementation.</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">The following represents the outcomes of our project:</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\"><strong>System software: Compiler and runtime system: </strong>our <em>omp2cd</em> compiler is at least competitive with regular OpenMP implementations: we have run both NWCHEM-SCF and Graph500 with OpenMP/GCC and OpenMP/Codelets. In the case of the SCF kernel, results show that we are on-par: speedups vary between 0.9x and 1.1x, depending on how many atoms we wish to simulate.</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\"><strong>Hilbert-Inspired Tile Scheduling for Matrix Multiplication:</strong> In the case of the Hilbert-inspired curves for tiled matrix multiplication on a manycore chip with inverted memory hierarchy (compared to a general-purpose multi-core processor; the hierarchy is similar to a GPU&rsquo;s), our simulation results show a significant reduction in energy expenditure compared to a classical hierarchical tiling scheme.</p>\n<p class=\"Default\"><strong>&nbsp;</strong></p>\n<p class=\"Default\"><strong>Fine-Grain Synchronization on General Purpose Many-Core Compute Nodes: </strong>In the case of the 2D stencil kernel we implemented for DARTS and OpenMP, we reached up to 1.75x compared to the regular coarse-grain synchronization version we implemented in OpenMP.</p>\n<p class=\"Default\">&nbsp;</p>\n<p><strong>Performance evaluation of multicore chips:&nbsp; </strong>We have extended Amdahl&rsquo;s law by considering the overhead of Data Preparation (ODP) for multicore systems, and demonstrated that the overhead of data preparation had become an unavoidable key parameter.&nbsp; Our analysis clearly shows that the higher parallelism gained from either computation or data preparation brings greater energy-efficiency.</p>\n<p>&nbsp;</p>\n<p><strong>Profile-based Dynamic Adaptive Workload Balance on Heterogeneous Architectures:</strong> we have extended homogeneous DARTS to support heterogeneous hardware resources (CPUs, GPUs).&nbsp; Also, we have developed a workload balance scheduler which combines an offline machine algorithm and online dynamic scheduling methodology. Experiment results show that our heterogeneous resources scheduler is either on-par or far outperforms whichever yields the best results (CPU or GPU).&nbsp;</p>\n<p>&nbsp;</p>\n<p class=\"Default\"><strong>CogNoC: cognitive NoCs Architecture: </strong>We have developed a CogNoc network that can recognize and adapt to changes in the environment, learn from experiences and retain knowledge gained for future usage.<strong> </strong></p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\"><strong>OWN: Optical Wireless Network-on-Chip (NoC) for Large-scale Heterogeneous Architectures:</strong> This proposed architecture combines the benefits of high-bandwidth, low-power, and low-latency on-chip silicon photonics with long range, low-latency flexible on-chip wireless technology to scale the NoC to 1024 cores. OWN provided much higher scalability as it provides a much higher sturation throughput when compared to other networks including wired mettalic interconnects, and photonic interconnects.</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\"><strong>LEAD: Learning-enabled Energy-Aware Dynamic Voltage/frequency scaling in NoCs: </strong>NoCs are the interconnect fabric of choice for multicore processors due to their superiority over traditional buses and crossbars in terms of scalability. While NoC&rsquo;s offer several advantages, they still suffer from high static and dynamic power consumption. In this paper, we propose LEAD - Learning-enabled Energy-Aware Dynamic voltage/frequency scaling for NoC architectures wherein we use machine learning techniques to enable energy-performance trade-offs at reduced overhead cost.<strong></strong></p>\n<p class=\"Default\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/18/2018<br>\n\t\t\t\t\tModified by: Jean-Luc&nbsp;Gaudiot</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "This project was a collaborative between University of California Irvine (Gaudiot), University of Deleware (Gao), and the University of Arizona (Louri).  In this project, we developed a new model of computation called SPARTA (Stream-based Processor And Run Time Architecture) to address the many programmability challenges.  The proposed model combines a new runtime and compiler technologies with a hierarchical heterogeneous many-core chip and features hardware mechanisms for stream-based fine-grain program execution models to be reflected in different new software/hardware systems.  Many issues were tackled, including programmability, scalability, performance evaluation, and power efficiency. This included major challenges and obstacles found toward an efficient exploitation of parallelism and scalability.  To do so, we have reexamined traditional approaches by a study of a collection of representative programs. We have identified a vertical design methodology to effectively address these challenges through our SPARTA approach and its implementation.\n \nThe following represents the outcomes of our project:\n \nSystem software: Compiler and runtime system: our omp2cd compiler is at least competitive with regular OpenMP implementations: we have run both NWCHEM-SCF and Graph500 with OpenMP/GCC and OpenMP/Codelets. In the case of the SCF kernel, results show that we are on-par: speedups vary between 0.9x and 1.1x, depending on how many atoms we wish to simulate.\n \nHilbert-Inspired Tile Scheduling for Matrix Multiplication: In the case of the Hilbert-inspired curves for tiled matrix multiplication on a manycore chip with inverted memory hierarchy (compared to a general-purpose multi-core processor; the hierarchy is similar to a GPU?s), our simulation results show a significant reduction in energy expenditure compared to a classical hierarchical tiling scheme.\n \nFine-Grain Synchronization on General Purpose Many-Core Compute Nodes: In the case of the 2D stencil kernel we implemented for DARTS and OpenMP, we reached up to 1.75x compared to the regular coarse-grain synchronization version we implemented in OpenMP.\n \n\nPerformance evaluation of multicore chips:  We have extended Amdahl?s law by considering the overhead of Data Preparation (ODP) for multicore systems, and demonstrated that the overhead of data preparation had become an unavoidable key parameter.  Our analysis clearly shows that the higher parallelism gained from either computation or data preparation brings greater energy-efficiency.\n\n \n\nProfile-based Dynamic Adaptive Workload Balance on Heterogeneous Architectures: we have extended homogeneous DARTS to support heterogeneous hardware resources (CPUs, GPUs).  Also, we have developed a workload balance scheduler which combines an offline machine algorithm and online dynamic scheduling methodology. Experiment results show that our heterogeneous resources scheduler is either on-par or far outperforms whichever yields the best results (CPU or GPU). \n\n \nCogNoC: cognitive NoCs Architecture: We have developed a CogNoc network that can recognize and adapt to changes in the environment, learn from experiences and retain knowledge gained for future usage. \n \nOWN: Optical Wireless Network-on-Chip (NoC) for Large-scale Heterogeneous Architectures: This proposed architecture combines the benefits of high-bandwidth, low-power, and low-latency on-chip silicon photonics with long range, low-latency flexible on-chip wireless technology to scale the NoC to 1024 cores. OWN provided much higher scalability as it provides a much higher sturation throughput when compared to other networks including wired mettalic interconnects, and photonic interconnects.\n \n \nLEAD: Learning-enabled Energy-Aware Dynamic Voltage/frequency scaling in NoCs: NoCs are the interconnect fabric of choice for multicore processors due to their superiority over traditional buses and crossbars in terms of scalability. While NoC?s offer several advantages, they still suffer from high static and dynamic power consumption. In this paper, we propose LEAD - Learning-enabled Energy-Aware Dynamic voltage/frequency scaling for NoC architectures wherein we use machine learning techniques to enable energy-performance trade-offs at reduced overhead cost.\n \n\n\t\t\t\t\tLast Modified: 09/18/2018\n\n\t\t\t\t\tSubmitted by: Jean-Luc Gaudiot"
 }
}