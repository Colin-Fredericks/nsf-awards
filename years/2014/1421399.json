{
 "awd_id": "1421399",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Optimal Allocation of Crowdsourced Resources for IR Evaluation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "James French",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 499677.0,
 "awd_amount": 499677.0,
 "awd_min_amd_letter_date": "2014-08-10",
 "awd_max_amd_letter_date": "2014-08-10",
 "awd_abstract_narration": "Evaluating the performance of information retrieval systems such as search engines is critical to their effective development. Current \"gold standard\" performance evaluation methodologies generally rely on the use of expert assessors to judge the quality of documents or web pages retrieved by search engines, at great cost in time and expense.  The advent of \"crowd sourcing,\" such as available through Amazon's Mechanical Turk service, holds out the promise that these performance evaluations can be performed more rapidly and at far less cost through the use of many (though generally less skilled) \"crowd workers\"; however, the quality of the resulting performance evaluations generally suffer greatly.  The thesis of this project is that one can obtain the best of both worlds -- performance evaluations with the quality of experts but at the cost of crowd workers -- by optimally leveraging both experts and crowd workers in asking the \"right\" assessor the \"right\" question at the \"right\" time.  For example, one might ask inexpensive crowd workers what are likely to be \"easy\" questions while reserving what are likely to be \"hard\" questions for the expensive experts.  While the project focuses on the performance evaluation of search engines as its use case, the techniques developed will be more broadly applicable to many domains where one wishes to efficiently and effectively harness experts and crowd workers with disparate levels of cost and expertise.\r\n\r\nTo enable the vision described above, a probabilistic framework will be developed within which one can quantify the uncertainty about a performance evaluation as well as the cost and expected utility of asking any assessor (expert or crowd worker) any question (e.g. a nominal judgment for a document or a preference judgment between two documents) at any time.  The goal is then to ask the \"right\" question of the \"right\" assessor at any time in order to maximize the expected utility gained per unit cost incurred and then to optimally aggregate such responses in order to efficiently and effectively evaluate performance.  For further information, see the project website at: http://www.ccs.neu.edu/home/jaa/IIS-1421399/.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Javed",
   "pi_last_name": "Aslam",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Javed A Aslam",
   "pi_email_addr": "jaa@ccs.neu.edu",
   "nsf_id": "000312612",
   "pi_start_date": "2014-08-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 499677.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Evaluating the performance of information retrieval systems such as search engines is critical to their effective development. Current \"gold standard\" performance evaluation methodologies generally rely on the use of expert assessors to judge the quality of documents or web pages retrieved by search engines, at great cost in time and expense. The advent of \"crowd sourcing,\" such as available through Amazon's Mechanical Turk service, holds out the promise that these performance evaluations can be performed more rapidly and at far less cost through the use of many (though generally less skilled) \"crowd workers\"; however, the quality of the resulting performance evaluations generally suffer greatly.&nbsp;</p>\n<p>The major goal of this project is to obtain the best of both worlds---performance evaluations with the quality of experts but at the cost of crowd workers---through the development of models, metrics, and analyses for performance evaluation via crowd-sourced relevance judgments. &nbsp;The specific goals include the development of models (1) for evaluating crowd worker quality, (2) for evaluating assessment task difficulty, (3) for combining crowd worker assessments, (4) for measuring system performance via these assessments, and (5) for exploring the utility of these techniques beyond evaluation.</p>\n<p>Our work included (1) the development of a probabilistic model for crowd worker quality that can be contextualized based on indicators of task difficulty, (2) the development of a Bayesian framework for soliciting and combining crowd worker assessments based on this crowd worker quality model, (3) the development of a unified model for aggregating crowdsourced relevance assessment and evaluating crowd worker quality with applications to performance evaluation and machine learning, and (4) the development of a machine learning algorithm for sentiment analysis trained and tested on crowdsourced data.</p>\n<p>As part of this project, we additionally developed collection construction, relevance assessment, and system evaluation techniques for the Temporal Summarization Track at TREC, a research venue for academia and industry sponsored and run by the US National Institute of Standards and Technology.</p>\n<p>Finally, this project provided thesis topics for three PhD students who published multiple research papers related to this project and who have gone on to pursue careers in academia and industrial research.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/23/2018<br>\n\t\t\t\t\tModified by: Javed&nbsp;A&nbsp;Aslam</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nEvaluating the performance of information retrieval systems such as search engines is critical to their effective development. Current \"gold standard\" performance evaluation methodologies generally rely on the use of expert assessors to judge the quality of documents or web pages retrieved by search engines, at great cost in time and expense. The advent of \"crowd sourcing,\" such as available through Amazon's Mechanical Turk service, holds out the promise that these performance evaluations can be performed more rapidly and at far less cost through the use of many (though generally less skilled) \"crowd workers\"; however, the quality of the resulting performance evaluations generally suffer greatly. \n\nThe major goal of this project is to obtain the best of both worlds---performance evaluations with the quality of experts but at the cost of crowd workers---through the development of models, metrics, and analyses for performance evaluation via crowd-sourced relevance judgments.  The specific goals include the development of models (1) for evaluating crowd worker quality, (2) for evaluating assessment task difficulty, (3) for combining crowd worker assessments, (4) for measuring system performance via these assessments, and (5) for exploring the utility of these techniques beyond evaluation.\n\nOur work included (1) the development of a probabilistic model for crowd worker quality that can be contextualized based on indicators of task difficulty, (2) the development of a Bayesian framework for soliciting and combining crowd worker assessments based on this crowd worker quality model, (3) the development of a unified model for aggregating crowdsourced relevance assessment and evaluating crowd worker quality with applications to performance evaluation and machine learning, and (4) the development of a machine learning algorithm for sentiment analysis trained and tested on crowdsourced data.\n\nAs part of this project, we additionally developed collection construction, relevance assessment, and system evaluation techniques for the Temporal Summarization Track at TREC, a research venue for academia and industry sponsored and run by the US National Institute of Standards and Technology.\n\nFinally, this project provided thesis topics for three PhD students who published multiple research papers related to this project and who have gone on to pursue careers in academia and industrial research.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/23/2018\n\n\t\t\t\t\tSubmitted by: Javed A Aslam"
 }
}