{
 "awd_id": "1460651",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Research in Student Peer Review: A Cooperative Web-Services Approach",
 "cfda_num": "47.076",
 "org_code": "11040200",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ron Buckmire",
 "awd_eff_date": "2014-09-13",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 201753.0,
 "awd_amount": 201753.0,
 "awd_min_amd_letter_date": "2014-09-04",
 "awd_max_amd_letter_date": "2014-09-04",
 "awd_abstract_narration": "Hundreds of thousands of students have used online peer review applications to review their classmates' work. While learning gains from peer review have been documented repeatedly, current systems do not always produce accurate scores and often give inadequate guidance to students about what constitutes a good review, resulting in haphazard feedback. This research addresses these issues with a common set of web services that can be used by any peer review system, as well as new visualizations that identify students' strengths and weaknesses, and gauge improvement over time.\r\n\r\nThis project differs from previous research that typically involves a single peer review system. It will develop a set of web services that will be usable by any peer review system in the same way that Google Maps is available to any website that wants to display location data. This common implementation will allow the project team to gather data from large numbers of students in a wide variety of contexts, thereby giving us the statistical power to produce more convincing, highly generalizable results\r\n\r\nPeer review during the writing process is an example of formative assessment, feedback that is received while the recipient still has a chance to improve his/her work. Several studies have found that the students who benefit most from formative assessment are those who typically underperform as measured by exams and standardized tests. Formative assessment tends to level the playing field for underrepresented minorities by allowing these students to receive input from their peers when they are not stressed about how their grade is being affected.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Ford",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Eric W Ford",
   "pi_email_addr": "ewford@uab.edu",
   "nsf_id": "000666667",
   "pi_start_date": "2014-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins Bloomberg School of Public Health",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212052113",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "153600",
   "pgm_ele_name": "S-STEM-Schlr Sci Tech Eng&Math"
  },
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8209",
   "pgm_ref_txt": "Improv Undergrad STEM Ed(IUSE)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0414",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001415DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "13XX",
   "app_name": "H-1B FUND, EHR, NSF",
   "app_symb_id": "045176",
   "fund_code": "1300XXXXDB",
   "fund_name": "H-1B FUND, EDU, NSF",
   "fund_symb_id": "045176"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 201753.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Peer-to-peer assessment is widely used at across education levels.  Students give feedback tp their classmates and recieve feedback in  return. The review and feedback processes produce a wealth of  information that can potentially be used to improve the assessment  process.</p>\n<p>There are several learning management systems that have  implemented peer-to-peer feedback as a pedagogical tool. Many of the  systems have been built by univeristies for specific purposes using a  general understanding. Thus far, each online peer- assessment system has  been an entity unto itself often using different approaches to the  tasks that need to be completed. For example, the rubrics or the  structuring of the assessment processes are common to all systems but  differ in their implementation. The PeerLogic project is an attempt to  change that. We are constructing a data warehouse of millions of peer  reviews, from at least half-a-dozen systems, that can be mined to  determine how differences in the assessment processes translate into  differences in peer assessments.</p>\n<p>We create the research framework  to create a taxonomy of online educational peer-assessment systems. The  framework enables researchers in technology-supported peer assessment to  the state-of-the-art of learning management technologies supporting  student peer review and assessment. The framework helps identify the  major themes in existing and potential research and formulate an agenda  for future studies. It also informs educators and system design practitioners about use cases and design options.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/05/2017<br>\n\t\t\t\t\tModified by: Eric&nbsp;Ford</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPeer-to-peer assessment is widely used at across education levels.  Students give feedback tp their classmates and recieve feedback in  return. The review and feedback processes produce a wealth of  information that can potentially be used to improve the assessment  process.\n\nThere are several learning management systems that have  implemented peer-to-peer feedback as a pedagogical tool. Many of the  systems have been built by univeristies for specific purposes using a  general understanding. Thus far, each online peer- assessment system has  been an entity unto itself often using different approaches to the  tasks that need to be completed. For example, the rubrics or the  structuring of the assessment processes are common to all systems but  differ in their implementation. The PeerLogic project is an attempt to  change that. We are constructing a data warehouse of millions of peer  reviews, from at least half-a-dozen systems, that can be mined to  determine how differences in the assessment processes translate into  differences in peer assessments.\n\nWe create the research framework  to create a taxonomy of online educational peer-assessment systems. The  framework enables researchers in technology-supported peer assessment to  the state-of-the-art of learning management technologies supporting  student peer review and assessment. The framework helps identify the  major themes in existing and potential research and formulate an agenda  for future studies. It also informs educators and system design practitioners about use cases and design options.\n\n\t\t\t\t\tLast Modified: 12/05/2017\n\n\t\t\t\t\tSubmitted by: Eric Ford"
 }
}