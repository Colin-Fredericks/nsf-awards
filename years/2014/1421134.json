{
 "awd_id": "1421134",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: MatCam: A Camera that Sees Materials",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2014-08-29",
 "awd_max_amd_letter_date": "2014-08-29",
 "awd_abstract_narration": "This project develops the first material camera, or MatCam, that outputs a per-pixel label of object material and its properties that can be used in visual computing tasks. In the everyday real world there are a vast number of materials that are useful to discern including concrete, metal, plastic, velvet, satin, water layer on asphalt, carpet, tile, wood, and marble.  A device for identifying materials has important implications in developing new technologies. For example, a mobile robot may use a MatCam to determine whether the terrain is grass, gravel, pavement, or snow in order to optimize mechanical control.  In e-commerce, the material composition of objects can be tagged by a MatCam for advertising and inventory.  The potential applications are limitless in areas such as robotics, digital architecture, human-computer interaction, intelligent vehicles and advanced manufacturing. Furthermore, material maps have foundational importance in nearly all vision algorithms including segmentation, feature matching, scene recognition, image-based rendering, context-based search, and object recognition and motion estimation. The camera brings material recognition to the broader scientific and engineering communities, in a similar way that depth cameras are currently used in many fields outside of computer vision. \r\n\r\nThis research brings high accuracy material estimation out of the lab and into the real-world for fast high-accuracy per-pixel material estimates.  The program has three technical aims.  First, a material appearance database is captured and stored with an exploration robot viewing surfaces from multiple angles.   This large, structured and actionable visual dataset is then used to develop computational appearance models.  A novel methodology using angular reflectance gradients is integrated for characterizing features of surface appearance.  Using the training data and statistical inference methods, these models are designed for hardware implementation. The final aim is the material camera implementation as a near real-time prototype of point-and-shoot material acquisition that extends RGB-D cameras to RGB-DM cameras that provide color, depth, and material.  The hardware implementation of the material appearance models utilizes FPGA and SoC (system-on-chip) technology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kristin",
   "pi_last_name": "Dana",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Kristin J Dana",
   "pi_email_addr": "kristin.dana@rutgers.edu",
   "nsf_id": "000148773",
   "pi_start_date": "2014-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "94 Brett Road, ECE",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Color and geometry are not a full measure of the richness of visual appearance. Material composition of a physical surface point determines the characteristics of light interaction and the reflection to an observer. In the everyday real world there are a vast number of materials that are useful to discern including concrete, metal, plastic, velvet, satin, asphalt, carpet, tile, skin, hair, wood, and marble. A computational method for identifying these materials has important implications in developing new algorithms and new technologies for a broad set of application domains. For example, a mobile robot or autonomous automobile can use material recognition to determine whether the terrain is asphalt, grass, gravel, ice or snow in order to optimize mechanical control. An indoor mobile robot can distinguish among wood, tile, or carpet for cleaning tasks. The material composition of objects can be tagged for an e-commerce inventory or for characterizing multi-composite 3D printed&nbsp;objects. The potential applications are limitless in areas such as robotics, digital architecture, human-computer interaction, intelligent vehicles, and advanced manufacturing. Furthermore, just as computer vision algorithms now use depth sensors directly from RGB-D cameras, material sensors can have foundational importance in nearly all vision algorithms including segmentation, feature matching, scene recognition, image-based rendering, context-based search, object recognition, and motion estimation.</p>\n<p>We have developed a suite of&nbsp; algorithms for material recognition as part of this project. These algorithms can be run in near real-time and therefore can be used as a material camera to see and recognize real world materials. Our work has been accepted to premier conferences (e.g. IEEE Computer Vision and Pattern Recognition) with low acceptance rate and a peer-review process that requires demonstrating that an algorithm works better than the state of the art.&nbsp;</p>\n<p>In order to support this algorithm development, we have collected several novel and large visual datasets with ground truth material that can be used not only for our algorithms but also to check and support future algorithms.&nbsp; These databases and our algorithm code have been made publicly available.&nbsp;</p>\n<p>The algorithm set that we have developed are as follows:</p>\n<p>1)&nbsp; Reflectance Disks and Reflectance Hashing: We present a database of reflectance images comprised of twenty different diverse material classes including wood, velvet, ceramic and automotive paint with 10 spot measurements per surface and with three different surface instances per class. Measurements include three on-axis illumination angles and ten random spot measurements over the surface. Each spot measurement is a reflectance disk composed of a dense sampling of viewing angles totaling thousands of reflectance angles per disk. The database of 3600 images or reflectance disks is made publicly available.&nbsp;For recognition, we combine binary hash coding and texton boosting for a new framework called reflectance hashing for efficient and accurate recognition of materials.&nbsp;&nbsp;</p>\n<p>2)&nbsp; Deep Texture Encoding Network:&nbsp; We develop a Deep Texture Encoding Network (Deep-TEN) with a novel computational component called the&nbsp; Encoding Layer integrated on top of convolutional layers, which ports the entire dictionary learning and encoding pipeline into a single model.&nbsp;</p>\n<p>3)&nbsp;&nbsp;Differential Angular Imaging for Material Recognition.&nbsp; We develop Differential Angular Imaging for a sparse spatial distribution of angular gradients that provides key cues for material recognition. We collect&nbsp; The GTOS (Ground Terrain for Outdoor Scenes) Dataset with ground terrain imaged by systematic in-scene measurement of partial reflectance instead of in-lab reflectance measurements. The database contains 34,243 images with 40 surface classes, 18 viewing directions, 4 illumination conditions, 3 exposure settings per sample and several instances/samples per class. We develop and evaluate an architecture for using differential angular imaging, showing superior results for differential inputs as compared to original images. Our work in measuring and modeling outdoor surfaces has important implications for applications such as robot navigation (determining control parameters based on current ground terrain) and automatic driving (determining road conditions by partial real time reflectance measurements).&nbsp;<br /><br /><br />4) Deep Texture Manifold for Ground Terrain Recognition: We develop a new recognition network called Deep Encoding Pooling Network (DEP) for the task of ground terrain recognition.&nbsp; This method uses the Encoding Layar of Deep-TEN but focuses on ground terrain material recognition. We also collect the GTOS-mobile database comprised of 81 ground terrains videos of similar terrain classes as GTOS, captured with a handheld mobile phone to evaluate knowledge-transfer between different image capture methods but within the the same domain.&nbsp;<br /><br /><br />5) Friction from Reflectance.&nbsp; We have presented the first-of-its kind friction-from-reflectance framework. This is a non-contact sensor that estimates friction from a distance using visual features.&nbsp; Friction-from-prediction will have good practical value in applications including automated assessment of road conditions prior to vehicular contact, planning robotic grasping of unknown objects,attribute tagging in scene analysis.&nbsp;<br /><br /></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/07/2018<br>\n\t\t\t\t\tModified by: Kristin&nbsp;J&nbsp;Dana</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nColor and geometry are not a full measure of the richness of visual appearance. Material composition of a physical surface point determines the characteristics of light interaction and the reflection to an observer. In the everyday real world there are a vast number of materials that are useful to discern including concrete, metal, plastic, velvet, satin, asphalt, carpet, tile, skin, hair, wood, and marble. A computational method for identifying these materials has important implications in developing new algorithms and new technologies for a broad set of application domains. For example, a mobile robot or autonomous automobile can use material recognition to determine whether the terrain is asphalt, grass, gravel, ice or snow in order to optimize mechanical control. An indoor mobile robot can distinguish among wood, tile, or carpet for cleaning tasks. The material composition of objects can be tagged for an e-commerce inventory or for characterizing multi-composite 3D printed objects. The potential applications are limitless in areas such as robotics, digital architecture, human-computer interaction, intelligent vehicles, and advanced manufacturing. Furthermore, just as computer vision algorithms now use depth sensors directly from RGB-D cameras, material sensors can have foundational importance in nearly all vision algorithms including segmentation, feature matching, scene recognition, image-based rendering, context-based search, object recognition, and motion estimation.\n\nWe have developed a suite of  algorithms for material recognition as part of this project. These algorithms can be run in near real-time and therefore can be used as a material camera to see and recognize real world materials. Our work has been accepted to premier conferences (e.g. IEEE Computer Vision and Pattern Recognition) with low acceptance rate and a peer-review process that requires demonstrating that an algorithm works better than the state of the art. \n\nIn order to support this algorithm development, we have collected several novel and large visual datasets with ground truth material that can be used not only for our algorithms but also to check and support future algorithms.  These databases and our algorithm code have been made publicly available. \n\nThe algorithm set that we have developed are as follows:\n\n1)  Reflectance Disks and Reflectance Hashing: We present a database of reflectance images comprised of twenty different diverse material classes including wood, velvet, ceramic and automotive paint with 10 spot measurements per surface and with three different surface instances per class. Measurements include three on-axis illumination angles and ten random spot measurements over the surface. Each spot measurement is a reflectance disk composed of a dense sampling of viewing angles totaling thousands of reflectance angles per disk. The database of 3600 images or reflectance disks is made publicly available. For recognition, we combine binary hash coding and texton boosting for a new framework called reflectance hashing for efficient and accurate recognition of materials.  \n\n2)  Deep Texture Encoding Network:  We develop a Deep Texture Encoding Network (Deep-TEN) with a novel computational component called the  Encoding Layer integrated on top of convolutional layers, which ports the entire dictionary learning and encoding pipeline into a single model. \n\n3)  Differential Angular Imaging for Material Recognition.  We develop Differential Angular Imaging for a sparse spatial distribution of angular gradients that provides key cues for material recognition. We collect  The GTOS (Ground Terrain for Outdoor Scenes) Dataset with ground terrain imaged by systematic in-scene measurement of partial reflectance instead of in-lab reflectance measurements. The database contains 34,243 images with 40 surface classes, 18 viewing directions, 4 illumination conditions, 3 exposure settings per sample and several instances/samples per class. We develop and evaluate an architecture for using differential angular imaging, showing superior results for differential inputs as compared to original images. Our work in measuring and modeling outdoor surfaces has important implications for applications such as robot navigation (determining control parameters based on current ground terrain) and automatic driving (determining road conditions by partial real time reflectance measurements). \n\n\n4) Deep Texture Manifold for Ground Terrain Recognition: We develop a new recognition network called Deep Encoding Pooling Network (DEP) for the task of ground terrain recognition.  This method uses the Encoding Layar of Deep-TEN but focuses on ground terrain material recognition. We also collect the GTOS-mobile database comprised of 81 ground terrains videos of similar terrain classes as GTOS, captured with a handheld mobile phone to evaluate knowledge-transfer between different image capture methods but within the the same domain. \n\n\n5) Friction from Reflectance.  We have presented the first-of-its kind friction-from-reflectance framework. This is a non-contact sensor that estimates friction from a distance using visual features.  Friction-from-prediction will have good practical value in applications including automated assessment of road conditions prior to vehicular contact, planning robotic grasping of unknown objects,attribute tagging in scene analysis. \n\n\n\n \n\n\t\t\t\t\tLast Modified: 12/07/2018\n\n\t\t\t\t\tSubmitted by: Kristin J Dana"
 }
}