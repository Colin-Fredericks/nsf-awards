{
 "awd_id": "1447861",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: DKM: Collaborative Research: Scalable Middleware for Managing and Processing Big Data on Next Generation HPC Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 359999.0,
 "awd_amount": 359999.0,
 "awd_min_amd_letter_date": "2014-08-25",
 "awd_max_amd_letter_date": "2014-08-25",
 "awd_abstract_narration": "Managing and processing large volumes of data and gaining meaningful insights is a significant challenge facing the Big Data community.  Thus, it is critical that data-intensive computing middleware (such as Hadoop, HBase and Spark) to process such data are diligently designed, with high performance and scalability, in order to meet the growing demands of such Big Data applications.  While Hadoop, Spark and HBase are gaining popularity for processing Big Data applications, these middleware and the associated Big Data applications are not able to take advantage of the advanced features on modern High Performance Computing (HPC) systems widely deployed all over the world, including many of of the multi-Petaflop systems in the XSEDE environment.  Modern HPC systems and the associated middleware (such as MPI and Parallel File systems) have been exploiting the advances in HPC technologies (multi/many-core architectures, RDMA-enabled networking, NVRAMs and SSDs) during the last decade. However, Big Data middleware (such as Hadoop, HBase and Spark) have not embraced such technologies. These disparities are taking HPC and Big Data processing into \"divergent trajectories.\" \r\n\r\nThe proposed research, undertaken by a team of computer and application scientists from OSU and SDSC, aim to bring HPC and Big Data processing into a \"convergent trajectory.\" The investigators will specifically address the following challenges: 1) designing novel communication and I/O runtime for Big Data processing while exploiting the features of modern multi-/many-core, networking and storage technologies; 2) redesigning Big Data middleware (such as Hadoop, HBase and Spark) to deliver performance and scalability on modern and next-generation HPC systems; and 3) demonstrating the benefits of the proposed approach for a set of driving Big Data applications on HPC system.  The proposed work targets four major workloads and applications in the Big Data community (namely data analytics, query, interactive, and iterative) using the popular Big Data middleware (Hadoop, HBase and Spark).  The proposed framework will be validated on a variety of Big Data benchmarks and applications.  The proposed middleware and runtimes will be made publicly available to the community.  The research enables curricular advancements via research in pedagogy for key courses in the new data analytics program at Ohio State and SDSC -- among the first of its kind nationwide.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amitava",
   "pi_last_name": "Majumdar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Amitava Majumdar",
   "pi_email_addr": "majumdar@sdsc.edu",
   "nsf_id": "000413199",
   "pi_start_date": "2014-08-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mahidhar",
   "pi_last_name": "Tatineni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mahidhar Tatineni",
   "pi_email_addr": "Mahidhar@sdsc.edu",
   "nsf_id": "000643983",
   "pi_start_date": "2014-08-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 359999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Advances in technology have enabled us to collect large amounts of<br /> data from all walks of life.&nbsp; Managing and processing such large<br /> volumes of data, or \"Big Data\", and gaining meaningful insights is a<br /> significant challenge facing the Big Data community. This has<br /> significant impact in a wide range of domains including health care,<br /> bio-medical research, Internet search, finance and business<br /> informatics, and scientific computing. As data-gathering technologies<br /> and data-sources witness an explosion in the amount of input data, it<br /> is expected that in the future massive quantities of data in the order<br /> of hundreds or thousands of petabytes will need to be processed. Thus,<br /> it is critical that data-intensive computing middleware (such as<br /> Hadoop, Spark, and HBase) to process such data are diligently<br /> designed, with high performance and scalability, in order to meet the<br /> growing demands of such Big Data applications.<br /> <br /> While Hadoop, Spark, and HBase are gaining popularity for processing<br /> Big Data applications, these middleware and the associated Big Data<br /> applications are not able to fully take advantage of the advanced<br /> features on modern High Performance Computing (HPC) systems widely<br /> deployed all over the world, including many of of the multi-Petaflop<br /> systems in the XSEDE environment. Modern HPC systems and the<br /> associated middleware (such as MPI and Parallel File systems) have<br /> been exploiting the advances in HPC technologies (multi/many-core<br /> architectures, RDMA-enabled networking, NVRAMs and SSDs) during the<br /> last decade.&nbsp; However, Big Data middleware (such as Hadoop, Spark, and<br /> HBase) have not embraced such technologies. These disparities are<br /> taking HPC and Big Data processing into \"divergent trajectories\".<br /> This leads to the following broad challenges: \"Can novel runtimes be<br /> used to redesign Big Data middleware (such as Hadoop, Spark, and<br /> HBase) to deliver performance and scalability on modern and<br /> next-generation HPC systems?\"<br /> <br /> In this project, we have proposed new runtimes and re-designed the Big<br /> Data middleware stacks to take advantage of modern HPC technologies<br /> and systems. Challenges have been addressed in the following three<br /> directions: 1) Designed novel communication and I/O runtime for Big<br /> Data processing while exploiting the features of modern<br /> multi-/many-core, networking and storage technologies; 2) Redesigned<br /> Big Data middleware (such as Hadoop, Spark, and HBase) to deliver<br /> performance and scalability on modern and next-generation HPC systems;<br /> and 3) Demonstrated the benefits of the proposed approach for a set of<br /> driving Big Data benchmarks and applications on HPC systems.&nbsp; The<br /> proposed designs have brought HPC and Big Data processing into a<br /> \"convergent trajectory\".<br /> <br /> Contributions were made in all three areas and evaluated with a range<br /> of Big Data benchmarks and applications including - TeraGen, Sort,<br /> TeraSort, TestDFSIO, GroupBy, PUMA, YCSB, CCIndex, HiBench,<br /> CloudBurst, Deep Learning, and Astronomy.&nbsp; Some highlights of these<br /> results are:<br /> <br /> * The new RDMA-HDFS design delivers a speedup of 2.3x for TeraGen of<br /> &nbsp; 200 GBytes of dataset<br /> <br /> * There is an improvement of 25% for 120 GB Sort using the new<br /> &nbsp; MapReduce over Luster<br /> <br /> * The RDMA-Spark design over 1536 cores improves the HiBench PageRank<br /> &nbsp; time by 43%.<br /> <br /> * An astronomy application using 65GB dataset is accelerated by 21%<br /> &nbsp;&nbsp;using the proposed RDMA-Spark design.<br /> <br /> * Up to 2.4x speedup for the YCSB workload A with RDMA-based design<br /> &nbsp; for HBase.<br /> <br /> The results of this research (new designs, performance results,<br /> benchmarks, etc.) have been made available to the community through<br /> RDMA-Hadoop, RDMA-Spark, and RDMA-HBase libraries and OSU HiBD<br /> (High-Performance Big Data) benchmark suite (multiple versions and<br /> libraries).&nbsp; The latest versions of these libraries are currently<br /> running on many large-scale InfiniBand and RoCE systems including SDSC<br /> Comet. Currently, the HiBD libraries are being used by more than 275<br /> organizations in 34 countries. The HiBD libraries and the associated<br /> enhancements are being used by a large number of users of these<br /> systems to accelerate Big Data applications.<br /> &nbsp; <br /> In each of these releases, information about the tuned designs for<br /> various components (HDFS, MapReduce, Spark, HBase, etc.) has been<br /> shared with the HiBD user community through mailing lists. The<br /> applications-based results have been made available to the community<br /> through the \"Performance\" link of the HiBD project web page.&nbsp; In<br /> addition to the software distribution, the results have been presented<br /> at various conferences and events through talks, tutorials, and<br /> hands-on sessions.&nbsp; Multiple Ph.D and Masters students have performed<br /> research work and received their Ph.D and M.S. degrees as a part of<br /> this project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/26/2017<br>\n\t\t\t\t\tModified by: Amitava&nbsp;Majumdar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAdvances in technology have enabled us to collect large amounts of\n data from all walks of life.  Managing and processing such large\n volumes of data, or \"Big Data\", and gaining meaningful insights is a\n significant challenge facing the Big Data community. This has\n significant impact in a wide range of domains including health care,\n bio-medical research, Internet search, finance and business\n informatics, and scientific computing. As data-gathering technologies\n and data-sources witness an explosion in the amount of input data, it\n is expected that in the future massive quantities of data in the order\n of hundreds or thousands of petabytes will need to be processed. Thus,\n it is critical that data-intensive computing middleware (such as\n Hadoop, Spark, and HBase) to process such data are diligently\n designed, with high performance and scalability, in order to meet the\n growing demands of such Big Data applications.\n \n While Hadoop, Spark, and HBase are gaining popularity for processing\n Big Data applications, these middleware and the associated Big Data\n applications are not able to fully take advantage of the advanced\n features on modern High Performance Computing (HPC) systems widely\n deployed all over the world, including many of of the multi-Petaflop\n systems in the XSEDE environment. Modern HPC systems and the\n associated middleware (such as MPI and Parallel File systems) have\n been exploiting the advances in HPC technologies (multi/many-core\n architectures, RDMA-enabled networking, NVRAMs and SSDs) during the\n last decade.  However, Big Data middleware (such as Hadoop, Spark, and\n HBase) have not embraced such technologies. These disparities are\n taking HPC and Big Data processing into \"divergent trajectories\".\n This leads to the following broad challenges: \"Can novel runtimes be\n used to redesign Big Data middleware (such as Hadoop, Spark, and\n HBase) to deliver performance and scalability on modern and\n next-generation HPC systems?\"\n \n In this project, we have proposed new runtimes and re-designed the Big\n Data middleware stacks to take advantage of modern HPC technologies\n and systems. Challenges have been addressed in the following three\n directions: 1) Designed novel communication and I/O runtime for Big\n Data processing while exploiting the features of modern\n multi-/many-core, networking and storage technologies; 2) Redesigned\n Big Data middleware (such as Hadoop, Spark, and HBase) to deliver\n performance and scalability on modern and next-generation HPC systems;\n and 3) Demonstrated the benefits of the proposed approach for a set of\n driving Big Data benchmarks and applications on HPC systems.  The\n proposed designs have brought HPC and Big Data processing into a\n \"convergent trajectory\".\n \n Contributions were made in all three areas and evaluated with a range\n of Big Data benchmarks and applications including - TeraGen, Sort,\n TeraSort, TestDFSIO, GroupBy, PUMA, YCSB, CCIndex, HiBench,\n CloudBurst, Deep Learning, and Astronomy.  Some highlights of these\n results are:\n \n * The new RDMA-HDFS design delivers a speedup of 2.3x for TeraGen of\n   200 GBytes of dataset\n \n * There is an improvement of 25% for 120 GB Sort using the new\n   MapReduce over Luster\n \n * The RDMA-Spark design over 1536 cores improves the HiBench PageRank\n   time by 43%.\n \n * An astronomy application using 65GB dataset is accelerated by 21%\n   using the proposed RDMA-Spark design.\n \n * Up to 2.4x speedup for the YCSB workload A with RDMA-based design\n   for HBase.\n \n The results of this research (new designs, performance results,\n benchmarks, etc.) have been made available to the community through\n RDMA-Hadoop, RDMA-Spark, and RDMA-HBase libraries and OSU HiBD\n (High-Performance Big Data) benchmark suite (multiple versions and\n libraries).  The latest versions of these libraries are currently\n running on many large-scale InfiniBand and RoCE systems including SDSC\n Comet. Currently, the HiBD libraries are being used by more than 275\n organizations in 34 countries. The HiBD libraries and the associated\n enhancements are being used by a large number of users of these\n systems to accelerate Big Data applications.\n   \n In each of these releases, information about the tuned designs for\n various components (HDFS, MapReduce, Spark, HBase, etc.) has been\n shared with the HiBD user community through mailing lists. The\n applications-based results have been made available to the community\n through the \"Performance\" link of the HiBD project web page.  In\n addition to the software distribution, the results have been presented\n at various conferences and events through talks, tutorials, and\n hands-on sessions.  Multiple Ph.D and Masters students have performed\n research work and received their Ph.D and M.S. degrees as a part of\n this project.\n\n\t\t\t\t\tLast Modified: 12/26/2017\n\n\t\t\t\t\tSubmitted by: Amitava Majumdar"
 }
}