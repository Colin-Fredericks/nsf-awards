{
 "awd_id": "1447771",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: F: DKM: Collaborative Research: PXFS: ParalleX Based Transformative I/O System for Big Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2014-09-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2014-08-22",
 "awd_max_amd_letter_date": "2014-08-22",
 "awd_abstract_narration": "Recent decades have seen the development of computational science where modeling and data analysis are critical to exploration, discovery, and refinement of new innovations in science and engineering. More recently the techniques have been applied to arts, social, political and other fields less traditionally reliant on high performance computing. This innovation has grown out of realization some 20 years ago that I/O (input/output) support for high performance parallel and distributed architectures had lagged behind that of pure computational speed, and further that bring I/O up to speed was both critical, and a rather difficult problem. The core hurdle of contemporary I/O on large HPC machines relates to issues of latency in large parts caused by the deficiencies of the historical I/O model that was relevant when computers were exclusively large, centralized, single processor systems shared by many time-sharing programs. In order to improve I/O on scalability on future hardware architectures novel approaches are required.\r\n\r\nThis project is conducting research on an extension of ParalleX, a new highly innovative parallel execution model. The extension provides a powerful I/O interface that allows researchers to create highly efficient data management, discovery, and analysis codes for Big Data applications.  This new extension, known as PXFS, is based on HPX, an implementation of ParalleX based on C++, and OrangeFS, a high performance parallel file system.  The research goal driving PXFS is to extend HPX objects into I/O space so that the objects become persistent and storage becomes another class of memory, all accessed as a single virtual address space and managed by an event driven dynamic adaptive computation environment.  Critical aspects of this approach include futures-based synchronization, dynamic locality management, dynamic resource management, hierarchical name space, and an active global address space (AGAS).  The overall goals of PXFS are to eliminate the division of programming imposed by conventional file system through the unification of name spaces and their management, and to minimize global synchronization in order to support asynchronous concurrency.  The research methodology is to implement a Map/Reduce application framework using PXFS and evaluate its effectiveness in both performance and ease of use.\r\n\r\nThis project is conducted at three major research universities involving undergraduate and graduate students, post-docs, and high-school teachers and their students.  The project includes a PI from the functional genomics field acting as domain science expert in order to focus the development efforts on real world problems. Graduate students and post-docs involved in the project are trained in these areas to promote scientists who understanding both aspects of Big Data problems.  The project engages under represented minorities with the goal to inspire them to pursue a career in computer science or genomics. The software developed by the project is available open-source and archived using an integrated source code revision repository, wiki, and bug tracking software system in addition to code releases with accompanying documentation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Walter",
   "pi_last_name": "Ligon",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Walter B Ligon",
   "pi_email_addr": "walt@clemson.edu",
   "nsf_id": "000359301",
   "pi_start_date": "2014-08-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Feltus",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Frank A Feltus",
   "pi_email_addr": "ffeltus@clemson.edu",
   "nsf_id": "000261582",
   "pi_start_date": "2014-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Clemson University",
  "inst_street_address": "201 SIKES HALL",
  "inst_street_address_2": "",
  "inst_city_name": "CLEMSON",
  "inst_state_code": "SC",
  "inst_state_name": "South Carolina",
  "inst_phone_num": "8646562424",
  "inst_zip_code": "296340001",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "SC03",
  "org_lgl_bus_name": "CLEMSON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "H2BMNX7DSKU8"
 },
 "perf_inst": {
  "perf_inst_name": "Clemson University",
  "perf_str_addr": "300 Brackett Hall, Box 345702",
  "perf_city_name": "Clemson",
  "perf_st_code": "SC",
  "perf_st_name": "South Carolina",
  "perf_zip_code": "296340001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "SC03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project has been to bring two HPC system software resources together to support Big Data computing.&nbsp; The two technologies are OrangeFS, a high performance parallel file system and HPX, a parallel programming runtime environment that provides for running computing tasks, passing data, synchronization, etc.&nbsp; The science driver was functional genomics which involves processing large volumes of genetic data across many species in order to isolate common genes and deduce their function.&nbsp; The jobs involve first downloading terrabyes of data and then preparing the files for processing in order to identify and the match genes.&nbsp; There is a significant amount of domain specific information in these files, and specialized code that has been developed and accepted by the genomics community over the years.&nbsp; Unlike many application areas in Big Data and HPC, it is not generally feasible to rewrite the programs for parallel execution because of this integrated domain-specific code.&nbsp; As a result we focused on a very coarse grain approach, which directed our system-level development is a different direction than expected.&nbsp; In particular we used the resources (HPX and OrangeFS) to develop a Big Data project manager that could run the domain-specific codes as tasks without altering the code, and extracted parallelism by running many instances of these codes while managing system resources such as memory and network bandwith.</p>\n<p>From this point, we sought to design a complete Big Data programming and runtime environment using HPX as a basis, integrating OrangeFS into HPX to handle IO streams, and building the task manager to support&nbsp; user code.&nbsp; The primary product produced by this project is the design of this infrastructure.&nbsp; This project was limited in time, thus we were only able to produce prototypes of some these features in order to explore their implementation and performance.</p>\n<p>In addition to this, a major issue is that most Big Data applicatin areas process data that is produced over a wide area - across a state, country, or the world.&nbsp; Genomics research is no different.&nbsp; Thus a major issue is transferring this data to a local site.&nbsp; This is a task that has been worked on for years in various forms, but our research showed that most such sofware packages do no perform very well when we consider large-scale transfer from one high-performance storage system, across a long distance, and into another sorage system.&nbsp; Thus part of our research involved working in this area learning to tune file-systems, networks, and a myriad of parameters to achieve as close to maximum theoretical throughput as possible.&nbsp; Thus another product of this project is the results of this study.</p>\n<p>The final report on this project contains the details of these developments, references to papers and other pertinent information.&nbsp; Similarly, the final reports submitted by our colaborators LSU and Indiana U further fill in the blanks of their portion of the project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/08/2018<br>\n\t\t\t\t\tModified by: Walter&nbsp;B&nbsp;Ligon</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project has been to bring two HPC system software resources together to support Big Data computing.  The two technologies are OrangeFS, a high performance parallel file system and HPX, a parallel programming runtime environment that provides for running computing tasks, passing data, synchronization, etc.  The science driver was functional genomics which involves processing large volumes of genetic data across many species in order to isolate common genes and deduce their function.  The jobs involve first downloading terrabyes of data and then preparing the files for processing in order to identify and the match genes.  There is a significant amount of domain specific information in these files, and specialized code that has been developed and accepted by the genomics community over the years.  Unlike many application areas in Big Data and HPC, it is not generally feasible to rewrite the programs for parallel execution because of this integrated domain-specific code.  As a result we focused on a very coarse grain approach, which directed our system-level development is a different direction than expected.  In particular we used the resources (HPX and OrangeFS) to develop a Big Data project manager that could run the domain-specific codes as tasks without altering the code, and extracted parallelism by running many instances of these codes while managing system resources such as memory and network bandwith.\n\nFrom this point, we sought to design a complete Big Data programming and runtime environment using HPX as a basis, integrating OrangeFS into HPX to handle IO streams, and building the task manager to support  user code.  The primary product produced by this project is the design of this infrastructure.  This project was limited in time, thus we were only able to produce prototypes of some these features in order to explore their implementation and performance.\n\nIn addition to this, a major issue is that most Big Data applicatin areas process data that is produced over a wide area - across a state, country, or the world.  Genomics research is no different.  Thus a major issue is transferring this data to a local site.  This is a task that has been worked on for years in various forms, but our research showed that most such sofware packages do no perform very well when we consider large-scale transfer from one high-performance storage system, across a long distance, and into another sorage system.  Thus part of our research involved working in this area learning to tune file-systems, networks, and a myriad of parameters to achieve as close to maximum theoretical throughput as possible.  Thus another product of this project is the results of this study.\n\nThe final report on this project contains the details of these developments, references to papers and other pertinent information.  Similarly, the final reports submitted by our colaborators LSU and Indiana U further fill in the blanks of their portion of the project.\n\n\t\t\t\t\tLast Modified: 01/08/2018\n\n\t\t\t\t\tSubmitted by: Walter B Ligon"
 }
}