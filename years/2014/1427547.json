{
 "awd_id": "1427547",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI: Collaborative Research: Modeling and Verification of Language-based Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Miller",
 "awd_eff_date": "2014-08-15",
 "awd_exp_date": "2019-07-31",
 "tot_intn_awd_amt": 525000.0,
 "awd_amount": 525000.0,
 "awd_min_amd_letter_date": "2014-08-18",
 "awd_max_amd_letter_date": "2015-03-10",
 "awd_abstract_narration": "Many autonomous systems today, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this  interaction with a model that guarantees robot performance.  This research brings together key elements that are just now reaching a sufficient level of maturity for integration: firstly, natural language processing and probabilistic modeling to capture human input, and secondly probabilistic synthesis and verification of the combined human-robot systems to ensure correct performance. The outcome will be theory and software to enable correct, effective and natural interactions between robots and humans to be realized. This research will impact most future autonomous systems which require interactions with humans, including service, personal and planetary robots. \r\n\r\nThe goal of this research is to develop models and algorithms for synthesizing and verifying an integrated human-plus-robot system based on natural language interaction. Algorithms are being developed for probabilistic modeling and inference of natural language, including the grounding of the constituents of the language into the physical world and the human's expectations. These models will enable the development of a distribution over specifications for control synthesis, which will in turn enable the development and verification of correct-by-construction controllers to a particular level of probability. The out years will consider interactive human-robot dialogue to resolve conflicts, and \"open world\" scenarios to enable on-line learning of new models over time. It is expected that this research will enable high reliability and performance in many autonomous systems because of the inherent interaction with humans.  Outcomes include open source data and software; community workshops; and undergraduate and graduate student education in the unique area of language, modeling and verification for robotics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nicholas",
   "pi_last_name": "Roy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nicholas Roy",
   "pi_email_addr": "nickroy@mit.edu",
   "nsf_id": "000226870",
   "pi_start_date": "2014-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 525000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many current autonomous systems, such as personal or service robots, are&nbsp;designed primarily to perform tasks independently and in&nbsp;isolation. Integrating these robots with human partners can often result in&nbsp;poor performance, as the robot does not know how to interpret human&nbsp;interaction, and cannot merge information from this interaction with a model&nbsp;that guarantees robot performance. This research project developed and&nbsp;validated models and algorithms for synthesizing and verifying an integrated&nbsp;human+robot system based on natural language interaction in conjunction with&nbsp;researchers at Cornell University and the University of Rochester.&nbsp;</p>\n<p>The following summarizes the specific outcomes of the project:</p>\n<p><strong>Verifiable models for natural language symbol grounding:</strong> We developed a new&nbsp;symbolic representation that encoded propositions, sensors, and actions for&nbsp;use with a verification framework. This model, called the Verifiable Distributed&nbsp;Correspondence Graph (V-DCG), addresses one of the principal research&nbsp;challenges of this proposal by demonstrating how we could extract&nbsp;specifications that could be verified for correctness from natural language in&nbsp;a collaborative human-robot interaction setting.</p>\n<p><strong>Abstract symbol grounding:</strong> We developed a new model of language&nbsp;grounding that introduced abstract symbols, and showed not only how abstract&nbsp;symbol grounding can be learned efficiently, but abstract symbols can make&nbsp;overall inference faster.&nbsp;</p>\n<p><strong>Declarative symbol grounding:</strong> We developed a novel mode of incorporating&nbsp;declarative information into our approach to symbol grounding, called the&nbsp;Temporal Grounding Graph (TGG). To support this inference, we developed a&nbsp;novel Markov logic network model that enables a robot to infer symbolic plans&nbsp;from natural language commands in scenarios where the workspace is partially&nbsp;observed or the robot's background knowledge is insufficient. This model uses&nbsp;background conceptual knowledge encompassing semantic attributes,&nbsp;relationships, affordances etc. can be used to infer \"missing\" plan&nbsp;constituents (such as action subjects or goal locations) based on statistical&nbsp;correlations and taxonomic affinities. The model incorporates a probabilistic&nbsp;relational model over symbolic knowledge learned from noisy textual&nbsp;descriptions and taxonomic data bases.</p>\n<p><strong>Data Collection:</strong> The project integrated natural language parsing and&nbsp;controller synthesis from logic formulas into a system (simulation and&nbsp;experiment) that was used for data collection of a corpus of natural language&nbsp;instructions, and validation experiments. Given natural language instructions&nbsp;and a world model, the end-to-end system synthesizes robot controllers that a&nbsp;physical robot (Rethink Robotics' Baxter) then executes.&nbsp;</p>\n<p><strong>Validation experiments via Baxter:</strong> This project designed and validated&nbsp;integrated algorithmic approaches for human-robot collaboration via the Baxter&nbsp;robot. The task is a Baxter robot collaboratively working with a human to take&nbsp;a mixture of blocks, arrange them in an order, and then build an object. Human&nbsp;natural language is used to define specifications and describe the scene. The&nbsp;robot perceives the environment and determines the logical values of the&nbsp;prepositions. Automatic controllers are generated for the robot. Run time&nbsp;evaluations are conducted to understand if/when assumptions are not met; if&nbsp;not met, then specifications are changed and controllers are&nbsp;re-generated. Feedback to the human includes an unrealizable specification,&nbsp;added assumptions about the environment that are automatically inferred (if&nbsp;any), and feedback regarding failure of action.</p>\n<p><strong>Transition and Training:</strong> In addition to the generation of algorithms and data,&nbsp;a total of 10 conference and journal papers were published. Training included&nbsp;funding and advising two graduate students and three postdocs on the&nbsp;project. All three postdocs are now faculty.&nbsp;&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/05/2020<br>\n\t\t\t\t\tModified by: Nicholas&nbsp;Roy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany current autonomous systems, such as personal or service robots, are designed primarily to perform tasks independently and in isolation. Integrating these robots with human partners can often result in poor performance, as the robot does not know how to interpret human interaction, and cannot merge information from this interaction with a model that guarantees robot performance. This research project developed and validated models and algorithms for synthesizing and verifying an integrated human+robot system based on natural language interaction in conjunction with researchers at Cornell University and the University of Rochester. \n\nThe following summarizes the specific outcomes of the project:\n\nVerifiable models for natural language symbol grounding: We developed a new symbolic representation that encoded propositions, sensors, and actions for use with a verification framework. This model, called the Verifiable Distributed Correspondence Graph (V-DCG), addresses one of the principal research challenges of this proposal by demonstrating how we could extract specifications that could be verified for correctness from natural language in a collaborative human-robot interaction setting.\n\nAbstract symbol grounding: We developed a new model of language grounding that introduced abstract symbols, and showed not only how abstract symbol grounding can be learned efficiently, but abstract symbols can make overall inference faster. \n\nDeclarative symbol grounding: We developed a novel mode of incorporating declarative information into our approach to symbol grounding, called the Temporal Grounding Graph (TGG). To support this inference, we developed a novel Markov logic network model that enables a robot to infer symbolic plans from natural language commands in scenarios where the workspace is partially observed or the robot's background knowledge is insufficient. This model uses background conceptual knowledge encompassing semantic attributes, relationships, affordances etc. can be used to infer \"missing\" plan constituents (such as action subjects or goal locations) based on statistical correlations and taxonomic affinities. The model incorporates a probabilistic relational model over symbolic knowledge learned from noisy textual descriptions and taxonomic data bases.\n\nData Collection: The project integrated natural language parsing and controller synthesis from logic formulas into a system (simulation and experiment) that was used for data collection of a corpus of natural language instructions, and validation experiments. Given natural language instructions and a world model, the end-to-end system synthesizes robot controllers that a physical robot (Rethink Robotics' Baxter) then executes. \n\nValidation experiments via Baxter: This project designed and validated integrated algorithmic approaches for human-robot collaboration via the Baxter robot. The task is a Baxter robot collaboratively working with a human to take a mixture of blocks, arrange them in an order, and then build an object. Human natural language is used to define specifications and describe the scene. The robot perceives the environment and determines the logical values of the prepositions. Automatic controllers are generated for the robot. Run time evaluations are conducted to understand if/when assumptions are not met; if not met, then specifications are changed and controllers are re-generated. Feedback to the human includes an unrealizable specification, added assumptions about the environment that are automatically inferred (if any), and feedback regarding failure of action.\n\nTransition and Training: In addition to the generation of algorithms and data, a total of 10 conference and journal papers were published. Training included funding and advising two graduate students and three postdocs on the project. All three postdocs are now faculty.  \n\n \n\n\t\t\t\t\tLast Modified: 08/05/2020\n\n\t\t\t\t\tSubmitted by: Nicholas Roy"
 }
}