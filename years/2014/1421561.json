{
 "awd_id": "1421561",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Lightning in Clouds: Detection and Characterization of Very Short Bottlenecks",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2014-10-01",
 "awd_exp_date": "2017-09-30",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2014-08-05",
 "awd_max_amd_letter_date": "2014-08-05",
 "awd_abstract_narration": "A plausible explanation for the persistent low utilization of data centers (around 18% by Gartner reports) is the managerial need to maintain quality of service against the well-known Latency Long Tail problem, where some apparently random requests that normally return within milliseconds would suddenly take multiple seconds. The latency long tail problem arises at moderate utilization levels (e.g., 50%) with all resources far from saturation. Despite the efforts to remedy the latency long tail problem in various ways, its causes have remained elusive: In most cases, the very requests that took several seconds actually return within milliseconds when executed by themselves. Studying and solving the latency long tail problem will contribute to better utilization while maintaining quality of service, leading to lower costs for cloud users, higher return on investment for cloud providers, and lower power consumption for the environment. The main goal of this project is the investigation of the class of very short bottlenecks, in which the CPU becomes saturated only for a small fraction of a second, as a significant cause of latency long tail problems. Despite their short lifespan, very short bottlenecks can lead to significant response time increases (several seconds) by propagating queuing effects up and down the request chain in an n-tier application system because of strong dependencies among the tiers during request processing. \r\n\r\nThis project runs large scale experiments in clouds and simulators to generate extensive fine-grain monitoring data in the investigation of very short bottlenecks, which are virtually invisible under typical performance monitoring tools with sampling periods of seconds or minutes. To match the time scale of very short bottlenecks, special instrumentation software tools are being refined to sample intra-server resource utilization at millisecond resolution and timestamp inter-server messages at microsecond resolution. Preliminary studies of n-tier application benchmarks with naturally bursty workloads have found very short bottlenecks that cause latency long tail in several system layers: systems software (JVM garbage collection), processor architecture (dynamic voltage and frequency scaling), and consolidation of applications in virtualized cloud environments. They show the potential for many other sources of very short bottlenecks, e.g., kernel daemon processes that use 100% of CPU for several milliseconds. Through careful distributed event analysis of the experimental data, new kinds of very short bottlenecks can be discovered, verified, reproduced, and studied in detail. Concrete solutions for specific very short bottlenecks have been developed, e.g., an improved Java garbage collector. However, other very short bottlenecks have no specific bug-fixes, e.g., those created by consolidated workload overlapping bursts of statistical nature. As an alternative to bug-fixes, more general solutions that disrupt queuing propagation are being explored. As a concrete example, instead of using a classic request/response approach, where waiting threads participate in the queuing propagation, asynchronous requests with notification of responses to reduce overall queuing is being investigated as a potential solution to eliminate or reduce the impact of several kinds of very short bottlenecks.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Calton",
   "pi_last_name": "Pu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Calton Pu",
   "pi_email_addr": "calton@cc.gatech.edu",
   "nsf_id": "000432066",
   "pi_start_date": "2014-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 North Avenue NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Web-facing applications (e.g., e-commerce) have stringent quality of service requirements (typical response time of milliseconds) due to intense competitive pressure. A study by Amazon reported that every increase of 100 milliseconds in page loading time is correlated to roughly 1% loss in sales. Similarly, Google found that a 500ms additional delay in returning search results could reduce revenues by up to 20%. However, the persistence of response time long tail problem over the last two decades, where a relatively small number of requests (a few percent) take a very long time (several seconds) to return, remains a serious research challenge despite its growing practical importance. Companies really want to reduce the response time long tails to the 99<sup>th</sup> and 99.9<sup>th</sup> percentiles, preferably achieving 100% near-zero-latency response time.</p>\n<p>This research project focused on the experimental results that led to the development of the Millibottleneck Theory to explain the very long response time (VLRT) requests that form the response time long tail problem. According to the Millibottleneck Theory, many VLRT requests are caused by millisecond-scale performance bugs called <em>millibottlenecks</em>. Informally stated, millibottlenecks (with duration of tens to hundreds of milliseconds) can cause ripple effects through a large distributed system due to dependencies among the components. Thus, initially modest queueing effects in one component can become amplified, resulting in dropped packets and VLRT requests. The millibottlenecks may happen with many kinds of resources (e.g., software such as Java garbage collector, or hardware such as dynamic voltage and frequency scaling &ndash; DVFS) and involve various kinds of dependencies (e.g., resource sharing through locks, or synchronous control flow using remote procedure calls &ndash; RPC). The simple, yet powerful, Millibottleneck Theory for millisecond-scale performance bugs is the first major outcome of the project.</p>\n<p>The second major outcome of the project is a set of experiments that validate the predictions of Millibottleneck Theory. These concrete results (made publicly available as part of the Alpha release of the MilliMonitor toolkit &ndash; see below) include: scripts that created the experiments, data produced by the experiments, and analysis of the experimental data in published papers. A concrete example consists of the noisy neighbor problem in a virtual machine (VM) consolidation environment: The millibottleneck is on CPU, the dependencies are due to RPC-style synchronous invocations among servers (Apache, Tomcat, and MySQL), and the observable results consist of reliably reproducible VLRT requests each time the co-located noisy neighbor VM generates a sufficiently high burst of requests.</p>\n<p>The attached figure illustrates the noisy neighbor VM scenario. The x-axis shows the timeline from the beginning of the experiment and all 5 parts of the figure (from (a) to (e)) refer to the same 20 seconds during the experiment. Part (a), top of the figure, shows presence of the response time long tail problem, with 5 peaks of requests taking a long time. These VLRT requests are shown in part (b). Around the same time of these groups of VLRT requests (about 2s, 5s, 9s, 15s, and 17s in all parts), the queues in the Apache and Tomcat servers are shown in part (c), and they explain the dropped packets that caused the VLRT requests. The queues are explained by CPU millibottlenecks in part (d), which in turn are caused by the generated workload bursts shown in part (e). Through this kind of experimental data, the VLRT requests are shown to be linked to the millibottlenecks from various sources.</p>\n<p>The third major outcome of the project is the Alpha release of the MilliMonitor toolkit, with three main components. The first component consists of fine-grain resource monitors that generate data to enable the finding of millibottlenecks similar to the ones shown in part (d) of the attached figure. The second component of MilliMonitor consists of detailed specialized event monitors that record the timestamps of arrival/departure times of each request and response. The detailed event data allow the finding of queues as shown in part (c) of the attached figure. The third component of MilliMonitor consists of the n-tier benchmark (RUBBoS) that creates the workload, including the bursts shown in part (e) and the job statistics shown in parts (a) and (b) of the attached figure.</p>\n<p>In summary, the project was successful in defining a simple and powerful model (the Millibottleneck Theory) to explain the appearance of VLRT requests and the difficulties in studying them due to the millisecond-scale of these performance bugs. Significant experimental data show the prevalence of the millibottlenecks (e.g., CPU, memory, and disks). Scientifically, the Alpha release of the MilliMonitor toolkit will allow the experimental computer science research community to study millibottlenecks and reproduce or disprove the findings from the project. Practically, the identification of millibottlenecks will enable a better understanding of large scale distributed systems and their prevention will enable a higher utilization level of data centers and computing clouds.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/22/2017<br>\n\t\t\t\t\tModified by: Calton&nbsp;Pu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1421561/1421561_10328199_1508730257593_outcome-VM-figure--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1421561/1421561_10328199_1508730257593_outcome-VM-figure--rgov-800width.jpg\" title=\"Millibottlenecks and Very Long Response Time Requests Found in Cloud Environments\"><img src=\"/por/images/Reports/POR/2017/1421561/1421561_10328199_1508730257593_outcome-VM-figure--rgov-66x44.jpg\" alt=\"Millibottlenecks and Very Long Response Time Requests Found in Cloud Environments\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Experimental confirmation of millibottlenecks causing Very Long Response Time Requests in n-tier applications running in cloud environments: the case of noisy neighbor in consolidated virtual machine environments.</div>\n<div class=\"imageCredit\">Georgia Institute of Technology</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Calton&nbsp;Pu</div>\n<div class=\"imageTitle\">Millibottlenecks and Very Long Response Time Requests Found in Cloud Environments</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nWeb-facing applications (e.g., e-commerce) have stringent quality of service requirements (typical response time of milliseconds) due to intense competitive pressure. A study by Amazon reported that every increase of 100 milliseconds in page loading time is correlated to roughly 1% loss in sales. Similarly, Google found that a 500ms additional delay in returning search results could reduce revenues by up to 20%. However, the persistence of response time long tail problem over the last two decades, where a relatively small number of requests (a few percent) take a very long time (several seconds) to return, remains a serious research challenge despite its growing practical importance. Companies really want to reduce the response time long tails to the 99th and 99.9th percentiles, preferably achieving 100% near-zero-latency response time.\n\nThis research project focused on the experimental results that led to the development of the Millibottleneck Theory to explain the very long response time (VLRT) requests that form the response time long tail problem. According to the Millibottleneck Theory, many VLRT requests are caused by millisecond-scale performance bugs called millibottlenecks. Informally stated, millibottlenecks (with duration of tens to hundreds of milliseconds) can cause ripple effects through a large distributed system due to dependencies among the components. Thus, initially modest queueing effects in one component can become amplified, resulting in dropped packets and VLRT requests. The millibottlenecks may happen with many kinds of resources (e.g., software such as Java garbage collector, or hardware such as dynamic voltage and frequency scaling &ndash; DVFS) and involve various kinds of dependencies (e.g., resource sharing through locks, or synchronous control flow using remote procedure calls &ndash; RPC). The simple, yet powerful, Millibottleneck Theory for millisecond-scale performance bugs is the first major outcome of the project.\n\nThe second major outcome of the project is a set of experiments that validate the predictions of Millibottleneck Theory. These concrete results (made publicly available as part of the Alpha release of the MilliMonitor toolkit &ndash; see below) include: scripts that created the experiments, data produced by the experiments, and analysis of the experimental data in published papers. A concrete example consists of the noisy neighbor problem in a virtual machine (VM) consolidation environment: The millibottleneck is on CPU, the dependencies are due to RPC-style synchronous invocations among servers (Apache, Tomcat, and MySQL), and the observable results consist of reliably reproducible VLRT requests each time the co-located noisy neighbor VM generates a sufficiently high burst of requests.\n\nThe attached figure illustrates the noisy neighbor VM scenario. The x-axis shows the timeline from the beginning of the experiment and all 5 parts of the figure (from (a) to (e)) refer to the same 20 seconds during the experiment. Part (a), top of the figure, shows presence of the response time long tail problem, with 5 peaks of requests taking a long time. These VLRT requests are shown in part (b). Around the same time of these groups of VLRT requests (about 2s, 5s, 9s, 15s, and 17s in all parts), the queues in the Apache and Tomcat servers are shown in part (c), and they explain the dropped packets that caused the VLRT requests. The queues are explained by CPU millibottlenecks in part (d), which in turn are caused by the generated workload bursts shown in part (e). Through this kind of experimental data, the VLRT requests are shown to be linked to the millibottlenecks from various sources.\n\nThe third major outcome of the project is the Alpha release of the MilliMonitor toolkit, with three main components. The first component consists of fine-grain resource monitors that generate data to enable the finding of millibottlenecks similar to the ones shown in part (d) of the attached figure. The second component of MilliMonitor consists of detailed specialized event monitors that record the timestamps of arrival/departure times of each request and response. The detailed event data allow the finding of queues as shown in part (c) of the attached figure. The third component of MilliMonitor consists of the n-tier benchmark (RUBBoS) that creates the workload, including the bursts shown in part (e) and the job statistics shown in parts (a) and (b) of the attached figure.\n\nIn summary, the project was successful in defining a simple and powerful model (the Millibottleneck Theory) to explain the appearance of VLRT requests and the difficulties in studying them due to the millisecond-scale of these performance bugs. Significant experimental data show the prevalence of the millibottlenecks (e.g., CPU, memory, and disks). Scientifically, the Alpha release of the MilliMonitor toolkit will allow the experimental computer science research community to study millibottlenecks and reproduce or disprove the findings from the project. Practically, the identification of millibottlenecks will enable a better understanding of large scale distributed systems and their prevention will enable a higher utilization level of data centers and computing clouds.\n\n\t\t\t\t\tLast Modified: 10/22/2017\n\n\t\t\t\t\tSubmitted by: Calton Pu"
 }
}