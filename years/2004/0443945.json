{
 "awd_id": "0443945",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:   I/UCRC:    Safety Security Rescue Research Center (SSR-RC)",
 "cfda_num": "47.041",
 "org_code": "07050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Lawrence A. Hornak",
 "awd_eff_date": "2004-08-15",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 532750.0,
 "awd_min_amd_letter_date": "2004-08-12",
 "awd_max_amd_letter_date": "2013-06-12",
 "awd_abstract_narration": "This multi-university Industry/University Cooperative Research Center for Safety, Security and Rescue Research located at the University of South Florida and the University of Minnesota will bring together industry, academe, and public sector users together to provide integrative robotics and artificial intelligence solutions in robotics for activities conducted by the police, FBI, FEMA, firefighting, transportation safety, and emergency response to mass casuality-related activities.  The need for safety, security, and rescue technologies has accelerated in the aftermath of 9/11 and a new research community is forming, as witnessed by the first IEEE Workshop on Safety, Security and Rescue Robotics in February 2003.  \r\n\r\nThe Center will be built upon the knowledge and expertise of multi-disciplinary researchers in computer science, engineering, industrial organization, psychology, public health, and marine sciences at the University of South Florida (the lead institution) and the University of Minnesota.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "EEC",
 "org_div_long_name": "Division of Engineering Education and Centers",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nikolaos",
   "pi_last_name": "Papanikolopoulos",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Nikolaos P Papanikolopoulos",
   "pi_email_addr": "npapas@cs.umn.edu",
   "nsf_id": "000205563",
   "pi_start_date": "2013-06-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Voyles",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Richard M Voyles",
   "pi_email_addr": "rvoyles@purdue.edu",
   "nsf_id": "000293618",
   "pi_start_date": "2004-08-12",
   "pi_end_date": "2007-07-20"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Maria",
   "pi_last_name": "Gini",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Maria L Gini",
   "pi_email_addr": "gini@cs.umn.edu",
   "nsf_id": "000468121",
   "pi_start_date": "2004-08-12",
   "pi_end_date": "2007-07-31"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Nikolaos",
   "pi_last_name": "Papanikolopoulos",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Nikolaos P Papanikolopoulos",
   "pi_email_addr": "npapas@cs.umn.edu",
   "nsf_id": "000205563",
   "pi_start_date": "2004-08-12",
   "pi_end_date": "2007-07-20"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Stergios",
   "pi_last_name": "Roumeliotis",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Stergios I Roumeliotis",
   "pi_email_addr": "stergios@cs.umn.edu",
   "nsf_id": "000487045",
   "pi_start_date": "2004-08-12",
   "pi_end_date": "2013-06-12"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tasoulla",
   "pi_last_name": "Hadjiyanni",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tasoulla Hadjiyanni",
   "pi_email_addr": "thadjiya@umn.edu",
   "nsf_id": "000282991",
   "pi_start_date": "2008-10-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "2221 UNIVERSITY AVE SE STE 100",
  "perf_city_name": "MINNEAPOLIS",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554143074",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "576100",
   "pgm_ele_name": "IUCRC-Indust-Univ Coop Res Ctr"
  },
  {
   "pgm_ele_code": "289000",
   "pgm_ele_name": "CISE Research Resources"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1049",
   "pgm_ref_txt": "INDUSTRY-UNIV COOPERATIVE RSCH PROJECTS"
  },
  {
   "pgm_ref_code": "115E",
   "pgm_ref_txt": "RESEARCH EXP FOR TEACHERS"
  },
  {
   "pgm_ref_code": "116E",
   "pgm_ref_txt": "RESEARCH EXP FOR UNDERGRADS"
  },
  {
   "pgm_ref_code": "122E",
   "pgm_ref_txt": "CENTERS: OTHER INDUSTRY-UNIV"
  },
  {
   "pgm_ref_code": "5761",
   "pgm_ref_txt": "INDUSTRY/UNIV COOP RES CENTERS"
  },
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "8039",
   "pgm_ref_txt": "Information, Communication & Computing"
  },
  {
   "pgm_ref_code": "9177",
   "pgm_ref_txt": "ELEMENTARY/SECONDARY EDUCATION"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0104",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0105",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "0100999999",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2004,
   "fund_oblg_amt": 50000.0
  },
  {
   "fund_oblg_fiscal_yr": 2005,
   "fund_oblg_amt": 50000.0
  },
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 50000.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 60000.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 145000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 169750.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>One of the projects at the Center studied the contents and use of shared mental models for mixed teams of humans, robots, and agents in the context of urban search and rescue. The study was supported by an agent based simulator that was used for modeling complex systems with hundreds of agents in a large city and to study their behaviors under different circumstances. Agents were modeled as autonomous entities that interacted with other agents while moving around the city to search for victims and to stop fires from spreading. Humans&nbsp; interacted with the agents in making decisions and carrying them out. We&nbsp; focused on the use of shared team mental models in those mixed human-robot-agent teams, in situations where information was uncertain, and communication was limited and unreliable. Specific outcomes were:<br />1. development of &nbsp;shared mental models for mixed teams of humans, robots, and agents;</p>\n<p>2. study of the effects of different levels of shared mental models on team performance;</p>\n<p>3. measurement of &nbsp;the performance of the shared team mental models in complex urban search and rescue environments with uncertain knowledge and limited communication.</p>\n<p>&nbsp;</p>\n<p>Another outcome focused on incremental learning of people appearances for tracking across cameras. Modern surveillance systems have come a long way in the last decade having being transformed into automata providing security warnings about activities recorded in video streams. However, it is unrealistic to believe that these systems can be fully automatic given the unacceptable high rates of false alarms under which they operate. It is also often the case that security warnings generated by the various cameras in a surveillance network are viewed in isolation. Therefore, interpretation of activities resulting from analysis of video sequences characterized by limited temporal and spatial scope is an impossible task not only for computer/camera systems but for humans as well. This work put forth a new paradigm of video interpretation and information linking amongst multiple cameras. It assumed availability of information extracted by low level processing video streams, it continued with supervised learning for data organization and management across time and space and it finally ended with visualization of activity pertinent to human initiated queries. Specifically, we introduced a technique for dynamic clustering of people appearances by taking advantage of availability of data extracted from low level vision processes such as tracking and labeled data provided by the user who escalates security hypotheses. We also used supervised clustering to explore the manifold of people appearances in an incremental (or dynamic) fashion.<br /><br />Another outcome was about dictionary learning for robust background subtraction.&nbsp; Background subtraction is a&nbsp;fundamental task in many computer vision applications, such as robotics and automated surveillance systems. The performance of high-level vision tasks such as object detection and tracking is dependent on effective foreground detection techniques. In this work, we have developed a novel background modeling algorithm that represents the background as a linear combination of dictionary atoms and the foreground as a sparse error, when one uses the respective set of dictionary atoms as basis elements to linearly approximate/ reconstruct a new image. The&nbsp;dictionary atoms represent variations of the background model and they are learned from a batch of randomly sampled training frames. The sparse foreground is estimated during both training and performance phases, and this is formulated as a Lasso problem, while the dictionary update step in the training phase is motivated from the K-SVD algorithm. Our method works well in the presence of foreground in the training frames, and also gives the foreground masks for the ...",
  "por_txt_cntn": "\nOne of the projects at the Center studied the contents and use of shared mental models for mixed teams of humans, robots, and agents in the context of urban search and rescue. The study was supported by an agent based simulator that was used for modeling complex systems with hundreds of agents in a large city and to study their behaviors under different circumstances. Agents were modeled as autonomous entities that interacted with other agents while moving around the city to search for victims and to stop fires from spreading. Humans  interacted with the agents in making decisions and carrying them out. We  focused on the use of shared team mental models in those mixed human-robot-agent teams, in situations where information was uncertain, and communication was limited and unreliable. Specific outcomes were:\n1. development of  shared mental models for mixed teams of humans, robots, and agents;\n\n2. study of the effects of different levels of shared mental models on team performance;\n\n3. measurement of  the performance of the shared team mental models in complex urban search and rescue environments with uncertain knowledge and limited communication.\n\n \n\nAnother outcome focused on incremental learning of people appearances for tracking across cameras. Modern surveillance systems have come a long way in the last decade having being transformed into automata providing security warnings about activities recorded in video streams. However, it is unrealistic to believe that these systems can be fully automatic given the unacceptable high rates of false alarms under which they operate. It is also often the case that security warnings generated by the various cameras in a surveillance network are viewed in isolation. Therefore, interpretation of activities resulting from analysis of video sequences characterized by limited temporal and spatial scope is an impossible task not only for computer/camera systems but for humans as well. This work put forth a new paradigm of video interpretation and information linking amongst multiple cameras. It assumed availability of information extracted by low level processing video streams, it continued with supervised learning for data organization and management across time and space and it finally ended with visualization of activity pertinent to human initiated queries. Specifically, we introduced a technique for dynamic clustering of people appearances by taking advantage of availability of data extracted from low level vision processes such as tracking and labeled data provided by the user who escalates security hypotheses. We also used supervised clustering to explore the manifold of people appearances in an incremental (or dynamic) fashion.\n\nAnother outcome was about dictionary learning for robust background subtraction.  Background subtraction is a fundamental task in many computer vision applications, such as robotics and automated surveillance systems. The performance of high-level vision tasks such as object detection and tracking is dependent on effective foreground detection techniques. In this work, we have developed a novel background modeling algorithm that represents the background as a linear combination of dictionary atoms and the foreground as a sparse error, when one uses the respective set of dictionary atoms as basis elements to linearly approximate/ reconstruct a new image. The dictionary atoms represent variations of the background model and they are learned from a batch of randomly sampled training frames. The sparse foreground is estimated during both training and performance phases, and this is formulated as a Lasso problem, while the dictionary update step in the training phase is motivated from the K-SVD algorithm. Our method works well in the presence of foreground in the training frames, and also gives the foreground masks for the training frames as a by-product of the batch training phase. Experimental validation was provided on standard datasets with ground truth info..."
 }
}