{
 "awd_id": "9602082",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Learning to Cooperate in Repeated Games",
 "cfda_num": "47.075",
 "org_code": "04050100",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Daniel H. Newlon",
 "awd_eff_date": "1996-08-01",
 "awd_exp_date": "1998-11-09",
 "tot_intn_awd_amt": 182019.0,
 "awd_amount": 182019.0,
 "awd_min_amd_letter_date": "1996-08-08",
 "awd_max_amd_letter_date": "1998-06-24",
 "awd_abstract_narration": "A primary reason for studying repeated games is to understand how selfish players can coordinate their actions to achieve improvements without a collusive agreement.  Unfortunately existing game-theoretic models admit so many outcomes that it is impossible to predict whether coordination will emerge.  Also analysis postulates a rational agent who has unbounded computational capability and perfect foresight.  These assumptions are critical for equilibrium models but also rather unrealistic.  This project explores alternative models in which perfectly rational agents are replaced by `boundedly rational` agents who have only limited computational capabilities, and who cannot perfectly foresee the strategy of other players, which they have to learn from the past experiences.  This approach is shown to capture learning dynamics and to permit applications to a wide class of repeated and dynamic games which have a `big` player who can influence the long run outcome of the model.   Applications include international debt   and optimal growth with moral hazard.  More specifically, the project examines two person repeated games where each player learns the opponent's strategy according to the gradient method by assuming that the opponent is playing according to a linear strategy.  In addition, each player artificially adds random noise that disappears slowly in order to experiment against the opponent's strategy.  No restrictions are imposed on feasible strategies, but the forecast of each player must be a linear function of past observations.  The reason for selecting this particular class of strategies is that these strategies are simple enough to be parameterized easily.  Then, each player can learn the opponent's strategy using least squares estimation.  The agent's preference is also modified slightly so that he is selecting a best response while minimizing the complexity of the decision making process.  then, a recursive least squares learning model is obtained, where each player updates his belief as well as his operated game strategy as the game proceeds.  The learning dynamics converges with probability 1 and in the limit, both players have an identical estimator.  Consequently the behavior of the two players is highly correlated, and the limit frequency of outcomes can be sustained by some Nash equilibrium in linear strategies.  In the prisoner's dilemma game, for example, the limit frequency of outcomes must be a strict convex combination of cooperation and defection, which implies that the players must learn to cooperative with positive probability.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "In-Koo",
   "pi_last_name": "Cho",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "In-Koo Cho",
   "pi_email_addr": "in-koo.cho@emory.edu",
   "nsf_id": "000198663",
   "pi_start_date": "1996-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "1 PROSPECT ST",
  "perf_city_name": "PROVIDENCE",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132000",
   "pgm_ele_name": "Economics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0196",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0196",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0197",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0197",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0198",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0198",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 1996,
   "fund_oblg_amt": 60000.0
  },
  {
   "fund_oblg_fiscal_yr": 1997,
   "fund_oblg_amt": 59562.0
  },
  {
   "fund_oblg_fiscal_yr": 1998,
   "fund_oblg_amt": 17462.0
  }
 ],
 "por": null
}