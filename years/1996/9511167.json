{
 "awd_id": "9511167",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Computational Studies of Parameter Setting in Language",
 "cfda_num": "47.075",
 "org_code": "04040500",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Catherine N. Ball",
 "awd_eff_date": "1995-11-01",
 "awd_exp_date": "1999-10-31",
 "tot_intn_awd_amt": 295000.0,
 "awd_amount": 295000.0,
 "awd_min_amd_letter_date": "1995-11-02",
 "awd_max_amd_letter_date": "1997-11-26",
 "awd_abstract_narration": "The problem of developing an appropriate and workable  computational theory of parameter-setting in language is a  central problem for linguistic theory, language acquisition,  learning theory and cognitive science in general. The purpose of  this grant is to make a significant increase, compared to  previous studies, in the size and scope of the syntactic  parameter spaces, and to do computational parameter-setting  investigations of these parameter spaces. We plan to investigate  the learning of a space of 256 grammars or larger, based on 8  parameters (plus a \"lexical\" parameter).   The major questions that we will investigate include:  1. With a larger set of parameters, does the theory of  parameter-setting work (i.e., for all natural languages stated in  terms of those parameters, does the algorithm converge)?  2. Do different algorithms work better than others?   3. What are the kinds of special assumptions about markedness or  default values that work in the specific parameter spaces that we  study? Are there general principles of markedness or default  values that seem to apply to many different parameters? Are  default values necessary in general?  4. What happens to computational results in parameter-setting  when we \"scale up\" ? That is, if a certain pattern of results  obtains for a parameter-space, do we find that this pattern  remains when the space is embedded in a larger space? Or were the  results artifacts of the smaller space?  5. Suppose that an algorithm converges on the correct  parameter-settings for a class of parameter-settings that we know  is instantiated (i.e., there are natural languages which exhibit  these parameter-settings), but doesn't converge for some other  parameter-settings. Can we show that these parameter-settings are  not instantiated in natural language, so that in fact there are  reasons of learnability that some parameter-settings don't show  up?   6. How fast do particular algorithms converge? This can be stated  in terms of the number of examples t hat need to be given. Are  some algorithms more realistic than others in this regard?  7. What is the relation of the properties of the algorithms to  empirical research in language acquisition? Can early properties  of acquisition be shown to relate to the algorithms?",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kenneth",
   "pi_last_name": "Wexler",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Kenneth N Wexler",
   "pi_email_addr": "wexler@mit.edu",
   "nsf_id": "000208338",
   "pi_start_date": "1995-11-02",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Edward",
   "pi_last_name": "Gibson",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Edward A Gibson",
   "pi_email_addr": "egibson@mit.edu",
   "nsf_id": "000215436",
   "pi_start_date": "1995-11-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0196",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0196",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0197",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0197",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0198",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0198",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 1996,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 1997,
   "fund_oblg_amt": 95000.0
  },
  {
   "fund_oblg_fiscal_yr": 1998,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": null
}