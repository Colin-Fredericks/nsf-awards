{
 "awd_id": "0535140",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Learning and Reconstructing Generative 3D Human Models from Monocular Video",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2006-03-01",
 "awd_exp_date": "2010-02-28",
 "tot_intn_awd_amt": 336929.0,
 "awd_amount": 336929.0,
 "awd_min_amd_letter_date": "2006-02-22",
 "awd_max_amd_letter_date": "2006-02-22",
 "awd_abstract_narration": "Technical description\r\n\r\nThe main goal of this project is to automate the construction and  \r\nmanipulation of very high-level, three-dimensional structural and  \r\nappearance representations of humans from un-instrumented monocular  \r\nvideo. This technology would enable a broad spectrum of applications  \r\nincluding video browsing and indexing (content-based access to  \r\ndigital libraries), entertainment, virtual reality or human-computer  \r\ninteraction. Our methodology involves an alliance between supervised  \r\nand unsupervised statistical modeling and learning methods, non- \r\nlinear optimization and sampling techniques and computer vision. The  \r\ngoal of these procedures is to automate the model construction  \r\nprocess. We aim for compact representations that have the optimal  \r\nlevel of complexity in order to ensure stable and reliable perceptual  \r\ninferences.\r\n\r\nBroad project significance\r\n\r\nThe purpose of this research is to derive artificial systems that are  \r\nable to recover accurate three-dimensional models of human structure  \r\nand appearance from video sequences filmed with a single camera (this  \r\ninclude movies, sports or cultural events like ballet, or home  \r\nrecorded videos). Human are the prevailing subjects in the existing  \r\nvideo data, which typically records their motions, actions or  \r\nexpressions, the fine or coarse details of their behavior, the way  \r\nthey collaborate and communicate. Visualizing or analyzing scenes  \r\nwith complex life events based on reconstructed human models is an  \r\nimportant problem for the advancement of a variety of technological  \r\nfields including digital libraries and archives, video coding,  \r\nentertainment, animation and virtual reality, as well as intelligent  \r\nhuman-computer interfaces, protection and security\r\n\r\nHuman analysis in video is an open research problem facing important  \r\nscientific and computational challenges. The proportions of the human  \r\nbody vary across individuals due to gender, weight, age or race.  \r\nAside from this variability, any single human body has many degrees  \r\nof freedom due to articulation and the individual limbs are  \r\ndeformable due to muscle and clothing. Finally many real-world scenes  \r\ninvolve multiple interacting humans occluded by each other or by  \r\nother objects. The scene conditions may also vary due to the camera  \r\nmotion or lighting changes. These factors make accurate 3d human  \r\nmodels difficult to build and difficult to reconstruct reliably from  \r\nflat 2d images. In order to address these challenges, this research  \r\nwill involve synergies between optimization algorithms, computer  \r\nvision and image processing technologies. A key component of our  \r\napproach is the use of large scale statistical learning methods in  \r\norder to automatically acquire compact models of humans directly from  \r\nvideos filmed in the real world. In this respect, this research can  \r\nlead to fruitful connections with other computer science disciplines  \r\nlike computer graphics or computer animation. These areas are known  \r\nfor their highly realistic, but extraordinary complex to construct  \r\nmodels of the physical world, which often require intensive  \r\nlaboratory design by skilled artists.\r\n\r\nURL: http://ttic.uchicago.edu/~crismin/human_models_from_video.html\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cristian",
   "pi_last_name": "Sminchisescu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Cristian Sminchisescu",
   "pi_email_addr": "crismin@tti-c.org",
   "nsf_id": "000200568",
   "pi_start_date": "2006-02-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Toyota Technological Institute at Chicago",
  "inst_street_address": "6045 S KENWOOD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7738340409",
  "inst_zip_code": "606372803",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "TOYOTA TECHNOLOGICAL INSTITUTE AT CHICAGO",
  "org_prnt_uei_num": "ERBJF4DMW6G4",
  "org_uei_num": "ERBJF4DMW6G4"
 },
 "perf_inst": {
  "perf_inst_name": "Toyota Technological Institute at Chicago",
  "perf_str_addr": "6045 S KENWOOD AVE",
  "perf_city_name": "CHICAGO",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606372803",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "733900",
   "pgm_ele_name": "COMPUTER VISION"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6857",
   "pgm_ref_txt": "DIGITAL LIBRARIES AND ARCHIVES"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 336929.0
  }
 ],
 "por": null
}