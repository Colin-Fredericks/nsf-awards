{
 "awd_id": "0545772",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Extraction and Integration of Voice Source Features into the Acoustical Analysis of Spoken Affect",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2006-02-01",
 "awd_exp_date": "2012-09-30",
 "tot_intn_awd_amt": 499979.0,
 "awd_amount": 529911.0,
 "awd_min_amd_letter_date": "2006-01-05",
 "awd_max_amd_letter_date": "2011-08-17",
 "awd_abstract_narration": "This CAREER project is focused on voice source extraction directly from recorded speech signals for the enhancement of acoustical models in speaker affect analysis.  The project has four primary research goals.  The first goal is the creation of new techniques in voice source estimation from the acoustic speech signal without the use of auxiliary devices (e.g., EGG's).  This is accomplished through the development of metrics for assessing glottal quality and choosing estimates that reflect the best approximation of the voice source given the available speech data.  A second goal is the automated parameterization of the voice source in both the time and frequency domain allowing the shape and characteristics of the voice source to be quantified for analysis.  The third goal is to integrate the parameterized voice source into an acoustical framework for describing the characteristics of a speaker's affective/emotional state.  Current databases consisting of representations of speaker emotion, deception, and stress are under analysis for the incorporation of voice source features not previously available.  The final goal of the project is the creation and dissemination of a robust set of extraction and analysis tools of the voice source to the scientific community to encourage and enable the analysis of voice source parameters in all forms of speech research.  The project contributes greatly to speech analysis applications by enhancing the theoretical knowledge of assessing the quality of voice source estimates and providing the tools to enhance the acoustical models of the voice.  Additionally, the project allows for the design of educational demonstrations for outreach activities focused on improving community exposure to science and engineering research and increasing the participation of underrepresented groups. \r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Elliot",
   "pi_last_name": "Moore",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Elliot Moore",
   "pi_email_addr": "em80@mail.gatech.edu",
   "nsf_id": "000062117",
   "pi_start_date": "2006-01-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "225 NORTH AVE NW",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "727400",
   "pgm_ele_name": "HUMAN LANGUAGE & COMMUNICATION"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 105992.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 105995.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 99996.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 99998.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 107998.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 9932.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The core goal of the work in this project has been to take part in<br />the investigation of technology that can provide analysis of the human state<br />aside from the most basic objective analysis. Current technology in practice can extract the content of speech (speech recognition) and determine who is<br />speaking (facial recognition) but it can not analyze the affective state of the<br />person. Such an analysis can be important in law enforcement, health<br />monitoring, and general customer service applications. The focus of the work in this proposal has been on the extraction and integration of voice source features (related to the vocal fold motion) in the analysis and classification of<br />emotion in speech.</p>\n<p><span style=\"text-decoration: underline;\">Intellectual Merit</span></p>\n<p>The work in this proposal has produced several outcomes of<br />relevance to the analysis of emotion in speech including: 1) methodologies for<br />the extraction of voice source information, 2) analysis of pertinent features<br />related various states of affect (including various emotions and deception),<br />and 3) strategies for performing cross-database training and testing with the<br />hope of improving the ability to create a stable emotion classification model.</p>\n<p><span style=\"text-decoration: underline;\">Broader Impact</span></p>\n<p>All results of the project have been and are continuing to be disseminated through publication and presentations.&nbsp; The algorithms developed during the course of project are freely available upon request.&nbsp; Additionally, the PI has worked extensively in STEM outreach that has produced a number of MATLAB based demonstrations and presentations on signal processing concepts that are freely available upon request.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/26/2013<br>\n\t\t\t\t\tModified by: Elliot&nbsp;Moore</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe core goal of the work in this project has been to take part in\nthe investigation of technology that can provide analysis of the human state\naside from the most basic objective analysis. Current technology in practice can extract the content of speech (speech recognition) and determine who is\nspeaking (facial recognition) but it can not analyze the affective state of the\nperson. Such an analysis can be important in law enforcement, health\nmonitoring, and general customer service applications. The focus of the work in this proposal has been on the extraction and integration of voice source features (related to the vocal fold motion) in the analysis and classification of\nemotion in speech.\n\nIntellectual Merit\n\nThe work in this proposal has produced several outcomes of\nrelevance to the analysis of emotion in speech including: 1) methodologies for\nthe extraction of voice source information, 2) analysis of pertinent features\nrelated various states of affect (including various emotions and deception),\nand 3) strategies for performing cross-database training and testing with the\nhope of improving the ability to create a stable emotion classification model.\n\nBroader Impact\n\nAll results of the project have been and are continuing to be disseminated through publication and presentations.  The algorithms developed during the course of project are freely available upon request.  Additionally, the PI has worked extensively in STEM outreach that has produced a number of MATLAB based demonstrations and presentations on signal processing concepts that are freely available upon request.\n\n\t\t\t\t\tLast Modified: 04/26/2013\n\n\t\t\t\t\tSubmitted by: Elliot Moore"
 }
}