{
 "awd_id": "0534419",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Content-Driven Techniques for Non-Visual Web Access",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2005-12-01",
 "awd_exp_date": "2010-05-31",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 550623.0,
 "awd_min_amd_letter_date": "2005-11-28",
 "awd_max_amd_letter_date": "2009-09-18",
 "awd_abstract_narration": "The World Wide Web has evolved into an indispensable medium for dissemination of information, entertainment, commerce and education.  However, the graphical nature of most browsing software, coupled with the diversity and complexity of web content, have limited access to this technology for an entire community of people with visual disabilities.  Existing audio browsers that are based on text-to-speech conversion (e.g., screen readers) are not capable of describing the conceptual organization of a document's content or of letting a user select parts of a document to listen to.  As a result, people with visual disabilities can find it difficult to perform common tasks, such as distinguishing topics or correlating similar items, which are key to understanding the organization of documents, and so waste considerable time and attention listening to irrelevant information.  In this project the PI and his team will develop, test and disseminate the HearSay web browser, which will bring the browsing experience of people with visual disabilities closer to that of sighted people.  HearSay is based on automated techniques for structuring the content of web documents into labeled partitions consisting of logically related items.  By enabling interactive speech-driven guided exploration in which the system presents the document's labeled content and the user selects which parts of the content to listen to and when to navigate to a new page, HearSay will make non-visual browsing far less cumbersome.  Furthermore, for repetitive browsing tasks, HearSay will let users create and retrieve personalized content in different ways, ranging from content-based voice-marking of selected partitions in a page to powerful personal information assistants that gather and present user-defined information at the user's command.  To assure that the HearSay system is really useful \"as advertised\" to the intended community, the PIs have established a collaboration with Helen Keller Services for the Blind in Hempstead, NY, which trains people with visual disabilities, and will consult on the design and evaluation of HearSay.\r\n \r\nBroader Impacts:  This research will result in new algorithms and powerful technologies enabling end users to navigate web content using audio.  This capability will be especially valuable to people who are visually impaired in that it will enable them to browse and customize web content by themselves, but it will also be useful for mobile users of small-form devices, reducing their dependence on specialized content providers.  The PIs will develop a special version of HearSay for use with the Blackboard educational system, which will be advertised and disseminated to educational institutions so as to improve access to educational materials for students with visual disabilities.  In coordination with this project, two workshops on technology for accessibility will be organized, one focusing on accessibility technology in general and the second on accessibility technology for postgraduate and adult education.  These workshops will serve to raise awareness of the issues faced by those with visual disabilities and the potential for content-based techniques to improve information access for users who are mobile and/or have disabilities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "IV",
   "pi_last_name": "Ramakrishnan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "IV Ramakrishnan",
   "pi_email_addr": "ram@cs.stonybrook.edu",
   "nsf_id": "000365929",
   "pi_start_date": "2005-11-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Kifer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Kifer",
   "pi_email_addr": "kifer@cs.stonybrook.edu",
   "nsf_id": "000172694",
   "pi_start_date": "2005-11-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Susan",
   "pi_last_name": "Brennan",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Susan E Brennan",
   "pi_email_addr": "susan.brennan@stonybrook.edu",
   "nsf_id": "000106933",
   "pi_start_date": "2005-11-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Amanda",
   "pi_last_name": "Stent",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Amanda Stent",
   "pi_email_addr": "amanda.stent@gmail.com",
   "nsf_id": "000203133",
   "pi_start_date": "2005-11-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "perf_city_name": "STONY BROOK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117940001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "684600",
   "pgm_ele_name": "UNIVERSAL ACCESS"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6846",
   "pgm_ref_txt": "UNIVERSAL ACCESS"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 369132.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 169491.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 12000.0
  }
 ],
 "por": null
}