{
 "awd_id": "0546262",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Categorization and Identification of Visual Scenes",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2006-06-01",
 "awd_exp_date": "2011-05-31",
 "tot_intn_awd_amt": 519430.0,
 "awd_amount": 519430.0,
 "awd_min_amd_letter_date": "2006-05-26",
 "awd_max_amd_letter_date": "2010-04-25",
 "awd_abstract_narration": "CAREER: Categorization and Identification of Visual Scenes\r\nPI: Aude Oliva\r\n\r\nOne remarkable aspect of visual recognition is that humans are able to recognize the meaning (or \"gist\") of complex visual scenes within 1/20 of a second, independently of the quantity of objects in the scene. This rapid understanding phenomenon can be experienced while looking at rapid sequences in television advertisements and quick cuts in modern movie trailers. How is this remarkable feat accomplished?  Research over the last decade has made substantial progress toward understanding the mechanisms underlying single object recognition, but less progress has been made toward understanding scene recognition. For example, computer systems fall well short of human performance in tasks that require recognizing the gist of a scene. Dr. Aude Oliva has undertaken a novel approach to this challenging question by studying mechanisms of analysis that are global in nature, focusing on statistically robust features describing the spatial layout of the scene (e.g. its volume, its perspective, its level of clutter) and not merely its components (e.g., the objects in a scene). With National Science Foundation support, Dr. Aude Oliva will conduct a five-year CAREER award study to examine how a global approach to image analysis can explain humans remarkable ability to recognize scenes and objects. Moreover she will use this approach to define operational strategies for machine vision systems.  This program of research will combine a number of methodologies, including behavioral experiments (psychophysics, eye tracking), cognitive neuroscience methods (event-related potentials), and computational modeling. Applications of this work might include scene and space recognition systems to assist drivers, automatic systems that could provide semantic descriptions of the contents of large image databases, and computer assisted systems to aid the visually-impaired in navigating through visual space. The educational mission proposed by Dr. Oliva includes laboratory training of graduate and undergraduate students in the cognitive and computational methods of scene understanding, as well as a new course on computational visual cognition, and a winter tutorial together with an annual symposium both on scene understanding.\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aude",
   "pi_last_name": "Oliva",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Aude Oliva",
   "pi_email_addr": "oliva@mit.edu",
   "nsf_id": "000096241",
   "pi_start_date": "2006-05-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7327",
   "pgm_ref_txt": "CRCNS"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 300964.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 107394.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 111072.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Computer vision started with the goal of building machines that can see like humans. Currently, many techniques in computational visual understanding are inspired by the versatility of human object and scene recognition, and the ease with which visual information is stored in memory. Dr. Aude Oliva&rsquo;s National Science Foundation CAREER award has enabled her to investigate computational perception and cognition, building on the synergy between human and machine vision to help solve important computer vision problems such as how to teach a computer to understand scenes, perceive space, and find objects like a human. This work, in part, is accomplished by modeling human eye fixations, as well as mathematically predicting subjective properties of images.<br /> <br /> <br /> Whereas the complex arrangement of objects in natural scenes can create the impression that there is too much to see at once, people are able to interpret the meaning of multifaceted and complex scene images and remember which image they saw, e.g. a wedding, a birthday party, or a stadium crowd. Remarkably, people interpret complex scenes within a fraction of a second, which is roughly the same time it takes to identify that a <em>single</em> object is a face, a dog or a car. This feat of the human brain can be experienced at the movies when, with a the rapid scene cuts from a movie trailer, we are able to understand aspects of the storyline that would take considerably more time to explain verbally. The same phenomenon happens when quickly changing television channels or flipping pages of a magazine: one single glance is often enough to recognize a popular TV personality, a high-speed car chase, a football game, etc. Perceiving scenes in a glance is analogous to looking at a painting of a landscape and recognizing it as a &ldquo;forest&rdquo; without necessarily seeing the &ldquo;trees&rdquo; that comprise it. Inspired by this strategy of human scene understanding, Dr. Oliva and her team have proposed computer vision algorithms that automatically recognize the type of environment (i.e. a busy street, an alley, a park, etc) shown in an image, and predict where people would look if they were searching for an object (like where pedestrians could be found in street scenes, see illustration).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/07/2012<br>\n\t\t\t\t\tModified by: Aude&nbsp;Oliva</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2012/0546262/0546262_10021590_1331162001803_MultiGuidanceModel--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2012/0546262/0546262_10021590_1331162001803_MultiGuidanceModel--rgov-800width.jpg\" title=\"Where does a computational model of attention &quot;think&quot; people will look when searching for people?\"><img src=\"/por/images/Reports/POR/2012/0546262/0546262_10021590_1331162001803_MultiGuidanceModel--rgov-66x44.jpg\" alt=\"Where does a computational model of attention &quot;think&quot; people will look when searching for people?\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The 3D maps show model's predictions: the map height represents how likely that region is to attract the eye. The Scene model selects a region of the ground plane. The Target Features model selects regions that are \"person-like\". The Saliency model selects regions that usually attract attention.</div>\n<div class=\"imageCredit\">Ehinger, K.A., Hidalgo-Sotelo, B., Torralba, A. & Oliva, A. (2009). Modelling Search for People in 9...",
  "por_txt_cntn": "\nComputer vision started with the goal of building machines that can see like humans. Currently, many techniques in computational visual understanding are inspired by the versatility of human object and scene recognition, and the ease with which visual information is stored in memory. Dr. Aude Oliva\u00c6s National Science Foundation CAREER award has enabled her to investigate computational perception and cognition, building on the synergy between human and machine vision to help solve important computer vision problems such as how to teach a computer to understand scenes, perceive space, and find objects like a human. This work, in part, is accomplished by modeling human eye fixations, as well as mathematically predicting subjective properties of images.\n \n \n Whereas the complex arrangement of objects in natural scenes can create the impression that there is too much to see at once, people are able to interpret the meaning of multifaceted and complex scene images and remember which image they saw, e.g. a wedding, a birthday party, or a stadium crowd. Remarkably, people interpret complex scenes within a fraction of a second, which is roughly the same time it takes to identify that a single object is a face, a dog or a car. This feat of the human brain can be experienced at the movies when, with a the rapid scene cuts from a movie trailer, we are able to understand aspects of the storyline that would take considerably more time to explain verbally. The same phenomenon happens when quickly changing television channels or flipping pages of a magazine: one single glance is often enough to recognize a popular TV personality, a high-speed car chase, a football game, etc. Perceiving scenes in a glance is analogous to looking at a painting of a landscape and recognizing it as a \"forest\" without necessarily seeing the \"trees\" that comprise it. Inspired by this strategy of human scene understanding, Dr. Oliva and her team have proposed computer vision algorithms that automatically recognize the type of environment (i.e. a busy street, an alley, a park, etc) shown in an image, and predict where people would look if they were searching for an object (like where pedestrians could be found in street scenes, see illustration).\n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/07/2012\n\n\t\t\t\t\tSubmitted by: Aude Oliva"
 }
}