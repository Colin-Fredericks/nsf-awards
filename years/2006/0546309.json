{
 "awd_id": "0546309",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Interacting with Autonomy",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2006-03-15",
 "awd_exp_date": "2012-02-29",
 "tot_intn_awd_amt": 420888.0,
 "awd_amount": 464888.0,
 "awd_min_amd_letter_date": "2006-03-14",
 "awd_max_amd_letter_date": "2010-05-12",
 "awd_abstract_narration": "Over the past three years, the PI has conducted studies of human-robot interaction (HRI) in robot systems designed for urban search and rescue (USAR) and robotic wheelchairs.  As a consequence of these studies, she has observed several problems arising from current designs: users do not switch modes effectively; users are unable to intervene after a long period of autonomy; users have a lack of situation awareness; information is presented ineffectively; and bystanders are confused when the robot acts in an unexpected fashion.  Based upon these issues, the PI has identified four challenge areas for interacting with autonomous systems: blending user and robot inputs to create sliding scale autonomy; automatically changing autonomy levels; summarizing robot state and past actions; and interacting with bystanders.  During the first three year of this project, the PI will design systems and conduct experiments in and effort to find answers to these challenge areas.  She will test her theories in two robot applications: urban search and rescue, which requires the operator to be remotely located; and robotic wheelchairs, where the operator is sitting on the robot and bystanders can be affected by the operation of the system.   In the final two years of the project, the PI will validate her theories in a third application: a robot assistant for the elderly or disabled.  Throughout the grant period, the research will be integrated into courses (undergraduate as well as graduate), as well as outreach activities in middle and high schools.  The PI will also develop a new course, Interacting with Autonomous Systems, which will address both human-computer interaction (HCI) and human-robot interaction (HRI).  The intellectual merits of this project includes: formulation and testing of theories for determining how to appropriately set and switch autonomy levels for robotic systems; creation of methods for blending autonomy levels to create new autonomy levels; development of an improved understanding of interaction with robotic systems; and validation of all hypotheses through the development of a new robotic system.\r\n\r\nBroader Impacts:  All of the robot application areas to be explored in this research have the potential to impact society.  The theories regarding interacting with autonomy will apply to non-robotic systems as well, including space systems such as life support.  Inclusion of undergraduate students in the research will encourage them to pursue graduate studies, and the outreach program will introduce middle and high school students to robotics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Holly",
   "pi_last_name": "Yanco",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Holly A Yanco",
   "pi_email_addr": "holly@cs.uml.edu",
   "nsf_id": "000278965",
   "pi_start_date": "2006-03-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Lowell Research Foundation",
  "inst_street_address": "600 Suffolk Street",
  "inst_street_address_2": "2nd Floor South",
  "inst_city_name": "Lowell",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "9789344723",
  "inst_zip_code": "018543692",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MA03",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": null
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Lowell",
  "perf_str_addr": "220 PAWTUCKET ST STE 400",
  "perf_city_name": "LOWELL",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "018543573",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MA03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "749600",
   "pgm_ele_name": "COLLABORATIVE SYSTEMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 83302.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 83728.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 96164.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 100615.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 101079.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our research resulted in new methods for designing robot hardware, control software, and interfaces to make the robots easier for people to use. &nbsp;We investigated interaction methods for a wide variety of user types, including people with disabilities. &nbsp;Our research also allows for the design of systems that will better interact with bystanders and be easily learnable and usable. Our research identified, developed and utilized metrics for assessing the effectiveness of human-robot interaction in many application domains, including telepresence robots, assistive technology, and urban search and rescue.</p>\n<p>One of the results of our research is the DREAM (<strong>d</strong>ynamically <strong>r</strong>esizing, <strong>e</strong>rgonomic <strong>a</strong>nd <strong>m</strong>ulti-touch) Controller.&nbsp; While large multi-touch tables could be used to display maps and issue commands to groups of robots, there was no easy way to control a single robot on a multi-touch table without switching the view away from the overhead map view or connecting an external joystick or game pad.&nbsp; Taking inspiration from a game pad, we designed and programmed the ability for a DREAM Controller to be drawn around a hand when it was placed on a multi-touch screen, including algorithms to rapidly distinguish whether there was one or more hands on the screen or just a collection of other points.&nbsp; Through user testing with first responders, we found that the DREAM Controller could be learned quickly and that even people with no video game experience could drive a robot effectively with just a few minutes of training. &nbsp;&nbsp;Our work also created other interface designs and robot control software for remotely controlled robots used for urban search and rescue.</p>\n<p>We investigated how remote employees could use telepresence robots to meet with co-workers at a facility that housed the robot.&nbsp; Our research resulted in a set of design guidelines for telepresence robots, both their hardware and software.&nbsp; Based upon this initial research, we developed a prototype telepresence robot system, which we named Hugo.&nbsp; The additional sensors and mini-computer on Hugo allowed it to have greater levels of autonomy than commercially available telepresence robots.&nbsp; These additionally capabilities allow a robot to be commanded to go to a particular room in a building, removing the need for its user to joystick the robot around a building.&nbsp; We also designed and evaluated prototype interfaces that would allow people with motor and/or cognitive disabilities to use the telepresence robots to stay connected with friends, family, school, and work.</p>\n<p>The research results were disseminated in 2 journal articles, 1 PhD thesis, 2 MS theses, 1 book chapter, and 17 refereed conference and workshop papers.&nbsp; Videos of the resulting projects were also posted on our lab&rsquo;s YouTube channel: youtube.com/user/umlrobotics.</p>\n<p>We also developed and ran professional development workshops for K-12 teachers, which we ran in the summers of 2009, 2010, 2011 and 2012.&nbsp; The workshops were entitled, \"STREAM: Using Robotics to Teach Science, Technology, Engineering and Math.\"&nbsp; Over the four summers, over 100 teachers attended our workshops, learning a variety of materials and curriculum that could be used in their classrooms.&nbsp; More information can be found at stream.cs.uml.edu.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/01/2012<br>\n\t\t\t\t\tModified by: Holly&nbsp;A&nbsp;Yanco</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div c...",
  "por_txt_cntn": "\nOur research resulted in new methods for designing robot hardware, control software, and interfaces to make the robots easier for people to use.  We investigated interaction methods for a wide variety of user types, including people with disabilities.  Our research also allows for the design of systems that will better interact with bystanders and be easily learnable and usable. Our research identified, developed and utilized metrics for assessing the effectiveness of human-robot interaction in many application domains, including telepresence robots, assistive technology, and urban search and rescue.\n\nOne of the results of our research is the DREAM (dynamically resizing, ergonomic and multi-touch) Controller.  While large multi-touch tables could be used to display maps and issue commands to groups of robots, there was no easy way to control a single robot on a multi-touch table without switching the view away from the overhead map view or connecting an external joystick or game pad.  Taking inspiration from a game pad, we designed and programmed the ability for a DREAM Controller to be drawn around a hand when it was placed on a multi-touch screen, including algorithms to rapidly distinguish whether there was one or more hands on the screen or just a collection of other points.  Through user testing with first responders, we found that the DREAM Controller could be learned quickly and that even people with no video game experience could drive a robot effectively with just a few minutes of training.   Our work also created other interface designs and robot control software for remotely controlled robots used for urban search and rescue.\n\nWe investigated how remote employees could use telepresence robots to meet with co-workers at a facility that housed the robot.  Our research resulted in a set of design guidelines for telepresence robots, both their hardware and software.  Based upon this initial research, we developed a prototype telepresence robot system, which we named Hugo.  The additional sensors and mini-computer on Hugo allowed it to have greater levels of autonomy than commercially available telepresence robots.  These additionally capabilities allow a robot to be commanded to go to a particular room in a building, removing the need for its user to joystick the robot around a building.  We also designed and evaluated prototype interfaces that would allow people with motor and/or cognitive disabilities to use the telepresence robots to stay connected with friends, family, school, and work.\n\nThe research results were disseminated in 2 journal articles, 1 PhD thesis, 2 MS theses, 1 book chapter, and 17 refereed conference and workshop papers.  Videos of the resulting projects were also posted on our lab\u00c6s YouTube channel: youtube.com/user/umlrobotics.\n\nWe also developed and ran professional development workshops for K-12 teachers, which we ran in the summers of 2009, 2010, 2011 and 2012.  The workshops were entitled, \"STREAM: Using Robotics to Teach Science, Technology, Engineering and Math.\"  Over the four summers, over 100 teachers attended our workshops, learning a variety of materials and curriculum that could be used in their classrooms.  More information can be found at stream.cs.uml.edu.\n\n\t\t\t\t\tLast Modified: 08/01/2012\n\n\t\t\t\t\tSubmitted by: Holly A Yanco"
 }
}