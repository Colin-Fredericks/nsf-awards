{
 "awd_id": "0534359",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Structure Induction for Manipulative and Interactive Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2006-02-01",
 "awd_exp_date": "2010-01-31",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 491598.0,
 "awd_min_amd_letter_date": "2006-02-14",
 "awd_max_amd_letter_date": "2008-07-09",
 "awd_abstract_narration": "\r\nStructure Induction for Manipulative and Interactive Tasks\r\n\r\nThis project will develop methods for inferring models, similar to  those traditionally used for speech and language, for structured, repetitive, interactive manipulation tasks.  Such tasks are typically composed of more elementary tasks that follow a nominal plan of action to  achieve a set of desired outcomes. Examples include assembly tasks, various forms of surgery, gesture-based  interfaces, maneuvering in traffic, sports, dancing or gymnastics to name a few.  \r\n\r\nThe project will be carried out in the context of observing and  modeling minimally invasive surgical procedures. Motion and video streams will be acquired from surgeons  during simulated surgical tasks using the da Vinci surgical robot. Surgical tasks will be represented using  grammars on a set of simple, elementary surgical gestures modeled using finite-state machines. Both the  elementary gestures and the task-dependent grammatical constraints will be inferred directly from data. A key  element of the proposed approach is to first develop accurate models from simple, low-dimensional datasets  such as direct measurements of surgical motion, and then to use these models to boostrap model training to more complex, high-dimensional synchronized video of the same procedure.\r\n\r\nThe core research problems to be addressed are thus:  Motion sequence modeling for elementary tasks: Develop algorithms that are able to learn finite-state machines modeling simple, context-independent elementary motion or manipulation tasks in terms of atomic gestures using a limited set of manually labeled motion data.  Multi-modal models of elementary tasks: Create effective methods for augmenting elementary task models with complementary and contextual features from synchronized video data.  Grammar induction:  Use elementary task models to automatically label more, larger data sequences and infer high-level grammatical structures from these labelings.  Model bootstrapping: Use elementary task and grammatical models to label even larger data corpi that are then used to retrain elemetary task models using only video data.\r\n\r\nIn summary, this project will study the problem of modeling structured human manipulative actions such as those that occur during minimally invasive surgery. Using statistical modeling methods originally developed for speech understanding, models for the language of surgical motion will be created and evaluated. These\r\nmodels will first be created using motion measurements from a surgical robot, but will then be transferred to video data from the same system. These specific models will have  applicability to surgical training, skill assessment, and robotic surgical augmentation.  More generally, this project will advance our ability to develop structured models for understanding com-plex data sources, which has wide ranging implications. It will facilitate a variety of applications that require understanding  of human action such gesture-based systems for human-machine interaction, methods for interpreting or detecting action in video surveillance data, or assessment and measurement of movement for bio-mechanics and medicine. At the same time, it will advance the basic science of modeling large and complex data streams such as video, text, speech, or medical images.\r\n\r\nURL: http://www.cs.jhu.edu/CIRL/projects/SurgicalModeling/\r\n\r\n\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Hager",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Hager",
   "pi_email_addr": "hager@cs.jhu.edu",
   "nsf_id": "000385453",
   "pi_start_date": "2006-02-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sanjeev",
   "pi_last_name": "Khudanpur",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Sanjeev P Khudanpur",
   "pi_email_addr": "khudanpur@jhu.edu",
   "nsf_id": "000236251",
   "pi_start_date": "2006-02-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Izhak",
   "pi_last_name": "Shafran",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Izhak Shafran",
   "pi_email_addr": "zakshafran@gmail.com",
   "nsf_id": "000096166",
   "pi_start_date": "2007-04-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N CHARLES ST",
  "perf_city_name": "BALTIMORE",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182608",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7496",
   "pgm_ref_txt": "COLLABORATIVE SYSTEMS"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 479998.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 5800.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 5800.0
  }
 ],
 "por": null
}