{
 "awd_id": "0546900",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Algorithms and Models for Random Structures",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Balasubramanian Kalyanasundaram",
 "awd_eff_date": "2006-02-15",
 "awd_exp_date": "2013-01-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2006-01-23",
 "awd_max_amd_letter_date": "2010-05-20",
 "awd_abstract_narration": "Random structures have become an indispensable tool of computer science over the last two decades. When used to model input distributions, they provide an easily accessible experimental platform for exploring and \r\ntesting new algorithms: a number of practically important algorithmic developments have been made by designing algorithms that perform well on certain simple input distributions. When used to model the formation and growth of networks, they have been the dominant analytical tool in an area that has seen explosive growth in recent years: random graphs are now central objects in areas ranging from communications and information retrieval to biology and game theory.\r\n\r\nThis research project focuses on i) the analysis of algorithms on random constraint satisfaction problems, such as random k-SAT and, ii) the development of analytically tractable models of random networks that \r\nincorporate realistic constraints, such as low-dimensionality and total connection length. The goal of the\r\nfirst part is to address a number of questions relating the performance of algorithms on random constraint satisfaction problems to structural properties of their solution--spaces. It is partly motivated by the \r\nremarkable experimental performance of a new algorithm for such problems, Survey Propagation, that employs a number of deep, but non-rigorous, ideas from statistical physics. The goal of the second part is to remove a number of unrealistic, hard-coded assumptions underlying existing models of random networks by introducing generative models in which networks are maximum entropy solutions of optimization tradeoffs.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dimitris",
   "pi_last_name": "Achlioptas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dimitris Achlioptas",
   "pi_email_addr": "optas@cs.ucsc.edu",
   "nsf_id": "000429292",
   "pi_start_date": "2006-01-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Cruz",
  "inst_street_address": "1156 HIGH ST",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA CRUZ",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8314595278",
  "inst_zip_code": "950641077",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "CA19",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA SANTA CRUZ",
  "org_prnt_uei_num": "",
  "org_uei_num": "VXUFPE4MCZH5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Cruz",
  "perf_str_addr": "1156 HIGH ST",
  "perf_city_name": "SANTA CRUZ",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "950641077",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "CA19",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "286000",
   "pgm_ele_name": "THEORY OF COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 155329.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 159836.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 84835.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"p1\"><span class=\"s1\">The project&rsquo;s goal has been to build bridges between two distinct scientific fields. The first field is the&nbsp;study of computational complexity. That is, the study of which problems are amenable to sophisticated&nbsp;solution methods (algorithms) vs. which problems are &ldquo;intractable&rdquo;, meaning that we can not hope for&nbsp;solution methods significantly more efficient than exhaustively enumerating and evaluating all&nbsp;possibilities.&nbsp;The second field is statistical physics, whose traditional object of study has been the&nbsp;emergence of macroscopic properties, such as temperature, in large systems of interacting particles.</span></p>\n<p class=\"p2\"><span class=\"s1\">&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">The methodological vehicle for connecting the two fields are random structures, such as random&nbsp;networks and random instances of Constraint Satisfaction Problems (CSPs). As a toy example of a generating a random CSP instance imagine that we start with a fully&nbsp;solved Sudoku puzzle, that is, one where all 81 squares are properly filled in. Next, imagine that we&nbsp;randomly select and erase some of the squares. Finally, imagine that we hand the resulting puzzle to someone, asking them to fill in the empty squares to form a valid, complete solution (not necessarily the one we started with). How hard will this task be?&nbsp;</span></p>\n<p class=\"p2\"><span class=\"s1\">&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">The extreme cases are intuitively clear: if we erase only a handful of squares, then&nbsp;the ones that remain will impose very strong restrictions on the empty ones, strongly pointing towards the&nbsp;solution we started with, making the task easy; if, on the other hand, we erase most squares, then it's very likely that there will now be a multitude of valid complete&nbsp;solutions, again making the task easy (for example, if we erase all squares, then any valid&nbsp;Sudoku is a valid answer). Yet, for some intermediate number of random erasures, the resulting puzzle&nbsp;will be quite hard. The reason is that in this intermediate regime the following is true: for&nbsp;each&nbsp;empty square there is a unique correct value, yet if one guesses a wrong value for that square and tries to&nbsp;complete the rest of the puzzle, many other squares have to be filled in, in many different combinations, to&nbsp;demonstrate the falsensess of the original guess. That is, there is no &ldquo;short proof&rdquo; of the falseness.&nbsp;</span></p>\n<p class=\"p2\"><span class=\"s1\">&nbsp;</span></p>\n<p class=\"p1\"><span class=\"s1\">Even though the above is a toy example, in the project we mathematically prove that the underlying&nbsp;phenomenon is extremely general and dub it shattering. That is, we prove that in random Constraint Satisfaction Problems,&nbsp;a very small change in the control parameter (in the Sudoku example this is the number of non-empty&nbsp;squares) can have a dramatic impact on computational hardness, as it can cause a massive change in&nbsp;the geometry of the set of solutions (and, more generally, to the &ldquo;energy landscape&rdquo; of the problem). Such a massive change is a phase transition,&nbsp;completely comparable to the change of&nbsp;water from liquid to solid following a minute change in temperature (the control parameter). Indeed, the methods used in the project&nbsp;borrow and connect several ideas from statistical physics to the study of mathematical algorithms. Moreover, they&nbsp;repay some of the debt to physics by developing new mathematical techniques&nbsp;which, while motivated by the study of algorithms, can be used to analyze physical problems.<br /><br />The impact of the proposal lies in giving both computer scientists and physicists new tools to reason about large systems of interacting entities. Computer science is enriched by the understanding that phase tran...",
  "por_txt_cntn": "The project\u00c6s goal has been to build bridges between two distinct scientific fields. The first field is the study of computational complexity. That is, the study of which problems are amenable to sophisticated solution methods (algorithms) vs. which problems are \"intractable\", meaning that we can not hope for solution methods significantly more efficient than exhaustively enumerating and evaluating all possibilities. The second field is statistical physics, whose traditional object of study has been the emergence of macroscopic properties, such as temperature, in large systems of interacting particles.\n \nThe methodological vehicle for connecting the two fields are random structures, such as random networks and random instances of Constraint Satisfaction Problems (CSPs). As a toy example of a generating a random CSP instance imagine that we start with a fully solved Sudoku puzzle, that is, one where all 81 squares are properly filled in. Next, imagine that we randomly select and erase some of the squares. Finally, imagine that we hand the resulting puzzle to someone, asking them to fill in the empty squares to form a valid, complete solution (not necessarily the one we started with). How hard will this task be? \n \nThe extreme cases are intuitively clear: if we erase only a handful of squares, then the ones that remain will impose very strong restrictions on the empty ones, strongly pointing towards the solution we started with, making the task easy; if, on the other hand, we erase most squares, then it's very likely that there will now be a multitude of valid complete solutions, again making the task easy (for example, if we erase all squares, then any valid Sudoku is a valid answer). Yet, for some intermediate number of random erasures, the resulting puzzle will be quite hard. The reason is that in this intermediate regime the following is true: for each empty square there is a unique correct value, yet if one guesses a wrong value for that square and tries to complete the rest of the puzzle, many other squares have to be filled in, in many different combinations, to demonstrate the falsensess of the original guess. That is, there is no \"short proof\" of the falseness. \n \nEven though the above is a toy example, in the project we mathematically prove that the underlying phenomenon is extremely general and dub it shattering. That is, we prove that in random Constraint Satisfaction Problems, a very small change in the control parameter (in the Sudoku example this is the number of non-empty squares) can have a dramatic impact on computational hardness, as it can cause a massive change in the geometry of the set of solutions (and, more generally, to the \"energy landscape\" of the problem). Such a massive change is a phase transition, completely comparable to the change of water from liquid to solid following a minute change in temperature (the control parameter). Indeed, the methods used in the project borrow and connect several ideas from statistical physics to the study of mathematical algorithms. Moreover, they repay some of the debt to physics by developing new mathematical techniques which, while motivated by the study of algorithms, can be used to analyze physical problems.\n\nThe impact of the proposal lies in giving both computer scientists and physicists new tools to reason about large systems of interacting entities. Computer science is enriched by the understanding that phase transition phenomena can have computational character, while statistical physics from the notion that the absence of short proofs of impossibility, can lead to very long relaxation times.\n\n\t\t\t\t\tLast Modified: 04/30/2015\n\n\t\t\t\t\tSubmitted by: Dimitris Achlioptas"
 }
}