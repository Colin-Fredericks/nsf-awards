{
 "awd_id": "0546136",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: MauveDB: Model-Based User Views over Sensor Data",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gia-Loi Le Gruenwald",
 "awd_eff_date": "2006-01-01",
 "awd_exp_date": "2011-12-31",
 "tot_intn_awd_amt": 479902.0,
 "awd_amount": 479902.0,
 "awd_min_amd_letter_date": "2005-12-22",
 "awd_max_amd_letter_date": "2010-02-02",
 "awd_abstract_narration": "IIS-0546136\r\nAmol V Deshpande <amol@cs.umd.edu>\r\nUniversity of Maryland College Park\r\n\r\nCAREER: MauveDB: Model-based User Views over Sensor Data\r\n\r\nReal-world data --- especially when generated by distributed measurement infrastructures such as sensor networks --- tends to be incomplete, imprecise, and erroneous, making it impossible to present it to the users or to feed it directly into applications.  The goal of this project is to develop MauveDB, a data management system that offers a principled approach to dealing with this problem by supporting a new abstraction called \"model-based views.\"  MauveDB is built by leveraging the Apache Derby Database Management System (DBMS) codebase.  Analogous to traditional database views, model-based views provide independence from the details of the underlying data generating mechanism and hide the irregularities of the data by using statistical and probabilistic \"models\" to present a consistent view of the data to the users. MauveDB supports a declarative language for defining model-based views, and supports declarative querying over model-based views using an extended version of SQL that supports continuous and probabilistic queries. Being a full-fledged DBMS, MauveDB also enables easy storage and archival and querying of historical data. By relieving the users of the burden of dealing with the noisy real-world data, MauveDB enables a high-impact new class of real-world applications based on networks of monitoring and sensing devices, such as traffic monitoring, location-based services, environmental monitoring, health services, and military applications.  This research will be used to develop code, datasets and sensor network deployments that will be used in the new course modules being developed at the University of Maryland; the code and the course material developed will also be made freely available at the project web site http://www.cs.umd.edu/~amol/MauveDB.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amol",
   "pi_last_name": "Deshpande",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Amol V Deshpande",
   "pi_email_addr": "amol@cs.umd.edu",
   "nsf_id": "000486255",
   "pi_start_date": "2005-12-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland, College Park",
  "perf_str_addr": "3112 LEE BUILDING",
  "perf_city_name": "COLLEGE PARK",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "749600",
   "pgm_ele_name": "COLLABORATIVE SYSTEMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0106",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0106",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2006,
   "fund_oblg_amt": 94171.0
  },
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 90899.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 94486.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 98225.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 102121.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this NSF project was to develop data management abstractions and tools to store, manage, and process noisy and incomplete data generated in a wide range of practical application domains.&nbsp; The data uncertainties may be a result of the fundamental limitations of the underlying measurement infrastructures, or the inherent ambiguity in the domain, or they may be a side-effect of the rich <em>probabilistic modeling</em> typically performed to extract high-level events from the data. For example, there has been a tremendous increase in the number of distributed measurement infrastructures such as wireless sensor networks that continuously generate invaluable data about our everyday world. However, the potential of that data has been hard to realize because of the typically incomplete and erroneous nature of the data, that makes it to hard to base critical decisions on it. Experimental data generated in scientific domains also exhibits many of the similar uncertainties. Similarly, when attempting to integrate heterogeneous data sources on the Internet (\"data integration\") or extracting structured information from text (\"information extraction\"), the results are approximate and uncertain at best. Lacking functionally rich and easy-to-use data management tools that can reason about large volumes of uncertain data, the information about the uncertainty is often either discarded or reasoned about only superficially.<br /><br />The key outcomes of the project can be broadly divided into two parts.<br /><br />First, we developed abstractions and techniques to <em>integrate a variety of statistical modeling techniques into relational database systems</em><span> to seamlessly manage and process uncertain data.&nbsp; Statistical analysis and modeling are perhaps the most ubiquitous tasks that need to be performed on real-world data, especially on uncertain data. Models can often be an end unto themselves, e.g., in scientific data analysis, but are also widely used in non-scientific application domains for smoothing of noisy data, predicting missing values, forecasting, pattern recognition, and event or anomaly detection. However, statistical modeling techniques typically cannot scale to the very large volumes of data commonly encountered in today's world. On the other hand, relational database systems can easily handle very large data volumes, but do not have the rich processing capabilities of statistical models. This forces the users to employ an awkward combination of databases and external tools like R, <span>Matlab</span> and <span>SAS</span>, for their analysis. To address this problem, we proposed a new declarative abstraction called \"model-based views\" that enables scalable and real-time statistical modeling of data by tightly integrating statistical models in the core query processing engine of a database system. We illustrated the viability and benefits of model-based views by building a prototype system that supports this abstraction. We showed how to efficiently integrate a large variety of statistical models into relational databases, thus enabling us to significantly enrich the capabilities of relational database systems, without compromising the ability to efficiently query large volumes of data.</span><br /><br />Second, we developed <em>techniques and algorithms for representing and querying large volumes of data annotated with \"probabilities\" in relational databases</em>. Such annotations are one of the most common ways to both associate uncertainty with data and reason about it. However, traditional data management tools cannot properly reason or operate upon such annotated data. Further, even simple analysis of such data turns out to be intractable in many cases. During the course of the project, we addressed many different facets of this challengi<span>ng problem. We developed a uniform framework for managing probabilistic data that combines t...",
  "por_txt_cntn": "\nThe goal of this NSF project was to develop data management abstractions and tools to store, manage, and process noisy and incomplete data generated in a wide range of practical application domains.  The data uncertainties may be a result of the fundamental limitations of the underlying measurement infrastructures, or the inherent ambiguity in the domain, or they may be a side-effect of the rich probabilistic modeling typically performed to extract high-level events from the data. For example, there has been a tremendous increase in the number of distributed measurement infrastructures such as wireless sensor networks that continuously generate invaluable data about our everyday world. However, the potential of that data has been hard to realize because of the typically incomplete and erroneous nature of the data, that makes it to hard to base critical decisions on it. Experimental data generated in scientific domains also exhibits many of the similar uncertainties. Similarly, when attempting to integrate heterogeneous data sources on the Internet (\"data integration\") or extracting structured information from text (\"information extraction\"), the results are approximate and uncertain at best. Lacking functionally rich and easy-to-use data management tools that can reason about large volumes of uncertain data, the information about the uncertainty is often either discarded or reasoned about only superficially.\n\nThe key outcomes of the project can be broadly divided into two parts.\n\nFirst, we developed abstractions and techniques to integrate a variety of statistical modeling techniques into relational database systems to seamlessly manage and process uncertain data.  Statistical analysis and modeling are perhaps the most ubiquitous tasks that need to be performed on real-world data, especially on uncertain data. Models can often be an end unto themselves, e.g., in scientific data analysis, but are also widely used in non-scientific application domains for smoothing of noisy data, predicting missing values, forecasting, pattern recognition, and event or anomaly detection. However, statistical modeling techniques typically cannot scale to the very large volumes of data commonly encountered in today's world. On the other hand, relational database systems can easily handle very large data volumes, but do not have the rich processing capabilities of statistical models. This forces the users to employ an awkward combination of databases and external tools like R, Matlab and SAS, for their analysis. To address this problem, we proposed a new declarative abstraction called \"model-based views\" that enables scalable and real-time statistical modeling of data by tightly integrating statistical models in the core query processing engine of a database system. We illustrated the viability and benefits of model-based views by building a prototype system that supports this abstraction. We showed how to efficiently integrate a large variety of statistical models into relational databases, thus enabling us to significantly enrich the capabilities of relational database systems, without compromising the ability to efficiently query large volumes of data.\n\nSecond, we developed techniques and algorithms for representing and querying large volumes of data annotated with \"probabilities\" in relational databases. Such annotations are one of the most common ways to both associate uncertainty with data and reason about it. However, traditional data management tools cannot properly reason or operate upon such annotated data. Further, even simple analysis of such data turns out to be intractable in many cases. During the course of the project, we addressed many different facets of this challenging problem. We developed a uniform framework for managing probabilistic data that combines the rich modeling power of probabilistic graphical models with the efficiency of database storage and querying. This enables us to represent and query complex correlations tha..."
 }
}