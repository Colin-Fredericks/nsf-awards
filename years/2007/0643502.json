{
 "awd_id": "0643502",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:  Enhancing Vocal Communication using Graphical Social Cues",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2007-03-01",
 "awd_exp_date": "2013-02-28",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2007-01-23",
 "awd_max_amd_letter_date": "2011-03-21",
 "awd_abstract_narration": "This proposal outlines a challenging goal of combining the ease of voice with the visual feedback of graphics to create a new communication medium. The project will create a graphical language for visualizing communication. Different versions of the graphical language will be used to mediate remote and co-located conversation, to create learning tools for acquiring new language skills and conducting speech therapy, to create new visualization techniques combining time and phase analysis, and to produce novel methods of archiving audio, speech, and voice.\r\n\r\nIntellectual Merit. The intellectual merit of this project is the consideration audio, speech, and voice processing from a perspective and goal different from that of traditional audio engineers. The project begins with simple yet effective techniques that have not yet been studied and progresses to developing new analysis and visualization techniques. The graphical languages will be evaluated through applications catered to their use. This research studies the use of social cues and signals that are more readily apparent in mediated interaction than in face-to-face interaction; the parameters in voice that are most effective and easy to interpret; the use of color, shape, and motion mapped onto these voice parameters. The research will help us understand how visualizing conversation publicly affects our vocal interaction when in co-located spaces and in remote spaces; and how much value is added by combining voice data with graphical data.\r\n\r\nBroader Impact. This research will alter vocal communication interaction in online environments and in physical co-located spaces. The act of archiving in real-time will have new consequences for voice communication and collaboration. The research will provide novel tools for helping users learn the subtleties of vocalization. These tools will be used for learning new languages and for speech therapy. The research extends the vocalization tools to tangible toy objects to encourage vocalization in children who have social impairments. The research will provide an alternate graphical method for archiving large bodies of audio that can be searched at a glance rather than with a search engine. The research results will be disseminated in new courses and in research publications.\r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Karrie",
   "pi_last_name": "Karahalios",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karrie Karahalios",
   "pi_email_addr": "kkarahal@cs.uiuc.edu",
   "nsf_id": "000313162",
   "pi_start_date": "2007-01-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 95007.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 193710.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 103598.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 107685.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This work explored using visualization to emphasize and uncover cues in spoken language.&nbsp; The work initially focused on two domains:&nbsp; (1) visualizing small group conversation around a tabletop and (2) visualization to encourage vocalization in those diagnosed with Autism Spectrum Disorders (ASD).&nbsp; We later explored cues in some written discourse.&nbsp; We highlight the projects below.</p>\n<p>&nbsp;</p>\n<p>The Conversation Clock:&nbsp; The Conversation Clock explored small group dynamics about a tabletop. A visualization explicitly rendered participant contribution visible from a third person perspective to the group.&nbsp; The system highlighted cues such as turn-taking, silence, domination, mimicry, &ldquo;yes&rdquo;-behavior, laughter, interruption and prosody. In a series of studies of this visualization in groups of 3-4 people, we found the visualization resulted in decreased participation of participants that previously spoke 'above average'and increased participation in participants who previously spoke 'below average'.&nbsp; We found significant differences in how participants modified their vocal behavior with respects to number of turns and length of turns in conversation.</p>\n<p>We placed this tool in three additional contexts:&nbsp; (1) &nbsp; to analyze and teach turn-taking skills in teenagers and adults with Asperger&rsquo;s Syndrome.&nbsp; We found that &ldquo;seeing&rdquo; conversation helped participants better understand goals.&nbsp; (2)&nbsp; to explore visual &ldquo;lies&rdquo; in the clock interface.&nbsp; We made slight modifications to the visualization to suggest participants had spoken more than they actually had.&nbsp; We found we could influence participation to a degree.&nbsp; Participants whose visualization suggested they spoke more spoke less. &nbsp;When modified above a threshold, participants felt the microphones in the system were faulty.</p>\n<p>&nbsp;</p>\n<p>Conversation Votes:&nbsp; Conversation Votes extended the Conversation Clock by allowing human annotation in addition to vocal contribution by voting on moments of significance.&nbsp; A precursor to the Facebook &ldquo;Like&rdquo; button, it is used in real time during a vocal conversation.&nbsp; We wanted to highlight participants that spoke less but had something important to say.&nbsp; Earlier work found those higher on a power hierarchy dominate conversation.&nbsp; We wanted to democratize participation.&nbsp; This tool resulted in similar vocal contribution behavior changes as the Conversation Clock. &nbsp;While we initially anticipated that participants who spoke less would vote more, we found that those who spoke more voted overwhelmingly more.&nbsp; In both the Conversation Clock and Conversation Votes, the table further served as a tool for indexing vocal conversation.&nbsp; Conversation Votes had the additional features of intentional annotation cues.</p>\n<p>&nbsp;</p>\n<p>Conversation Clusters:&nbsp; &nbsp;We created a tool that visualizes conversation topics spoken in groups.&nbsp; It shows when topics split, when they merge and who contributed topics.</p>\n<p>&nbsp;</p>\n<p>The Spoken Impact Project:&nbsp; We created a series of visualizations designed to encourage vocalization in children with Autism Spectrum Disorders (ASD). &nbsp;The first series was designed to increase speech-like and non-speech-like focalization. &nbsp;The second was designed to increase speech-like vocalization. We conducted a single subject multiple-baseline study over the course of a year with 5 children. &nbsp;Four out of the five children increased vocalization in short amounts of time throughout the course of the study. &nbsp;We followed up this study with a wizard-of-oz segment that showed rewards when there was incremental improvement. &nbsp;An unanticipated finding was that for 3 our of the 5 children, even though audio, video, and audio-video feedback was helpful, audio...",
  "por_txt_cntn": "\nThis work explored using visualization to emphasize and uncover cues in spoken language.  The work initially focused on two domains:  (1) visualizing small group conversation around a tabletop and (2) visualization to encourage vocalization in those diagnosed with Autism Spectrum Disorders (ASD).  We later explored cues in some written discourse.  We highlight the projects below.\n\n \n\nThe Conversation Clock:  The Conversation Clock explored small group dynamics about a tabletop. A visualization explicitly rendered participant contribution visible from a third person perspective to the group.  The system highlighted cues such as turn-taking, silence, domination, mimicry, \"yes\"-behavior, laughter, interruption and prosody. In a series of studies of this visualization in groups of 3-4 people, we found the visualization resulted in decreased participation of participants that previously spoke 'above average'and increased participation in participants who previously spoke 'below average'.  We found significant differences in how participants modified their vocal behavior with respects to number of turns and length of turns in conversation.\n\nWe placed this tool in three additional contexts:  (1)   to analyze and teach turn-taking skills in teenagers and adults with Asperger\u00c6s Syndrome.  We found that \"seeing\" conversation helped participants better understand goals.  (2)  to explore visual \"lies\" in the clock interface.  We made slight modifications to the visualization to suggest participants had spoken more than they actually had.  We found we could influence participation to a degree.  Participants whose visualization suggested they spoke more spoke less.  When modified above a threshold, participants felt the microphones in the system were faulty.\n\n \n\nConversation Votes:  Conversation Votes extended the Conversation Clock by allowing human annotation in addition to vocal contribution by voting on moments of significance.  A precursor to the Facebook \"Like\" button, it is used in real time during a vocal conversation.  We wanted to highlight participants that spoke less but had something important to say.  Earlier work found those higher on a power hierarchy dominate conversation.  We wanted to democratize participation.  This tool resulted in similar vocal contribution behavior changes as the Conversation Clock.  While we initially anticipated that participants who spoke less would vote more, we found that those who spoke more voted overwhelmingly more.  In both the Conversation Clock and Conversation Votes, the table further served as a tool for indexing vocal conversation.  Conversation Votes had the additional features of intentional annotation cues.\n\n \n\nConversation Clusters:   We created a tool that visualizes conversation topics spoken in groups.  It shows when topics split, when they merge and who contributed topics.\n\n \n\nThe Spoken Impact Project:  We created a series of visualizations designed to encourage vocalization in children with Autism Spectrum Disorders (ASD).  The first series was designed to increase speech-like and non-speech-like focalization.  The second was designed to increase speech-like vocalization. We conducted a single subject multiple-baseline study over the course of a year with 5 children.  Four out of the five children increased vocalization in short amounts of time throughout the course of the study.  We followed up this study with a wizard-of-oz segment that showed rewards when there was incremental improvement.  An unanticipated finding was that for 3 our of the 5 children, even though audio, video, and audio-video feedback was helpful, audio feedback was more powerful than the visual feedback.\n\n \n\nVCode/VData:  Analyzing the Spoken Impact Project proved quite challenging. Existing annotation tools lacked specific phonetic tools required in the coding and metrics for success.  We created an annotation tool called VCode/VData.  While designed for our project, it has since been downloaded over 10,..."
 }
}