{
 "awd_id": "0716245",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CT-ISG: Cross-Leveraging Cryptography with Learning Theory",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2007-09-01",
 "awd_exp_date": "2011-08-31",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 396236.0,
 "awd_min_amd_letter_date": "2007-08-27",
 "awd_max_amd_letter_date": "2010-08-31",
 "awd_abstract_narration": "This research involves a detailed study of the connections between Cryptography and Computational Learning Theory.  Cryptography is about manipulating information in order to achieve confidentiality, integrity, privacy, etc., while learning theory is about efficiently extracting information from some unknown object.  Learning theory provides a rigorous basis for the practically important field of machine learning, and cryptography plays a similar role for the crucial area of computer security.  In this research the investigators work to obtain new cryptographic results based on the presumed hardness of various problems in computational learning theory, and work to obtain new learning results via cryptography, thus extending and deepening the current understanding of both areas and the connections between them.  The research is integrated with a plan to achieve broader impact through education by developing an advanced course on the connections between cryptography and learning theory at Columbia University and advising and guiding a diverse group of graduate students in their development as researchers and educators.\r\n\r\nIn more detail, the research involves (i) constructing and applying new cryptographic primitives, such as public-key cryptosystems and pseudorandom generators with very low circuit complexity, from learning problems that are widely believed to be hard; (ii) continuing ongoing work on exploring the average-case learnability of various well-studied concept classes such as decision trees and DNF formulas; (iii) applying computational hardness of learning to establishing computational hardness of learning for various Boolean function classes, using tools from cryptography; (v) working to obtain computational separations between pairs of well-studied learning models by showing that learning problems that have polynomial-time algorithms in one model are intractable (under a cryptographic assumption) in the other model; and (vi) exploring the foundational issue of what are the minimal assumptions required to prove computational hardness of learning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rocco",
   "pi_last_name": "Servedio",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Rocco A Servedio",
   "pi_email_addr": "rocco@cs.columbia.edu",
   "nsf_id": "000232661",
   "pi_start_date": "2007-08-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tal",
   "pi_last_name": "Malkin",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Tal G Malkin",
   "pi_email_addr": "tal@cs.columbia.edu",
   "nsf_id": "000096938",
   "pi_start_date": "2007-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "615 W 131ST ST",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100277922",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "737100",
   "pgm_ele_name": "CYBER TRUST"
  },
  {
   "pgm_ele_code": "745600",
   "pgm_ele_name": "ITR-CYBERTRUST"
  },
  {
   "pgm_ele_code": "779500",
   "pgm_ele_name": "TRUSTWORTHY COMPUTING"
  },
  {
   "pgm_ele_code": "792700",
   "pgm_ele_name": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 175000.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 21236.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our project addressed the interface between cryptography and learning<br />theory. Roughly speaking, cryptography is about manipulating and<br />encoding information so that it is difficult to reconstruct the original<br />information (for instance, so that sensitive information like a credit<br />card number can be sent securely over the Internet).&nbsp; Learning theory is<br />about efficiently extracting information from an unknown object (such as<br />learning a underlying prediction rule that explains a large data set).<br />So learning theory and cryptography can be viewed as \"two sides of the<br />same coin\" -- if a scheme for processing information is<br />cryptographically secure then it is hard to learn the underlying<br />information, while on the other hand any time we have an efficient<br />learning algorithm it means that there is a provable violation of<br />cryptographic security.<br /><br />One of our major research findings was establishing a new connection<br />between cryptography and learnability for an important class of<br />functions that are widely studied in computer science, called \"monotone<br />functions\". A monotone function is one for which the output of the<br />function always increases whenever the input to the function increases.<br />So for example, the function f(x) = x^2 - 4x + 4 is not monotone<br />(because increasing the input from 1 to 2 causes the output of the<br />function to go from 1 to 0), but the function f(x) = 3x + 4 is monotone<br />(because increasing x always causes f(x) to increase).&nbsp; It's clear that<br />if a function is monotone then it is at least a little bit predictable,<br />since we know that increasing the input to such a function can only cause the<br />function to increase and never to decrease.&nbsp; But how much help does<br />monotonicity really give us if we are trying to learn an unknown function<br />to high accuracy?&nbsp; Are monotone functions always easy to learn?<br /><br />This is an important question because many real-world phenomena that<br />scientists are interested in modelling correspond to monotone functions.<br />For example, the probability of developing a disease might plausibly be<br />a monotone function of exposure to various environmental toxins (more<br />exposure means a higher likelihood of developing the disease), but we<br />don't know the precise nature of this relationship.&nbsp; If we had efficient<br />and general ways to learn relationships like this which correspond to<br />monotone functions, it could be a very useful tool.<br /><br />Versions of this question have been studied for quite a while in<br />learning theory, and preliminary evidence before our work suggested that<br />monotone functions might be much easier to learn than nonmonotone<br />functions.&nbsp; We used tools from cryptography to show that in fact, for<br />many learning problems monotonicity is not very helpful if we want to<br />learn an unknown function to high accuracy.&nbsp; To put it another way,<br />while monotoncity does provide a certain \"weak\" form of predictability<br />(because increasing inputs always imply increasing outputs), there are<br />relatively simple monotone functions which are almost as hard to learn<br />to high accuracy as nonmonotone functions.<br /><br />While this particular finding of ours was \"bad news\" in terms of coming<br />up with efficient learning algorithms, there are some bright sides.&nbsp; For<br />one thing, knowing what we *can't* do is important for guiding research<br />efforts -- our findings may save future researchers from wasting time on<br />to develop efficient learning algorithms for problems which we now know<br />provably don't have such algorithms.&nbsp; Another positive aspect is that<br />with our new understanding of how monotone functions can be<br />\"cryptographically hard\", we or other researchers may be able to build<br />ne...",
  "por_txt_cntn": "\nOur project addressed the interface between cryptography and learning\ntheory. Roughly speaking, cryptography is about manipulating and\nencoding information so that it is difficult to reconstruct the original\ninformation (for instance, so that sensitive information like a credit\ncard number can be sent securely over the Internet).  Learning theory is\nabout efficiently extracting information from an unknown object (such as\nlearning a underlying prediction rule that explains a large data set).\nSo learning theory and cryptography can be viewed as \"two sides of the\nsame coin\" -- if a scheme for processing information is\ncryptographically secure then it is hard to learn the underlying\ninformation, while on the other hand any time we have an efficient\nlearning algorithm it means that there is a provable violation of\ncryptographic security.\n\nOne of our major research findings was establishing a new connection\nbetween cryptography and learnability for an important class of\nfunctions that are widely studied in computer science, called \"monotone\nfunctions\". A monotone function is one for which the output of the\nfunction always increases whenever the input to the function increases.\nSo for example, the function f(x) = x^2 - 4x + 4 is not monotone\n(because increasing the input from 1 to 2 causes the output of the\nfunction to go from 1 to 0), but the function f(x) = 3x + 4 is monotone\n(because increasing x always causes f(x) to increase).  It's clear that\nif a function is monotone then it is at least a little bit predictable,\nsince we know that increasing the input to such a function can only cause the\nfunction to increase and never to decrease.  But how much help does\nmonotonicity really give us if we are trying to learn an unknown function\nto high accuracy?  Are monotone functions always easy to learn?\n\nThis is an important question because many real-world phenomena that\nscientists are interested in modelling correspond to monotone functions.\nFor example, the probability of developing a disease might plausibly be\na monotone function of exposure to various environmental toxins (more\nexposure means a higher likelihood of developing the disease), but we\ndon't know the precise nature of this relationship.  If we had efficient\nand general ways to learn relationships like this which correspond to\nmonotone functions, it could be a very useful tool.\n\nVersions of this question have been studied for quite a while in\nlearning theory, and preliminary evidence before our work suggested that\nmonotone functions might be much easier to learn than nonmonotone\nfunctions.  We used tools from cryptography to show that in fact, for\nmany learning problems monotonicity is not very helpful if we want to\nlearn an unknown function to high accuracy.  To put it another way,\nwhile monotoncity does provide a certain \"weak\" form of predictability\n(because increasing inputs always imply increasing outputs), there are\nrelatively simple monotone functions which are almost as hard to learn\nto high accuracy as nonmonotone functions.\n\nWhile this particular finding of ours was \"bad news\" in terms of coming\nup with efficient learning algorithms, there are some bright sides.  For\none thing, knowing what we *can't* do is important for guiding research\nefforts -- our findings may save future researchers from wasting time on\nto develop efficient learning algorithms for problems which we now know\nprovably don't have such algorithms.  Another positive aspect is that\nwith our new understanding of how monotone functions can be\n\"cryptographically hard\", we or other researchers may be able to build\nnew cryptographic schemes for securely encoding and communicating\ninformation using monotone functions.  Since monotone functions are\noften simpler or more efficient to compute than nonmonotone functions,\nthis could lead to simpler, faster and better schemes for\ncryptographically secure communication in the future.\n\n\t\t\t\t\tLast Modified: 02/23/2012\n\n\t\t\t\t\tSubmitted by: Rocco A Servedio"
 }
}