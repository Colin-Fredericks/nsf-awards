{
 "awd_id": "0720878",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CSR/PDOS: Concurrent, Direct Network Access: High-performance, Low-overhead Network I/O Virtualization",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Krishna Kant",
 "awd_eff_date": "2007-09-01",
 "awd_exp_date": "2011-08-31",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 461988.0,
 "awd_min_amd_letter_date": "2007-08-22",
 "awd_max_amd_letter_date": "2010-06-30",
 "awd_abstract_narration": "The economics of supporting a growing number of Internet-based applications has created a demand for server consolidation, and thus a resurgence of interest in virtualization.  Virtualization systems for commodity hardware virtualize processor, memory, and I/O devices in software.  Although this enables these systems to support a wide range of hardware, it also leads to significant overheads.\r\n\r\nWe have developed Concurrent, Direct Network Access (CDNA), a new I/O virtualization architecture combining software and hardware that reduces the overhead of network virtualization.  This architecture provides untrusted virtual machines safe, direct access to the network interface.  While CDNA dramatically improves the efficiency of I/O virtualization, a notable gap between native and virtualized I/O performance still exists.\r\n\r\nThis project's objective is to eliminate this performance gap without sacrificing the generality and manageability of software-based I/O virtualization.  This leads to three main thrusts.  First, we are working to mitigate the remaining overheads of the CDNA I/O virtualization architecture, which include memory protection and the scheduling of interrupts.  Second, we are developing mechanisms to support full virtualization, memory protection using IOMMU hardware, and protected DMA by a conventional NIC.  This will improve the generality of the CDNA I/O virtualization architecture.  Finally, we are developing mechanisms to enable migration among systems with and without CDNA and to provide mechanisms for network resource provisioning.  This will facilitate system managament for virtualized servers using the CDNA I/O virtualization architecture.  Altogether, this research will make CDNA a complete I/O virtualization solution that provides efficient, general, and manageable I/O virtualization.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Cox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan Cox",
   "pi_email_addr": "alc@rice.edu",
   "nsf_id": "000277789",
   "pi_start_date": "2007-08-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Scott",
   "pi_last_name": "Rixner",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Scott Rixner",
   "pi_email_addr": "rixner@rice.edu",
   "nsf_id": "000102366",
   "pi_start_date": "2007-08-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "6100 MAIN ST",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 250000.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 6000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 5988.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In many organizations, the economics of supporting a growing number of<br />Internet-based application services has created a demand for server<br />consolidation.&nbsp; Consequently, there has been a resurgence of interest<br />in machine virtualization.&nbsp; A virtual machine monitor (VMM) enables<br />multiple virtual machines, each encapsulating one or more application<br />services, to share the same physical machine safely and fairly.&nbsp; It<br />provides isolation between the virtual machines and manages their<br />access to hardware resources.</p>\n<p>Modern VMMs for commodity hardware, such as VMWare and Xen, can<br />virtualize processors, memory, and I/O devices in software.&nbsp; Such<br />software-based approaches enable these VMMs to support a wide range of<br />hardware, but they also lead to significant performance overheads.&nbsp; To<br />reduce the performance overhead of processor and memory<br />virtualization, both AMD and Intel have introduced hardware<br />virtualization support that reduces the frequency and duration of<br />calls into the VMM.&nbsp; However, there has been no similar consensus on<br />how to improve the efficiency of I/O virtualization, because existing<br />hardware-based approaches sacrifice the generality and manageability<br />of software-based I/O virtualization.</p>\n<p>The overall objective of this project has been to advance the state-<br />of-the-art in I/O virtualization.&nbsp; To this end, we have worked both on<br />developing new approaches to I/O virtualization and on developing new<br />mechanisms to support existing approaches.&nbsp; At the same time, we have<br />looked at problems that are common to all of these approaches, such as<br />how I/O virtualization interacts with virtual machine scheduling.</p>\n<p>* New approaches</p>\n<p>Several VMMs --- including Xen, L4, and Microsoft Hyper-V --- use the<br />driver domain model to virtualize I/O devices.&nbsp; The driver domain is a<br />virtual machine that runs a largely unmodified operating system.<br />Consequently, it is able to use all of the device drivers that are<br />available for that operating system.&nbsp; This greatly simplifies the<br />complexity of providing support for a wide variety of devices in a<br />virtualized environment.&nbsp; In addition, the driver domain model<br />provides a safe execution environment for physical device drivers,<br />enabling improved fault isolation over alternative models that locate<br />device drivers in the VMM.&nbsp; However, while the driver domain provides<br />several benefits, it also incurs significant performance overheads.<br />Previous work by our collaborators at HP labs has argued that these<br />overheads can be eliminated, in part through the use of a new<br />generation of commodity NICs that support multiple transmit and<br />receive queues for packets.&nbsp; This project has fully realized that<br />vision, contributing a complete design and implementation of<br />multi-queue NIC support for the driver domain model in Xen.</p>\n<p>* New mechanisms</p>\n<p>Xen's mechanism for memory sharing and protection, called the grant<br />mechanism, is used to share I/O buffers in guest virtual machines'<br />memory with a driver domain.&nbsp; Previous studies have identified the<br />grant mechanism as a significant source of network I/O overhead in<br />Xen.&nbsp; We have developed a redesigned grant mechanism to significantly<br />reduce the associated overheads.&nbsp; Unlike the original grant mechanism,<br />the new mechanism allows guest domains to unilaterally issue and revoke<br />grants.&nbsp; As a result, the new mechanism makes it simple for the guest<br />OS to reduce the number of grant issue and revoke operations that are<br />needed for I/O by taking advantage of temporal and/or spatial locality<br />in its use of I/O buffers.</p>\n<p>* Common problems</p>\n<p>We explored the relationship be...",
  "por_txt_cntn": "\nIn many organizations, the economics of supporting a growing number of\nInternet-based application services has created a demand for server\nconsolidation.  Consequently, there has been a resurgence of interest\nin machine virtualization.  A virtual machine monitor (VMM) enables\nmultiple virtual machines, each encapsulating one or more application\nservices, to share the same physical machine safely and fairly.  It\nprovides isolation between the virtual machines and manages their\naccess to hardware resources.\n\nModern VMMs for commodity hardware, such as VMWare and Xen, can\nvirtualize processors, memory, and I/O devices in software.  Such\nsoftware-based approaches enable these VMMs to support a wide range of\nhardware, but they also lead to significant performance overheads.  To\nreduce the performance overhead of processor and memory\nvirtualization, both AMD and Intel have introduced hardware\nvirtualization support that reduces the frequency and duration of\ncalls into the VMM.  However, there has been no similar consensus on\nhow to improve the efficiency of I/O virtualization, because existing\nhardware-based approaches sacrifice the generality and manageability\nof software-based I/O virtualization.\n\nThe overall objective of this project has been to advance the state-\nof-the-art in I/O virtualization.  To this end, we have worked both on\ndeveloping new approaches to I/O virtualization and on developing new\nmechanisms to support existing approaches.  At the same time, we have\nlooked at problems that are common to all of these approaches, such as\nhow I/O virtualization interacts with virtual machine scheduling.\n\n* New approaches\n\nSeveral VMMs --- including Xen, L4, and Microsoft Hyper-V --- use the\ndriver domain model to virtualize I/O devices.  The driver domain is a\nvirtual machine that runs a largely unmodified operating system.\nConsequently, it is able to use all of the device drivers that are\navailable for that operating system.  This greatly simplifies the\ncomplexity of providing support for a wide variety of devices in a\nvirtualized environment.  In addition, the driver domain model\nprovides a safe execution environment for physical device drivers,\nenabling improved fault isolation over alternative models that locate\ndevice drivers in the VMM.  However, while the driver domain provides\nseveral benefits, it also incurs significant performance overheads.\nPrevious work by our collaborators at HP labs has argued that these\noverheads can be eliminated, in part through the use of a new\ngeneration of commodity NICs that support multiple transmit and\nreceive queues for packets.  This project has fully realized that\nvision, contributing a complete design and implementation of\nmulti-queue NIC support for the driver domain model in Xen.\n\n* New mechanisms\n\nXen's mechanism for memory sharing and protection, called the grant\nmechanism, is used to share I/O buffers in guest virtual machines'\nmemory with a driver domain.  Previous studies have identified the\ngrant mechanism as a significant source of network I/O overhead in\nXen.  We have developed a redesigned grant mechanism to significantly\nreduce the associated overheads.  Unlike the original grant mechanism,\nthe new mechanism allows guest domains to unilaterally issue and revoke\ngrants.  As a result, the new mechanism makes it simple for the guest\nOS to reduce the number of grant issue and revoke operations that are\nneeded for I/O by taking advantage of temporal and/or spatial locality\nin its use of I/O buffers.\n\n* Common problems\n\nWe explored the relationship between virtual machine scheduling and\nI/O performance.  Traditionally, VMM schedulers have focused on fairly\nsharing the processor resources among virtual machines while leaving\nthe scheduling of I/O resources as a secondary concern.  However, this\ncan result in poor and/or unpredictable application performance,\nmaking virtualization less desirable for applications that require\nefficient and consistent I/O behavior.  This project wa..."
 }
}