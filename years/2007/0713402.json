{
 "awd_id": "0713402",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Collaborative Research: Discriminative Knowledge-Rich Language Modeling for Machine Translation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2007-09-01",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 325216.0,
 "awd_amount": 390214.0,
 "awd_min_amd_letter_date": "2007-08-29",
 "awd_max_amd_letter_date": "2010-09-21",
 "awd_abstract_narration": "This project investigates a novel approach for assessing the fluency and\r\ngrammaticality of alternative translation hypotheses that are created within\r\nsearch-based Machine Translation (MT) systems.  This task, commonly termed\r\n\"Language Modeling\" (LM), has been explored primarily in the context of speech\r\nrecognition; however, current state-of-the-art language models (LMs) are not\r\neffective at distinguishing between more fluent grammatical translations and\r\ntheir poor alternatives.  In contrast, the proposed approach, \"Discriminative\r\nKnowledge-Rich Language Modeling\" (DKRLM), is explicitly designed to find the\r\nmost fluent and grammatical translations within the search space by comparing\r\nthe linguistic features of the translation hypotheses against very large\r\n\"clean\" monolingual corpora. The intuition is that more grammatical\r\ntranslation hypotheses should contain higher proportions of features seen in\r\nthe large corpora.  An important contribution of the project is in exploring\r\ndifferent types of linguistic features to identify those that are most\r\ninformative for the comparisons.  Moreover, discriminative training is\r\nperformed to incorporate the features into a system-independent scoring\r\nfunction, replacing traditional LMs in MT systems.  The broader impacts of the\r\nproposed work include both broader adoption for the methodology as well as\r\nwider use of the new DKRLM functions to other search-based NLP applications\r\nthat aim at generating fluent grammatical text.  This includes search-based\r\napproaches to Speech Recognition, Natural Language Generation (NLG), Optical\r\nCharacter Recognition (OCR), Summarization, and others.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alon",
   "pi_last_name": "Lavie",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alon Lavie",
   "pi_email_addr": "alavie@cs.cmu.edu",
   "nsf_id": "000215959",
   "pi_start_date": "2007-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "801000",
   "pgm_ele_name": "Computing in the Cloud"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8010",
   "pgm_ref_txt": "Computing in the Cloud"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 107525.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 106412.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 111279.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 64998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>There has been dramatic progress in the accuacy and fluency of fully-automated language translation systems over the past decade. &nbsp;State-of-the-art Machine Translation (MT) has increasingly become an essential technology in the service of governments and global enterprises in recent years. &nbsp;Yet in many instances and application scenarios, the accuracy, and particularly the fluency, of MT is still far from human levels. &nbsp;The main goal of this research project &nbsp;was to explore novel ways for leveraging information about the occurance patterns and likelihood of sequences of words, as they are found in web-scale monolonigual corpora, in order to improve the accuracy and fluency of ssuch MT systems.</p>\n<p>The first two years of the project focused on constructing a classification framework for identifying ungrammatical and disfluent points in MT-generated translations, based on a variety of linguistic features and on occurrence statistics for 'n-grams' - sequences of words that can be observed in large-scale human-generated text corpora.</p>\n<p>The second half of the project focused on applying advanced n-gram statistics for improving MT system combination, and investigated the feasibility of incorporating the Microsoft Web-scale N-gram service as an additional source of information within MT system combination. In collaboration with colleagues at the University of Pittsburgh, we also explored identifying and improving translation of \"difficult-to-translate\" phrases within sentences.</p>\n<p>The results and outcomes of this project were published in several publications at premier conferences in the field of Computational Linguistics and Natural Language Processing (NLP). &nbsp;</p>\n<p>In a closely related project, a PhD student partially supported by this project developed KenLM - a new highly optimized language modeling software package for MT and other statsitical language applications. KenLM has quickly become the defacto standard language modeling software toolkit used by the majority of the research and commercial NLP development communities.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/05/2015<br>\n\t\t\t\t\tModified by: Alon&nbsp;Lavie</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThere has been dramatic progress in the accuacy and fluency of fully-automated language translation systems over the past decade.  State-of-the-art Machine Translation (MT) has increasingly become an essential technology in the service of governments and global enterprises in recent years.  Yet in many instances and application scenarios, the accuracy, and particularly the fluency, of MT is still far from human levels.  The main goal of this research project  was to explore novel ways for leveraging information about the occurance patterns and likelihood of sequences of words, as they are found in web-scale monolonigual corpora, in order to improve the accuracy and fluency of ssuch MT systems.\n\nThe first two years of the project focused on constructing a classification framework for identifying ungrammatical and disfluent points in MT-generated translations, based on a variety of linguistic features and on occurrence statistics for 'n-grams' - sequences of words that can be observed in large-scale human-generated text corpora.\n\nThe second half of the project focused on applying advanced n-gram statistics for improving MT system combination, and investigated the feasibility of incorporating the Microsoft Web-scale N-gram service as an additional source of information within MT system combination. In collaboration with colleagues at the University of Pittsburgh, we also explored identifying and improving translation of \"difficult-to-translate\" phrases within sentences.\n\nThe results and outcomes of this project were published in several publications at premier conferences in the field of Computational Linguistics and Natural Language Processing (NLP).  \n\nIn a closely related project, a PhD student partially supported by this project developed KenLM - a new highly optimized language modeling software package for MT and other statsitical language applications. KenLM has quickly become the defacto standard language modeling software toolkit used by the majority of the research and commercial NLP development communities.\n\n\t\t\t\t\tLast Modified: 06/05/2015\n\n\t\t\t\t\tSubmitted by: Alon Lavie"
 }
}