{
 "awd_id": "0713698",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HRI: Learning Mixed-Initiative Dialogue Strategies",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2007-09-15",
 "awd_exp_date": "2011-08-31",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 472038.0,
 "awd_min_amd_letter_date": "2007-09-07",
 "awd_max_amd_letter_date": "2010-05-18",
 "awd_abstract_narration": "This research project enables next generation dialogue systems to be able to collaborate with a user without the limitations of system-initiative interaction, in order to solve complex tasks in an optimal manner. The research develops reinforcement learning (RL) strategies to learn dialogue policies that are mixed-initiative. The specific aims of this are to (a) extend RL to mixed-initiative dialogue interaction; (b) allow the system policy to adapt to different user types, such as people with poor memory, or poor problem-solving skills; and (c) simultaneously learn the policy for the simulated user. \r\n\r\nThis approach will allow more advanced dialogue systems to be deployed, such as assisting the elderly so they can live independently longer, and helping provide health care information to rural areas. The proposed research project will result in a toolkit that will allow a wide range of users to easily develop dialogue policies. The toolkit will (a) allow students to be effectively trained in this area, (b) lower the barrier for other researchers to contribute to the field, and (c) help transfer this new technology to industry. \r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Heeman",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Peter A Heeman",
   "pi_email_addr": "heemanp@ohsu.edu",
   "nsf_id": "000171635",
   "pi_start_date": "2007-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon Health & Science University",
  "inst_street_address": "3181 SW SAM JACKSON PARK RD",
  "inst_street_address_2": "",
  "inst_city_name": "PORTLAND",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5034947784",
  "inst_zip_code": "972393011",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "OR01",
  "org_lgl_bus_name": "OREGON HEALTH & SCIENCE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPSNT86JKN51"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon Health & Science University, West Campus",
  "perf_str_addr": "20000 N.W. Walker Road",
  "perf_city_name": "Beaverton",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "970068921",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "OR01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 152289.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 296749.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 13750.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 9250.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Project Outcomes Report</strong><br /><br />The long-term goal of this research project is to enable next generation dialogue systems that will be able to collaborate with a user without the limitations of system-initiative interaction, in order to solve complex tasks in an optimal manner. &nbsp;Our previous work has been directed towards this goal by allowing reinforcement learning (RL) to use a well-defined formalism for expressing the preconditions and effects of actions that the system will take, and by allowing the dialogue policy to be learned using a simulated user, which is simultaneously learned with RL. &nbsp;The research objective of this proposal is to further develop the methodology to allow RL to learn dialogue policies that are mixed-initiative.&nbsp;<br /><br /><strong>Intellectual Merit:</strong><br /><br />Most applications of RL for dialogue management focus on form-filling dialogues, in which the system just has to have the user provide answers to a set number of fixed questions. &nbsp;To extend the use of RL, we investigated a task that requires collaboration, in which the user and system each have a set of preferences, and must find the solution that best fits these preferences. &nbsp;In order for RL to learn an optimal policy, we constructed the state to include both action &nbsp; selection variables and bookkeeping variables. &nbsp;The bookkeeping variables track how the user and system reached agreement, which allows RL to properly propagate the costs of reaching an agreement back to the appropriate state-action pairs.<br /><br />Turn-taking is a major component of mixed-initiative interaction. &nbsp;We proposed a new approach to turn-taking for spoken dialogue systems, based on evidence of how people manage turn-taking. After each utterance, the system and user bid for making the next utterance. &nbsp;We used reinforcement learning to determine a system policy for both what the system should say and how much it should bid for the turn. &nbsp;The use of RL allows the system's bid to be based on how important the system believes its proposed turn is terms of finishing the dialogue. &nbsp;We also proposed an extension of this model in which we also separated attention actions, which determine what the system will talk about next, and learned all three action types using RL.<br /><br />A key part of building a dialogue system is to determine what mechanisms the system needs to use to participate in a dialogue, such as turn-taking, showing understanding, and how to repair mistakes. &nbsp;We investigated how the dialogue of children with typical development (TD) differs from that of children with Developmental Language Disorder (DLD) and Autism Spectrum Disorder (ASD). &nbsp;Children with ASD have social and language impairments. &nbsp;Children with DLD tend to display similar language deficits as children with ASD, but without social impairments. &nbsp;We found that children with ASD use the filler `uh' at the same rate as children with DLD and TD; while using `um' at a much lower rate. &nbsp;We also found that children with TD and DLD were more likely to produce a pause after `um' then were the children with ASD. &nbsp;This work suggests that `um' might be a result of social reasoning (a dialogue coordination mechanism), while `uh' might be due to speaker processing, and not intended for the other conversant.<br /><br /><strong>Broader Impacts:</strong><br /><br />This grant partially funded two Ph.D. students, both U.S. citizens, one who is now in his fourth year, and a second who is about to graduate. Two supplements to this grant (Research Experiences for Undergraduates) enabled two undergraduate students to become involved in the research project, one student for one summer, and a second for two summers.<br /><br />The PI created a lecture sequence on using Reinforcement Learning with Information State Update (ISU) rules, along wit...",
  "por_txt_cntn": "\nProject Outcomes Report\n\nThe long-term goal of this research project is to enable next generation dialogue systems that will be able to collaborate with a user without the limitations of system-initiative interaction, in order to solve complex tasks in an optimal manner.  Our previous work has been directed towards this goal by allowing reinforcement learning (RL) to use a well-defined formalism for expressing the preconditions and effects of actions that the system will take, and by allowing the dialogue policy to be learned using a simulated user, which is simultaneously learned with RL.  The research objective of this proposal is to further develop the methodology to allow RL to learn dialogue policies that are mixed-initiative. \n\nIntellectual Merit:\n\nMost applications of RL for dialogue management focus on form-filling dialogues, in which the system just has to have the user provide answers to a set number of fixed questions.  To extend the use of RL, we investigated a task that requires collaboration, in which the user and system each have a set of preferences, and must find the solution that best fits these preferences.  In order for RL to learn an optimal policy, we constructed the state to include both action   selection variables and bookkeeping variables.  The bookkeeping variables track how the user and system reached agreement, which allows RL to properly propagate the costs of reaching an agreement back to the appropriate state-action pairs.\n\nTurn-taking is a major component of mixed-initiative interaction.  We proposed a new approach to turn-taking for spoken dialogue systems, based on evidence of how people manage turn-taking. After each utterance, the system and user bid for making the next utterance.  We used reinforcement learning to determine a system policy for both what the system should say and how much it should bid for the turn.  The use of RL allows the system's bid to be based on how important the system believes its proposed turn is terms of finishing the dialogue.  We also proposed an extension of this model in which we also separated attention actions, which determine what the system will talk about next, and learned all three action types using RL.\n\nA key part of building a dialogue system is to determine what mechanisms the system needs to use to participate in a dialogue, such as turn-taking, showing understanding, and how to repair mistakes.  We investigated how the dialogue of children with typical development (TD) differs from that of children with Developmental Language Disorder (DLD) and Autism Spectrum Disorder (ASD).  Children with ASD have social and language impairments.  Children with DLD tend to display similar language deficits as children with ASD, but without social impairments.  We found that children with ASD use the filler `uh' at the same rate as children with DLD and TD; while using `um' at a much lower rate.  We also found that children with TD and DLD were more likely to produce a pause after `um' then were the children with ASD.  This work suggests that `um' might be a result of social reasoning (a dialogue coordination mechanism), while `uh' might be due to speaker processing, and not intended for the other conversant.\n\nBroader Impacts:\n\nThis grant partially funded two Ph.D. students, both U.S. citizens, one who is now in his fourth year, and a second who is about to graduate. Two supplements to this grant (Research Experiences for Undergraduates) enabled two undergraduate students to become involved in the research project, one student for one summer, and a second for two summers.\n\nThe PI created a lecture sequence on using Reinforcement Learning with Information State Update (ISU) rules, along with a 5 homework sequence, building hand-crafted system policies with an ISU engine, augmenting the ISU engine to simulate dialogues between a user and a system, augmenting the ISU engine to learn the system dialogue policy using RL, and using the resulting RL-ISU toolkit to expe..."
 }
}