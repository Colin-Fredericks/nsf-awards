{
 "awd_id": "0725357",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  Applying Hardware-Inspired Methods for Multi-Core Software Design",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2007-10-01",
 "awd_exp_date": "2010-09-30",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 462494.0,
 "awd_min_amd_letter_date": "2007-08-06",
 "awd_max_amd_letter_date": "2010-01-27",
 "awd_abstract_narration": "0725350\r\nCollaborative Research: Applying Hardware-Inspired Methods for Multi- Core Software Design Brian C. Demsky University of California, Irvine\r\n\r\n0725357\r\nCollaborative Research: Applying Hardware-Inspired Methods for Multi- Core Software Design Michael B. Taylor University of California, San Diego\r\n\r\nIn the past, improvements in microprocessor capabilities were expressed largely through a combination of clock frequency increases and microarchitectural enhancements that were invisible to the typical developer. More recently, due to power and microarchitectural scalability issues, microprocessor designs have diverged from this path and have begun to focus on exposing improved semiconductor process capabilities through the multi-core abstraction, which integrates multiple independent processors into a single chip. The deployment of such explicitly-parallel multi-core processors has deep implications on the future of software systems. While parallel software has been largely unnecessary in desktop systems, it will become essential if we are to expect continued increases in software functionality and programmer productivity like those that society has enjoyed over the last 35 years.\r\n\r\nThis research investigates a new design methodology for developing the parallel software systems that are necessary to take advantage of multi-core processors. This methodology leverages concepts from hardware chip-design methodologies, which scale to millions of communicating parallel entities.  This new design process enables the software developer to create flexible system designs that easily accommodate refinement of how the computation is realized. It does this by separating the functional design of the software system from the specification of how to organize the computation.  To validate this new design methodology, the research project investigates the construction of synthesis and profiling tools that can be used to develop and refine these functional and organizational specifications. These specifications are in turn used to create an executable that is optimized for the specific multi-core microprocessor.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Taylor",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Michael B Taylor",
   "pi_email_addr": "profmbt@cs.washington.edu",
   "nsf_id": "000069038",
   "pi_start_date": "2007-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "765200",
   "pgm_ele_name": "SCIENCE OF DESIGN"
  },
  {
   "pgm_ele_code": "794300",
   "pgm_ele_name": "PROGRAMMING LANGUAGES"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 446494.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As multicore processors enter mainstream computing, program parallelization is getting more attention. Although many tools have been proposed to help programmers parallelize their code, one important question has been largely overlooked: &rdquo;which parts of the program should I spend time parallelizing?&rdquo;</p>\n<p><br />Profilers such as gprof provide a solution to a similar problem in the domain of serial optimization. A typical profiler produces a list of regions ordered by their work coverage (we call it a &rdquo;plan&rdquo;) as a region with larger work is likely to bring higher benefit from optimization, guiding to more effective optimization activities.</p>\n<p><br />In this work, we created a prototype for Kremlin, a profiling tool for par- allelization. Kremlin adopts the time-tested gprof usage model. Whereas gprof uses the work coverage of a region as a metric, Kremlin uses the program speedup as a metric. According to Moore&rsquo;s law, we can calculate the program speedup upon a region parallelization if region-localized parallelism and work coverage of the region is known. Region-localized parallelism represents the parallelism available in a region excluding the parallelism originated from its subregions.</p>\n<p>&nbsp;</p>\n<p>The major challenge in the design of Kremlin is the lack of a technique to localize parallelism to a region. Conventional critical path analysis (CPA)<br />termine the overall parallelism in a program, but it cannot localize the parallelism to a specific region. Kremlin overcomes this prob- lem by employing a new technique called hierarchical critical path analysis (HCPA). Based on the region hierarchy of a program and CPA results for each region, HCPA extracts region-localized parallelism.</p>\n<p><br />From our preliminary evaluation with NAS Parallel Bench (NPB), a parallelization guided by Kremlin achieves a com- parable speedup to manual parallelization with a smaller number of parallelized regions, effectively reducing a programmer&rsquo;s efforts without sacrificing the quality of parallelization. A user study also indicates parallelization can be more effective with Kremlin.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/12/2011<br>\n\t\t\t\t\tModified by: Michael&nbsp;B&nbsp;Taylor</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2011/0725357/0725357_10023319_1294820698745_overview--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2011/0725357/0725357_10023319_1294820698745_overview--rgov-800width.jpg\" title=\"Kremlin Design\"><img src=\"/por/images/Reports/POR/2011/0725357/0725357_10023319_1294820698745_overview--rgov-66x44.jpg\" alt=\"Kremlin Design\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The internal workings of the Kremlin tool.</div>\n<div class=\"imageCredit\">UCSD</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Michael&nbsp;B&nbsp;Taylor</div>\n<div class=\"imageTitle\">Kremlin Design</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nAs multicore processors enter mainstream computing, program parallelization is getting more attention. Although many tools have been proposed to help programmers parallelize their code, one important question has been largely overlooked: \"which parts of the program should I spend time parallelizing?\"\n\n\nProfilers such as gprof provide a solution to a similar problem in the domain of serial optimization. A typical profiler produces a list of regions ordered by their work coverage (we call it a \"plan\") as a region with larger work is likely to bring higher benefit from optimization, guiding to more effective optimization activities.\n\n\nIn this work, we created a prototype for Kremlin, a profiling tool for par- allelization. Kremlin adopts the time-tested gprof usage model. Whereas gprof uses the work coverage of a region as a metric, Kremlin uses the program speedup as a metric. According to Moore\u00c6s law, we can calculate the program speedup upon a region parallelization if region-localized parallelism and work coverage of the region is known. Region-localized parallelism represents the parallelism available in a region excluding the parallelism originated from its subregions.\n\n \n\nThe major challenge in the design of Kremlin is the lack of a technique to localize parallelism to a region. Conventional critical path analysis (CPA)\ntermine the overall parallelism in a program, but it cannot localize the parallelism to a specific region. Kremlin overcomes this prob- lem by employing a new technique called hierarchical critical path analysis (HCPA). Based on the region hierarchy of a program and CPA results for each region, HCPA extracts region-localized parallelism.\n\n\nFrom our preliminary evaluation with NAS Parallel Bench (NPB), a parallelization guided by Kremlin achieves a com- parable speedup to manual parallelization with a smaller number of parallelized regions, effectively reducing a programmer\u00c6s efforts without sacrificing the quality of parallelization. A user study also indicates parallelization can be more effective with Kremlin.\n\n\t\t\t\t\tLast Modified: 01/12/2011\n\n\t\t\t\t\tSubmitted by: Michael B Taylor"
 }
}