{
 "awd_id": "0724271",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase II:   Robust Speech-to-Text Messaging",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Glenn H. Larsen",
 "awd_eff_date": "2007-09-01",
 "awd_exp_date": "2010-08-31",
 "tot_intn_awd_amt": 0.0,
 "awd_amount": 716000.0,
 "awd_min_amd_letter_date": "2007-08-30",
 "awd_max_amd_letter_date": "2010-05-26",
 "awd_abstract_narration": "This Small Business Innovation Research (SBIR) Phase II research project proposes to develop techniques for the hands-free input of text to mobile devices. Specifically, this project extends the results of the Phase I effort to produce a speech-recognition system for mobile devices and personal appliances that is robust in the presence of background noise. To increase the speech recognition accuracy, four techniques are employed: 1) Spellation where the users have to speak and partially spell the words as they dictate, 2) VoiceTap which requires that, for each character, the user says that character and the following character in the alphabet, 3) Voice Predict where the user has to say the word and input the first character of the word using the keyboard or VoiceTap, and 4) multi-modal speech to text, where the user speaks and uses the keyboard simultaneously. The research effort will focus on developing modules that allow speech to be dictated using a combination of whole words and spelled words. \r\n\r\nThe outcome of the proposed research has significant commercial potential. Because the front end or client-side can be ported to a variety of operating systems and processors, the flexibility of this technology should enable wide licensing of the technology to telecommunication device manufacturers. The mobile wireless industry is very large and growing industry, and  multi-modal input technology is important to mobile customers who demand more efficient and accurate methods for communication. Improvements in accuracy could be very significant and would potentially have widespread applicability. \r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ashwin",
   "pi_last_name": "Rao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ashwin Rao",
   "pi_email_addr": "ashwin@travellingwave.com",
   "nsf_id": "000064962",
   "pi_start_date": "2007-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "TravellingWave",
  "inst_street_address": "13617 88TH PL NE",
  "inst_street_address_2": "",
  "inst_city_name": "KIRKLAND",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "4252736933",
  "inst_zip_code": "980341710",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "WA01",
  "org_lgl_bus_name": "TRAVELLINGWAVE INC",
  "org_prnt_uei_num": "",
  "org_uei_num": "H292KLASH9D3"
 },
 "perf_inst": {
  "perf_inst_name": "TravellingWave",
  "perf_str_addr": "13617 88TH PL NE",
  "perf_city_name": "KIRKLAND",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "980341710",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "WA01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "148000",
   "pgm_ele_name": "ERC-Eng Research Centers"
  },
  {
   "pgm_ele_code": "537300",
   "pgm_ele_name": "SBIR Phase II"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "124E",
   "pgm_ref_txt": "CENTERS: BIOENG & HEALTH CARE"
  },
  {
   "pgm_ref_code": "1480",
   "pgm_ref_txt": "ENGINEERING RESEARCH CENTERS"
  },
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "5373",
   "pgm_ref_txt": "SMALL BUSINESS PHASE II"
  },
  {
   "pgm_ref_code": "9107",
   "pgm_ref_txt": "BIOELECTRONICS AND BIONETWORKS"
  },
  {
   "pgm_ref_code": "9139",
   "pgm_ref_txt": "INFORMATION INFRASTRUCTURE & TECH APPL"
  },
  {
   "pgm_ref_code": "9231",
   "pgm_ref_txt": "SUPPL FOR UNDERGRAD RES ASSIST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "BIOT",
   "pgm_ref_txt": "BIOTECHNOLOGY"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 200000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>VOICEPREDICT, A MULTIMODAL TEXT INPUT THAT COMBINES VOICE AND TOUCH</p>\n<p><strong>Background</strong></p>\n<p>Facilitating text input into computers and handheld devices is a work in progress. Well-known solutions include mobile triple-tapping, ambiguous/unambiguous text prediction, mini-qwerty keyboards, on-screen soft-key displays, and handwriting/gesture recognition. In theory, speech-to-text would be a natural alternative: if one could simply speak into their computer or device and have the text magically appear on the screen.</p>\n<p>Unfortunately, speech-to-text has been historically plagued with problems, including infinite language perplexity, background and channel noises, varied pronunciations, unacceptable speaker-training methods, and lack of intuitive error-correction. As a result, speech-to-text continues to be a futuristic technology; except for specialized applications wherein the lexicon is fairly compressed as in call-center automation.</p>\n<p><strong>Multimodal Technology Underlying VoicePredict</strong></p>\n<p>TravellingWave has taken an innovative approach that combines redundant information from multiple modes, namely the keyboard and the microphone, to significantly enhance accuracies of both voice recognition and text prediction. Specifically, Voice Powered Text Prediction or VoicePredict technology predicts words using the speech rendered by a user in addition to the letters inputted by the user; traditional predictive text input systems rely only on letters; speech-to-text systems rely on speech only.</p>\n<p><strong>VoicePredict System Components</strong></p>\n<p>The following components form the basis for the VoicePredict system are now briefly described.</p>\n<p><strong>Frequency Localized Temporal processing</strong></p>\n<p>At the very front-end of VoicePredict lies TravellingWave's proprietary signal processing module called RAGs Algorithm (Rao-Aronov-Garafutdinov speech processing algorithm). It is based on published research on compact features modeling the traveling wave phenomena in the human cochlea.</p>\n<p>RAGs extracts modulation information from speech, as opposed to traditional power spectrum analysis. For example, instead of relying on the spectral energy envelope, RAGs computes the locations of several resonances (similar to speech formants); their slowly varying temporal trajectories; rich modulations coding the harmonics around those resonances; local bandwidths, syllable onset and offset times; durations of phones and other proprietary acoustic-phonetic features.</p>\n<p>RAGs output is then employed by the acoustic-phonetic models to make decisions about word prediction. Overall, this enables VoicePredict system to perform reliably in a variety of noisy environments.</p>\n<p><strong>Acoustic Modeling</strong></p>\n<p>VoicePredict combines traditional acoustic modeling techniques (based on modeling phonemes using statistical models) with acoustic-phonetic modeling. In VoicePredict, the latter incorporates spectrally localized temporal features, in conjunction with features like phonetic durations, syllable boundaries and formant energies.</p>\n<p><strong>Language Modeling</strong></p>\n<p>VoicePredict adapts its language model based on the frequency of word usage. New words that are not in the large built-in dictionary (tens of thousands of words) are learnt on the fly. Currently VoicePredict employs unigram language models; meaning VoicePredict does not rely on a sentence context. The unigram modeling techniques make use of VoicePredict's inherent multimodality, resulting in an extremely robust language model.</p>\n<p>In this project, the Frequency Localized Temporal processing module was first developed and integrated with the overall VoicePredict system; by the TravellingWave researchers. &nbsp;A software development kit (SDK) was sunsequently designed and developed for the overall system. This SDK was then integrated by researc...",
  "por_txt_cntn": "\nVOICEPREDICT, A MULTIMODAL TEXT INPUT THAT COMBINES VOICE AND TOUCH\n\nBackground\n\nFacilitating text input into computers and handheld devices is a work in progress. Well-known solutions include mobile triple-tapping, ambiguous/unambiguous text prediction, mini-qwerty keyboards, on-screen soft-key displays, and handwriting/gesture recognition. In theory, speech-to-text would be a natural alternative: if one could simply speak into their computer or device and have the text magically appear on the screen.\n\nUnfortunately, speech-to-text has been historically plagued with problems, including infinite language perplexity, background and channel noises, varied pronunciations, unacceptable speaker-training methods, and lack of intuitive error-correction. As a result, speech-to-text continues to be a futuristic technology; except for specialized applications wherein the lexicon is fairly compressed as in call-center automation.\n\nMultimodal Technology Underlying VoicePredict\n\nTravellingWave has taken an innovative approach that combines redundant information from multiple modes, namely the keyboard and the microphone, to significantly enhance accuracies of both voice recognition and text prediction. Specifically, Voice Powered Text Prediction or VoicePredict technology predicts words using the speech rendered by a user in addition to the letters inputted by the user; traditional predictive text input systems rely only on letters; speech-to-text systems rely on speech only.\n\nVoicePredict System Components\n\nThe following components form the basis for the VoicePredict system are now briefly described.\n\nFrequency Localized Temporal processing\n\nAt the very front-end of VoicePredict lies TravellingWave's proprietary signal processing module called RAGs Algorithm (Rao-Aronov-Garafutdinov speech processing algorithm). It is based on published research on compact features modeling the traveling wave phenomena in the human cochlea.\n\nRAGs extracts modulation information from speech, as opposed to traditional power spectrum analysis. For example, instead of relying on the spectral energy envelope, RAGs computes the locations of several resonances (similar to speech formants); their slowly varying temporal trajectories; rich modulations coding the harmonics around those resonances; local bandwidths, syllable onset and offset times; durations of phones and other proprietary acoustic-phonetic features.\n\nRAGs output is then employed by the acoustic-phonetic models to make decisions about word prediction. Overall, this enables VoicePredict system to perform reliably in a variety of noisy environments.\n\nAcoustic Modeling\n\nVoicePredict combines traditional acoustic modeling techniques (based on modeling phonemes using statistical models) with acoustic-phonetic modeling. In VoicePredict, the latter incorporates spectrally localized temporal features, in conjunction with features like phonetic durations, syllable boundaries and formant energies.\n\nLanguage Modeling\n\nVoicePredict adapts its language model based on the frequency of word usage. New words that are not in the large built-in dictionary (tens of thousands of words) are learnt on the fly. Currently VoicePredict employs unigram language models; meaning VoicePredict does not rely on a sentence context. The unigram modeling techniques make use of VoicePredict's inherent multimodality, resulting in an extremely robust language model.\n\nIn this project, the Frequency Localized Temporal processing module was first developed and integrated with the overall VoicePredict system; by the TravellingWave researchers.  A software development kit (SDK) was sunsequently designed and developed for the overall system. This SDK was then integrated by researchers at the Human-Computer-Interface laboratory of the Carnegie Mellon University, into a simulator. The objective was to study the effectiveness of the novel multimodal interface, VoicePredict, compared to regular virtual keyboard and 9-digit keypad; in writing ou..."
 }
}