{
 "awd_id": "0642728",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Accumulating Evidence for Smooth Paths",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Lawrence Gottlob",
 "awd_eff_date": "2007-03-01",
 "awd_exp_date": "2010-02-28",
 "tot_intn_awd_amt": 349999.0,
 "awd_amount": 349999.0,
 "awd_min_amd_letter_date": "2007-02-27",
 "awd_max_amd_letter_date": "2009-03-11",
 "awd_abstract_narration": "Humans have an amazing ability to pick out an object of interest from a cluttered scene. This can be a difficult task; for example, many animals, from caterpillars to tigers, have coloration and markings that allow them to blend into their environments. However, context and knowledge can help reveal the hidden form, and are particularly useful in selecting relevant information when a person searches for a target. \r\n\r\nWith support from the National Science Foundation, Dr. Verghese will examine the detection of two kinds of targets in cluttered backgrounds--those with smooth contours in static scenes and those with smooth motion trajectories in dynamic scenes. Previous studies have shown that humans are extremely good at finding smooth contours and smooth motion paths even when visibility is very poor. The usefulness of these aspects for identifying forms in visual input has presumably been learned by experience: Contours and motion paths in real-world scenes tend to change direction slowly and smoothly. Dr. Verghese will examine how the visual system comes, over time and experience, to be able to exploit this smoothness property for both static and moving paths.  This will be done by asking observers to report the location and direction of a path hidden in a background at various distances from a starting point, and at various times after display onset. Both choice responses and eye movements will be compared to predictive models to help reveal how evidence for the extended path accumulates. These studies will shed light on how humans predict the future path of moving targets, and how they integrate local contour fragments into full contours that define the boundaries of objects. Path and boundary perception are fundamental visual processes that may have important implications for perceptual training and for machine vision. The project will also make special efforts to draw high school and college students into the scientific work.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Preeti",
   "pi_last_name": "Verghese",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Preeti Verghese",
   "pi_email_addr": "preeti@ski.org",
   "nsf_id": "000173956",
   "pi_start_date": "2007-02-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Smith-Kettlewell Eye Research Foundation",
  "inst_street_address": "2318 FILLMORE ST",
  "inst_street_address_2": "",
  "inst_city_name": "SAN FRANCISCO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4153452035",
  "inst_zip_code": "941151813",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "CA11",
  "org_lgl_bus_name": "THE SMITH-KETTLEWELL EYE RESEARCH INSTITUTE",
  "org_prnt_uei_num": "VT22KLLMNQL6",
  "org_uei_num": "VT22KLLMNQL6"
 },
 "perf_inst": {
  "perf_inst_name": "Smith-Kettlewell Eye Research Foundation",
  "perf_str_addr": "2318 FILLMORE ST",
  "perf_city_name": "SAN FRANCISCO",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "941151813",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "CA11",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 85532.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 131597.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 132870.0
  }
 ],
 "por": null
}