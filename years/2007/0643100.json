{
 "awd_id": "0643100",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Human interaction with large numbers of unmanned vehicles",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2007-02-15",
 "awd_exp_date": "2013-01-31",
 "tot_intn_awd_amt": 499636.0,
 "awd_amount": 530136.0,
 "awd_min_amd_letter_date": "2007-02-06",
 "awd_max_amd_letter_date": "2012-04-13",
 "awd_abstract_narration": "Future unmanned vehicle (UV) systems will be deployed for homeland security missions including Chemical, Biological, Nuclear, Radiological, and Explosive (CBRNE) event response. UV systems that incorporate large numbers of ground and aerial UVs [67] (large, mixed-type UV systems) are envisioned. This project will develop visualization techniques that provide a scalable interface incorporating integrated and easily understood information, thus permitting supervision of large, mixed-type UV systems. This project includes three Human-Robotic Interaction (HRI) challenge areas: the development of a data abstraction framework; the development of scalable interface visualization techniques; and the development of visualization transition techniques.  During the first four years, the PI will develop and evaluate data abstractions, visualization, and transition techniques. The fifth year focuses on CBRNE field evaluations. A new module for the PI's Complex Human-Machine Interaction course and a new Introduction to Robotics course will be developed. Each summer an undergraduate student and a high school teacher will join the research team. The intellectual merits of this project include: formulation of a data abstraction framework for providing scalable, integrated visualizations; creation and evaluation of visualization and visualization transition techniques; and validation of all hypotheses via quantitative and qualitative usability evaluations with simulated and real UVs.\r\nBroader impacts: The development of visualization techniques for large, mixed-type UV systems that allow emergency responders to quickly assess a situation while reducing exposure to contaminants. The development of HRI techniques that will impact UV system development for homeland security that increase personnel capabilities, reduce exposure to dangerous situations, and cover difficult terrain. The development of visualizations may influence interface design for complex system domains such as air traffic control and nuclear process monitoring. The inclusion of high school teachers in summer research projects encourages inclusion of research examples in their courses and may result in increasing student interest in engineering. \r\n\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Julie",
   "pi_last_name": "Adams",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Julie A Adams",
   "pi_email_addr": "julie.a.adams@oregonstate.edu",
   "nsf_id": "000146838",
   "pi_start_date": "2007-02-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Vanderbilt University",
  "inst_street_address": "110 21ST AVE S",
  "inst_street_address_2": "",
  "inst_city_name": "NASHVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "6153222631",
  "inst_zip_code": "372032416",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "TN05",
  "org_lgl_bus_name": "VANDERBILT UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "GTNBNWXJ12D5"
 },
 "perf_inst": {
  "perf_inst_name": "Vanderbilt University",
  "perf_str_addr": "110 21ST AVE S",
  "perf_city_name": "NASHVILLE",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "372032416",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "TN05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "763200",
   "pgm_ele_name": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7632",
   "pgm_ref_txt": "HUMAN-ROBOT INTERACTION"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0407",
   "app_name": "NSF,Education & Human Resource",
   "app_symb_id": "490106",
   "fund_code": "app-0407",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 306166.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 99921.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 107549.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 8500.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research objective was to develop methods for humans to interact with large numbers of robots via a scalable interface incorporating information from all robots so that humans can understand what the robots are doing and provide guidance to the robots. Monitoring and directing robots that are located out of the humans&rsquo; line-of-sight limits understanding of the information gathered by the robots&rsquo; when displayed on a desktop computer, tablet or smartphone.</p>\n<p>The research domain was first responders (e.g., fire, police personnel) responding to a chemical, biological, radiological, nuclear or explosive device events (e.g., 9-11 terrorist attack). The response structure incorporates a hierarchy of responders with differing responsibilities. The incident commander oversees the entire response and provides direction to the lower level responders (e.g., police cordon off streets).</p>\n<p>A framework representing the large amounts of data and information provided by the robots in a format appropriate for the different users (e.g., incident commander, fire personnel) was developed. The Cognitive Information Flow Analysis combines results from analysis techniques into a representation of how information is passed from the lowest level users to the highest level and how information is combined and changed by users at the different levels. This analysis is crucial when robots are tasked with providing the same information as human responders.&nbsp; Eight response tasks incorporating robots were identified.</p>\n<p>The novel General Visualization and Abstraction algorithm dynamically minimizes visual screen clutter by reducing the sizes of icons representing information based on age or relevance to current tasks. The algorithm groups icons representing the same types of information (e.g., victims with similar injuries), further minimizing clutter. The algorithm supports multiple users by displaying the user&rsquo;s information based on the algorithm and others&rsquo; information in the reduced visualization, thus making the most task relevant information salient. The results demonstrate that the algorithm lowers users&rsquo; cognitive demands and improves overall situation awareness.&nbsp; A limitation is that the grouping is information type specific, representing one element of a semantic context. Another limitation is that information grouping does not consider temporal and spatial contexts. Finally, the algorithm does not reduce the visual clutter sufficiently for hand-held devices.</p>\n<p>The novel Feature Sets visualization method reduces visual clutter based on geospatial, temporal and semantic contexts. Feature sets combine related information into a single chronologically ordered component, while providing access to information detail. Feature Sets present dynamic changes more readily than Points of Interest (e.g., location pins on Google maps), thus allowing quick identification of new information. The results demonstrate that Feature Sets provide better task performance than Points of Interest with high information densities and allow faster identification of dynamically updated information.</p>\n<p>The initial desktop interface allows task specification, interaction with robots, access to robot provided information, etc. The associated tablet interface supports basic voice interaction. The current interface incorporates hardware and operating system independent components usable across desktops, tablets and smart phones. This focus requires effective interaction with large amounts of information on smaller displays, such as specifying and selecting information on hand-held devices.</p>\n<p>LocalSwipes, a multi-touch interaction method for hand-held devices provides standard widgets (e.g., buttons, drop down boxes) without requiring widgets to be oversized for direct touch interaction. LocalSwipes performs better than direct touch for specifying robo...",
  "por_txt_cntn": "\nThe research objective was to develop methods for humans to interact with large numbers of robots via a scalable interface incorporating information from all robots so that humans can understand what the robots are doing and provide guidance to the robots. Monitoring and directing robots that are located out of the humans\u00c6 line-of-sight limits understanding of the information gathered by the robots\u00c6 when displayed on a desktop computer, tablet or smartphone.\n\nThe research domain was first responders (e.g., fire, police personnel) responding to a chemical, biological, radiological, nuclear or explosive device events (e.g., 9-11 terrorist attack). The response structure incorporates a hierarchy of responders with differing responsibilities. The incident commander oversees the entire response and provides direction to the lower level responders (e.g., police cordon off streets).\n\nA framework representing the large amounts of data and information provided by the robots in a format appropriate for the different users (e.g., incident commander, fire personnel) was developed. The Cognitive Information Flow Analysis combines results from analysis techniques into a representation of how information is passed from the lowest level users to the highest level and how information is combined and changed by users at the different levels. This analysis is crucial when robots are tasked with providing the same information as human responders.  Eight response tasks incorporating robots were identified.\n\nThe novel General Visualization and Abstraction algorithm dynamically minimizes visual screen clutter by reducing the sizes of icons representing information based on age or relevance to current tasks. The algorithm groups icons representing the same types of information (e.g., victims with similar injuries), further minimizing clutter. The algorithm supports multiple users by displaying the user\u00c6s information based on the algorithm and others\u00c6 information in the reduced visualization, thus making the most task relevant information salient. The results demonstrate that the algorithm lowers users\u00c6 cognitive demands and improves overall situation awareness.  A limitation is that the grouping is information type specific, representing one element of a semantic context. Another limitation is that information grouping does not consider temporal and spatial contexts. Finally, the algorithm does not reduce the visual clutter sufficiently for hand-held devices.\n\nThe novel Feature Sets visualization method reduces visual clutter based on geospatial, temporal and semantic contexts. Feature sets combine related information into a single chronologically ordered component, while providing access to information detail. Feature Sets present dynamic changes more readily than Points of Interest (e.g., location pins on Google maps), thus allowing quick identification of new information. The results demonstrate that Feature Sets provide better task performance than Points of Interest with high information densities and allow faster identification of dynamically updated information.\n\nThe initial desktop interface allows task specification, interaction with robots, access to robot provided information, etc. The associated tablet interface supports basic voice interaction. The current interface incorporates hardware and operating system independent components usable across desktops, tablets and smart phones. This focus requires effective interaction with large amounts of information on smaller displays, such as specifying and selecting information on hand-held devices.\n\nLocalSwipes, a multi-touch interaction method for hand-held devices provides standard widgets (e.g., buttons, drop down boxes) without requiring widgets to be oversized for direct touch interaction. LocalSwipes performs better than direct touch for specifying robot tasks during seated or walking task specifications. A study incorporating LocalSwipes, users walking continuously, and users receiving r..."
 }
}