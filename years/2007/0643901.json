{
 "awd_id": "0643901",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Breaking the phonetic code: novel acoustic-lexical modeling techniques for robust automatic speech recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2006-12-15",
 "awd_exp_date": "2012-11-30",
 "tot_intn_awd_amt": 502952.0,
 "awd_amount": 502952.0,
 "awd_min_amd_letter_date": "2006-12-06",
 "awd_max_amd_letter_date": "2010-12-03",
 "awd_abstract_narration": "Spontaneous speech, accented speech, and speech in noise continue to\r\r\nprovide automatic speech recognition (ASR) technology with significant\r\r\nchallenges; error rates of ASR systems are still unacceptably high for\r\r\nthese types of speech.  This project establishes a consistent\r\r\nframework that seeks to cope with all of these conditions. The novel\r\r\napproach to phonetic variability investigated here views the problem\r\r\nas one of phonetic information underspecification: some subset of\r\r\ninformation that the listener receives will be missing or uncertain.\r\r\nLexical access is thus a phonetic code-breaking problem --- how can a\r\r\nsystem accumulate phonetic cues in each of these conditions to\r\r\nrecognize words on the basis of incomplete evidence?\r\r\n\r\r\nThe research program of this project takes a multidisciplinary\r\r\napproach to integrating linguistic theory with speech recognition\r\r\ntechnology; discriminative statistical models of linguistic features\r\r\nare employed to model nonlinear, overlapping phonological effects\r\r\nobserved in speech.  The framework allows derivation of new linguistic\r\r\ninsights through analysis of trained systems.\r\r\n\r\r\nThe educational program fosters interdisciplinary research (with\r\r\ncross-disciplinary graduate seminars) and increases participation of\r\r\nunderrepresented students in Computer Science by introducing language\r\r\ntechnology topics early into the undergraduate curriculum and\r\r\nencouraging undergraduate research.\r\r\n\r\r\nApart from cultivating a new way of thinking about pronunciation\r\r\nvariation for ASR, the broader impacts of this research are to provide\r\r\ncollaborative resources for the ASR and linguistics communities to\r\r\ndiscuss in tutorial and workshop settings.  Addressing noise, accent,\r\r\nand speaking style in a consistent framework will also improve ASR\r\r\ntechnology for many who are underserved by current systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Fosler-Lussier",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Eric Fosler-Lussier",
   "pi_email_addr": "fosler@cse.ohio-state.edu",
   "nsf_id": "000182577",
   "pi_start_date": "2006-12-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University Research Foundation -DO NOT USE",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "Columbus",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888734",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "QR7NH79713E5"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "1960 KENNY RD",
  "perf_city_name": "COLUMBUS",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 98316.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 95111.0
  },
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 99076.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 103218.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 107231.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main scientific premise of this project is that the way that we perceive speech in the face of noise and varying accents can bethought of as problem of breaking a phonetic code: humans perceive incomplete evidence that they are able to reassemble into messages. Computer models of speech for the process of getting computers to recognize what was said (the Automatic Speech Recognition problem) could be improved by including evidence combination techniques (a form of machine learning). &nbsp;Moreover, the ability to think about these machine learning techniques in an interdisciplinary way (combining insights from linguistics and computer science) can lead to new ways to think about general problems in linguistics.</p>\n<p><br />The main outcomes of the project included two general findings (summarized from roughly 25 publications):</p>\n<p><span>First, statistical methods called Conditional Random Fields (<span>CRFs</span></span>) that are relatively new to the Automatic Speech Recognition field have been shown to be effective combiners of linguistic information. &nbsp;For example, one view of speech sounds represents the sound patterns as whole blocks in time (known as phones); these are the traditional building blocks of <span><span>ASR</span></span> systems. &nbsp;However, these sounds can be broken into \"phonological feature\" categories -- a multi-dimensional representation of speech sounds. &nbsp;<span><span>CRFs</span></span><span> are shown to be a much more effective combination method of these different representations of speech than the tradition Hidden Markov Model (<span>HMM</span></span>), and can decrease the errors made by a system much more than either representation alone. &nbsp;Our explorations examined how we can think about feature combinations both within short, local windows of speech, or over longer timescales.</p>\n<p><br />A second outcome was a new method of thinking about how phonetic information is impacted by noise. &nbsp;When the human ear hears speech and noise together, some frequencies of the speech are blocked by the noise, in a process called masking; this is similar to the visual phenomenon of distant objects being partially obscured by closer objects in the line of sight. &nbsp;Noise severely degrades <span><span>ASR</span></span> performance (i.e., it increases the error). &nbsp;Previous methods tried to estimate what parts of the signal were noise-masked, and reconstruct the underlying speech. &nbsp;However, our research showed that, surprisingly, treating the masked components as completely absent was a better strategy than other reconstruction techniques. &nbsp;It is likely better to focus on mask estimation rather than reconstruction. &nbsp;Tying into the phonetic code aspect of the project, we found that one could improve mask estimation by using information from a speech recognition system, and then using machine learning techniques similar to <span><span>CRFs</span></span> to improve prediction of the mask. &nbsp;Another thread of research showed how <span><span>CRFs</span></span> could be used directly for mask prediction; this overall approach shows how we may be able to think about speech recognition and speech enhancement as complimentary, cooperative processes.</p>\n<p><br />The educational outcomes of this project included new techniques for teaching advanced machine learning concepts in speech and language technology classes, and helping students in linguistics disciplines&nbsp;utilize machine learning in their own dissertations.</p>\n<p>In terms of outreach, a tutorial on this material was presented at a major international conference, and three tutorial-style journal articles co-authored by the PI were influenced by this grant. &nbsp;The project fostered interdisciplinary research by being an example project presented at the new Buckeye Language Network, and members of the project participated in the <span><span>Ohi...",
  "por_txt_cntn": "\nThe main scientific premise of this project is that the way that we perceive speech in the face of noise and varying accents can bethought of as problem of breaking a phonetic code: humans perceive incomplete evidence that they are able to reassemble into messages. Computer models of speech for the process of getting computers to recognize what was said (the Automatic Speech Recognition problem) could be improved by including evidence combination techniques (a form of machine learning).  Moreover, the ability to think about these machine learning techniques in an interdisciplinary way (combining insights from linguistics and computer science) can lead to new ways to think about general problems in linguistics.\n\n\nThe main outcomes of the project included two general findings (summarized from roughly 25 publications):\n\nFirst, statistical methods called Conditional Random Fields (CRFs) that are relatively new to the Automatic Speech Recognition field have been shown to be effective combiners of linguistic information.  For example, one view of speech sounds represents the sound patterns as whole blocks in time (known as phones); these are the traditional building blocks of ASR systems.  However, these sounds can be broken into \"phonological feature\" categories -- a multi-dimensional representation of speech sounds.  CRFs are shown to be a much more effective combination method of these different representations of speech than the tradition Hidden Markov Model (HMM), and can decrease the errors made by a system much more than either representation alone.  Our explorations examined how we can think about feature combinations both within short, local windows of speech, or over longer timescales.\n\n\nA second outcome was a new method of thinking about how phonetic information is impacted by noise.  When the human ear hears speech and noise together, some frequencies of the speech are blocked by the noise, in a process called masking; this is similar to the visual phenomenon of distant objects being partially obscured by closer objects in the line of sight.  Noise severely degrades ASR performance (i.e., it increases the error).  Previous methods tried to estimate what parts of the signal were noise-masked, and reconstruct the underlying speech.  However, our research showed that, surprisingly, treating the masked components as completely absent was a better strategy than other reconstruction techniques.  It is likely better to focus on mask estimation rather than reconstruction.  Tying into the phonetic code aspect of the project, we found that one could improve mask estimation by using information from a speech recognition system, and then using machine learning techniques similar to CRFs to improve prediction of the mask.  Another thread of research showed how CRFs could be used directly for mask prediction; this overall approach shows how we may be able to think about speech recognition and speech enhancement as complimentary, cooperative processes.\n\n\nThe educational outcomes of this project included new techniques for teaching advanced machine learning concepts in speech and language technology classes, and helping students in linguistics disciplines utilize machine learning in their own dissertations.\n\nIn terms of outreach, a tutorial on this material was presented at a major international conference, and three tutorial-style journal articles co-authored by the PI were influenced by this grant.  The project fostered interdisciplinary research by being an example project presented at the new Buckeye Language Network, and members of the project participated in the OhioSpeaks workshop.  The PI presented research from this and related projects on Capitol Hill as part of the 2008 Coalition for National Science Funding research day.  The PI also gave a talk on how machine learning can improve autism research at the Central Ohio Autism Society.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/11/2013\n\n\t\t\t\t\tSubmitted by: Eric Fosler-Lussier"
 }
}