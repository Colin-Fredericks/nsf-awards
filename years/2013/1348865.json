{
 "awd_id": "1348865",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Prototyping an Urban Data Cyberinfrastructure for Computational Social Sciences",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922247",
 "po_email": "rchadduc@nsf.gov",
 "po_sign_block_name": "Robert Chadduck",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 299998.0,
 "awd_amount": 299998.0,
 "awd_min_amd_letter_date": "2013-09-05",
 "awd_max_amd_letter_date": "2013-09-05",
 "awd_abstract_narration": "Cities are the crucibles of civilization, and accelerating global urbanization raises challenges and opportunities related to density and scale in areas including transportation; food production and distribution; human health and wellbeing; education; social policy and services; and management of water and energy. Seeking to understand the human, social, and economic to help develop effective education or public policy scientists, and thus city officials, have traditionally been limited to qualitative studies or to using sparse, often stale data sources. The open data movement is making an increasingly rich set of urban data available, but the cyber infrastructure technologies and tools used to make this data available were designed primarily to support the analysis of individual data sets rather than exploring relationships among many data sets. Consequently, urban scientists from sociology, economics, behavioral sciences, education, engineering, operations research, and other disciplines lack the tools and infrastructure to fully harness urban data for their research. The questions these researchers ask are therefore constrained by the data they have in hand. Two new cyber infrastructure capabilities have potential to unleash these data sources, both exploiting the fact that most of the published urban data sets share the attributes of location and time. The first is to allow a scientist to assemble data from multiple, independent, data sources for a specific geographical location point (latitude/longitude), city unit (street segment, census tract, block), or area (polygon). The second is to select a window of time and to normalize the selected data sources using a common sampling interval, merging them into a composite structure for computational and statistical analysis. Taken together, these capabilities will allow a scientist to study urban areas, over specific time periods, with varied, relevant data represented as a time series of vectors. We propose to develop, in partnership with urban scientists and City officials initially from Chicago and eventually from New York City, a proof-of-concept with these capabilities.\r\n\r\nThe prototype will draw data from open data portals, allowing a researcher to specify a location, a window of time, a sampling period, and a list of data sets. The system will provide a matrix with one row per time sample and columns representing each data set. By merging and transforming urban data into matrices we will enable urban scientists to apply the tools of mathematics and computation to understand urban challenges ranging from youth violence and crime to graduate rates to employment and economic decline and revitalization.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Catlett",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Charles E Catlett",
   "pi_email_addr": "catlett@anl.gov",
   "nsf_id": "000117217",
   "pi_start_date": "2013-09-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "5735 S Ellis Ave",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606371433",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 299998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to provide a prototype platform that (a) draws on open place-based data from many sources, (b) enables a person to discover what data is available associated with a selected geographic place or area, (c) specify a window of time and a sampling period for selected data associated with that place or area, (d) evaluate the general character (completeness, dynamics, geographic distribution/density, etc.) of the data, and (e) extract multiple selected data sets in a form that is aligned spatially and temporally to facilitate advanced data analysis.</p>\n<p>The team worked with social scientists and data scientists from multiple universities as well as the City of Chicago to develop an open data discovery and exploration capability that could be used by non-data-scientists such as social or economic scientists or policy makers.&nbsp;</p>\n<p>The goal was to create a web-based open data resource for non-data scientists, with discovery, exploration, and integration capabilities in order to reduce the amount of work needed to find and combine data sets together before asking science questions. &nbsp;This meant providing three fundamental capabilities:</p>\n<p>(1) Discovery.&nbsp; Open data is fragmented across many open data portals, which provide internal search but no global search capability exists across these silos.&nbsp;</p>\n<p>(2) Exploration.&nbsp; We wanted to provide visual methods for exploring the spatial and temporal characteristics of open data, focusing primarily on place-based questions.</p>\n<p>(3) Integration.&nbsp; We wanted to provide a way for a scientist to take two or more data sets and analyze them without first having to align them temporally and spatially, given the diverse methods used for representing space and the many different temporal (and spatial) resolutions.</p>\n<p>Working with social, economic, and other scientists we determined that a map-based, calendar-based search paradigm would be ideal, allowing a scientist or policymaker to ask \"what data is available about this geographic area (city, neighborhood, county) for a given period of time\" - where the geographic area and period of time correspond to a research question the scientist or policymaker wishes to ask, of an evaluation they wish to perform.&nbsp; By starting with place and time we leave room for the user to discover data that might be relevant, but that they may not have considered (or known existed).</p>\n<p>We then leveraged the fact that the vast majority of open data portals use either Socrata (commercial) or CKAN (open source) platforms, and thus we could provide scriptable extract-transform-load (ETL) capabilities to import data from many sources.&nbsp; The ETL allows any user to find a data set on the web (ideally in a Socrata or CKAN portal), copy the URL for the data set of interest, and paste into a form on the http://plenar.io website.&nbsp; Within hours the data is imported and aligned.</p>\n<p>We keyed on data sets with common attributes of location and time, merging all data sets into a single scalable geospatial database, aligned over space and time.&nbsp; We developed application programming interfaces APIs to support portal or application development, and build the http://plenar.io portal with these APIs.</p>\n<p>The quality of APIs and the potential for data from many sources resulted in the City of Chicago using Plenario as its back-end for a community-facing portal (http://opengrid.io) -- rather than using their own data portal (from which Plenario draws Chicago data).</p>\n<p>The City of Chicago's OpenGrid system, like Plenario, is open source and runs on commercial cloud services (Amazon Web Services, but engineered using portable methods that allow for trivial movement of the systems to other cloud systems, commercial or private, or to physical servers.)</p>\n<p>The project provided unique opportunities for undergraduate, graduate, and high school students. Each summer our programmers supervised teams of students who worked on the platform, interacting with data and social scientists frequently in context of (a) faculty members at UChicago, (b) the Data Science for Social Good program at UChicago, and (c) the scientists working in the Chief Data Officer's group within the City of Chicago.</p>\n<p>All source code is open in Github (https://github.com/UrbanCCD-UChicago/plenario), and the Plenar.io system is featured as a resource for programs such as UChicago's Data Science for Social Good.</p>\n<p>Plenario has also been extended to support sensor network data, and thus will be the primary access method for the NSF-funded Array of Things project (http://arrayofthings.us).&nbsp; The extensions include new API calls to support applications and portals using sensor data as well as readily centering the default view over any city or geographic area (originally it defaulted to Chicago). For instance, three groups (a company, a university, and a not-for-profit) have already begun to independently develop mobile applications for air quality.&nbsp; Other companies, for instance Panasonic, are using the APIs to enable AoT data to be imported into smart city dashboards.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/10/2017<br>\n\t\t\t\t\tModified by: Charles&nbsp;E&nbsp;Catlett</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to provide a prototype platform that (a) draws on open place-based data from many sources, (b) enables a person to discover what data is available associated with a selected geographic place or area, (c) specify a window of time and a sampling period for selected data associated with that place or area, (d) evaluate the general character (completeness, dynamics, geographic distribution/density, etc.) of the data, and (e) extract multiple selected data sets in a form that is aligned spatially and temporally to facilitate advanced data analysis.\n\nThe team worked with social scientists and data scientists from multiple universities as well as the City of Chicago to develop an open data discovery and exploration capability that could be used by non-data-scientists such as social or economic scientists or policy makers. \n\nThe goal was to create a web-based open data resource for non-data scientists, with discovery, exploration, and integration capabilities in order to reduce the amount of work needed to find and combine data sets together before asking science questions.  This meant providing three fundamental capabilities:\n\n(1) Discovery.  Open data is fragmented across many open data portals, which provide internal search but no global search capability exists across these silos. \n\n(2) Exploration.  We wanted to provide visual methods for exploring the spatial and temporal characteristics of open data, focusing primarily on place-based questions.\n\n(3) Integration.  We wanted to provide a way for a scientist to take two or more data sets and analyze them without first having to align them temporally and spatially, given the diverse methods used for representing space and the many different temporal (and spatial) resolutions.\n\nWorking with social, economic, and other scientists we determined that a map-based, calendar-based search paradigm would be ideal, allowing a scientist or policymaker to ask \"what data is available about this geographic area (city, neighborhood, county) for a given period of time\" - where the geographic area and period of time correspond to a research question the scientist or policymaker wishes to ask, of an evaluation they wish to perform.  By starting with place and time we leave room for the user to discover data that might be relevant, but that they may not have considered (or known existed).\n\nWe then leveraged the fact that the vast majority of open data portals use either Socrata (commercial) or CKAN (open source) platforms, and thus we could provide scriptable extract-transform-load (ETL) capabilities to import data from many sources.  The ETL allows any user to find a data set on the web (ideally in a Socrata or CKAN portal), copy the URL for the data set of interest, and paste into a form on the http://plenar.io website.  Within hours the data is imported and aligned.\n\nWe keyed on data sets with common attributes of location and time, merging all data sets into a single scalable geospatial database, aligned over space and time.  We developed application programming interfaces APIs to support portal or application development, and build the http://plenar.io portal with these APIs.\n\nThe quality of APIs and the potential for data from many sources resulted in the City of Chicago using Plenario as its back-end for a community-facing portal (http://opengrid.io) -- rather than using their own data portal (from which Plenario draws Chicago data).\n\nThe City of Chicago's OpenGrid system, like Plenario, is open source and runs on commercial cloud services (Amazon Web Services, but engineered using portable methods that allow for trivial movement of the systems to other cloud systems, commercial or private, or to physical servers.)\n\nThe project provided unique opportunities for undergraduate, graduate, and high school students. Each summer our programmers supervised teams of students who worked on the platform, interacting with data and social scientists frequently in context of (a) faculty members at UChicago, (b) the Data Science for Social Good program at UChicago, and (c) the scientists working in the Chief Data Officer's group within the City of Chicago.\n\nAll source code is open in Github (https://github.com/UrbanCCD-UChicago/plenario), and the Plenar.io system is featured as a resource for programs such as UChicago's Data Science for Social Good.\n\nPlenario has also been extended to support sensor network data, and thus will be the primary access method for the NSF-funded Array of Things project (http://arrayofthings.us).  The extensions include new API calls to support applications and portals using sensor data as well as readily centering the default view over any city or geographic area (originally it defaulted to Chicago). For instance, three groups (a company, a university, and a not-for-profit) have already begun to independently develop mobile applications for air quality.  Other companies, for instance Panasonic, are using the APIs to enable AoT data to be imported into smart city dashboards.\n\n \n\n\t\t\t\t\tLast Modified: 02/10/2017\n\n\t\t\t\t\tSubmitted by: Charles E Catlett"
 }
}