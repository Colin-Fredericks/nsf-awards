{
 "awd_id": "1340984",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CC-NIE Integration: High Performance Computing with Data and Networking Acceleration (HPCDNA)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2016-10-31",
 "tot_intn_awd_amt": 644609.0,
 "awd_amount": 644609.0,
 "awd_min_amd_letter_date": "2013-09-11",
 "awd_max_amd_letter_date": "2013-09-11",
 "awd_abstract_narration": "Future cyber-infrastructures are increasingly expected to revolve around the integration of big data, computation, and high performance networking.  The High Performance Computing with Data and Networking Acceleration (HPCDNA) project focuses on these issues in the context of campus science and computational requirements.  Many researchers have application specific data and a variety of desired compute environments, including purpose built small lab compute resources, medium scale campus High Performance Computing (HPC), cloud computing,  and national scale distributed or centralized resources.  The inability to flexibly and seamlessly get data to the most appropriate compute resource is often the limiting factor determining where the computation is run and what computation is performed.  The HPCDNA project is developing technologies to greatly improve the ability to utilize common data sets across this diverse set of computational resources.\r\n\r\nThe HPCDNA architecture and technologies include a Network Embedded Storage (NES) system based on a distributed high performance parallel file system deployed in the core of the Mid-Atlantic Crossroads regional network.  This system is enhanced for high performance via tight integration with campus HPC, campus networks, and wide area networks.   An NES client interface is being developed, which coordinates storage with dynamic network provisioning, and enables a variety of high performance access methods by workflow processes.  \r\n\r\nIt is expected that the HPCDNA system will facilitate expanded use of HPC and clouds by researchers, which will allow the adoption of more ambitious goals with the knowledge that their computing environments can scale up along with their problem space.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tripti",
   "pi_last_name": "Sinha",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tripti Sinha",
   "pi_email_addr": "tsinha@umd.edu",
   "nsf_id": "000621387",
   "pi_start_date": "2013-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Lehman",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas W Lehman",
   "pi_email_addr": "tom.w.lehman@gmail.com",
   "nsf_id": "000240166",
   "pi_start_date": "2013-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "Torrens",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Paul M Torrens",
   "pi_email_addr": "torrens@nyu.edu",
   "nsf_id": "000301178",
   "pi_start_date": "2013-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Saurabh",
   "pi_last_name": "Channan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Saurabh Channan",
   "pi_email_addr": "channan@terrauplse.com",
   "nsf_id": "000627876",
   "pi_start_date": "2013-09-11",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xi",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xi Yang",
   "pi_email_addr": "maxyang@umd.edu",
   "nsf_id": "000644580",
   "pi_start_date": "2013-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "3112 LEE BLDG",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "772600",
   "pgm_ele_name": "Data Cyberinfrastructure"
  },
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 644609.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The High Performance Computing with Data and Networking Acceleration (HPCDNA) project developed technologies and cyberinfrastructure to facilitate the integration of data, compute systems, networks, and application codes.&nbsp; The motivation for this work was experience working with domain science researchers who were often locked into a single compute environment that limited the type and scale of approaches they can apply to their science problems.&nbsp;&nbsp; This lack of flexibility was often due to cyberinfrastructure that does not allow easy data movement or flexible access to a variety of compute infrastructures.&nbsp;</p>\n<p>In response, this project developed an edge infrastructure which integrated host virtualization, Software Defined Networking (SDN), and high performance parallel file systems to develop a system referred to as a &ldquo;Software Defined ScienceDMZ&rdquo;.&nbsp;&nbsp; This edge infrastructure provides a well connected location for applications to host their codes, conduct high speed data transfers, access local compute systems, and build customized hybrid cloud topologies.&nbsp; Built upon technologies similar to those used in modern data centers, the result is a scalable, multi-tenancy environment, where each user or user group can allocate dedicated resources and use in an isolated, independent and elastic fashion.&nbsp; In addition this facility includes integration of external resources such as direct connections to public clouds along with research and education network services.&nbsp; The result is a campus edge facility where users can obtain traditional ScienceDMZ data transfer service, and/or have specialized topologies built which integrate local cloud and HPC based compute, storage, and external connections to public clouds or external destinations.</p>\n<p>User driven service instantiation on the Software-Defined ScienceDMZ was realized thru an open source &ldquo;full-stack model driven orchestration&rdquo; system, StackV (github.com/MAX-UMD/StackV.community), to integrate a diverse set of resources via utilization of semantic web based modeling and dynamic orchestrated workflows.&nbsp; This allows users to present highly abstracted and simple requests to obtain complex high-end services which are customized to their use cases and objectives.&nbsp; This solution has been used in practice to integrate local HPC, Amazon Web Services, local and wide area SDN, Network Function Virtualization, OpenStack, Ceph File Systems, Data Transfer Nodes, and interface to users with abstract service models.</p>\n<p>A Software-Defined ScienceDMZ facility was deployed at the University of Maryland College Park Rivertech Cyberinfrastructure facility and connected to the Mid-Atlantic Crossroads regional network.&nbsp; We worked with domain science researchers from the large scale simulation and climate science communities to test and verify the Software-Defined ScienceDMZ architecture and deployment.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/29/2017<br>\n\t\t\t\t\tModified by: Thomas&nbsp;W&nbsp;Lehman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe High Performance Computing with Data and Networking Acceleration (HPCDNA) project developed technologies and cyberinfrastructure to facilitate the integration of data, compute systems, networks, and application codes.  The motivation for this work was experience working with domain science researchers who were often locked into a single compute environment that limited the type and scale of approaches they can apply to their science problems.   This lack of flexibility was often due to cyberinfrastructure that does not allow easy data movement or flexible access to a variety of compute infrastructures. \n\nIn response, this project developed an edge infrastructure which integrated host virtualization, Software Defined Networking (SDN), and high performance parallel file systems to develop a system referred to as a \"Software Defined ScienceDMZ\".   This edge infrastructure provides a well connected location for applications to host their codes, conduct high speed data transfers, access local compute systems, and build customized hybrid cloud topologies.  Built upon technologies similar to those used in modern data centers, the result is a scalable, multi-tenancy environment, where each user or user group can allocate dedicated resources and use in an isolated, independent and elastic fashion.  In addition this facility includes integration of external resources such as direct connections to public clouds along with research and education network services.  The result is a campus edge facility where users can obtain traditional ScienceDMZ data transfer service, and/or have specialized topologies built which integrate local cloud and HPC based compute, storage, and external connections to public clouds or external destinations.\n\nUser driven service instantiation on the Software-Defined ScienceDMZ was realized thru an open source \"full-stack model driven orchestration\" system, StackV (github.com/MAX-UMD/StackV.community), to integrate a diverse set of resources via utilization of semantic web based modeling and dynamic orchestrated workflows.  This allows users to present highly abstracted and simple requests to obtain complex high-end services which are customized to their use cases and objectives.  This solution has been used in practice to integrate local HPC, Amazon Web Services, local and wide area SDN, Network Function Virtualization, OpenStack, Ceph File Systems, Data Transfer Nodes, and interface to users with abstract service models.\n\nA Software-Defined ScienceDMZ facility was deployed at the University of Maryland College Park Rivertech Cyberinfrastructure facility and connected to the Mid-Atlantic Crossroads regional network.  We worked with domain science researchers from the large scale simulation and climate science communities to test and verify the Software-Defined ScienceDMZ architecture and deployment.\n\n\t\t\t\t\tLast Modified: 01/29/2017\n\n\t\t\t\t\tSubmitted by: Thomas W Lehman"
 }
}