{
 "awd_id": "1320059",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Binaural Sound Source Separation Robust to Listener Head Movements",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 186624.0,
 "awd_amount": 186624.0,
 "awd_min_amd_letter_date": "2013-08-06",
 "awd_max_amd_letter_date": "2013-08-06",
 "awd_abstract_narration": "The goal of this project is to develop a new binaural model to separate sounds in complex environments. The new aspect of the model is that it can utilize head movements to improve its localization performance by analyzing dynamic localization cues and combining these with information about its own head position. In addition, the model uses a dual approach to eliminate the influence of room reflections on sound source localization and segregation. In the first stage, specular reflections are eliminated using an autocorrelation- based algorithm. In the second stage, diffuse reverberation is removed by measuring interaural cross correlation across time/frequency bins, knowing that these values decrease with decreasing direct-to- reverberant energy ratio. The model development is accompanied by a behavioral study to better understand the underlying principles of how humans can perform robustly in complex scenarios. The results are also used as a benchmark test for the model algorithms.\r\n\r\nThis project intends to bridge the gap that exists between fundamentally knowing how the auditory system processes binaural tasks for simple multiple-sound-source scenarios, and understanding and modeling how it performs when the environment reaches real-life complexity. The resulting model is expected to operate in real time to localize sound sources in robot or surveillance applications or serve as a front end for sound- source separation algorithms, speech recognizers, predictors for acoustical quality of rooms, and Computational Auditory Scene Analysis (CASA) models.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jonas",
   "pi_last_name": "Braasch",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jonas Braasch",
   "pi_email_addr": "braasj@rpi.edu",
   "nsf_id": "000186127",
   "pi_start_date": "2013-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rensselaer Polytechnic Institute",
  "inst_street_address": "110 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "TROY",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5182766000",
  "inst_zip_code": "121803590",
  "inst_country_name": "United States",
  "cong_dist_code": "20",
  "st_cong_dist_code": "NY20",
  "org_lgl_bus_name": "RENSSELAER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "U5WBFKEBLMX3"
 },
 "perf_inst": {
  "perf_inst_name": "Rensselaer Polytechnic Institute",
  "perf_str_addr": "110 8th Street",
  "perf_city_name": "Troy",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "121803522",
  "perf_ctry_code": "US",
  "perf_cong_dist": "20",
  "perf_st_cong_dist": "NY20",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 186624.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><br />The aim of this project was to develop a sound source segregation model that can operate under realistic conditions in a reverberant environment and utilize head movements to achieve a&nbsp;more human-like performance of sound source segregation. The system can be divided into three components: (i) a binaural model to localize reverberant sound sources, (ii) a mechanism to utilize head movements, and (iii) a source segregation algorithm based on the Equalization/Cancellation (EC) method. Figure 1 depicts the current structure of the model.&nbsp;</p>\n<p>The sound localization model uses a binaural signal&nbsp;or a signal that is measured at the two eardrums of a human subject. Based on this signal the model can localize a sound source in the presence of multiple wall reflections. The model also estimates a room impulse response from a running binaural signal and determines the spatial locations and delays of early reflections, without any prior or additional knowledge of the source. A dual-layer cross-correlation/auto-correlation algorithm is used to determine the interaural time difference of the direct sound source component and to estimate a binaural activity pattern like the one shown in Figure 2.&nbsp;</p>\n<p>The localization model is used by a sound source-segregation model. The latter takes a mixture of two simultaneous speech signals at two unique positions which is also obtained as a binaural signal. The model localizes the two stationary sources by analyzing interaural time and level differences, and then virtually rotates its head to find the orientation for the best resulting signal-to-noise ratio to optimally remove the undesired sound source from the mixture. For this purpose, the model extracts either source using an inverse filter approach similar to the Durlach&rsquo;s Equalization/Cancellation (EC) method to detect time/frequency segments in which the target sound source is present &ndash; see Figure 3. Here, the model only selects those time/frequency bins that contain the target signal, and thus it rejects the masking signal. The model then generates a binary masking map from the selected target bins and overlays this on the mixed signal's spectrogram to extract only the target signal. The resulting output signal is then reconstructed in time and frequency, leaving only one desired target signal.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/01/2016<br>\n\t\t\t\t\tModified by: Jonas&nbsp;Braasch</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973343723_Figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973343723_Figure1--rgov-800width.jpg\" title=\"Model Structure\"><img src=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973343723_Figure1--rgov-66x44.jpg\" alt=\"Model Structure\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">General Model Structure of the Binaural Localization and Source Segregation Model.</div>\n<div class=\"imageCredit\">Jonas Braasch & Nikhil Desphande</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jonas&nbsp;Braasch</div>\n<div class=\"imageTitle\">Model Structure</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973442147_Figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973442147_Figure2--rgov-800width.jpg\" title=\"Binaural Activity Pattern\"><img src=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973442147_Figure2--rgov-66x44.jpg\" alt=\"Binaural Activity Pattern\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Binaural Activity Pattern extracted from a speech clip located in the front with four early wall reflections.</div>\n<div class=\"imageCredit\">Jonas Braasch</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jonas&nbsp;Braasch</div>\n<div class=\"imageTitle\">Binaural Activity Pattern</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973535015_Figure3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973535015_Figure3--rgov-800width.jpg\" title=\"Source Segregation Algorithm\"><img src=\"/por/images/Reports/POR/2016/1320059/1320059_10264431_1477973535015_Figure3--rgov-66x44.jpg\" alt=\"Source Segregation Algorithm\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The model's source segregation process. The top graph shows male (black) and female (gray) voice signals that are presented in a mixture. The second and third row show spectrograms of the female and male voice signals. The fourth row depicts the binary mask for extracting the male voice, and the bot</div>\n<div class=\"imageCredit\">Nikhil Deshpande & Jonas Braasch</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jonas&nbsp;Braasch</div>\n<div class=\"imageTitle\">Source Segregation Algorithm</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\nThe aim of this project was to develop a sound source segregation model that can operate under realistic conditions in a reverberant environment and utilize head movements to achieve a more human-like performance of sound source segregation. The system can be divided into three components: (i) a binaural model to localize reverberant sound sources, (ii) a mechanism to utilize head movements, and (iii) a source segregation algorithm based on the Equalization/Cancellation (EC) method. Figure 1 depicts the current structure of the model. \n\nThe sound localization model uses a binaural signal or a signal that is measured at the two eardrums of a human subject. Based on this signal the model can localize a sound source in the presence of multiple wall reflections. The model also estimates a room impulse response from a running binaural signal and determines the spatial locations and delays of early reflections, without any prior or additional knowledge of the source. A dual-layer cross-correlation/auto-correlation algorithm is used to determine the interaural time difference of the direct sound source component and to estimate a binaural activity pattern like the one shown in Figure 2. \n\nThe localization model is used by a sound source-segregation model. The latter takes a mixture of two simultaneous speech signals at two unique positions which is also obtained as a binaural signal. The model localizes the two stationary sources by analyzing interaural time and level differences, and then virtually rotates its head to find the orientation for the best resulting signal-to-noise ratio to optimally remove the undesired sound source from the mixture. For this purpose, the model extracts either source using an inverse filter approach similar to the Durlach?s Equalization/Cancellation (EC) method to detect time/frequency segments in which the target sound source is present &ndash; see Figure 3. Here, the model only selects those time/frequency bins that contain the target signal, and thus it rejects the masking signal. The model then generates a binary masking map from the selected target bins and overlays this on the mixed signal's spectrogram to extract only the target signal. The resulting output signal is then reconstructed in time and frequency, leaving only one desired target signal. \n\n\t\t\t\t\tLast Modified: 11/01/2016\n\n\t\t\t\t\tSubmitted by: Jonas Braasch"
 }
}