{
 "awd_id": "1330110",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Distributed Sensing, Learning and Control in Dynamic Environments",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2019-03-31",
 "tot_intn_awd_amt": 1000000.0,
 "awd_amount": 1000000.0,
 "awd_min_amd_letter_date": "2013-09-12",
 "awd_max_amd_letter_date": "2018-07-20",
 "awd_abstract_narration": "The objective of this project is to improve the performance of autonomous systems \r\nin dynamic environments, such as disaster recovery, by integrating perception, planning\r\nparadigms, learning, and databases. For the next generation of autonomous systems to be\r\ntruly effective in terms of tangible performance improvements (e.g., long-term operations,\r\ncomplex and rapidly changing environments), a new level of intelligence must be attained.\r\nThis project improves the state of robotic systems by enhancing their ability to coordinate\r\nactivities (such as searching a disaster zone), recognize objects or people, account for\r\nuncertainty, and  \"most important\" learn, so the system's performance is continuously \r\nimproving. To do this, the project takes an interdisciplinary approach to developing \r\ntechniques in core areas and at the interface of perception, planning, learning, and databases \r\nto achieve robustness. \r\n\r\nThis project seeks to significantly improve the performance of cyber-physical systems \r\nfor time-critical applications such as disaster monitoring, search and rescue, autonomous \r\nnavigation, and security and surveillance. It enables the development of techniques and \r\ntools to augment all decision making processes and applications which are characterized\r\nby continuously changing operating conditions, missions and environments. The project\r\ncontributes to education and a diverse engineering workforce by training students at the\r\nUniversity of California, Riverside, one of the most diverse research institutions in US \r\nand an accredited Hispanic Serving Institution. Instruction and research opportunities\r\ncross traditional disciplinary boundaries, and the project serves as the basis for \r\nundergraduate capstone design projects and a new graduate course. The software and \r\ntestbeds from this project will be shared with the cyber-physical system research community,\r\nindustry, and end users. The project plans to present focused workshops/tutorials at major \r\nIEEE and ACM conferences. The results will be broadly disseminated through the \r\nproject website.\r\n\r\nFor further information see the project website at:  \r\nhttp://vislab.ucr.edu/RESEARCH/DSLC/DSLC.php",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bir",
   "pi_last_name": "Bhanu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bir Bhanu",
   "pi_email_addr": "bhanu@ece.ucr.edu",
   "nsf_id": "000126207",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chinya",
   "pi_last_name": "Ravishankar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chinya Ravishankar",
   "pi_email_addr": "ravi@cs.ucr.edu",
   "nsf_id": "000338299",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Campbell",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Mark E Campbell",
   "pi_email_addr": "mc288@cornell.edu",
   "nsf_id": "000214501",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Amit",
   "pi_last_name": "Roy-Chowdhury",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Amit K Roy-Chowdhury",
   "pi_email_addr": "amitrc@ece.ucr.edu",
   "nsf_id": "000309390",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Riverside",
  "inst_street_address": "200 UNIVERSTY OFC BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "RIVERSIDE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9518275535",
  "inst_zip_code": "925210001",
  "inst_country_name": "United States",
  "cong_dist_code": "39",
  "st_cong_dist_code": "CA39",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA AT RIVERSIDE",
  "org_prnt_uei_num": "",
  "org_uei_num": "MR5QC5FCAVH5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Riverside",
  "perf_str_addr": "900 University Avenue",
  "perf_city_name": "Riverside",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "925210001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "39",
  "perf_st_cong_dist": "CA39",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 1000000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>For the next generation of autonomous systems to be truly enabling in terms of tangible performance improvements such as long-term operations and complex scenarios, a new level of intelligence is required. This intelligence can only come from a tighter coupling of the perception and planning paradigms with learning and databases so the desired sensory data are effectively converted to <em>usable</em> information for both planning and perception. Such a system will be able to intelligently overcome uncertainties that will inevitably arise and to learn over time, so its real-world performance is robust. The project has brought an interdisciplinary team from the University of California at Riverside and Cornell University with backgrounds in Electrical Engineering, Mechanical/Aerospace Engineering, and Computer Science to develop cyber-physical systems in the context of robotic networks equipped with cameras and operating in complex dynamic environments with a focus on scenarios where the sensors must collaborate in space and time to provide scene understanding.</p>\n<h2>Intellectual Merit</h2>\n<p>The novelty of the approach is that it addresses perception, learning and databases in dynamic applications for increasingly robust performance over time. The achievements of the project are:</p>\n<p>(a) <em>&nbsp;Perception, Learning, Tracking, Re-identification and Representations:</em> The project developed algorithms and systems for a group of fixed (pan-tilt and zoom video cameras) and mobile sensors (robots) to collaborate on scene understanding in scenarios, which are characterized by highly dynamic and uncertain environments. Key outcomes are: (i) A wide range of perception and learning algorithms were developed, including distributed algorithms, for tracking and data association of objects (people and vehicles) with linear, non-linear motion, social behavior of humans in groups and incorporating human walking models in crowds in a network of overlapping and non-overlapping sensors. (ii) The project developed extensive algorithms based on different principles (reference-based methods, efficient canonical correlation based analysis, deep learning and active learning methods, feature warps, network constraints, etc.) for re-identification in non-overlapping cameras in a video network, including work on fusing infrared and video cameras. The framework and algorithms are experimentally validated.</p>\n<p>(b) &nbsp;<em>Distributed Estimation and Information Planning:</em> The project carried out a tight integration of perception and action in a probabilistic framework for truly intelligent robotic systems. It explored synergies across three areas: control, video understanding and perception, and handling data with uncertainty. It developed algorithms to enable a wider range of behaviors in cooperative robotic teams, which subsequently will enable a better handling of uncertainty, and improved sensing, information collection and decision making. The key outcomes are: (i) Information based planner with guarantees where a receding horizon approach to real time work was developed using a novel tail cost approximation.&nbsp;The work was verified with an indoor robot over multiple trials. (ii) A formal solution to the multi-robot multi-object exploration and tracking problems simultaneously using a hierarchical architecture.</p>\n<p>(c) <em>Management and Querying of Uncertain Data:</em> The key outcomes are: (i) Inferential time-decaying filters are introduced, which integrate Bayesian priors and information latent in bloom filters to make penalty-optimal, query-specific decisions. These methods can support novel query types and sliding window queries with dynamic error penalties. (ii) Planning and discovering assemblies of a large number of moving objects using partial information is challenging when object arrival observations are sparse due to inadequate sensor coverage or object countermeasures. A novel class of assembly queries is introduced to model these scenarios and developed a unified scheme for it. A formal model is developed and achieved excellent performance using Contraction Hierarchies. (iii) The project explored ways of using existing uncertain databases, both with respect to uncertainty management as well as query evaluation. It found that a significant drawback of these databases is that they are useful for managing uncertain relational data, and are not appropriate for applications that require the storage and retrieval of uncertain graphs.</p>\n<h2>Broader Impacts</h2>\n<p>The project has contributed to the education and training of a diverse engineering students at UCR and Cornell University. It has impacted the education and training of 19 graduate students (including four female students) and two post-docs. The results of the project have been broadly disseminated through the project website, publications, and release of software tools, data and demonstrations. All of the work has been published in high quality journals and conferences. New datasets were collected and made available to the community for multi-camera person tracking and re-identification, object recognition in unconstrained environment, algorithms and software for lidar and video data fusion for tracking people from moving robots in the presence of uncertainty and tracking methodology, and an information optimal exploration planner. Three Special Issues of Journals (IEEE Computer; IEEE Sensor; and Computer Vision and Image Understanding) were published. The perception, learning, planning and querying capabilities will enable a wider range of behaviors in cooperative robotic teams, which subsequently will enable a better handling of uncertainty, and improved sensing, information collection and decision making.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/11/2019<br>\n\t\t\t\t\tModified by: Bir&nbsp;Bhanu</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560224603829_fig1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560224603829_fig1--rgov-800width.jpg\" title=\"Challenges for re-identification\"><img src=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560224603829_fig1--rgov-66x44.jpg\" alt=\"Challenges for re-identification\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Challenges for re-identification</div>\n<div class=\"imageCredit\">public domain</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu</div>\n<div class=\"imageTitle\">Challenges for re-identification</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560224905515_fig3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560224905515_fig3--rgov-800width.jpg\" title=\"Framework for unbiased representation\"><img src=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560224905515_fig3--rgov-66x44.jpg\" alt=\"Framework for unbiased representation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Unbiased spatial representation and unbiased temporal representation</div>\n<div class=\"imageCredit\">Xiu Zhang</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu</div>\n<div class=\"imageTitle\">Framework for unbiased representation</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560226451723_tabel1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560226451723_tabel1--rgov-800width.jpg\" title=\"Comparison of re-id results\"><img src=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560226451723_tabel1--rgov-66x44.jpg\" alt=\"Comparison of re-id results\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Re-id results.M. Ye, et al. ICCV 2017Z. Liu, et al. ICCV 2017Y. Wu, et al. CVPR 2018</div>\n<div class=\"imageCredit\">X. Zhang</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu</div>\n<div class=\"imageTitle\">Comparison of re-id results</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560226978016_testingimage--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560226978016_testingimage--rgov-800width.jpg\" title=\"Sample image used for testing at Cornell\"><img src=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560226978016_testingimage--rgov-66x44.jpg\" alt=\"Sample image used for testing at Cornell\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">sample images used for multi objective, multi-sensor detection and tracking of pedestrians on a mobile robot</div>\n<div class=\"imageCredit\">M. Campbell</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu</div>\n<div class=\"imageTitle\">Sample image used for testing at Cornell</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560227395112_group--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560227395112_group--rgov-800width.jpg\" title=\"Group structure preserving tracking\"><img src=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560227395112_group--rgov-66x44.jpg\" alt=\"Group structure preserving tracking\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Pedestrian tracking in a camera network</div>\n<div class=\"imageCredit\">Z. Jin</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu</div>\n<div class=\"imageTitle\">Group structure preserving tracking</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560227687038_tableofresults--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560227687038_tableofresults--rgov-800width.jpg\" title=\"Tracking performance\"><img src=\"/por/images/Reports/POR/2019/1330110/1330110_10279290_1560227687038_tableofresults--rgov-66x44.jpg\" alt=\"Tracking performance\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Tracking performance on segments from PETS 2009 S2.L2 for pedestrian in groups only</div>\n<div class=\"imageCredit\">Z. Jin</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bir&nbsp;Bhanu</div>\n<div class=\"imageTitle\">Tracking performance</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nFor the next generation of autonomous systems to be truly enabling in terms of tangible performance improvements such as long-term operations and complex scenarios, a new level of intelligence is required. This intelligence can only come from a tighter coupling of the perception and planning paradigms with learning and databases so the desired sensory data are effectively converted to usable information for both planning and perception. Such a system will be able to intelligently overcome uncertainties that will inevitably arise and to learn over time, so its real-world performance is robust. The project has brought an interdisciplinary team from the University of California at Riverside and Cornell University with backgrounds in Electrical Engineering, Mechanical/Aerospace Engineering, and Computer Science to develop cyber-physical systems in the context of robotic networks equipped with cameras and operating in complex dynamic environments with a focus on scenarios where the sensors must collaborate in space and time to provide scene understanding.\nIntellectual Merit\n\nThe novelty of the approach is that it addresses perception, learning and databases in dynamic applications for increasingly robust performance over time. The achievements of the project are:\n\n(a)  Perception, Learning, Tracking, Re-identification and Representations: The project developed algorithms and systems for a group of fixed (pan-tilt and zoom video cameras) and mobile sensors (robots) to collaborate on scene understanding in scenarios, which are characterized by highly dynamic and uncertain environments. Key outcomes are: (i) A wide range of perception and learning algorithms were developed, including distributed algorithms, for tracking and data association of objects (people and vehicles) with linear, non-linear motion, social behavior of humans in groups and incorporating human walking models in crowds in a network of overlapping and non-overlapping sensors. (ii) The project developed extensive algorithms based on different principles (reference-based methods, efficient canonical correlation based analysis, deep learning and active learning methods, feature warps, network constraints, etc.) for re-identification in non-overlapping cameras in a video network, including work on fusing infrared and video cameras. The framework and algorithms are experimentally validated.\n\n(b)  Distributed Estimation and Information Planning: The project carried out a tight integration of perception and action in a probabilistic framework for truly intelligent robotic systems. It explored synergies across three areas: control, video understanding and perception, and handling data with uncertainty. It developed algorithms to enable a wider range of behaviors in cooperative robotic teams, which subsequently will enable a better handling of uncertainty, and improved sensing, information collection and decision making. The key outcomes are: (i) Information based planner with guarantees where a receding horizon approach to real time work was developed using a novel tail cost approximation. The work was verified with an indoor robot over multiple trials. (ii) A formal solution to the multi-robot multi-object exploration and tracking problems simultaneously using a hierarchical architecture.\n\n(c) Management and Querying of Uncertain Data: The key outcomes are: (i) Inferential time-decaying filters are introduced, which integrate Bayesian priors and information latent in bloom filters to make penalty-optimal, query-specific decisions. These methods can support novel query types and sliding window queries with dynamic error penalties. (ii) Planning and discovering assemblies of a large number of moving objects using partial information is challenging when object arrival observations are sparse due to inadequate sensor coverage or object countermeasures. A novel class of assembly queries is introduced to model these scenarios and developed a unified scheme for it. A formal model is developed and achieved excellent performance using Contraction Hierarchies. (iii) The project explored ways of using existing uncertain databases, both with respect to uncertainty management as well as query evaluation. It found that a significant drawback of these databases is that they are useful for managing uncertain relational data, and are not appropriate for applications that require the storage and retrieval of uncertain graphs.\nBroader Impacts\n\nThe project has contributed to the education and training of a diverse engineering students at UCR and Cornell University. It has impacted the education and training of 19 graduate students (including four female students) and two post-docs. The results of the project have been broadly disseminated through the project website, publications, and release of software tools, data and demonstrations. All of the work has been published in high quality journals and conferences. New datasets were collected and made available to the community for multi-camera person tracking and re-identification, object recognition in unconstrained environment, algorithms and software for lidar and video data fusion for tracking people from moving robots in the presence of uncertainty and tracking methodology, and an information optimal exploration planner. Three Special Issues of Journals (IEEE Computer; IEEE Sensor; and Computer Vision and Image Understanding) were published. The perception, learning, planning and querying capabilities will enable a wider range of behaviors in cooperative robotic teams, which subsequently will enable a better handling of uncertainty, and improved sensing, information collection and decision making.\n\n\t\t\t\t\tLast Modified: 06/11/2019\n\n\t\t\t\t\tSubmitted by: Bir Bhanu"
 }
}