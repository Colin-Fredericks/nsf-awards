{
 "awd_id": "1319979",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Optimal Iterative Estimation in Signal Processing, Information Theory and Machine Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2013-06-01",
 "awd_exp_date": "2018-05-31",
 "tot_intn_awd_amt": 416160.0,
 "awd_amount": 416160.0,
 "awd_min_amd_letter_date": "2013-05-31",
 "awd_max_amd_letter_date": "2013-05-31",
 "awd_abstract_narration": "Modern imaging devices, sensors, data acquisition systems allow to gather data with unprecedented speed and\r\naccuracy. Most of the times, however, we are not interested in accumulating data per se, but rather to\r\nuncover some hidden patterns in the data. For instance, given a large network, we might want to discover \r\na small subset of notes that are tightly connected to each other. Such highly connected substructures\r\nare of interest in biological datasets, but also in social network analysis, and in signal processing. \r\nFinding such patterns requires highly efficient algorithms that can process large amount of data and\r\nuncover tenuous statistical signatures. The investigators develop new algorithms that simultaneously optimize\r\nboth metrics: statistical efficiency and computational efficiency.\r\n\r\nConsider in particular the problem of finding an anomalous submatrix in a large data matrix with independent \r\nrandom entries. If the anomalous submatrix has entries with a different distribution, this can be done via \r\nprincipal component analysis, as long as the submatrix has dimensions of the  order of the square root of\r\nthe ambient dimensions. The investigators introduce a class of first order methods with linear complexity,\r\nand determine the optimal algorithm within this class. This appears to provably outperform existing approaches. \r\nThe same framework is generalized to several other classes of high-dimensional estimation problems. \r\nOptimal iterative procedures are developed  under strict computational constraints.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Montanari",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea Montanari",
   "pi_email_addr": "montanari@stanford.edu",
   "nsf_id": "000107366",
   "pi_start_date": "2013-05-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "792600",
   "pgm_ele_name": "ALGORITHMS"
  },
  {
   "pgm_ele_code": "793600",
   "pgm_ele_name": "SIGNAL PROCESSING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 416160.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modern data analysis challenges require to fit complex models with tens or hundreds of thousandsof parameters. Such parameters fitting procedures are carried out by sophisticated algorithms, andmay require dedicated hardware. In this context, the key bottleneck to carry out a specific statistical taskis not due to a lack of data, but rather to the feasibility (or infeasibility) of a certain computational task.While computational tools (both algorithms and hardware) increase in sophistication at a rapid pace,these computational barriers in statistical estimation cannot be surpassed by future improvements.Further, they are ubiquitous and arise in a large number of easy-to-state problems. A prototypicalexample is the `hidden clique problem' whereby we are asked to identify a dense subgraph in an otherwiserandom graph. This is the simplest example of a class of problems that require to estimate hidden structurein network or matrix data. Another classical example is sparse principal component analysis,whereby we want to estimate the main direction of variability of a cloud of datapoints, knowing that theunderlying signal is sparse.For these and similar problems, computational barriers correspond to sharp phase transitionsas the number of samples (data points) crosses a critical threshold. When we have less data than thethreshold, the information contained in those data is still sufficient to perform accurate statisticalinference, but extracting that information requires an exponentially large computational power and is thereforepractically impossible. This project characterized such barriers in a number of problems, and developed new algorithms&nbsp; that are guaranteed to achieve those barriers. The algorithms studied belong broadly tothree classes: semidefinite programming relaxations, iterative message passing algorithms, and gradient descentalgorithms. We find that often several algorithmic strategies are characterized by the same computational limits.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/13/2018<br>\n\t\t\t\t\tModified by: Andrea&nbsp;Montanari</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nModern data analysis challenges require to fit complex models with tens or hundreds of thousandsof parameters. Such parameters fitting procedures are carried out by sophisticated algorithms, andmay require dedicated hardware. In this context, the key bottleneck to carry out a specific statistical taskis not due to a lack of data, but rather to the feasibility (or infeasibility) of a certain computational task.While computational tools (both algorithms and hardware) increase in sophistication at a rapid pace,these computational barriers in statistical estimation cannot be surpassed by future improvements.Further, they are ubiquitous and arise in a large number of easy-to-state problems. A prototypicalexample is the `hidden clique problem' whereby we are asked to identify a dense subgraph in an otherwiserandom graph. This is the simplest example of a class of problems that require to estimate hidden structurein network or matrix data. Another classical example is sparse principal component analysis,whereby we want to estimate the main direction of variability of a cloud of datapoints, knowing that theunderlying signal is sparse.For these and similar problems, computational barriers correspond to sharp phase transitionsas the number of samples (data points) crosses a critical threshold. When we have less data than thethreshold, the information contained in those data is still sufficient to perform accurate statisticalinference, but extracting that information requires an exponentially large computational power and is thereforepractically impossible. This project characterized such barriers in a number of problems, and developed new algorithms  that are guaranteed to achieve those barriers. The algorithms studied belong broadly tothree classes: semidefinite programming relaxations, iterative message passing algorithms, and gradient descentalgorithms. We find that often several algorithmic strategies are characterized by the same computational limits.\n\n\t\t\t\t\tLast Modified: 10/13/2018\n\n\t\t\t\t\tSubmitted by: Andrea Montanari"
 }
}