{
 "awd_id": "1302693",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF:Medium: Energy Efficient and Stochastically Robust Resource Allocation for Heterogeneous Computing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2013-05-15",
 "awd_exp_date": "2017-12-31",
 "tot_intn_awd_amt": 850000.0,
 "awd_amount": 850000.0,
 "awd_min_amd_letter_date": "2013-05-08",
 "awd_max_amd_letter_date": "2015-04-30",
 "awd_abstract_narration": "Parallel and distributed computing systems are often a heterogeneous mix of machines. As these systems continue to expand rapidly in capability, their computational energy expenditure has skyrocketed, requiring elaborate cooling facilities, which themselves consume significant energy. The need for energy-efficient resource management is thus paramount. Moreover, these systems frequently experience degraded performance and high power consumption due to circumstances that change unpredictably, such as thermal hotspots caused by load imbalances or sudden machine failures. As the complexity of systems grows, so does the importance of making system operation robust against these uncertainties. The goal of this award is to study stochastic-based models, metrics, and algorithmic strategies for deriving resource allocations that are energy-efficient and robust. The research focus is on deriving stochastic robustness and energy models from real-world data from heterogeneous computing machines; applying stochastic models for resource management strategies that co-optimize performance, robustness, computation energy, and cooling energy; developing novel schemes for real-time thermal modeling; and driving and validating the research with feedback collected from real-world petascale systems (Yellowstone at National Center of Atmospheric Research and Jaguar at Oak Ridge National Lab) and terascale systems (Colorado State University's ISTeC cluster and clusters at Oak Ridge National Lab).\r\n\r\nThe research is expected to realize resource management strategies that are resilient to various sources of uncertainty at run-time while also considering the dynamics of temperature variations and cooling capacity to meet performance guarantees with unprecedented gains in system energy-efficiency in high performance computing environments. By lowering the energy costs and impact of uncertainties associated with computing, this research will ultimately render high performance computing accessible to a wider population of researchers and scientific problems. In the long term, the theoretical foundations and tools that emerge from this research will play a vital role in achieving the grand promise of sustainable computing at extreme scales within realistic power budgets. The broader impacts of the research include: incorporate research results into all levels of teaching, including graduate, undergraduate, secondary, and even elementary education; increase participation by underrepresented groups; and foster close ties with industry and government labs to transfer the developed knowledge quickly into real-world deployments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sudeep",
   "pi_last_name": "Pasricha",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sudeep Pasricha",
   "pi_email_addr": "sudeep@colostate.edu",
   "nsf_id": "000513584",
   "pi_start_date": "2013-05-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Howard",
   "pi_last_name": "Siegel",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Howard J Siegel",
   "pi_email_addr": "hj@colostate.edu",
   "nsf_id": "000160596",
   "pi_start_date": "2013-05-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Patrick",
   "pi_last_name": "Burns",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Patrick J Burns",
   "pi_email_addr": "pburns@colostate.edu",
   "nsf_id": "000235688",
   "pi_start_date": "2013-05-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anthony",
   "pi_last_name": "Maciejewski",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Anthony A Maciejewski",
   "pi_email_addr": "aam@colostate.edu",
   "nsf_id": "000270838",
   "pi_start_date": "2013-05-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colorado State University",
  "inst_street_address": "601 S HOWES ST",
  "inst_street_address_2": "",
  "inst_city_name": "FORT COLLINS",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "9704916355",
  "inst_zip_code": "805212807",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": "COLORADO STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "LT9CXX8L19G1"
 },
 "perf_inst": {
  "perf_inst_name": "Colorado State University",
  "perf_str_addr": "200 W. Lake St.",
  "perf_city_name": "Fort Collins",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "805214593",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "794200",
   "pgm_ele_name": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 554144.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 295856.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High Performance Computing (HPC) systems are widely used in fields as diverse as weather modeling, financial predictions, fluid dynamics, and big data searches, and are opening the doors to new discoveries. For instance, the field of genomics relies heavily on very large supercomputers. Because of genomics, we have new drugs, ways of diagnosing disease, and crime investigation techniques. But the energy costs of operating HPC systems, whether it be supercomputers, data centers, or clusters of machines, are becoming prohibitive as HPC systems evolve. As an example, the K supercomputer in Japan consumes enough energy to power 10,000 homes, at a cost of $10 million/year. The petascale Yellowstone supercomputer at NCAR in Wyoming, USA has an energy expenditure of $2 million/year. Even smaller terascale HPC clusters, such as those at Colorado State University (CSU) and Oak Ridge National Lab (ORNL), have significant computational and cooling energy costs, ranging from $20K-200K per year. These energy costs impose a huge monetary burden on the scientific community, taking HPC systems out of the reach of those who have the greatest potential to make groundbreaking discoveries that can benefit society. Prior efforts to improve energy-efficiency in computing are unfortunately either not applicable to large HPC platforms or ignore important facets of the problem, such as cooling/thermal costs and uncertainties that often surface in HPC platforms at runtime.</p>\n<p class=\"Default\">The overarching theme of this proposal has been to devise a new software-based resource management framework that can intelligently manage the execution of applications on large-scale HPC platforms, while minimizing the energy needed for computation and cooling. The fundamental innovation that has emerged from this project is the discovery of the complex relationship between cooling and computation energy in HPC platforms, and its characterization using stochastic performance, robustness, and power models derived from real-world data (from terascale and petascale HPC systems). The insights from these models have guided the design of new strategies to co-optimize computing performance, robustness, computation power, and cooling power in large-scale HPC platforms. These strategies have further benefitted from new models that have been developed for 1) quantifying the impact of interference in shared memory and network subsystems; 2) fast real-time thermal characterization; and 3) cooling energy costs and capacity.</p>\n<p class=\"Default\">Rigorous experimental analysis has shown that the developed models and framework significantly outperform the best known prior efforts to quantify and optimize energy usage in HPC platforms. This project has also contributed to the integration of energy-efficient resource management in production HPC systems at ORNL and the Department of Defense (DoD). The research contributions ultimately represent unique and valuable solutions to overcome the energy challenge facing the design of future HPC platforms. The innovations from this research have been widely disseminated through over 50 peer-reviewed scientific journal/conference publications, as well as several invited industry and conference seminar talks, keynotes, and tutorials. The technical outcomes of this project have thus made significant and lasting contributions towards the goal of meeting current national needs for energy-efficient and cost-effective HPC systems.</p>\n<p>Beyond the technical objectives accomplished, this project also has had an immense broader impact. The techniques developed as part of this project have the potential to be applied to a variety of computing and communication system environments. As an example, the algorithms and models for energy-efficient HPC resource management from the project were successfully applied to solve multi-objective resource management problems in manycore electronic chip design. Several students have been fully or partially supported by this project. A total of eight Ph.D. students, two post-doctoral fellows, three M.S. students, and seven senior undergraduate students have conducted research with the faculty as part of this project. As part of K-12 outreach, four high school students also have been provided opportunities to work with the senior students on this project, and learn about the exciting opportunities in computer engineering. By exposing these students to the diverse aspects of modeling and analysis, optimization algorithms, emerging hardware, and software applications, and disseminating the developed ideas and outcomes via curriculum enhancements at CSU, the proposed research has also significantly contributed to an agile high-tech workforce that will maintain continued USA leadership in technological innovation.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/26/2018<br>\n\t\t\t\t\tModified by: Sudeep&nbsp;Pasricha</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1302693/1302693_10244169_1516989959872_1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1302693/1302693_10244169_1516989959872_1--rgov-800width.jpg\" title=\"Overview of energy-efficient and robust resource management in heterogeneous HPC systems\"><img src=\"/por/images/Reports/POR/2018/1302693/1302693_10244169_1516989959872_1--rgov-66x44.jpg\" alt=\"Overview of energy-efficient and robust resource management in heterogeneous HPC systems\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Block diagram of the resource management framework that optimizes computational and cooling energy in a large HPC data center. The specific HPC data center shown is for illustrative purposes only, and the framework can be applied to any data center configuration.</div>\n<div class=\"imageCredit\">Mark Oxley, Sudeep Pasricha</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sudeep&nbsp;Pasricha</div>\n<div class=\"imageTitle\">Overview of energy-efficient and robust resource management in heterogeneous HPC systems</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nHigh Performance Computing (HPC) systems are widely used in fields as diverse as weather modeling, financial predictions, fluid dynamics, and big data searches, and are opening the doors to new discoveries. For instance, the field of genomics relies heavily on very large supercomputers. Because of genomics, we have new drugs, ways of diagnosing disease, and crime investigation techniques. But the energy costs of operating HPC systems, whether it be supercomputers, data centers, or clusters of machines, are becoming prohibitive as HPC systems evolve. As an example, the K supercomputer in Japan consumes enough energy to power 10,000 homes, at a cost of $10 million/year. The petascale Yellowstone supercomputer at NCAR in Wyoming, USA has an energy expenditure of $2 million/year. Even smaller terascale HPC clusters, such as those at Colorado State University (CSU) and Oak Ridge National Lab (ORNL), have significant computational and cooling energy costs, ranging from $20K-200K per year. These energy costs impose a huge monetary burden on the scientific community, taking HPC systems out of the reach of those who have the greatest potential to make groundbreaking discoveries that can benefit society. Prior efforts to improve energy-efficiency in computing are unfortunately either not applicable to large HPC platforms or ignore important facets of the problem, such as cooling/thermal costs and uncertainties that often surface in HPC platforms at runtime.\nThe overarching theme of this proposal has been to devise a new software-based resource management framework that can intelligently manage the execution of applications on large-scale HPC platforms, while minimizing the energy needed for computation and cooling. The fundamental innovation that has emerged from this project is the discovery of the complex relationship between cooling and computation energy in HPC platforms, and its characterization using stochastic performance, robustness, and power models derived from real-world data (from terascale and petascale HPC systems). The insights from these models have guided the design of new strategies to co-optimize computing performance, robustness, computation power, and cooling power in large-scale HPC platforms. These strategies have further benefitted from new models that have been developed for 1) quantifying the impact of interference in shared memory and network subsystems; 2) fast real-time thermal characterization; and 3) cooling energy costs and capacity.\nRigorous experimental analysis has shown that the developed models and framework significantly outperform the best known prior efforts to quantify and optimize energy usage in HPC platforms. This project has also contributed to the integration of energy-efficient resource management in production HPC systems at ORNL and the Department of Defense (DoD). The research contributions ultimately represent unique and valuable solutions to overcome the energy challenge facing the design of future HPC platforms. The innovations from this research have been widely disseminated through over 50 peer-reviewed scientific journal/conference publications, as well as several invited industry and conference seminar talks, keynotes, and tutorials. The technical outcomes of this project have thus made significant and lasting contributions towards the goal of meeting current national needs for energy-efficient and cost-effective HPC systems.\n\nBeyond the technical objectives accomplished, this project also has had an immense broader impact. The techniques developed as part of this project have the potential to be applied to a variety of computing and communication system environments. As an example, the algorithms and models for energy-efficient HPC resource management from the project were successfully applied to solve multi-objective resource management problems in manycore electronic chip design. Several students have been fully or partially supported by this project. A total of eight Ph.D. students, two post-doctoral fellows, three M.S. students, and seven senior undergraduate students have conducted research with the faculty as part of this project. As part of K-12 outreach, four high school students also have been provided opportunities to work with the senior students on this project, and learn about the exciting opportunities in computer engineering. By exposing these students to the diverse aspects of modeling and analysis, optimization algorithms, emerging hardware, and software applications, and disseminating the developed ideas and outcomes via curriculum enhancements at CSU, the proposed research has also significantly contributed to an agile high-tech workforce that will maintain continued USA leadership in technological innovation. \n\n\t\t\t\t\tLast Modified: 01/26/2018\n\n\t\t\t\t\tSubmitted by: Sudeep Pasricha"
 }
}