{
 "awd_id": "1348109",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: A Multiagent Teacher/Student Framework for Sequential Decision Making Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032928074",
 "po_email": "jdonlon@nsf.gov",
 "po_sign_block_name": "James Donlon",
 "awd_eff_date": "2013-01-02",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 401931.0,
 "awd_amount": 421894.0,
 "awd_min_amd_letter_date": "2013-08-14",
 "awd_max_amd_letter_date": "2015-05-06",
 "awd_abstract_narration": "Physical (robotic) agents and virtual (software) agents are becoming increasingly common in industry, education, and domestic environments. Although recent research advances have enabled agents to learn how to complete tasks without human intervention, little is known about how best to have humans teach agents or agents teach other agents or even how agents might teach humans. Considering the full matrix of agent/human learning, in which either an agent or a human can play the role of teacher or student, would increase the potential benefits of leveraging human and agent expertise and knowledge. \r\n\r\nThis project aims to study agent/human learning in the context of sequential decision-making problems, a class of central importance for real-world agent systems. This project aims to develop a novel teacher/student framework that integrates autonomous learning with teaching by another agent or a human. The project plans to develop and evaluate a set of core algorithms to allow: (1) agents to teach agents, thus enabling robust knowledge sharing among agents; (2) humans to teach agents, thus allowing humans to share or transfer common sense or domain-specific knowledge with agents; and (3) agents to teach humans, thus helping humans better understand how to perform or recast sequential decision-making tasks already understood or performed by autonomous agents. In all cases, the goal is to develop methods that significantly improve learning performance relative to learning without guidance from a teacher. Issues to be explored include mismatch between teacher/student abilities, learning from multiple teachers, and shared knowledge representation between teacher/student. The PI plans to focus on several scenarios, each with different sets of assumptions about the knowledge or skill of the student or teacher and the kind of interaction possible between them (e.g., whether the teacher can tell the student what action to take). The techniques developed in the project will be evaluated in a variety of tests domains and will involve simulations as well as actual robots.\r\n\r\nThe teacher/student framework will enable agents to teach other agents and humans, as well as integrate autonomous learning with agent and human teaching. Understanding how to best teach agents is of key importance in developing deployable agent systems. The platform- and domain-independent approach incorporates ideas from multiagent systems, machine learning, human-computer interaction, and human-robot interaction communities, and has the potential to impact each of these areas. This work takes a step towards transitioning agents from specialized systems usable only by experts into useful tools and teammates for people without programming expertise. \r\n\r\nThis project has a strong educational component. The PI teaches at an undergraduate college and undergraduate students will play a crucial role throughout the project. Furthermore, the research produced by this project will be incorporated into five of the PI's courses, providing exciting new material to attract and retain computer science majors. The PI will also continue outreach to secondary school students as well as to underrepresented groups via Lafayette College's S-STEM and Higher Achievement programs.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Taylor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Taylor",
   "pi_email_addr": "taylorm@eecs.wsu.edu",
   "nsf_id": "000560224",
   "pi_start_date": "2013-08-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington State University",
  "inst_street_address": "240 FRENCH ADMINISTRATION BLDG",
  "inst_street_address_2": "",
  "inst_city_name": "PULLMAN",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "5093359661",
  "inst_zip_code": "991640001",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "WA05",
  "org_lgl_bus_name": "WASHINGTON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "XRJSGX384TD6"
 },
 "perf_inst": {
  "perf_inst_name": "Washington State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "991643140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "WA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 401931.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 9231.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 10732.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Physical (robotic) agents and virtual (software) agents are becoming increasingly common in industry, education, and domestic environments. Although recent research advances have enabled agents to learn how to complete tasks without human intervention via reinforcement learning techniques, relatively little research has been conducted on allowing on agent to teach another.</p>\n<p>&nbsp;</p>\n<p>This project primarily considered</p>\n<ol>\n<li>How a human should teach an agent</li>\n<li>How one agent should teach another agent</li>\n</ol>\n<p>In regard to #1, we invented methods and algorithms that allowed a human to teach via demonstration (e.g., teleoperating the agent) and via evaluative feedback (e.g., providing positive and negative reward signals). In both cases, agents provided this information were able to learn to accomplish the task. In the case of demonstration, we combined learning from human input with reinforcement learning (RL), where the goal is to maximize an environmental reward signal. Combining RL with learning from demonstration outperformed learning only from the environmental reward, as well as outperforming the human&rsquo;s performance. One long-term goal of this research thrust is to allow non-technical users to be able to define and teach agents sequential decision tasks in real-world settings.</p>\n<p>&nbsp;</p>\n<p>In regard to #2, we focused on the idea of &ldquo;action advice,&rdquo; where one agent would tell another agent what to do in a given state. In particular, we assumed there was a fixed budget, or number of times that the teacher agent could assist the student agent. We therefore aimed to maximize student performance by cleverly deciding when the teacher should provide advice. Our work empirically showed that it was possible for a student to significantly improve learning through relatively little advice. We also showed theoretically that a poor teacher would not affect the student in the long term. One long-term goal of this research thrust is to allow an experienced agent to teach a novice agent a sequential decision task, even if they have limited communication abilities and do not have representations of each other&rsquo;s internal knowledge structure.</p>\n<p>&nbsp;</p>\n<p>The primary intellectual merit outcomes of this work have been multiple peer-reviewed scientific journal articles and conference papers. The immediate broader impacts of this work have included 1) funding underrepresented minorities, undergraduates, and women in research; 2) incorporating outcomes of, and domains developed in, this research into both undergraduate and graduate courses taught at Washington State University; 3) the training of multiple PhD students; and 4) code releases to allow the easy replication and extension of the research.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2018<br>\n\t\t\t\t\tModified by: Matthew&nbsp;Taylor</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPhysical (robotic) agents and virtual (software) agents are becoming increasingly common in industry, education, and domestic environments. Although recent research advances have enabled agents to learn how to complete tasks without human intervention via reinforcement learning techniques, relatively little research has been conducted on allowing on agent to teach another.\n\n \n\nThis project primarily considered\n\nHow a human should teach an agent\nHow one agent should teach another agent\n\n\nIn regard to #1, we invented methods and algorithms that allowed a human to teach via demonstration (e.g., teleoperating the agent) and via evaluative feedback (e.g., providing positive and negative reward signals). In both cases, agents provided this information were able to learn to accomplish the task. In the case of demonstration, we combined learning from human input with reinforcement learning (RL), where the goal is to maximize an environmental reward signal. Combining RL with learning from demonstration outperformed learning only from the environmental reward, as well as outperforming the human?s performance. One long-term goal of this research thrust is to allow non-technical users to be able to define and teach agents sequential decision tasks in real-world settings.\n\n \n\nIn regard to #2, we focused on the idea of \"action advice,\" where one agent would tell another agent what to do in a given state. In particular, we assumed there was a fixed budget, or number of times that the teacher agent could assist the student agent. We therefore aimed to maximize student performance by cleverly deciding when the teacher should provide advice. Our work empirically showed that it was possible for a student to significantly improve learning through relatively little advice. We also showed theoretically that a poor teacher would not affect the student in the long term. One long-term goal of this research thrust is to allow an experienced agent to teach a novice agent a sequential decision task, even if they have limited communication abilities and do not have representations of each other?s internal knowledge structure.\n\n \n\nThe primary intellectual merit outcomes of this work have been multiple peer-reviewed scientific journal articles and conference papers. The immediate broader impacts of this work have included 1) funding underrepresented minorities, undergraduates, and women in research; 2) incorporating outcomes of, and domains developed in, this research into both undergraduate and graduate courses taught at Washington State University; 3) the training of multiple PhD students; and 4) code releases to allow the easy replication and extension of the research.\n\n \n\n\t\t\t\t\tLast Modified: 11/22/2018\n\n\t\t\t\t\tSubmitted by: Matthew Taylor"
 }
}