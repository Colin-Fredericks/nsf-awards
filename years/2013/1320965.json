{
 "awd_id": "1320965",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NeTS: Small: Scalable Network Virtualization for Multi-Core Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032922935",
 "po_email": "dmedhi@nsf.gov",
 "po_sign_block_name": "Deepankar Medhi",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 470315.0,
 "awd_amount": 470315.0,
 "awd_min_amd_letter_date": "2013-08-19",
 "awd_max_amd_letter_date": "2013-08-19",
 "awd_abstract_narration": "As virtualization spreads throughout the data center, the communication endpoints within the data center are becoming virtual machines (VMs), not physical servers. Therefore, the network facilities for packet processing and security must operate all the way to the VM. This last hop switching among virtual machines within the physical server has become a critically important component of the data center network.  This project will explore the design space of server/switch integration, in which software and hardware based switching are more efficiently integrated directly into the server. More specifically, it will decouple the switching operations and distribute their implementation throughout the virtualized system. This project will be comprised of the following thrusts: \r\n   - Efficient Software Switching via a decoupled software switching architecture using a flow-based approach that eliminates these\r\n     overheads and that enables efficient switching in software\r\n   - Judicious choice and use of hardware acceleration.  \r\n   - The Rack Becomes the Server via having the network control plane learning the hardware topology, monitoring the network traffic,\r\n     instructing the hypervisor to move communicating VMs closer to each other. \r\n\r\nAs the demand for data center capacity continues to increase at an incredible rate, it is becoming critical to minimize the number of physical machines. This research will transform the way in which networking is implemented on future systems, enabling the effective use of 100s of virtual machines per physical machine. This reduction in physical machines can have significant societal and economic impacts. If U.S. data centers used virtualization technology to achieve just half of the power reductions predicted by Intel, we would save $3.5 billion of electricity and prevent the release of 322 million pounds of CO2 per year.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Cox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alan Cox",
   "pi_email_addr": "alc@rice.edu",
   "nsf_id": "000277789",
   "pi_start_date": "2013-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "Rice University",
  "perf_str_addr": "6100 Main Street, MS 132",
  "perf_city_name": "Houston",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051892",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "TX",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736300",
   "pgm_ele_name": "Networking Technology and Syst"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 470315.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As the migration of computing into the cloud continues at an<br />incredible pace, so will the demand for data center capacity.&nbsp; To meet<br />this demand at low cost, it is critical to make effective use of the<br />physical servers in the data center.&nbsp; Historically, most of these<br />servers have been underutilized, wasting not only space but also<br />energy.&nbsp; Now, machine virtualization is widely recognized as a<br />successful approach to addressing these issues.<br /><br />As the use of machine virtualization has spread throughout the data<br />center, the communication endpoints (or hosts) within the data center<br />have become virtual machines (VMs), not physical servers.&nbsp; Therefore,<br />the network facilities for packet processing and security must operate<br />all the way to the VM.&nbsp; This last hop switching among virtual machines<br />within the physical server has become a critically important component<br />of the data center network.&nbsp; The aim of this research on the data<br />center network has been to transform the way in which networking is<br />implemented on future multi-core systems, eliminating one barrier to<br />effectively running hundreds of VMs per physical server.<br /><br />In current datacenter networks, failures have become the norm.<br />However, the latency to respond to these failures is increasingly<br />problematic as software recovery struggles or even fails to meet tight<br />service-level agreements.&nbsp; On the other hand, hardware recovery can be<br />near instantaneous.&nbsp; In this project, we have developed an approach to<br />practically implement t-resilient hardware, which protects against up<br />to t simultaneous failures.&nbsp; Moreover, we show that the next<br />generation switching chips that could be embedded within a virtualized<br />physical server can implement moderate yet interesting values of t.<br />Although forwarding table size is a limiting factor, we find that low<br />levels of resilience are highly effective at preventing failures.&nbsp; For<br />example, only 0.0002% of the pairwise paths between hosts are expected<br />to fail given 4-resilience and 64 edge failures on a 2048-host network<br />topology.<br /><br />To reduce forwarding table size, we have explored different approaches to<br />achieving forwarding table compression.&nbsp; First, we have developed<br />a new forwarding table compression algorithm.&nbsp; Then, we have<br />observed that only forwarding table entries that share both the<br />same output and the same packet modification action can be<br />compressed, which implies that the achievable compression ratio is<br />limited by the number of unique (output, action) pairs in the<br />table.&nbsp; Thus, this project's next two contributions were<br />explicitly conceived as ways to increase the number of common<br />outputs and actions.&nbsp; First, we introduced the concept of<br />compression-aware routing, which increases the number of entries<br />with common forwarding table outputs.&nbsp; Second, we created<br />Plinko, a new forwarding model in which all entries in the<br />forwarding table apply the same action.<br /><br />Lastly, storage area networking is driving data center switches to support<br />lossless Ethernet (DCB).&nbsp; Unfortunately, to enable DCB for all traffic<br />on arbitrary network topologies, we must address several problems that<br />can arise in lossless networks, e.g., large buffering delays,<br />unfairness, head of line blocking, and deadlock.&nbsp; To this end, we have<br />developed TCP-Bolt, a TCP variant that not only addresses the first<br />three problems but reduces flow completion times by as much as 70%.<br />We have also developed a simple, practical deadlock-free routing<br />scheme that eliminates deadlock while achieving aggregate network<br />throughput within 15% of ECMP routing.&nbsp; This small compromise in<br />potential routing capacity is well worth the gains in flow completion<br />time.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/12/2019<br>\n\t\t\t\t\tModified by: Alan&nbsp;Cox</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs the migration of computing into the cloud continues at an\nincredible pace, so will the demand for data center capacity.  To meet\nthis demand at low cost, it is critical to make effective use of the\nphysical servers in the data center.  Historically, most of these\nservers have been underutilized, wasting not only space but also\nenergy.  Now, machine virtualization is widely recognized as a\nsuccessful approach to addressing these issues.\n\nAs the use of machine virtualization has spread throughout the data\ncenter, the communication endpoints (or hosts) within the data center\nhave become virtual machines (VMs), not physical servers.  Therefore,\nthe network facilities for packet processing and security must operate\nall the way to the VM.  This last hop switching among virtual machines\nwithin the physical server has become a critically important component\nof the data center network.  The aim of this research on the data\ncenter network has been to transform the way in which networking is\nimplemented on future multi-core systems, eliminating one barrier to\neffectively running hundreds of VMs per physical server.\n\nIn current datacenter networks, failures have become the norm.\nHowever, the latency to respond to these failures is increasingly\nproblematic as software recovery struggles or even fails to meet tight\nservice-level agreements.  On the other hand, hardware recovery can be\nnear instantaneous.  In this project, we have developed an approach to\npractically implement t-resilient hardware, which protects against up\nto t simultaneous failures.  Moreover, we show that the next\ngeneration switching chips that could be embedded within a virtualized\nphysical server can implement moderate yet interesting values of t.\nAlthough forwarding table size is a limiting factor, we find that low\nlevels of resilience are highly effective at preventing failures.  For\nexample, only 0.0002% of the pairwise paths between hosts are expected\nto fail given 4-resilience and 64 edge failures on a 2048-host network\ntopology.\n\nTo reduce forwarding table size, we have explored different approaches to\nachieving forwarding table compression.  First, we have developed\na new forwarding table compression algorithm.  Then, we have\nobserved that only forwarding table entries that share both the\nsame output and the same packet modification action can be\ncompressed, which implies that the achievable compression ratio is\nlimited by the number of unique (output, action) pairs in the\ntable.  Thus, this project's next two contributions were\nexplicitly conceived as ways to increase the number of common\noutputs and actions.  First, we introduced the concept of\ncompression-aware routing, which increases the number of entries\nwith common forwarding table outputs.  Second, we created\nPlinko, a new forwarding model in which all entries in the\nforwarding table apply the same action.\n\nLastly, storage area networking is driving data center switches to support\nlossless Ethernet (DCB).  Unfortunately, to enable DCB for all traffic\non arbitrary network topologies, we must address several problems that\ncan arise in lossless networks, e.g., large buffering delays,\nunfairness, head of line blocking, and deadlock.  To this end, we have\ndeveloped TCP-Bolt, a TCP variant that not only addresses the first\nthree problems but reduces flow completion times by as much as 70%.\nWe have also developed a simple, practical deadlock-free routing\nscheme that eliminates deadlock while achieving aggregate network\nthroughput within 15% of ECMP routing.  This small compromise in\npotential routing capacity is well worth the gains in flow completion\ntime.\n\n\t\t\t\t\tLast Modified: 11/12/2019\n\n\t\t\t\t\tSubmitted by: Alan Cox"
 }
}