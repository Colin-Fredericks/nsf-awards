{
 "awd_id": "1316922",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Efficient Parallel Iterative Monte Carlo Methods for Statistical Analysis of Big Data",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 81575.0,
 "awd_amount": 81575.0,
 "awd_min_amd_letter_date": "2013-06-26",
 "awd_max_amd_letter_date": "2013-06-26",
 "awd_abstract_narration": "The integration of computer technology into science and daily life has enabled the collection of massive volumes of data. To analyze these data, one may have to resort to parallel and distributed architectures. While the parallel and distributed architectures present new capabilities for storage and manipulation of big data, it is unclear, from the inferential point of view, how the current statistical methodology can be transported to the paradigm of big data. Also, growing data size typically comes together with a growing complexity of data structures and of the models needed to account for the structures. Although iterative Monte Carlo algorithms, such as the Markov chain Monte Carlo (MCMC), stochastic approximation, and expectation-maximization (EM) algorithms, have proven to be very powerful and typically unique computational tools for analyzing data of complex structures, they are infeasible for big data as for which a large number of iterations and a complete scan of the full dataset for each iteration are typically required. Big data have put a great challenge on the current statistical methodology. The investigators propose a general principle for developing Monte Carlo algorithms that are feasible for big data and workable on parallel and distributed architectures; that is, using Monte Carlo averages calculated in parallel from subsamples to approximate the quantities that originally need to calculate from the full dataset. This principle avoids the requirement for repeated scans of full data in algorithm iterations, while enabling the algorithm to produce statistically sensible solutions to the problem under consideration. Under this principle, a general algorithm, the so-called subsampling approximation-based parallel stochastic approximation algorithm, is proposed for parameter estimation for big data problems. Unlike the existing algorithms, such as the bag of little bootstraps, aggregated estimation equation, and split-and-conquer algorithms, the proposed algorithm works for the problems for which the observations are generally dependent. Under the same principle, a subsampling approximation-based parallel Metropolis-Hastings algorithm is proposed for Bayesian analysis of big data, and a subsampling approximation-based parallel Monte Carlo EM algorithm is proposed for parameter estimation for the big data problems with missing observations. In addition to the subsampling approximation-based parallel iterative Monte Carlo algorithms, an embarrassingly parallel MCMC algorithm is proposed for Bayesian analysis of big data based on the popular idea of divide-and-conquer. Various schemes of dataset partition and results aggregation are proposed. The validity of the proposed parallel iterative Monte Carlo algorithms, including both the subsampling approximation-based and embarrassingly parallel ones, will be rigorously studied. The proposed algorithms will be applied to spatio-temporal modeling of satellite climate data, genome-wide association study, and stream data analysis.\r\n\r\nThe intellectual merit of this project is to propose a general principle for statistical analysis of big data: Using Monte Carlo averages of subsamples to approximate the quantities that originally need to calculate from the full dataset. This principle provides a general strategy for transporting the current statistical methodology to the paradigm of big data. Under this principle, a few subsampling approximation-based parallel iterative Monte Carlo algorithms are proposed. The proposed algorithms address the core problem of big data analysis?how to make a statistically sensible analysis for big data while avoiding repeated scans of the full dataset. This project will have broader impacts because big data are ubiquitous throughout almost all fields of science and technology. A successful research program in theory and methods of parallel iterative Monte Carlo computations can have immense benefit widely throughout science and technology.  The research results will be disseminated to the communities of interest, such as atmospheric science, biomedical science, engineering, and social science, via direct collaboration with researchers in these disciplines, conference presentations, books, and papers to be published in academic journals. The project will have also significant impacts on education through direct involvement of graduate students in the project and incorporation of results into undergraduate and graduate courses. In addition, the package Distributed Iterative Statistical Computing (DISC) that will be developed under this project is designed to provide a platform for Ph.D. students and researchers like the investigators with network-connected computers to experiment new ideas of developing efficient iterative Monte Carlo algorithms in parallel or, more exactly, grid computing environments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chuanhai",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chuanhai Liu",
   "pi_email_addr": "chuanhai@purdue.edu",
   "nsf_id": "000096911",
   "pi_start_date": "2013-06-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "250 N. University Street",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072066",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  },
  {
   "pgm_ele_code": "808400",
   "pgm_ele_name": "CDS&E"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 81575.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Iterative statistical&nbsp;algorithms such as Expectation-Maximization and Markov Chain Monte Carlo methods have proven to be indispensable&nbsp;tools for statistical analysis with complex models. Developing such methods for analyzing big data in parallel and distributed&nbsp;computing environment is useful but very challenging.&nbsp;In this collaborative research on&nbsp;Efficient Parallel Iterative Monte Carlo Methods for Statistical Analysis of Big Data, the investigator studied statistical computing software or environment for big data analysis. With new ideas appeared in the world of&nbsp;software development, the PI designed and built a prototype of a multi-threaded and distributed R-like software, temporarily called&nbsp;SupR.</p>\n<p>Over the period of three years, partially supported by this&nbsp;award,&nbsp;the PI started with an initial effort to develop a prototype of a software, purely written in R, for Distributed Iterative Statistical Computation (DISC). This prototype, based on the approach of Apache Hadoop,&nbsp;was&nbsp;created as an add-on R package. It is currently&nbsp;available at&nbsp;http://www.stat.purdue.edu/~chuanhai/disc/update.txt. This package was used at Purdue in graduate topic courses taught by the PI to discuss parallel and distributed statistical computing.</p>\n<p>The world of software development is an exciting place&nbsp;where new ideas and products appear quickly.&nbsp;During the period of the research, for example, Apache Spark and Apache Flink&nbsp;became very popular. The PI studied the Spark functionality for iterative statistical computing and, as an experiment,&nbsp;modified Spark system source code for efficient distributed iterative statistical computing. This work along with the modified source code and test examples can be found at&nbsp;http://www.stat.purdue.edu/~chuanhai/teaching/BigData2015/spp/index.html.</p>\n<p>For creating user-friendly software for data analysts, the PI also developed a prototype of a multi-threaded and distributed R-like software, temporarily called SupR.&nbsp;At a high level, SupR is a R-style implementation of a computing system for DISC. The prototype was&nbsp;built from&nbsp;<a href=\"http://www.r-project.org/\">R</a>, Version 3.1.1 and was focused&nbsp;on the implementation of (1) a R-style front-end by maintaining the existing R syntax and internal basic data structures, (2) a Java-like multithreading model, which would be the key to the success of big data analysis, (3) a Spark-like cluster computing environment, and (4) a builtin Simple Distributed File System, which, to some extent, represents a kind of cluster-wide namespace. The prototype of SupR is currently available at&nbsp;http://www.stat.purdue.edu/~chuanhai/SupR/index.html with documentation and simple examples such as EM and distributed linear regression.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/22/2017<br>\n\t\t\t\t\tModified by: Chuanhai&nbsp;Liu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIterative statistical algorithms such as Expectation-Maximization and Markov Chain Monte Carlo methods have proven to be indispensable tools for statistical analysis with complex models. Developing such methods for analyzing big data in parallel and distributed computing environment is useful but very challenging. In this collaborative research on Efficient Parallel Iterative Monte Carlo Methods for Statistical Analysis of Big Data, the investigator studied statistical computing software or environment for big data analysis. With new ideas appeared in the world of software development, the PI designed and built a prototype of a multi-threaded and distributed R-like software, temporarily called SupR.\n\nOver the period of three years, partially supported by this award, the PI started with an initial effort to develop a prototype of a software, purely written in R, for Distributed Iterative Statistical Computation (DISC). This prototype, based on the approach of Apache Hadoop, was created as an add-on R package. It is currently available at http://www.stat.purdue.edu/~chuanhai/disc/update.txt. This package was used at Purdue in graduate topic courses taught by the PI to discuss parallel and distributed statistical computing.\n\nThe world of software development is an exciting place where new ideas and products appear quickly. During the period of the research, for example, Apache Spark and Apache Flink became very popular. The PI studied the Spark functionality for iterative statistical computing and, as an experiment, modified Spark system source code for efficient distributed iterative statistical computing. This work along with the modified source code and test examples can be found at http://www.stat.purdue.edu/~chuanhai/teaching/BigData2015/spp/index.html.\n\nFor creating user-friendly software for data analysts, the PI also developed a prototype of a multi-threaded and distributed R-like software, temporarily called SupR. At a high level, SupR is a R-style implementation of a computing system for DISC. The prototype was built from R, Version 3.1.1 and was focused on the implementation of (1) a R-style front-end by maintaining the existing R syntax and internal basic data structures, (2) a Java-like multithreading model, which would be the key to the success of big data analysis, (3) a Spark-like cluster computing environment, and (4) a builtin Simple Distributed File System, which, to some extent, represents a kind of cluster-wide namespace. The prototype of SupR is currently available at http://www.stat.purdue.edu/~chuanhai/SupR/index.html with documentation and simple examples such as EM and distributed linear regression.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 02/22/2017\n\n\t\t\t\t\tSubmitted by: Chuanhai Liu"
 }
}