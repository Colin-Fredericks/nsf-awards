{
 "awd_id": "1309725",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "US-German Collaboration: Circuit models of form processing in primate V4",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 691465.0,
 "awd_amount": 691465.0,
 "awd_min_amd_letter_date": "2013-09-09",
 "awd_max_amd_letter_date": "2013-09-09",
 "awd_abstract_narration": "This collaborative study aims to advance the understanding of visual object recognition by combining electrophysiology, computational, modeling and psychophysics to probe the implications of newly discovered properties of neurons in visual cortical area V4, an important intermediate stage in the shape processing pathway of the brain.\r\n\r\nV4 neurons respond selectively to a variety of shape attributes, but recent studies demonstrate that they are also selective for the contrast polarity of stimuli and can be broadly classified into four categories based on their preference for the luminance contrast of shapes relative to a uniform background.  Specifically, Equiluminance cells respond best to stimuli defined purely by a chromatic contrast with no luminance contrast while Bright, Dark and Contrast cells respond best to positive contrasts, negative contrasts or either, respectively.  Because these categories are based on simple stimuli, it remains unknown how these cells respond to more naturalistic stimuli, where boundaries are seldom defined by a fixed luminance contrast, and whether the different cell classes have different functional roles for encoding objects. Characterizing V4 neurons with a parameterized set of naturalistic stimuli that are developed with rigorous psychophysical testing will provide novel insights into underlying circuitry and function and open new understanding about V4 and the ventral stream.\r\n\r\nComputational models of visual form processing in the brain have also been limited: they have not taken account of realistic physiological cell types known to exist from the retina to the visual cortex, they have been largely aimed at processing achromatic signals, and have relied heavily on feedfoward processing.  This study will generate models that overcome these limitations and are invaluable for gaining insights into the circuits and mechanisms underlying form processing. These models will be available in an open, online framework designed to set a standard for ease of use and transparency, to spur further collaboration between theoreticians and experimentalists, and to facilitate education.\r\n\r\nFinally, there has been a longstanding debate in vision science, motivated from the psychophysical literature, that questions whether and how chromatic signals contribute to form processing. The traditional view has been that boundary detection and segmentation are solely based on luminance contrast. Color then paints a surface within the confines of the identified boundary. Recent psychophysical and theoretical studies are at odds with this view and argue that color is important for segmentation and form processing in natural scenes, for example, fruit amidst leaves, where detection based on luminance contrast is very difficult. The experiments in this study will inject much needed physiology data into this debate and the models developed here will shed light on the functional organization of cortical pathways at multiple stages, revealing how different aspects of our natural visual input contribute to form perception.\r\n\r\nThis award is being co-funded by NSF's Office of the Director, International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wyeth",
   "pi_last_name": "Bair",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Wyeth Bair",
   "pi_email_addr": "wyeth0@uw.edu",
   "nsf_id": "000626703",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anitha",
   "pi_last_name": "Pasupathy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anitha Pasupathy",
   "pi_email_addr": "pasupat@u.washington.edu",
   "nsf_id": "000631457",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981950001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "729800",
   "pgm_ele_name": "International Research Collab"
  },
  {
   "pgm_ele_code": "732700",
   "pgm_ele_name": "CRCNS-Computation Neuroscience"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5936",
   "pgm_ref_txt": "GERMANY (F.R.G.)"
  },
  {
   "pgm_ref_code": "5980",
   "pgm_ref_txt": "WESTERN EUROPE PROGRAM"
  },
  {
   "pgm_ref_code": "7298",
   "pgm_ref_txt": "COLLABORATIVE RESEARCH"
  },
  {
   "pgm_ref_code": "7327",
   "pgm_ref_txt": "CRCNS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 691465.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>It is a major scientific challenge to understand the computations and neural circuitry underlying the remarkable capacity of the visual system of the human brain to recognize innumerable objects regardless of their size, position, viewpoint and surrounding clutter. &nbsp;We have a reasonably firm understanding of how the retinal image is initially represented by the brain in terms of local patches that differ in brightness, color, depth and line orientation as a result of experimental studies in macaque monkeys and other animals, but very little is known about how this representation leads to the representation of entire objects as the visual information passes deeper into the cerebral cortex. &nbsp;We therefore studied an intermediate stage in the primate visual cortex, cortical area V4, where object-part representations may just be beginning to form. &nbsp;We studied the signals carried by many single neurons in the brains of non-human primates, which have visual systems, visual abilities and visually-guided behavior very similar to our own, while they viewed visual patterns that were systematically designed to probe the representation of shape and luminance. &nbsp;We coordinated our experiments with computer modeling studies that approached the problem from two angles, (1) using biologically inspired models built on our knowledge of local, low-level visual processing, and (2) using artificial network models that arise from the overlap of computer vision and machine learning.</p>\n<p><br />Our research has uncovered evidence that strongly suggests that neurons in the visual cortex represent parts of objects using information about the shape of their boundary contour, their luminance and surface properties, and their size. &nbsp;For example, while we may perceive a picture of an apple and an outline drawing of an apple as representing the same object, we found that neurons that responded to the visual presentation of simple shapes may respond very differently to filled-in versus outline depictions of those shapes. &nbsp;This is important because it helps us to understand how the brain is combining information from different visual cues to represent objects. &nbsp;We also found that color was represented differently in neurons depending whether the color was painted on a shape that is typically associated with a color, such as a banana, versus a shape that is not obviously associated with a color, such as a triangle or square. &nbsp;This provides clues for us to understand how information about color is used in association with information about form in building up the visual representation in the brain. &nbsp;We also found that many neurons in brain regions that respond to color and shape also vary their response based on the size of the object being viewed. &nbsp;To understand how such neurons process information, we built and tested computer models that predict how neurons at different stages in the visual system will respond to images of simple objects. &nbsp;We found that existing models for mid-level visual processing of shape did not explain some key aspects of the responses of neurons. &nbsp;We built new models to replace the old ones on the basis of our understanding of the shortcomings of past models. &nbsp;We also found that artificial neural network models, known as &ldquo;deep convolutional neural nets&rdquo; from the field of computer vision and machine learning, were able to account for the responses of many neurons better than the models that had arisen within the field of neuroscience.</p>\n<p><br />Our findings in this study have created several new lines of research. &nbsp;First, we are now systematically using natural and realistic texture and rendering on simple shape stimuli to explore the various visual dimensions that are simultaneously encoded by single neurons. &nbsp;Second, we are probing artificial neural networks to understand how representations are built up and how they change layer by layer in these &ldquo;deep&rdquo; (i.e., many-layered) networks. &nbsp;An important impact of our work is to bring closer the fields of visual neuroscience and machine learning for vision.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/13/2017<br>\n\t\t\t\t\tModified by: Wyeth&nbsp;Bair</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIt is a major scientific challenge to understand the computations and neural circuitry underlying the remarkable capacity of the visual system of the human brain to recognize innumerable objects regardless of their size, position, viewpoint and surrounding clutter.  We have a reasonably firm understanding of how the retinal image is initially represented by the brain in terms of local patches that differ in brightness, color, depth and line orientation as a result of experimental studies in macaque monkeys and other animals, but very little is known about how this representation leads to the representation of entire objects as the visual information passes deeper into the cerebral cortex.  We therefore studied an intermediate stage in the primate visual cortex, cortical area V4, where object-part representations may just be beginning to form.  We studied the signals carried by many single neurons in the brains of non-human primates, which have visual systems, visual abilities and visually-guided behavior very similar to our own, while they viewed visual patterns that were systematically designed to probe the representation of shape and luminance.  We coordinated our experiments with computer modeling studies that approached the problem from two angles, (1) using biologically inspired models built on our knowledge of local, low-level visual processing, and (2) using artificial network models that arise from the overlap of computer vision and machine learning.\n\n\nOur research has uncovered evidence that strongly suggests that neurons in the visual cortex represent parts of objects using information about the shape of their boundary contour, their luminance and surface properties, and their size.  For example, while we may perceive a picture of an apple and an outline drawing of an apple as representing the same object, we found that neurons that responded to the visual presentation of simple shapes may respond very differently to filled-in versus outline depictions of those shapes.  This is important because it helps us to understand how the brain is combining information from different visual cues to represent objects.  We also found that color was represented differently in neurons depending whether the color was painted on a shape that is typically associated with a color, such as a banana, versus a shape that is not obviously associated with a color, such as a triangle or square.  This provides clues for us to understand how information about color is used in association with information about form in building up the visual representation in the brain.  We also found that many neurons in brain regions that respond to color and shape also vary their response based on the size of the object being viewed.  To understand how such neurons process information, we built and tested computer models that predict how neurons at different stages in the visual system will respond to images of simple objects.  We found that existing models for mid-level visual processing of shape did not explain some key aspects of the responses of neurons.  We built new models to replace the old ones on the basis of our understanding of the shortcomings of past models.  We also found that artificial neural network models, known as \"deep convolutional neural nets\" from the field of computer vision and machine learning, were able to account for the responses of many neurons better than the models that had arisen within the field of neuroscience.\n\n\nOur findings in this study have created several new lines of research.  First, we are now systematically using natural and realistic texture and rendering on simple shape stimuli to explore the various visual dimensions that are simultaneously encoded by single neurons.  Second, we are probing artificial neural networks to understand how representations are built up and how they change layer by layer in these \"deep\" (i.e., many-layered) networks.  An important impact of our work is to bring closer the fields of visual neuroscience and machine learning for vision.\n\n \n\n\t\t\t\t\tLast Modified: 02/13/2017\n\n\t\t\t\t\tSubmitted by: Wyeth Bair"
 }
}