{
 "awd_id": "1353400",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Establishing Trustworthy-Citizen-Created Data for Disaster Response and Humanitarian Action",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 70000.0,
 "awd_amount": 70000.0,
 "awd_min_amd_letter_date": "2013-09-04",
 "awd_max_amd_letter_date": "2013-09-04",
 "awd_abstract_narration": "Often referred to as microblogging, the practice of average citizens reporting on activities \"on-the-ground\" during a disaster is increasingly common. The contents of these message are potentially valuable to responder organizations and victims, but their volume makes it difficult to separate valuable messages from the stream. This project will examine microblogged messages sent during disasters to determine what aspects of the messages (individually and collectively) indicate that they are relevant, verifiable and actionable. Factors to be considered include the content of the messages, the identity of the sender and the overall pattern and spread of messages. The identified factors will then be used to instruct crowdsourced workers who will label messages to create a large corpus of labelled messages. \r\n\r\nThe project is important because microblogging data are seen as increasingly important: they are ubiquitous, rapid and accessible, and they are believed to empower average citizens to become more situationally aware during disasters and to coordinate to help themselves. The result of the project, if it is successful, will be evidence that it is possible to identify relevant, verifiable and actionable messages from a stream of microblogged messages and identification of the evidentiary factors. A further outcome will be a disaster-related, labeled dataset of messages, which will be useful to researchers, e.g., those seeking to automatically classify information within a microblogged data stream.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrea",
   "pi_last_name": "Tapia",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrea Tapia",
   "pi_email_addr": "atapia@ist.psu.edu",
   "nsf_id": "000131195",
   "pi_start_date": "2013-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anna",
   "pi_last_name": "Squicciarini",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anna Squicciarini",
   "pi_email_addr": "asquicciarini@ist.psu.edu",
   "nsf_id": "000501590",
   "pi_start_date": "2013-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 70000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Over the course of the EAGER project, we started the complex process of creating gold-standard datasets of disaster-related data. Defining <strong><em>gold-standard disaster-related, labeled datasets</em></strong> was a fundamental component of our research, and more in general of any project sharing the goal of understanding how microblogged information is disseminated during high-profile disasters. Given our collections of tweets related to the Sandy Hurricane and Boston bombings, we derived two sets of rules. The first set aimed at establishing labelers&rsquo; understanding of the mere content of the microblog itself, and included questions related to: tweets&rsquo; type of content (informative versus conversational), emotions being communicated, location information being available, etc. The second set of rules focused on the users&rsquo; perceived usefulness and veracity of the messages. These rules included labeling events as relevant, i.e., disaster or non-disaster events, and verifiable and their usefulness to first-responders. In deriving the rules, we did not distinguish between willfully miscommunicated (false) information and mistakenly communicated information (also false, but not intentionally or maliciously communicated as so), as we regarded all of them as unverifiable.&nbsp;</p>\n<p>Based on the derived rules, we used the Mechanical Turk (MTurk) to manually label about 5,000 tweets to date. The MTurk is &ldquo;a crowdsourcing Internet marketplace that enables computer programmers to co-ordinate the use of human intelligence to perform tasks that computers are currently unable to do.\" (Wikipedia). A requester posts Human Intelligence Tasks (HITs), whereas an MTurk worker selects from thousands of such tasks the ones to work on and be paid for. In our case, the HIT was the verifiability classification of disaster-related events. We choose several workers to annotate each micro-blog.&nbsp; Interestingly, we found significant agreement among workers in their annotations, for all questions except the one related to perceived trustworthiness (only about 26% agreement), demonstrating once again the challenges of assessing trust, even when human judgment is involved. This result seems to motivate our learning model, in that our supervised and semi-supervised models could learn from large amounts of social media streams to simplify the task of determining tweets trustworthiness.</p>\n<p>With the data in hand, we started a preliminary analysis. <em>First</em>, we identified the sentiment of tweets and further visualized these sentiments on a geographical map centered on the hurricane in order to understand the general mood during the Hurricane Sandy (Caragea et al., 2014). Using large numbers of tweets collected between 10/26/2012 and 11/12/2012 from the Sandy Hurricane, we found that extracting sentiments and mapping emotional intensity during a disaster could help responders develop stronger situational awareness of the disaster zone itself (Caragea et al., 2014). <em>Second</em>, we designed effective classification models to identify tweets that convey information and filter out those that are conversational in nature (Truong et al., 2014; Karavolia et al., 2014). By having a database of informative tweets, one can further understand, in real time, the effect that a disaster is having on the affected population. <em>Third</em>, PI Tapia identified areas of decision-making among humanitarian organizations and identified the sliding needs for accurate and trustworthy data within each area. She has further developed the concept of &ldquo;good enough&rdquo; data quality, focusing on identifying the threshold for actionable information by responding organizations, rather than on perfect data quality. Most importantly, she has discovered some of the unique qualities of trust networks within the humanitarian community, such as the focus on trusting individual members and organizations rather than information contained in the message, that will lead to building better automated systems leading to faster trust (Tapia &amp; Moore, 2014; Saab et al., 2013).</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/09/2015<br>\n\t\t\t\t\tModified by: Andrea&nbsp;Tapia</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOver the course of the EAGER project, we started the complex process of creating gold-standard datasets of disaster-related data. Defining gold-standard disaster-related, labeled datasets was a fundamental component of our research, and more in general of any project sharing the goal of understanding how microblogged information is disseminated during high-profile disasters. Given our collections of tweets related to the Sandy Hurricane and Boston bombings, we derived two sets of rules. The first set aimed at establishing labelers? understanding of the mere content of the microblog itself, and included questions related to: tweets? type of content (informative versus conversational), emotions being communicated, location information being available, etc. The second set of rules focused on the users? perceived usefulness and veracity of the messages. These rules included labeling events as relevant, i.e., disaster or non-disaster events, and verifiable and their usefulness to first-responders. In deriving the rules, we did not distinguish between willfully miscommunicated (false) information and mistakenly communicated information (also false, but not intentionally or maliciously communicated as so), as we regarded all of them as unverifiable. \n\nBased on the derived rules, we used the Mechanical Turk (MTurk) to manually label about 5,000 tweets to date. The MTurk is \"a crowdsourcing Internet marketplace that enables computer programmers to co-ordinate the use of human intelligence to perform tasks that computers are currently unable to do.\" (Wikipedia). A requester posts Human Intelligence Tasks (HITs), whereas an MTurk worker selects from thousands of such tasks the ones to work on and be paid for. In our case, the HIT was the verifiability classification of disaster-related events. We choose several workers to annotate each micro-blog.  Interestingly, we found significant agreement among workers in their annotations, for all questions except the one related to perceived trustworthiness (only about 26% agreement), demonstrating once again the challenges of assessing trust, even when human judgment is involved. This result seems to motivate our learning model, in that our supervised and semi-supervised models could learn from large amounts of social media streams to simplify the task of determining tweets trustworthiness.\n\nWith the data in hand, we started a preliminary analysis. First, we identified the sentiment of tweets and further visualized these sentiments on a geographical map centered on the hurricane in order to understand the general mood during the Hurricane Sandy (Caragea et al., 2014). Using large numbers of tweets collected between 10/26/2012 and 11/12/2012 from the Sandy Hurricane, we found that extracting sentiments and mapping emotional intensity during a disaster could help responders develop stronger situational awareness of the disaster zone itself (Caragea et al., 2014). Second, we designed effective classification models to identify tweets that convey information and filter out those that are conversational in nature (Truong et al., 2014; Karavolia et al., 2014). By having a database of informative tweets, one can further understand, in real time, the effect that a disaster is having on the affected population. Third, PI Tapia identified areas of decision-making among humanitarian organizations and identified the sliding needs for accurate and trustworthy data within each area. She has further developed the concept of \"good enough\" data quality, focusing on identifying the threshold for actionable information by responding organizations, rather than on perfect data quality. Most importantly, she has discovered some of the unique qualities of trust networks within the humanitarian community, such as the focus on trusting individual members and organizations rather than information contained in the message, that will lead to building better automated systems leading to faster trust (Tapia &amp; Moore, 2014; Saab et al., 2013).\n\n \n\n\t\t\t\t\tLast Modified: 09/09/2015\n\n\t\t\t\t\tSubmitted by: Andrea Tapia"
 }
}