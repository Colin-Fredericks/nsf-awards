{
 "awd_id": "1319312",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "\"AF:Small:Efficient and reliable low-rank approximation techniques and fast solutions to large sparse linear equations\"",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2013-08-21",
 "awd_max_amd_letter_date": "2013-08-21",
 "awd_abstract_narration": "In many computational and engineering problems, it is critical to solve large sparse linear systems of equations rapidly and reliably. Nevertheless, many practical but difficult large linear systems of equations remain out of reach computationally. In this project, the PI develops structured fast direct methods and pre-conditioners for solving such large sparse linear systems. These methods systematically exploit potentially rich numerical low-rank patterns within the fill-ins for large reductions in computational time and memory. The efficiency of these methods comes from three innovative design strategies: the PI develops randomized algorithms that can rapidly compute high quality low-rank approximations with low numerical compression overhead; the PI adapts these methods to preserve desirable properties of the original matrix for enhanced numerical reliability; and the PI re-organizes the computations so that no numerical compression and data communication is performed unless necessary.\r\n\r\nThe outcome of this research has the potential to create a novel class of direct solvers and pre-conditioners that in conjunction with iterative solvers can become powerful weapons for solving difficult large sparse linear systems, and the low-rank approximation schemes would become a valuable tool for the general scientific community, as effective data compression is essential in many areas of science and engineering. Mathematical software for rapidly solving large sparse linear systems of equations and for effective compression of large data sets will be made available to the scientific computing public.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ming",
   "pi_last_name": "Gu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ming Gu",
   "pi_email_addr": "mgu@math.berkeley.edu",
   "nsf_id": "000205573",
   "pi_start_date": "2013-08-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California, Berkeley Math Department",
  "perf_str_addr": "",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947203840",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "793300",
   "pgm_ele_name": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7933",
   "pgm_ref_txt": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<p>Three graduate students have received Ph.D. degrees under NSF support with this award. Two more graduate students have done the majority of their thesis work, and are due to graduate in Spring Semester, 2018.&nbsp;</p>\n<p>Two graduate students have presented/will present their findings at four well-recognized conferences in machine learning and high performance computing, with four publications in the conference proceedings.&nbsp;</p>\n<p><span>Intellectual merit:&nbsp; w</span>e have demonstrated that randomization can be an integral part of the standard techniques for solving classical problems in numerical linear algebra, leading to outstanding results and algorithms.&nbsp;Our work demonstrates that, when properly re-designed, the classical LU, QR, and Cholesky factorizations are perfectly capable of low-rank matrix approximation, at the same approximation quality but reduced computational costs.&nbsp; We have also developed randomized LU factorization with complete pivoting algorithm and its variant for symmetric LU factorization. It has long been well-known that the work-horse for solving linear systems of equations, the LU factorization with partial pivoting algorithm, can be numerically unstable even in practice. Yet, this algorithm remains the method of choice in practice because no stable algorithm is known to be as efficient. Our randomized LU factorization with complete pivoting algorithms will fill likely this gap.</p>\n<p><span>Broader impact:&nbsp;</span>One of the fundamental problems in large scale data analysis is the efficient and effective approximation of any given matrix by a matrix of much lower rank. This problem has now become of central importance in the era of large data.</p>\n<p>We have demonstrated that classical matrix factorizations, when properly redesigned, are perfectly capable of low-rank matrix approximation, at the same approximation quality but reduced computational costs. Thus our work has provided people in machine learning community faster and relaible methods for data compression.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/24/2017<br>\n\t\t\t\t\tModified by: Ming&nbsp;Gu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \n\nThree graduate students have received Ph.D. degrees under NSF support with this award. Two more graduate students have done the majority of their thesis work, and are due to graduate in Spring Semester, 2018. \n\nTwo graduate students have presented/will present their findings at four well-recognized conferences in machine learning and high performance computing, with four publications in the conference proceedings. \n\nIntellectual merit:  we have demonstrated that randomization can be an integral part of the standard techniques for solving classical problems in numerical linear algebra, leading to outstanding results and algorithms. Our work demonstrates that, when properly re-designed, the classical LU, QR, and Cholesky factorizations are perfectly capable of low-rank matrix approximation, at the same approximation quality but reduced computational costs.  We have also developed randomized LU factorization with complete pivoting algorithm and its variant for symmetric LU factorization. It has long been well-known that the work-horse for solving linear systems of equations, the LU factorization with partial pivoting algorithm, can be numerically unstable even in practice. Yet, this algorithm remains the method of choice in practice because no stable algorithm is known to be as efficient. Our randomized LU factorization with complete pivoting algorithms will fill likely this gap.\n\nBroader impact: One of the fundamental problems in large scale data analysis is the efficient and effective approximation of any given matrix by a matrix of much lower rank. This problem has now become of central importance in the era of large data.\n\nWe have demonstrated that classical matrix factorizations, when properly redesigned, are perfectly capable of low-rank matrix approximation, at the same approximation quality but reduced computational costs. Thus our work has provided people in machine learning community faster and relaible methods for data compression.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/24/2017\n\n\t\t\t\t\tSubmitted by: Ming Gu"
 }
}