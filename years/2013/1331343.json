{
 "awd_id": "1331343",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BSF:2012348:The Boundaries of Privacy",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 60000.0,
 "awd_amount": 60000.0,
 "awd_min_amd_letter_date": "2013-09-11",
 "awd_max_amd_letter_date": "2013-09-11",
 "awd_abstract_narration": "This project is funded as part of the United States-Israel Collaboration in Computer Science (USICCS) program. Through this program, NSF and the United States - Israel Binational Science Foundation (BSF) jointly support collaborations among US-based researchers and Israel-based researchers. The availability of fast and cheap computers coupled with massive storage devices has enabled the collection and mining of data on a scale previously unimaginable. This opens the door to potential abuse regarding individuals' information. This work explores the fundamental tradeoffs between privacy for individuals' data and the usefulness of the information one can obtain from these large datasets.\r\n\r\nDifferential privacy is a well-established paradigm aimed at mitigating the drawbacks of traditional anonymization techniques, as it provides a rigorous guarantee for the added risk to an individual in participating in a database. This research addresses fundamental questions clarifying the boundaries of what is possible under differential privacy and its relaxations, by exploring the fundamental conflicts between privacy and utility and the additional tensions introduced by computational efficiency. This work expands the potential impact of differentially private algorithms on real-world applications, and also  ensures broad impact via curriculum development, pedagogical development, and outreach activities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Katrina",
   "pi_last_name": "Ligett",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Katrina A Ligett",
   "pi_email_addr": "katrina@caltech.edu",
   "nsf_id": "000606549",
   "pi_start_date": "2013-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "California Institute of Technology",
  "inst_street_address": "1200 E CALIFORNIA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "PASADENA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6263956219",
  "inst_zip_code": "911250001",
  "inst_country_name": "United States",
  "cong_dist_code": "28",
  "st_cong_dist_code": "CA28",
  "org_lgl_bus_name": "CALIFORNIA INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "",
  "org_uei_num": "U2JMKHNS5TG4"
 },
 "perf_inst": {
  "perf_inst_name": "California Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "911250001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "28",
  "perf_st_cong_dist": "CA28",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "2878",
   "pgm_ref_txt": "SPECIAL PROJECTS - CCF"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 60000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Differential privacy is a paradigm aimed at mitigating the drawbacks of traditionalanonymization techniques, as it provides a rigorous guarantee for the added risk to an individual in participating in a database. Roughly speaking, a mechanism (a method for publishing data or for answering queries) satisfies differential privacy if for any possible output of themechanism and any possible set of data on individuals, the probability of obtaining this particular output changes only very little with the addition or deletion of the data on an individual.</p>\n<p>The fact that differential privacy comes with a clear guarantee, together with basic properties such as graceful degradation for ``group'' privacy, appealing composition properties, and powerful techniques, are someof the reasons it has seen remarkable acceptance and success.</p>\n<p>The focus of this grant was on improvements to fundamental privacy-accuracy tradeoffs for computations, and on applying the differential privacy notion in novel settings. Our results include:</p>\n<p><br />- Better privacy-accuracy tradeoffs for rectangle queries, one of the most basic queries one might issue against data</p>\n<p><br />- Investigation of privacy concerns in environments where agents might be strategic</p>\n<p><br />- Techniques for maximizing privacy of a computation, subject to accuracy constraints</p>\n<p><br />- New connections between notions of privacy and notions of robustness of learning algorithms</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/17/2017<br>\n\t\t\t\t\tModified by: Katrina&nbsp;A&nbsp;Ligett</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDifferential privacy is a paradigm aimed at mitigating the drawbacks of traditionalanonymization techniques, as it provides a rigorous guarantee for the added risk to an individual in participating in a database. Roughly speaking, a mechanism (a method for publishing data or for answering queries) satisfies differential privacy if for any possible output of themechanism and any possible set of data on individuals, the probability of obtaining this particular output changes only very little with the addition or deletion of the data on an individual.\n\nThe fact that differential privacy comes with a clear guarantee, together with basic properties such as graceful degradation for ``group'' privacy, appealing composition properties, and powerful techniques, are someof the reasons it has seen remarkable acceptance and success.\n\nThe focus of this grant was on improvements to fundamental privacy-accuracy tradeoffs for computations, and on applying the differential privacy notion in novel settings. Our results include:\n\n\n- Better privacy-accuracy tradeoffs for rectangle queries, one of the most basic queries one might issue against data\n\n\n- Investigation of privacy concerns in environments where agents might be strategic\n\n\n- Techniques for maximizing privacy of a computation, subject to accuracy constraints\n\n\n- New connections between notions of privacy and notions of robustness of learning algorithms\n\n\t\t\t\t\tLast Modified: 12/17/2017\n\n\t\t\t\t\tSubmitted by: Katrina A Ligett"
 }
}