{
 "awd_id": "1253950",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Robust Strategic Reasoning for Multi-Agent Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2013-02-15",
 "awd_exp_date": "2019-01-31",
 "tot_intn_awd_amt": 488288.0,
 "awd_amount": 488288.0,
 "awd_min_amd_letter_date": "2013-02-12",
 "awd_max_amd_letter_date": "2017-03-01",
 "awd_abstract_narration": "Many important decision problems in computational science involve multi-\u00adagent systems (MAS) in which multiple decision makers must choose strategies for action from a set of alternatives. (One example is selecting a security policy --- for example by police --- to protect critical infrastructure against attackers.) This CAREER award project develops methods for analyzing MAS to select strategies for an agent to follow that will lead to desirable outcomes. \r\n\r\nThe central tenant driving the project is that a good strategy should be robust in the sense that it will continue to perform well even when there is uncertainty about the outcomes or how other agents will choose their strategies.  An important component of the project is a multi-purpose web platform supporting both research experiments and education on game-playing agents. The PI will design course modules that use this platform to involve undergraduates in designing game-playing agents, offering an exciting way to practice basic programming and develop critical thinking and data analysis skills.  The agents designed by students also play an important role in the research; they will provide a diverse library of strategies to evaluate the robustness of reasoning methods for play against opponents with unanticipated behaviors.  This platform, and the data sets generated with it will serve the broader research and education communities in computer science and MAS.\r\n\r\nResearch on robust methods for strategic reasoning has the potential to significantly improve decision making in many important real-world problems, including decision support tools for homeland security operations. The availability of robust methods will also enable new applications of computational game theory in domains where confidence in the strategies, despite uncertainty, is critical. This award also supports broadening participation by developing research capacity at UTEP, a minority-\u00adserving institution. \r\n\r\nThe project's primary technical contributions are in computational game theory.  Typical game-theoretic solutions are not robust to the kinds of uncertainty that arise in real applications; these include payoff uncertainty, abstraction error, and opponent modeling error.  Robustness requires techniques that encompass unanticipated situations, as well as those where it is possible to precisely characterize the uncertainty.  The project extends the framework of meta-games to provide a methodology for evaluating robustness in the context of different kinds of uncertainty, including the three listed above. The PI uses meta-games to evaluate new concepts for robust strategic reasoning, including methods based on Bayesian games, interval-based approaches, and approaches drawn from behavioral game theory.  The web-based platform developed in the project facilitates extensive experiments with game-playing agents. This will be used to collect a diverse pool of unanticipated agent strategies (including ones designed by undergraduate students), enabling a more comprehensive investigation of robustness to unanticipated opponent behaviors.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Kiekintveld",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher D Kiekintveld",
   "pi_email_addr": "cdkiekintveld@utep.edu",
   "nsf_id": "000624514",
   "pi_start_date": "2013-02-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at El Paso",
  "inst_street_address": "500 W UNIVERSITY AVE",
  "inst_street_address_2": "",
  "inst_city_name": "EL PASO",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9157475680",
  "inst_zip_code": "799688900",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "TX16",
  "org_lgl_bus_name": "THE UNIVERSITY OF TEXAS AT EL PASO",
  "org_prnt_uei_num": "C1DEGMMKC7W7",
  "org_uei_num": "C1DEGMMKC7W7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at El Paso",
  "perf_str_addr": "",
  "perf_city_name": "El Paso",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "799125816",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "TX16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 88251.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 108916.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 111600.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 88627.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 90894.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Artificial Intelligence (AI) is rapidly becoming a widely deployed technology for applications ranging from autonomous vehicles to cybersecurity to managing financial transactions. The potential for AI to benefit society and address the most difficult problems we face is enormous, but it is critical that autonomous agents are reliable, even in uncertain and adverse situations.&nbsp;&nbsp;One of the greatest challenges in designing robust agents is that they often need to interact with other agents (including humans and other artificial agents) that are unpredictable, or even malicious.&nbsp;&nbsp;Game theory provides a framework for reasoning about how agents should make decisions in multi-agent settings where the best choice for one agent can depend on decisions that are made by other, self-interested agents. It is widely used for programming artificial agents that need to make complex decisions in multi-agent environments.&nbsp;&nbsp;However, the most common solutions in game theory (e.g., Nash equilibrium) focus on situations where all of the players are perfectly rational and their behavior is predicable, at least in principle.&nbsp;</p>\n<p>&nbsp;</p>\n<p>We focus on developing and evaluating new game theoretic approaches that prioritize robustness when interacting with other agents that are unpredictable or malicious. We have developed new mathematical models of games where the players may not know exactly what game they are playing, including being uncertain about the potential outcomes, the goals of the other players, and how the other players will play the game. We have developed several new solution concepts for making robust strategy choices in these games, and designed computationally efficient algorithms for solving large games based on these robust solution concepts. For example, we have developed algorithms for finding strategies that are robust against an adversary who is able to make (limited) changes to the game.&nbsp;&nbsp;By varying the amount and degree of changes the adversary can make we can make our own agent more or less robust to different types of uncertainty about the game or other player?s choices. We have also developed strategies for repeated games that combine game theory solutions with machine learning, so an agent can start playing conservatively, and adapt over repeated trials to optimize against a specific opponent.&nbsp;&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>Another important part of our work focuses on evaluating robustness. We have developed a testbed environment for running tournaments of different agents against one another so that we can evaluate how they perform, and especially how robust they are against different types of opponents.&nbsp;&nbsp;We can also manipulate how the agents are able to observe the game, so they may be given a noisy or abstracted view of the game they are really playing (which may differ from that of other players).&nbsp;&nbsp;Using this testbed we have run extensive simulations (millions of games) pitting different methods of selecting strategies for playing games against one another. Based on analysis of this data, we have identified the best methods to date for playing games robustly under different types of uncertainty. We expect that this platform will continue to produce valuable data for years to come, as we continue to add new candidate strategies and run additional simulations to strengthen the analysis of existing ones.&nbsp;&nbsp;In addition, the code for this testbed will be open source and available to other researchers to make use of in evaluating the robustness of their own strategy selection methods.&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>The tournament testbed also serves as a platform for developing several different types of project assignments for courses in artificial intelligence, decision theory, game theory, and cybersecurity.&nbsp;&nbsp;Students are able to program automated agents that play strategically for specific scenarios we developed, applying techniques they have been taught in the course (e.g., search methods, machine learning techniques, game theory solution concepts). The testbed is used to evaluate the agents against the other student submissions, as well as other benchmarks so students can get a real idea of the strength of their strategy.&nbsp;&nbsp;We have also collected a wide variety of different types of strategies that have been developed in these assignments, which we can also use in our research to evaluate our robust solution methods against novel, unexpected types of opponents.&nbsp;&nbsp;This allows the students to participate directly in the advancement of research for robust agents, even while they are learning about basic concepts in artificial intelligence and strategic reasoning.&nbsp;&nbsp;</p>\n<p>&nbsp;</p>\n<p>The fundamental research we have done on robust strategies for interacting with other agents has many potential real-world applications. One that we are focusing on currently is applications of AI to cybersecurity, and here the need for robustness is especially critical since we are facing highly skilled and creative adversaries.&nbsp;&nbsp;We have already applied some of the techniques and evaluation tools we developed in this project to game theoretic models used to strategically optimize cyber deception method (e.g. honeypots), and we expect many future applications of robust solution concepts in this area.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/21/2019<br>\n\t\t\t\t\tModified by: Christopher&nbsp;D&nbsp;Kiekintveld</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nArtificial Intelligence (AI) is rapidly becoming a widely deployed technology for applications ranging from autonomous vehicles to cybersecurity to managing financial transactions. The potential for AI to benefit society and address the most difficult problems we face is enormous, but it is critical that autonomous agents are reliable, even in uncertain and adverse situations.  One of the greatest challenges in designing robust agents is that they often need to interact with other agents (including humans and other artificial agents) that are unpredictable, or even malicious.  Game theory provides a framework for reasoning about how agents should make decisions in multi-agent settings where the best choice for one agent can depend on decisions that are made by other, self-interested agents. It is widely used for programming artificial agents that need to make complex decisions in multi-agent environments.  However, the most common solutions in game theory (e.g., Nash equilibrium) focus on situations where all of the players are perfectly rational and their behavior is predicable, at least in principle. \n\n \n\nWe focus on developing and evaluating new game theoretic approaches that prioritize robustness when interacting with other agents that are unpredictable or malicious. We have developed new mathematical models of games where the players may not know exactly what game they are playing, including being uncertain about the potential outcomes, the goals of the other players, and how the other players will play the game. We have developed several new solution concepts for making robust strategy choices in these games, and designed computationally efficient algorithms for solving large games based on these robust solution concepts. For example, we have developed algorithms for finding strategies that are robust against an adversary who is able to make (limited) changes to the game.  By varying the amount and degree of changes the adversary can make we can make our own agent more or less robust to different types of uncertainty about the game or other player?s choices. We have also developed strategies for repeated games that combine game theory solutions with machine learning, so an agent can start playing conservatively, and adapt over repeated trials to optimize against a specific opponent.   \n\n \n\nAnother important part of our work focuses on evaluating robustness. We have developed a testbed environment for running tournaments of different agents against one another so that we can evaluate how they perform, and especially how robust they are against different types of opponents.  We can also manipulate how the agents are able to observe the game, so they may be given a noisy or abstracted view of the game they are really playing (which may differ from that of other players).  Using this testbed we have run extensive simulations (millions of games) pitting different methods of selecting strategies for playing games against one another. Based on analysis of this data, we have identified the best methods to date for playing games robustly under different types of uncertainty. We expect that this platform will continue to produce valuable data for years to come, as we continue to add new candidate strategies and run additional simulations to strengthen the analysis of existing ones.  In addition, the code for this testbed will be open source and available to other researchers to make use of in evaluating the robustness of their own strategy selection methods.  \n\n \n\nThe tournament testbed also serves as a platform for developing several different types of project assignments for courses in artificial intelligence, decision theory, game theory, and cybersecurity.  Students are able to program automated agents that play strategically for specific scenarios we developed, applying techniques they have been taught in the course (e.g., search methods, machine learning techniques, game theory solution concepts). The testbed is used to evaluate the agents against the other student submissions, as well as other benchmarks so students can get a real idea of the strength of their strategy.  We have also collected a wide variety of different types of strategies that have been developed in these assignments, which we can also use in our research to evaluate our robust solution methods against novel, unexpected types of opponents.  This allows the students to participate directly in the advancement of research for robust agents, even while they are learning about basic concepts in artificial intelligence and strategic reasoning.  \n\n \n\nThe fundamental research we have done on robust strategies for interacting with other agents has many potential real-world applications. One that we are focusing on currently is applications of AI to cybersecurity, and here the need for robustness is especially critical since we are facing highly skilled and creative adversaries.  We have already applied some of the techniques and evaluation tools we developed in this project to game theoretic models used to strategically optimize cyber deception method (e.g. honeypots), and we expect many future applications of robust solution concepts in this area.\n\n \n\n\t\t\t\t\tLast Modified: 10/21/2019\n\n\t\t\t\t\tSubmitted by: Christopher D Kiekintveld"
 }
}