{
 "awd_id": "1319788",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small:  Learning and Testing Classes of Distributions",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2013-06-01",
 "awd_exp_date": "2016-05-31",
 "tot_intn_awd_amt": 471875.0,
 "awd_amount": 471875.0,
 "awd_min_amd_letter_date": "2013-06-03",
 "awd_max_amd_letter_date": "2013-06-03",
 "awd_abstract_narration": "A long and successful line of research in machine learning deals with algorithms that learn from \"labeled\" data, where a target function is assumed to provide a label for each data point.  A major focus of theoretical work has been to develop efficient algorithms for learning different classes of target functions.  Recent years have witnessed a data explosion across many domains of science and society, but much of this newly available data consists simply of example points (DNA sequences, sensor readings, smartphone user locations, etc) without any labels.  A natural model of such scenarios is that data points are generated according to some unknown probability distribution (typically over an extremely large domain).  The goal of the proposed work is to study the learnability of different classes of probability distributions given access to samples drawn from the distributions.  This is closely analogous to the framework of learning from labeled data sketched above, but with probability distributions playing the role of functions as the objects to be learned.\r\n\r\nIn this project, the PI will perform theoretical research on developing computationally efficient algorithms for learning and testing various natural types of probability distributions over extremely large domains. (Testing algorithms are algorithms which, instead of trying to accurately model an unknown distribution, have the more modest goal of testing whether or not the distribution has some property of interest.) Specific problems the PI will address include: (1) Developing efficient algorithms to learn and test univariate probability distributions that satisfy various natural kinds of \"shape constraints\" on the underlying probability density function.  Preliminary results suggest that dramatic improvements in efficiency may be possible for algorithms that are designed to exploit this type of structure.  (2) Developing efficient algorithms for learning and testing complex distributions that result from the aggregation of many independent simple sources of randomness.\r\n\r\nThe algorithms that the PI will work to develop can provide useful modelling tools in data-rich environments and may serve as a \"computational substrate\" on which large-scale machine learning applications can be developed for real-world problems spanning a broad range of application areas.  Other important focuses of the grant are to train graduate students through research collaboration, disseminate research results through seminar talks, survey articles and other publications, and to continue ongoing outreach activities aimed at increasing interest in theoretical computer science topics in elementary school students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rocco",
   "pi_last_name": "Servedio",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Rocco A Servedio",
   "pi_email_addr": "rocco@cs.columbia.edu",
   "nsf_id": "000232661",
   "pi_start_date": "2013-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "2960 Broadway",
  "perf_city_name": "New York, NY",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "792600",
   "pgm_ele_name": "ALGORITHMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 471875.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><pre>A well-known class of problems in statistics, usually referred to as \n\"density estimation\" problems, involve inferring an unknown probability \ndistribution on the basis of samples drawn from the distribution.  The \nmain goal of this project was to study problems of this sort from the \nvantage point of theoretical computer science.  The statistics \nperspective typically focuses on how much data is required to perform \nthe relevant inference tasks; in theoretical computer science, the \nemphasis is not only on data requirements, but on the computational \nefficiency of algorithms to solve the relevant problems.  This project \naimed to develop provably correct and efficient algorithms for inferring \nvarious types of probability distributions.  Below we describe two of \nthe main results achieved in the course of this project.\n\nOne significant result was a new and general approach for learning \n\"univariate\" probability distributions (these are simply distributions \nwhere each draw from the distribution is a single number; a very simple \nexample of such a distribution is the outcome obtained from rolling a \ndie, since the possible distinct outcomes are simply the numbers one \nthrough six).  The PI developed a general algorithm that can efficiently \nlearn any univariate distribution whose probability density function can \nbe approximated by a collection of polynomial curves over different \nportions of the domain (i.e. any distribution that can be approximated \nby a piecewise polynomial function).  The algorithm obtained is provably \nbest possible in terms of the amount of data that it requires in order \nto come up with a hypothesis distribution achieving a given accuracy \nlevel.  The PI also showed that a wide range of different well-studied \ntypes of probability distributions (such as k-modal distributions, \nmonotone hazard rate distributions, various types of mixture \ndistributions, etc) can be approximated by these piecewise polynomial \nfunctions, so the new general method has broad applicability.\n\nAnother significant result was for learning probability distributions \nthat are obtained by aggregating many simple independent distributions.  \nAs a concrete example, suppose that N (a very large number) of people \neach separately roll their own k-sided die, where each die is labeled \nwith the numbers 1,...,k but may be skewed in an unknown and arbitrary \nway (one die may output 2 one-quarter of the time and 4 three-quarters \nof the time; another may be fair and output all values 1,...,k \nequiprobably; etc).  The total of all die rolls is added up, and this \ncomprises a single draw from the aggregate distribution.  Is it possible \nto learn this aggregate distribution efficiently?  The PI gave an \nefficient algorithm to learn this aggregate distribution to high \naccuracy from an essentially optimal (minimal) number of samples --- \nperhaps surprisingly, the running time and amount of data required by \nthe algorithm is completely independent of the number N of component \nindependent distributions!  Ongoing work is underway to extend this \nresult to even more general types of aggregated distributions.  \n\nThe efficient algorithms described above for learning different types of \nprobability distributions may serve as a useful \"computational \nsubstrate\" for systems that do large-scale analysis of probabilistic \ndata.  In terms of broader impacts, besides the scientific impact of the \nwork, this award contributed to the development of human resources for \nthe STEM and academic workforce.  A number of Ph.D. students at Columbia \nUniversity received partial support and research training as a result of \nthis grant award.\n</pre>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/23/2016<br>\n\t\t\t\t\tModified by: Rocco&nbsp;A&nbsp;Servedio</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "A well-known class of problems in statistics, usually referred to as \n\"density estimation\" problems, involve inferring an unknown probability \ndistribution on the basis of samples drawn from the distribution.  The \nmain goal of this project was to study problems of this sort from the \nvantage point of theoretical computer science.  The statistics \nperspective typically focuses on how much data is required to perform \nthe relevant inference tasks; in theoretical computer science, the \nemphasis is not only on data requirements, but on the computational \nefficiency of algorithms to solve the relevant problems.  This project \naimed to develop provably correct and efficient algorithms for inferring \nvarious types of probability distributions.  Below we describe two of \nthe main results achieved in the course of this project.\n\nOne significant result was a new and general approach for learning \n\"univariate\" probability distributions (these are simply distributions \nwhere each draw from the distribution is a single number; a very simple \nexample of such a distribution is the outcome obtained from rolling a \ndie, since the possible distinct outcomes are simply the numbers one \nthrough six).  The PI developed a general algorithm that can efficiently \nlearn any univariate distribution whose probability density function can \nbe approximated by a collection of polynomial curves over different \nportions of the domain (i.e. any distribution that can be approximated \nby a piecewise polynomial function).  The algorithm obtained is provably \nbest possible in terms of the amount of data that it requires in order \nto come up with a hypothesis distribution achieving a given accuracy \nlevel.  The PI also showed that a wide range of different well-studied \ntypes of probability distributions (such as k-modal distributions, \nmonotone hazard rate distributions, various types of mixture \ndistributions, etc) can be approximated by these piecewise polynomial \nfunctions, so the new general method has broad applicability.\n\nAnother significant result was for learning probability distributions \nthat are obtained by aggregating many simple independent distributions.  \nAs a concrete example, suppose that N (a very large number) of people \neach separately roll their own k-sided die, where each die is labeled \nwith the numbers 1,...,k but may be skewed in an unknown and arbitrary \nway (one die may output 2 one-quarter of the time and 4 three-quarters \nof the time; another may be fair and output all values 1,...,k \nequiprobably; etc).  The total of all die rolls is added up, and this \ncomprises a single draw from the aggregate distribution.  Is it possible \nto learn this aggregate distribution efficiently?  The PI gave an \nefficient algorithm to learn this aggregate distribution to high \naccuracy from an essentially optimal (minimal) number of samples --- \nperhaps surprisingly, the running time and amount of data required by \nthe algorithm is completely independent of the number N of component \nindependent distributions!  Ongoing work is underway to extend this \nresult to even more general types of aggregated distributions.  \n\nThe efficient algorithms described above for learning different types of \nprobability distributions may serve as a useful \"computational \nsubstrate\" for systems that do large-scale analysis of probabilistic \ndata.  In terms of broader impacts, besides the scientific impact of the \nwork, this award contributed to the development of human resources for \nthe STEM and academic workforce.  A number of Ph.D. students at Columbia \nUniversity received partial support and research training as a result of \nthis grant award.\n\n\n \n\n\t\t\t\t\tLast Modified: 08/23/2016\n\n\t\t\t\t\tSubmitted by: Rocco A Servedio"
 }
}