{
 "awd_id": "1248991",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Emotionally Immersive Tele-Learning",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Glenn H. Larsen",
 "awd_eff_date": "2013-01-01",
 "awd_exp_date": "2013-06-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 155000.0,
 "awd_min_amd_letter_date": "2012-12-02",
 "awd_max_amd_letter_date": "2013-05-05",
 "awd_abstract_narration": "This Small Business Innovation Research (SBIR) Phase I project aims to incorporate novel machine vision functionality and innovative social networking capabilities into the technology of distance learning and online webinars. The primary objective is to make on-line training and virtual collaboration more engaging and compelling by replicating non-verbal feedback related to the rate and acceptance of information delivery in lectures, in order to make the experience of distance learning more emotionally immersive. This project contributes four significant innovations: 1) a machine-vision recognition system for head position, gaze direction, facial expressions of interest or comprehension, which when averaged across participants will provide simple feedback related to the rate and acceptance of information delivery; 2) a machine-vision-based hand detection system for motion and shape to detect hand raising or other gestures; 3) to enable hot-deployable third party pedagogical applications within the framework, aka \"side apps\"; and 4) to integrate social functionalities that replicate pre- and post-lecture socialization including pair-sharing, breakout groups, team teaching, and support for teaching assistance. In anticipation of support for this project, TSN has already built an evaluation test bed for tele-lectures and virtual classrooms.\r\n \r\nThe broader impact/commercial potential of this project is to significantly transform on-line education. On-line training also has the potential to radically alter the delivery of education. By 2018, the estimated cost of four-year public university education is expected to rise to $151,000. For private colleges, this cost will increase to over $300,000. To address this crisis, several colleges and startup companies have announced an increased use of on-line training. However existing systems for streaming video for lectures, and virtual group learning environments, have not advanced to the level that distance learning isn't considered to be a second-class citizen in the educational world. The proposed system can transform the fundamental efficacy of on-line training and spur new research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ian",
   "pi_last_name": "Bennett",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ian Bennett",
   "pi_email_addr": "ianmbennettc2y@gmail.com",
   "nsf_id": "000463799",
   "pi_start_date": "2012-12-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "The Spirituality Network, Inc.",
  "inst_street_address": "2275 East Bayshore Road",
  "inst_street_address_2": "Suite 130",
  "inst_city_name": "Palo Alto",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507969517",
  "inst_zip_code": "943033222",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "H7JCNFSGDGW3"
 },
 "perf_inst": {
  "perf_inst_name": "The Spirituality Network, Inc.",
  "perf_str_addr": "211 Cleveland Ct.",
  "perf_city_name": "Mill Valley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "949413515",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8031",
   "pgm_ref_txt": "Education Products"
  },
  {
   "pgm_ref_code": "8033",
   "pgm_ref_txt": "Hardware Software Integration"
  },
  {
   "pgm_ref_code": "8039",
   "pgm_ref_txt": "Information, Communication & Computing"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 155000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The objective of the SBIR Phase I project is to incorporate novel machine vision functionality and innovative social networking capabilities into the technology of distance learning and online webinars. The primary objective is to allow on-line training and virtual collaboration to be more engaging and compelling by replicating non-verbal meta-linguistic feedback related to the rate and acceptance of information delivery in lectures and to make the experience of distance learning more emotionally immersive. This system,&nbsp; &lsquo;Revolution e-Learning Platform&rsquo; and will be referred as &lsquo;Revolution&rsquo; throughout this report.</p>\n<p>Revolution implemented and integrated the following four innovations:</p>\n<p>&bull; Machine-vision recognition system for head position, gaze direction, facial expressions of interest or comprehension, which when averaged across participants will provide simple feedback related to the rate and acceptance of information delivery;</p>\n<p>&bull; Machine-vision-based hand detection system for motion and shape to detect hand raising or other gestures;</p>\n<p>&bull; Ability to enable hot-deployable third party pedagogical applications within the framework, aka &lsquo;side apps&rsquo;;</p>\n<p>&bull; The integration of social functionalities that replicate pre- and post-lecture socialization including pair-sharing, breakout groups, team teaching, and support for teaching assistance.</p>\n<p>The above innovations were made possibe by the following technical achievements: the creation of a machine vision system to gauge head position, gaze direction, facial expressions of interest paired with an analysis system for estimating comprehension; the development of a server application to aggregate the data coming in from many students to provide the lecturer with simple feedback about the rate of information delivery; the integration of novel social networking and media/content functionality; and the integration of these capabilities within a core webinar/collaboration&nbsp;system for the prototype pilot which provided a crisp and responsive user experience.</p>\n<p>This project not only achieved the key objectives as described above but also provided the framework for further dissemination of the research results to the community via publications and discussions at relevant upcoming professional conferences. The Revolution prototype demonstrates these key functionalities regarding machine vision, machine learning, intelligent data aggregation, and user experience. The objectives were achieved in the following process: first, we developed a complete specification for the framework and successfully implemented a prototype of the Revolution system, then we developed and integrated the systems, and then we deployed a pilot of the prototype within a live in-vivo test bed so actual users could provide us with deeper customer insights and market validation data. Although the engineering project was significantly more technically challenging than initially estimated, we successfully managed to make agile adjustments to the critical path dynamically in order to meet the feature set proposed for our prototype on budget and on time.</p>\n<p>A functional and operationally meaningful administrative system was also completed &ndash; this system provides control and admin functionality for lecturers and teaching assistants, and customized interfaces for students, lecturers and teaching assistants. Additionally, we were able to complete a first pass at media/content and so&shy;cial network data integration. Most importantly, via this prototype and pilot, we were able to learn more about the real and unarticulated needs of students, lecturers and teaching assistants &ndash; and the constituents of the entire eco-system of online learning including managers and school superintendents &ndash; to inform our design and validate marketability.</p>\n<p>&nbsp;</...",
  "por_txt_cntn": "\nThe objective of the SBIR Phase I project is to incorporate novel machine vision functionality and innovative social networking capabilities into the technology of distance learning and online webinars. The primary objective is to allow on-line training and virtual collaboration to be more engaging and compelling by replicating non-verbal meta-linguistic feedback related to the rate and acceptance of information delivery in lectures and to make the experience of distance learning more emotionally immersive. This system,  \u00e6Revolution e-Learning Platform\u00c6 and will be referred as \u00e6Revolution\u00c6 throughout this report.\n\nRevolution implemented and integrated the following four innovations:\n\n&bull; Machine-vision recognition system for head position, gaze direction, facial expressions of interest or comprehension, which when averaged across participants will provide simple feedback related to the rate and acceptance of information delivery;\n\n&bull; Machine-vision-based hand detection system for motion and shape to detect hand raising or other gestures;\n\n&bull; Ability to enable hot-deployable third party pedagogical applications within the framework, aka \u00e6side apps\u00c6;\n\n&bull; The integration of social functionalities that replicate pre- and post-lecture socialization including pair-sharing, breakout groups, team teaching, and support for teaching assistance.\n\nThe above innovations were made possibe by the following technical achievements: the creation of a machine vision system to gauge head position, gaze direction, facial expressions of interest paired with an analysis system for estimating comprehension; the development of a server application to aggregate the data coming in from many students to provide the lecturer with simple feedback about the rate of information delivery; the integration of novel social networking and media/content functionality; and the integration of these capabilities within a core webinar/collaboration system for the prototype pilot which provided a crisp and responsive user experience.\n\nThis project not only achieved the key objectives as described above but also provided the framework for further dissemination of the research results to the community via publications and discussions at relevant upcoming professional conferences. The Revolution prototype demonstrates these key functionalities regarding machine vision, machine learning, intelligent data aggregation, and user experience. The objectives were achieved in the following process: first, we developed a complete specification for the framework and successfully implemented a prototype of the Revolution system, then we developed and integrated the systems, and then we deployed a pilot of the prototype within a live in-vivo test bed so actual users could provide us with deeper customer insights and market validation data. Although the engineering project was significantly more technically challenging than initially estimated, we successfully managed to make agile adjustments to the critical path dynamically in order to meet the feature set proposed for our prototype on budget and on time.\n\nA functional and operationally meaningful administrative system was also completed &ndash; this system provides control and admin functionality for lecturers and teaching assistants, and customized interfaces for students, lecturers and teaching assistants. Additionally, we were able to complete a first pass at media/content and so&shy;cial network data integration. Most importantly, via this prototype and pilot, we were able to learn more about the real and unarticulated needs of students, lecturers and teaching assistants &ndash; and the constituents of the entire eco-system of online learning including managers and school superintendents &ndash; to inform our design and validate marketability.\n\n \n\n\t\t\t\t\tLast Modified: 02/04/2014\n\n\t\t\t\t\tSubmitted by: Ian Bennett"
 }
}