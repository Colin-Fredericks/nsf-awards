{
 "awd_id": "1251110",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Small: DA: Collaborative Research: From Data To Users: Providing Interpretable and Verifiable Explanations in Data Mining",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2013-09-13",
 "awd_max_amd_letter_date": "2013-09-13",
 "awd_abstract_narration": "The fruits of data mining pervade every aspect of our lives.  We have books and movies recommended; we are given differential pricing for insurance; screened for potential terror threats; diagnosed with various diseases; and targeted for political advertising. The ability to sift through massive data sets with sophisticated algorithms has resulted in applications with impressive predictive power.  And yet there is still a gap between what such tools can deliver, and what the users of data mining really need. It is often hard to interpret the answers produced by a learning algorithm, due to its sophistication and the use of large data sets to build models. The results of mining are often \"one-size-fits-all\", and convincing a user that results are actually relevant to them is difficult.  Finally, there is the important problem of validation. As the results of data mining affect more and more of our lives, the more crucial it is that the user be able to validate decisions made on their behalf and that affect them. The common theme tying these issues together is a user-centric perspective on the problems of data mining. Rather than asking \"What patterns can be found in this mountain of data?\" this work instead asks \"What structures in this data affect me?\" These issues arise precisely because of the vast amounts of data we now have the ability to mine, and the sophisticated methods at our disposal to analyze this data. In this research, the PIs develop a computational framework and key tools for user-centric data mining. A central theme in this research is the idea of interaction. In both machine learning and in the foundations of complexity theory, interaction has been used to allow a (weaker) entity to probe a much more powerful system and determine answers that it lacks the resources to compute directly itself. The PIs use formal interaction mechanisms both from the perspective of a user interacting with a powerful algorithm, as well as a client interacting with a computing source with access to large data, in order to enable the user to interpret and validate the results of data mining. \r\n\r\nThe goal of this project is to develop a computational framework for user-centric data mining  that enables existing users to tailor data analysis to their needs and facilitates the use of data mining in new areas where existing The team proposes interactive mechanisms that start with the results of a learning process and, via interaction with the user, produce an explanation expressed in terms of meaningful features, drawing on ideas from active learning, feature selection, and domain adaptation. 2. Locality: Answers that are relevant. Here, the focus is on providing information that depends more on a user?s local neighborhood, achieved via a new local notion of stability. 3. Verifiability: Answers you can check. The team proposes a framework for the validation of computationally-intensive data mining by the computationally-weak user, with ideas from interactive proof theory and stream algorithms. Tools for analyzing patient medical data have become more sophisticated and individual medical profiles play a far more significant role in diagnosis and treatment.The research examines user-centric data mining via three core primitives (classification, regression and clustering), and studies the three problems of interpreting results, providing local explanations, and validating the results of data mining. Firstly, the research draws on ideas from active learning, feature selection and domain adaptation to build interpretable results via interaction with users. Secondly, it introduces local notions of stability as a way of validating predictions for a specific user. Finally, it develops a general framework for validation of an analysis by a computationally-weak user, by drawing on ideas from the theory of interactive proofs and streaming algorithms.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "McGregor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew McGregor",
   "pi_email_addr": "mcgregor@cs.umass.edu",
   "nsf_id": "000536261",
   "pi_start_date": "2013-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039264",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The availability of massive amounts of data in various learning and data mining tasks has both advantages and disadvantages. With more data, we can hope to perform more sophisticated analysis and generate more accurate predictions. This could result in anything from better book recommendations from an online retailer to more detailed medical diagnoses and even to more accurate screening of potential terror threats. However, with more data comes more responsibility along with new computational challenges. Since the results of data mining can have negative consequences, e.g., an inappropriate treatment due to a misdiagnosis, we want to be able to validate the computations that led to the final results and understand the properties of the data set that may be relevant to the outcome.</p>\n<p>The intellectual merit of this project revolved around a) developing techniques for validation of computation on large data sets and b) designing efficient algorithms for analyzing properties of the data sets that are relevant to various data mining tasks. In particular, we developed the annotated data stream framework which enables the validation of computationally-intensive data mining by a computationally-weak user that has limited memory and is only able to take a single pass through the data. We then extended this to also support protocols that could have multiple rounds of interaction that would allow the user to verify the computation more efficiently. We then designed new data stream algorithms with provable accuracy guarantees for various data mining tasks including clustering (where we try to group together similar entities while ensuring dissimilar entities are in different groups), estimating the correlation coefficient of a network (a measure of how transitive the relationships in the data are), and constructing Bayesian networks that model dependencies in the data. Finally, we presented new results on reducing the dimension of high dimensional data sets in such a way that information-theoretic distances are approximately preserved.</p>\n<p>In terms of broader impact, the training of numerous graduate students was supported on this grant. In addition to publishing the results at conferences and journals, we also organized a tutorial at the leading machine learning conference. Finally, some of the results were included in a graduate course at the University of Massachusetts and the related teaching material has been made public for the benefit of other students and instructors.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/22/2018<br>\n\t\t\t\t\tModified by: Andrew&nbsp;Mcgregor</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe availability of massive amounts of data in various learning and data mining tasks has both advantages and disadvantages. With more data, we can hope to perform more sophisticated analysis and generate more accurate predictions. This could result in anything from better book recommendations from an online retailer to more detailed medical diagnoses and even to more accurate screening of potential terror threats. However, with more data comes more responsibility along with new computational challenges. Since the results of data mining can have negative consequences, e.g., an inappropriate treatment due to a misdiagnosis, we want to be able to validate the computations that led to the final results and understand the properties of the data set that may be relevant to the outcome.\n\nThe intellectual merit of this project revolved around a) developing techniques for validation of computation on large data sets and b) designing efficient algorithms for analyzing properties of the data sets that are relevant to various data mining tasks. In particular, we developed the annotated data stream framework which enables the validation of computationally-intensive data mining by a computationally-weak user that has limited memory and is only able to take a single pass through the data. We then extended this to also support protocols that could have multiple rounds of interaction that would allow the user to verify the computation more efficiently. We then designed new data stream algorithms with provable accuracy guarantees for various data mining tasks including clustering (where we try to group together similar entities while ensuring dissimilar entities are in different groups), estimating the correlation coefficient of a network (a measure of how transitive the relationships in the data are), and constructing Bayesian networks that model dependencies in the data. Finally, we presented new results on reducing the dimension of high dimensional data sets in such a way that information-theoretic distances are approximately preserved.\n\nIn terms of broader impact, the training of numerous graduate students was supported on this grant. In addition to publishing the results at conferences and journals, we also organized a tutorial at the leading machine learning conference. Finally, some of the results were included in a graduate course at the University of Massachusetts and the related teaching material has been made public for the benefit of other students and instructors.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/22/2018\n\n\t\t\t\t\tSubmitted by: Andrew Mcgregor"
 }
}