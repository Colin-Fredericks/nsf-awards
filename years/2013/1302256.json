{
 "awd_id": "1302256",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: BCSP: Automated Parameter Tuning of Large-Scale Spiking Neural Networks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 474996.0,
 "awd_amount": 474996.0,
 "awd_min_amd_letter_date": "2013-09-10",
 "awd_max_amd_letter_date": "2013-09-10",
 "awd_abstract_narration": "A framework will be developed to help scientists and engineers create brain-inspired, brain-sized networks that can carry out practical applications. Large-scale spiking neural networks, which follow the brain's architecture and activity, have been used to successfully model phenomena such as learning and memory, vision, auditory processing, neural oscillations, and many other important aspects of neural function. Additionally, spiking neural networks are particularly well suited to run on neuromorphic hardware, state of the art computers that emulate the brain?s structure and dynamics. These neuromorphic systems depend on the binary nature of spikes to lower communication bandwidth and energy consumption. Although significant progress has been made towards the specification and simulation of large-scale spiking neural networks on a variety of hardware platforms, many challenges remain before these neurobiologically inspired algorithms can be used in practical applications. While biology does provide increasingly abundant empirical data that can constrain these systems, many parameter values must be chosen manually by the designer to achieve appropriate neuronal dynamics, a task that is extremely tedious and often error-prone. To meet this challenge, an automated parametertuning framework will be developed that is capable of quickly and efficiently tuning large-scale spiking neural networks. The framework will leverage recent progress in evolutionary algorithms and optimization techniques for off-the-shelf graphics processing units (GPUs). The parameter search will be guided by the idea in neuroscience that biological networks adapt their responses to increase the amount of transmitted information, reduce redundancies, and span the stimulus space. This notion of efficient coding will guide the tuning process of the artificial spiking neural networks. Computer scientists and engineers will be able to use the resulting automated parameter-tuning framework to create brain inspired applications, such as vision and memory systems, on neuromorphic hardware. Moreover, the resulting framework will allow neuroscientists to more readily create models that better describe their empirical data and generate new quantitative hypotheses that can be tested in the laboratory.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kenneth",
   "pi_last_name": "De Jong",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Kenneth A De Jong",
   "pi_email_addr": "kdejong@gmu.edu",
   "nsf_id": "000329301",
   "pi_start_date": "2013-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Mason University",
  "inst_street_address": "4400 UNIVERSITY DR",
  "inst_street_address_2": "",
  "inst_city_name": "FAIRFAX",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "7039932295",
  "inst_zip_code": "220304422",
  "inst_country_name": "United States",
  "cong_dist_code": "11",
  "st_cong_dist_code": "VA11",
  "org_lgl_bus_name": "GEORGE MASON UNIVERSITY",
  "org_prnt_uei_num": "H4NRWLFCDF43",
  "org_uei_num": "EADLFP7Z72E5"
 },
 "perf_inst": {
  "perf_inst_name": "George Mason University",
  "perf_str_addr": "4400 University Drive",
  "perf_city_name": "Fairfax",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "220304422",
  "perf_ctry_code": "US",
  "perf_cong_dist": "11",
  "perf_st_cong_dist": "VA11",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 474996.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Normal1\">During the four years of this project, we developed a framework to help scientists and engineers create very large-scale neural networks for studying the brain and making practical applications. Large-scale spiking neural networks (SNNs), which follow the brain&rsquo;s architecture and activity, have been used to successfully model phenomena such as learning and memory, vision, auditory processing, neural oscillations, and many other important aspects of neural function. Although significant progress has been made towards the specification and simulation of large-scale spiking neural networks on a variety of hardware platforms, many challenges remain before these neurobiologically inspired algorithms can be used in practical applications. While biology does provide increasingly abundant empirical data that can constrain these systems, many parameter values must be chosen manually by the designer to achieve appropriate neuronal dynamics, a task that is extremely tedious and often error-prone.</p>\n<p class=\"Normal1\">To address the challenge of brain-scale simulations, we created an automated parameter tuning framework that leverages the optimization capabilities of evolutionary computation, and takes advantage of the parallel nature of graphical processing units (GPUs). The collaboration between George Mason University and the University of California, Irvine took advantage of their interdisciplinary expertise in neuroscience and computer science. Because SNN simulations and fitness evaluation are time-consuming operations, optimization time can be significantly reduced if multiple SNNs run concurrently, each with distinct parameters. In order to automate the parameter tuning process, one must define a way to minimize the difference between simulated and experimental data. However, for complex networks there can be many combinations of parameter values that produce low error rates. We took the approach of following a theory from neuroscience, known as the efficient coding hypothesis, with the idea being that biological neural networks adapt their responses to increase the amount of transmitted information by reducing redundancies and spanning the stimulus space. In other word, they try to encode information as efficiently as possible. This idea served as the basis for tuning our large-scale SNNs.</p>\n<p class=\"Normal1\">During the project, we were able to achieve our goals of: 1) using the automated tuning framework to replicate dynamics of real brain recordings from awake behaving animals, and use the resulting simulated brain to make predictions about learning and memory, 2) showing that encoding the most information using the least amount of energy, through sparse coding and dimensionality reduction, may be an organizing principle in the brain, 3) demonstrating this on multiple datasets, confirming that this was functionally equivalent to what was being done in our automated tuning framework, 4) developing a multi-GPU, multi-core version of our simulation software to increase usability and the scale of our SNNs, 5) enhancing the model tuning framework to reliably tune parameters to fit complex neuron responses found in brain areas such as the hippocampus, an area for learning and memory, 6) performing an analysis of complex dynamics observed in neurons, and 7) developing tools and techniques to analyze complex problem spaces, especially when they are noisy.</p>\n<p class=\"Normal1\">Over the last four years, the project had a major impact on science and education. We made multiple releases of our software for simulating and tuning spiking neural networks. The software, called CARLsim and ECJ, is publicly available and open source. CARLsim is now a part of the Neuroscience Gateway portal that facilitates the access and use of National Science Foundation (NSF) High Performance Computing (HPC) resources. The software is used around the world. It has had a particular impact on neuromorphic engineering by transitioning SNNs from toy problems to practical applications, on brain-inspired computer architectures. The use of evolutionary algorithms (EAs) to solve large-scale parameter optimization problems extends well beyond the neuroscience problems addressed in this research. The evolutionary algorithm improvements developed here are now part of the institutional research computing infrastructure and the international community&rsquo;s efforts to provide high-performance evolutionary algorithms.</p>\n<p class=\"Normal1\">Finally, the project has contributed to the education of many students at many levels. Several Ph.D. degrees, at both George Mason and UC Irvine, have come directly from the research on this project. Several undergraduates received training on this project through the NSF Research Experiences for Undergraduates. All personnel related to this project were a part of many outreach activities on computer science, neuroscience, and scientific discovery for K-12 students.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/12/2017<br>\n\t\t\t\t\tModified by: Kenneth&nbsp;A&nbsp;De Jong</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "During the four years of this project, we developed a framework to help scientists and engineers create very large-scale neural networks for studying the brain and making practical applications. Large-scale spiking neural networks (SNNs), which follow the brain?s architecture and activity, have been used to successfully model phenomena such as learning and memory, vision, auditory processing, neural oscillations, and many other important aspects of neural function. Although significant progress has been made towards the specification and simulation of large-scale spiking neural networks on a variety of hardware platforms, many challenges remain before these neurobiologically inspired algorithms can be used in practical applications. While biology does provide increasingly abundant empirical data that can constrain these systems, many parameter values must be chosen manually by the designer to achieve appropriate neuronal dynamics, a task that is extremely tedious and often error-prone.\nTo address the challenge of brain-scale simulations, we created an automated parameter tuning framework that leverages the optimization capabilities of evolutionary computation, and takes advantage of the parallel nature of graphical processing units (GPUs). The collaboration between George Mason University and the University of California, Irvine took advantage of their interdisciplinary expertise in neuroscience and computer science. Because SNN simulations and fitness evaluation are time-consuming operations, optimization time can be significantly reduced if multiple SNNs run concurrently, each with distinct parameters. In order to automate the parameter tuning process, one must define a way to minimize the difference between simulated and experimental data. However, for complex networks there can be many combinations of parameter values that produce low error rates. We took the approach of following a theory from neuroscience, known as the efficient coding hypothesis, with the idea being that biological neural networks adapt their responses to increase the amount of transmitted information by reducing redundancies and spanning the stimulus space. In other word, they try to encode information as efficiently as possible. This idea served as the basis for tuning our large-scale SNNs.\nDuring the project, we were able to achieve our goals of: 1) using the automated tuning framework to replicate dynamics of real brain recordings from awake behaving animals, and use the resulting simulated brain to make predictions about learning and memory, 2) showing that encoding the most information using the least amount of energy, through sparse coding and dimensionality reduction, may be an organizing principle in the brain, 3) demonstrating this on multiple datasets, confirming that this was functionally equivalent to what was being done in our automated tuning framework, 4) developing a multi-GPU, multi-core version of our simulation software to increase usability and the scale of our SNNs, 5) enhancing the model tuning framework to reliably tune parameters to fit complex neuron responses found in brain areas such as the hippocampus, an area for learning and memory, 6) performing an analysis of complex dynamics observed in neurons, and 7) developing tools and techniques to analyze complex problem spaces, especially when they are noisy.\nOver the last four years, the project had a major impact on science and education. We made multiple releases of our software for simulating and tuning spiking neural networks. The software, called CARLsim and ECJ, is publicly available and open source. CARLsim is now a part of the Neuroscience Gateway portal that facilitates the access and use of National Science Foundation (NSF) High Performance Computing (HPC) resources. The software is used around the world. It has had a particular impact on neuromorphic engineering by transitioning SNNs from toy problems to practical applications, on brain-inspired computer architectures. The use of evolutionary algorithms (EAs) to solve large-scale parameter optimization problems extends well beyond the neuroscience problems addressed in this research. The evolutionary algorithm improvements developed here are now part of the institutional research computing infrastructure and the international community?s efforts to provide high-performance evolutionary algorithms.\nFinally, the project has contributed to the education of many students at many levels. Several Ph.D. degrees, at both George Mason and UC Irvine, have come directly from the research on this project. Several undergraduates received training on this project through the NSF Research Experiences for Undergraduates. All personnel related to this project were a part of many outreach activities on computer science, neuroscience, and scientific discovery for K-12 students.\n\n \n\n\t\t\t\t\tLast Modified: 09/12/2017\n\n\t\t\t\t\tSubmitted by: Kenneth A De Jong"
 }
}