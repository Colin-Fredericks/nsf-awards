{
 "awd_id": "1253719",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Robotic Perception through Human Context",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "jeffrey trinkle",
 "awd_eff_date": "2013-03-15",
 "awd_exp_date": "2016-02-29",
 "tot_intn_awd_amt": 485000.0,
 "awd_amount": 284151.0,
 "awd_min_amd_letter_date": "2013-03-21",
 "awd_max_amd_letter_date": "2015-08-31",
 "awd_abstract_narration": "To successfully operate in human environments, a robot needs to be able to properly interpret its surroundings and manipulate human objects. In human environments such as a household, we can bring to bear significant information about the expected spatial layout and the function of common household objects. However, for most reasoning methods for robotic perception and manipulation, humans play a secondary role. In this proposal, we propose to make humans central to our reasoning algorithms. We argue that such explicit modeling and consideration of humans, even when they are not present, will enable robots to better perceive, manipulate, and plan. Through such reasoning, we will make advances in the following areas: (a) Modeling 3D Scenes with Objects and Humans, (b) Human Activity Detection and affordances, and (c) Robot Manipulation for assistive tasks.\r\n\r\nThis research has the potential to significantly impact the quality of life of a large number of elderly people. In fact, one of the most promising applications of this research is in assistive care for the elderly and those with difficulty in performing daily tasks. Assistive care for the elderly is an increasingly important problem: a growing fraction of the US population is elderly; by 2030 about 1 in 5 Americans will be over 65. Many elderly people (including about 50% of those 85 years or older) require some personal assistance.  Just to mention one basic example: a key concern in elderly care is that the subject does not drink enough water throughout the day and becomes dehydrated. Our  robot (or a monitoring system) can observe the water consumption, remind the subject to drink water when needed, and even bring some if desired. Other outreach activities include incorporating young students and minorities into robotics research, curriculum development, and open-source robotics software.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ashutosh",
   "pi_last_name": "Saxena",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ashutosh Saxena",
   "pi_email_addr": "asaxena@cs.cornell.edu",
   "nsf_id": "000539644",
   "pi_start_date": "2013-03-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "4159 Upson Hall",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148537501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 284151.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>To successfully operate, a robot needs to be able to properly sense and interpret its surroundings. In this project, we tackle the problem of sensing and understanding household environments. Although, general automated scene understanding, as studied in computer vision, is a longstanding and still largely open challenge, in a household setting, we can bring to bare significant information about the expected spatial layout and the function of common household objects. Extracting such information is also necessary for a robot to manipulate objects. Moreover, we add further information to the scene interpretation task from 3D sensing information, as provided by Kinect, in addition to active sensing, with the robot collecting new sensing data from different locations, when required. Finally, human activities in the environment provide yet a further source of information that a robot can use in interacting with the objects in the environment. We developed a web-based system (RoboBrain) that allows robots to share their acquired knowledge over the internet. We demonstrated how knowledge learned by a robot at Cornell could be used to by another robot for a related but different household task at Brown University (group of Prof. Stefanie Tellex). This demonstration shows the generality of our framework.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/22/2016<br>\n\t\t\t\t\tModified by: Ashutosh&nbsp;Saxena</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nTo successfully operate, a robot needs to be able to properly sense and interpret its surroundings. In this project, we tackle the problem of sensing and understanding household environments. Although, general automated scene understanding, as studied in computer vision, is a longstanding and still largely open challenge, in a household setting, we can bring to bare significant information about the expected spatial layout and the function of common household objects. Extracting such information is also necessary for a robot to manipulate objects. Moreover, we add further information to the scene interpretation task from 3D sensing information, as provided by Kinect, in addition to active sensing, with the robot collecting new sensing data from different locations, when required. Finally, human activities in the environment provide yet a further source of information that a robot can use in interacting with the objects in the environment. We developed a web-based system (RoboBrain) that allows robots to share their acquired knowledge over the internet. We demonstrated how knowledge learned by a robot at Cornell could be used to by another robot for a related but different household task at Brown University (group of Prof. Stefanie Tellex). This demonstration shows the generality of our framework.\n\n \n\n\t\t\t\t\tLast Modified: 08/22/2016\n\n\t\t\t\t\tSubmitted by: Ashutosh Saxena"
 }
}