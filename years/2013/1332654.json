{
 "awd_id": "1332654",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF:  Large:  Collaborative Research:  Reliable Performance for Modern Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2013-01-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 140061.0,
 "awd_amount": 140061.0,
 "awd_min_amd_letter_date": "2013-02-19",
 "awd_max_amd_letter_date": "2013-02-19",
 "awd_abstract_narration": "Today's computer systems are more powerful than ever, but have become so complex that it is now difficult for programmers to produce high-performance software. Even slight changes in programs or differences in a user's system can cause dramatic slowdowns. Currently, there is no way to guarantee that a program will perform as well as it did during testing. This situation makes it extremely difficult to track down the sources of inefficiencies or repair them. The result is reduced power and computational efficiency on servers, and a degraded user experience on client platforms.\r\n\r\nThis research aims to deliver reliable performance on modern computer systems. By introducing randomness into the way a computer runs programs, a reliably performant system will significantly reduce the probability that any small change will have a large impact on performance. For instance, consider a cache miss caused by a conflict. With standard caches, repeated access to the same elements would always cause misses, degrading performance.  In a randomized cache or with randomized object placement, it would be very unlikely for the same line to be repeatedly evicted. The investigators are designing and evaluating the use of both randomized algorithms in software and hardware, separately and in combination, to remedy the numerous sources of pathological behavior in modern systems. The result will enable performance-portable applications that are immune to unfortunate interactions with microprocessor components.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Jimenez",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel A Jimenez",
   "pi_email_addr": "djimenez@acm.org",
   "nsf_id": "000373616",
   "pi_start_date": "2013-02-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas Engineering Experiment Station",
  "perf_str_addr": "",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778454645",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "732900",
   "pgm_ele_name": "COMPILERS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7329",
   "pgm_ref_txt": "COMPILERS"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 68811.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 71250.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>These days, computer systems are highly complex. We want them to achieve high performance, but a small change in one aspect of a program or device might have a large unintended impact on performance. For example, microprocessors often use the memory addresses of instructions to allocate performance-related resources to those instructions. A technique called hashing is used for an efficient allocation of resources. However, this technique can cause two unrelated instructions to accidentally contend for the same resource, with a negative impact on performance. Whether or not this accident occurs is a consequence of arithmetic properties of the memory addresses, completely unrelated to the purpose of the program, and can change when a small change occurs in the program. Thus, performance of programs can be highly variable.&nbsp; Recognizing the causes of this variance allows us to investigate ways to reduce it or exploit it.<br /><br />This work involves designing new hardware for potential future computers as well as writing new software to enhance existing computers.&nbsp; In this work, we have used program variance to develop several novel techniques related to performance variance. For instance, we have artificially manipulated the causes of variance to act as a \"knob\" we can turn to raise or lower performance. By observing the change in the performance, we can accurately model the impact of new hardware techniques related to the knob. In another example, we have observed that memory accesses in computing systems tend to occur in rapid bursts rather than regularly over time.&nbsp; By predicting this variable behavior of memory accesses, we are able to do a better job of scheduling memory operations so that they don't interfere with one another, improving the performance of the computer.<br /><br />We have also seen how to improve performance by incorporating new technologies into the computer system.&nbsp; For example, spin torque transfer RAM (STT-RAM) is a new technology that enables a denser and higher-capacity memory, but it has some odd properties that can reduce performance if too many of those accidental events occurs. By predicting and handling the operations that would trigger these unfortunate events differently, we can build a high-performance and high-capacity memory system. We have applied similar techniques to help build a die-stacked DRAM-based cache. Die-stacked DRAM is a new technology that builds computer memories by stacking one memory chip on top of another, allowing potentially high capacity and high speed memory transfers.<br /><br />Results from the project were communicated via presentation at high-profile computer science conferences, journal articles, and a doctoral dissertation.&nbsp; The project has added to the knowledge and set of skills researchers have at their disposal to investigate high-performance computing systems. It has also contributed some practical techniques that can be put to use in new computing devices if industry chooses to adopt them. The research has potential impacts in many aspects of computing, including potentially reduced energy in mobile devices and improved performance and energy on cloud-based computing platforms.<br /><br />This project has involved the participation of two student researchers, both of whom are women. Unfortunately, women are under-represented in computing research, so it is gratifying that this project has been able to encourage these two young women in their pursuit of a graduate degree in computer science. As of this writing, one of the women has graduated with her doctorate and the other expects to graduate in May of 2015.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2014<br>\n\t\t\t\t\tModified by: Daniel&nbsp;A&nbsp;Jimenez</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThese days, computer systems are highly complex. We want them to achieve high performance, but a small change in one aspect of a program or device might have a large unintended impact on performance. For example, microprocessors often use the memory addresses of instructions to allocate performance-related resources to those instructions. A technique called hashing is used for an efficient allocation of resources. However, this technique can cause two unrelated instructions to accidentally contend for the same resource, with a negative impact on performance. Whether or not this accident occurs is a consequence of arithmetic properties of the memory addresses, completely unrelated to the purpose of the program, and can change when a small change occurs in the program. Thus, performance of programs can be highly variable.  Recognizing the causes of this variance allows us to investigate ways to reduce it or exploit it.\n\nThis work involves designing new hardware for potential future computers as well as writing new software to enhance existing computers.  In this work, we have used program variance to develop several novel techniques related to performance variance. For instance, we have artificially manipulated the causes of variance to act as a \"knob\" we can turn to raise or lower performance. By observing the change in the performance, we can accurately model the impact of new hardware techniques related to the knob. In another example, we have observed that memory accesses in computing systems tend to occur in rapid bursts rather than regularly over time.  By predicting this variable behavior of memory accesses, we are able to do a better job of scheduling memory operations so that they don't interfere with one another, improving the performance of the computer.\n\nWe have also seen how to improve performance by incorporating new technologies into the computer system.  For example, spin torque transfer RAM (STT-RAM) is a new technology that enables a denser and higher-capacity memory, but it has some odd properties that can reduce performance if too many of those accidental events occurs. By predicting and handling the operations that would trigger these unfortunate events differently, we can build a high-performance and high-capacity memory system. We have applied similar techniques to help build a die-stacked DRAM-based cache. Die-stacked DRAM is a new technology that builds computer memories by stacking one memory chip on top of another, allowing potentially high capacity and high speed memory transfers.\n\nResults from the project were communicated via presentation at high-profile computer science conferences, journal articles, and a doctoral dissertation.  The project has added to the knowledge and set of skills researchers have at their disposal to investigate high-performance computing systems. It has also contributed some practical techniques that can be put to use in new computing devices if industry chooses to adopt them. The research has potential impacts in many aspects of computing, including potentially reduced energy in mobile devices and improved performance and energy on cloud-based computing platforms.\n\nThis project has involved the participation of two student researchers, both of whom are women. Unfortunately, women are under-represented in computing research, so it is gratifying that this project has been able to encourage these two young women in their pursuit of a graduate degree in computer science. As of this writing, one of the women has graduated with her doctorate and the other expects to graduate in May of 2015.\n\n\t\t\t\t\tLast Modified: 10/30/2014\n\n\t\t\t\t\tSubmitted by: Daniel A Jimenez"
 }
}