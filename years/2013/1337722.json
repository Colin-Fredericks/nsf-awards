{
 "awd_id": "1337722",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Development of Large-Scale Dense Scene Capture and Tracking Instrument",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2013-08-15",
 "awd_max_amd_letter_date": "2016-03-09",
 "awd_abstract_narration": "Proposal #:\t13-37899\r\nPI(s):\t\tHahn, James K.\r\n\t\tLee, Taeyoung; Philbeck, John W.; Rickmond, Brian G.; Townsend, Gabe Sibley\r\nInstitution:\tGeorge Washington University \r\nTitle: \t\tMRI/Dev.: Large-Scale Dense Scene Capture and Tracking Instrument\r\nProject Proposed:\r\nThis project, developing a large-scale, dense 3D measurement instrument for capturing dynamic environments, integrates devices such as range-and-color sensing devices like depth cameras (RGB-D sensors) by designing and developing key technical methodologies to fuse the data received from remote networked sensors. The instrument will collectively cover a large space at a sampling resolution of at least 1cm with submillimeter resolution in localized regions. These data are then fused into a single underlying representation. The work involves developing a system that possesses both large-scale and real-time dense capture capabilities. Specifically, \r\n-\tExperimentally validating perception, planning and control algorithms of agile mobile robots (particularly those that operate with deformable objects) requires ground truth representation of those environments.\r\n-\tValidating computational tools for tether dynamics and control for flexible multibody systems requires the capture of their environment in a large environment.\r\n-\tStudy of human motion for biomechanics, physical therapy, and exercise science applications requires accurate capture of dynamically changing deformable human shapes in a large environment.\r\n-\tImage-guided surgical procedures require capture of localized dense patient anatomical surface registered in a larger surgical environment.\r\n-\tHuman visual perception and navigation require a dense model of the surrounding environments that include object in motion, thus advancing the state of eye movement analysis by enabling fast, automated and objective coding of object analysis by enabling fast, automated, and objective coding of objects people see as they move through the environment.\r\n-\tThe study of foot deformations enabled by dense shape capture during running and walking on real sediments will shed light on the evolution of gait and human anatomy, and the biomechanics of barefoot walking and running. \r\nThus, facilitating new research, the developed system enables rapid capture and construction of large dynamic high-resolution virtual environments that duplicate specific real-world environments, including deformable objects, with unprecedented density of detail.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Hahn",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "James K Hahn",
   "pi_email_addr": "hahn@gwu.edu",
   "nsf_id": "000255438",
   "pi_start_date": "2013-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Brian",
   "pi_last_name": "Richmond",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brian Richmond",
   "pi_email_addr": "brian.richmond@gmail.com",
   "nsf_id": "000478034",
   "pi_start_date": "2013-08-15",
   "pi_end_date": "2016-03-09"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Philbeck",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "John W Philbeck",
   "pi_email_addr": "philbeck@gwu.edu",
   "nsf_id": "000345356",
   "pi_start_date": "2013-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Taeyoung",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Taeyoung Lee",
   "pi_email_addr": "tylee@gwu.edu",
   "nsf_id": "000523355",
   "pi_start_date": "2013-08-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Sergio",
   "pi_last_name": "Almecija",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sergio Almecija",
   "pi_email_addr": "salmecija@amnh.org",
   "nsf_id": "000587610",
   "pi_start_date": "2016-03-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gabriel",
   "pi_last_name": "Sibley",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Gabriel T Sibley",
   "pi_email_addr": "gsibley@colorado.edu",
   "nsf_id": "000674922",
   "pi_start_date": "2013-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Washington University",
  "inst_street_address": "1918 F ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2029940728",
  "inst_zip_code": "200520042",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "GEORGE WASHINGTON UNIVERSITY (THE)",
  "org_prnt_uei_num": "",
  "org_uei_num": "ECR5E2LU5BL6"
 },
 "perf_inst": {
  "perf_inst_name": "George Washington University",
  "perf_str_addr": "",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200520058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to develop a novel instrument for large-scale, real-time, dense 3D surface and motion-capture of dynamic scenes. One of the key components of the instrument is a number of low-cost commodity RGB-D devices. This makes it possible to scale up the coverage area by adding a large number of relatively cheap devices. This instrument can measure, in real-time, dense 3D scene-structure and color-appearance information over a large volume at sub-centimeter resolution with focused volumes at sub-millimeter resolution. This gives users the ability to track, at unprecedented scale and resolution, scene-appearance, 3D-structure&nbsp;and their temporal evolution. In a broad range of fields, knowledge of the evolution of ground truth 3D structure and scene appearance is invaluable for validating scientific theories.</p>\n<p>3D surface and motion capture is still in its infancy. However, the technology has the potential to impact every segment of society. Smart phones are beginning to incorporate depth cameras (RGB-D cameras), such as the iPhone X. Commodity RGB-D devices such as the Microsoft Kinect is able to capture scenes at high resolution. Aside from the development of the instrument, the basic technology that was developed is bound to impact the future uses of these commodity devices.</p>\n<p>The instrument is housed in the Motion Capture and Analysis Laboratory (MOCA) at the George Washington University. MOCA has been used in robotics to track ground truth motions of flocks of unmanned flying robots. Anthropologists have used MOCA to track the human motion involved in locomotion, tool making, and throwing to study the biomechanics of early hominids. MOCA instrument and technology has also been used in the capture of motions involved in medical procedures, such as endotracheal intubation and intravenous injections. Psychologists have used MOCA to study the cognitive factors involved in human navigation.</p>\n<p>The instrument has also been used in non-technical domains. We are using the instrument in collaboration with the Jane Goodall Institute to create avatars of Jane Goodall. Dancers from Maida Withers Dance Company were scanned using the instrument. The data was then visualized during a live dance performance.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/10/2018<br>\n\t\t\t\t\tModified by: James&nbsp;K&nbsp;Hahn</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539122044328_moca_front--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539122044328_moca_front--rgov-800width.jpg\" title=\"MOCA Laboratory\"><img src=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539122044328_moca_front--rgov-66x44.jpg\" alt=\"MOCA Laboratory\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Motion Capture and Analysis Laboratory houses the large-scale, real-time, dense 3D motion-capture instrument.</div>\n<div class=\"imageCredit\">The George Washington University, Institute for Computer Graphics</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">James&nbsp;K&nbsp;Hahn</div>\n<div class=\"imageTitle\">MOCA Laboratory</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539122411155_GroupTest--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539122411155_GroupTest--rgov-800width.jpg\" title=\"Scanning human subjects\"><img src=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539122411155_GroupTest--rgov-66x44.jpg\" alt=\"Scanning human subjects\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The technology was used to scan 160 human subjects as a part of an NIH funded study to analyze the percentage of body fat.</div>\n<div class=\"imageCredit\">The George Washington University, Institute for Computer Graphics</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">James&nbsp;K&nbsp;Hahn</div>\n<div class=\"imageTitle\">Scanning human subjects</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539123281540_vrfuison--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539123281540_vrfuison--rgov-800width.jpg\" title=\"VR Fusion\"><img src=\"/por/images/Reports/POR/2018/1337722/1337722_10267589_1539123281540_vrfuison--rgov-66x44.jpg\" alt=\"VR Fusion\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">An avatar scanned from a subject is viewed by the subject himself using a virtual reality display.</div>\n<div class=\"imageCredit\">The George Washington University, Institute for Computer Graphics</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">James&nbsp;K&nbsp;Hahn</div>\n<div class=\"imageTitle\">VR Fusion</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to develop a novel instrument for large-scale, real-time, dense 3D surface and motion-capture of dynamic scenes. One of the key components of the instrument is a number of low-cost commodity RGB-D devices. This makes it possible to scale up the coverage area by adding a large number of relatively cheap devices. This instrument can measure, in real-time, dense 3D scene-structure and color-appearance information over a large volume at sub-centimeter resolution with focused volumes at sub-millimeter resolution. This gives users the ability to track, at unprecedented scale and resolution, scene-appearance, 3D-structure and their temporal evolution. In a broad range of fields, knowledge of the evolution of ground truth 3D structure and scene appearance is invaluable for validating scientific theories.\n\n3D surface and motion capture is still in its infancy. However, the technology has the potential to impact every segment of society. Smart phones are beginning to incorporate depth cameras (RGB-D cameras), such as the iPhone X. Commodity RGB-D devices such as the Microsoft Kinect is able to capture scenes at high resolution. Aside from the development of the instrument, the basic technology that was developed is bound to impact the future uses of these commodity devices.\n\nThe instrument is housed in the Motion Capture and Analysis Laboratory (MOCA) at the George Washington University. MOCA has been used in robotics to track ground truth motions of flocks of unmanned flying robots. Anthropologists have used MOCA to track the human motion involved in locomotion, tool making, and throwing to study the biomechanics of early hominids. MOCA instrument and technology has also been used in the capture of motions involved in medical procedures, such as endotracheal intubation and intravenous injections. Psychologists have used MOCA to study the cognitive factors involved in human navigation.\n\nThe instrument has also been used in non-technical domains. We are using the instrument in collaboration with the Jane Goodall Institute to create avatars of Jane Goodall. Dancers from Maida Withers Dance Company were scanned using the instrument. The data was then visualized during a live dance performance.\n\n \n\n\t\t\t\t\tLast Modified: 10/10/2018\n\n\t\t\t\t\tSubmitted by: James K Hahn"
 }
}