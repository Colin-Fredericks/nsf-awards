{
 "awd_id": "1343530",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CI-ADDO-NEW: Collaborative Research: A Repository for Annotating Multilingual Code Switched Data",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2013-02-01",
 "awd_exp_date": "2017-05-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2013-05-24",
 "awd_max_amd_letter_date": "2013-05-24",
 "awd_abstract_narration": "Linguistic code switching (LCS) is the practice of switching back and forth between the shared languages of bilingual or multilingual speakers. This phenomenon is particularly prevalent in geographic regions with linguistic boundaries or where there are large immigrant groups. Various levels of language (phonological, morphological, syntactic, semantic and discourse-pragmatic) may be implicated in LCS in different language pairs and/or genres. Computational algorithms trained for a single language quickly break down when the input includes LCS. A major barrier to research on LCS in computational linguistics (CL) has been the lack of large, accurately annotated corpora of LCS data. In this project, a large repository of LCS data is collected and a large annotation infrastructure is developed. It is consistently annotated in different modalities (speech and text), at various levels of linguistic granularity, and across different language pairs reflecting different linguistic typologies (Standard Arabic and Dialectal Arabic, Arabic-English, Spanish-English, Chinese-English, Hindi-English). The focus of the effort is on intra-sentential LCS.\r\n\r\nThis infrastructure and unified large LCS data resource is eagerly awaited by the CL research community, since annotated LCS data provides a natural test-bed for adaptive learning algorithms and the handling of diverse data sources, as well as a framework for genuine multilingual processing. It will also be of benefit to sociolinguistic and theoretical linguistic researchers, and provide a platform for collaborative interdisciplinary research. Finally, research on LCS helps overcome biases against multilingual speakers by demonstrating the creativity of such speakers in exploiting their verbal repertoires. Such a result is particularly important for K-12 education and testing policies in the USA with its diverse immigrant population.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mona",
   "pi_last_name": "Diab",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Mona T Diab",
   "pi_email_addr": "mdiab@andrew.cmu.edu",
   "nsf_id": "000244906",
   "pi_start_date": "2013-05-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Washington University",
  "inst_street_address": "1918 F ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2029940728",
  "inst_zip_code": "200520042",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "GEORGE WASHINGTON UNIVERSITY (THE)",
  "org_prnt_uei_num": "",
  "org_uei_num": "ECR5E2LU5BL6"
 },
 "perf_inst": {
  "perf_inst_name": "George Washington University",
  "perf_str_addr": "801 22nd St",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200520058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Code-switching (CS) is the phenomenon by which multilingual speakers switch back and forth between their common languages in written or spoken communication. CS may occur at the inter-utterance, intra-utterance (mixing of words from multiple languages in the same utterance) and even morphological (mixing of morphemes from different languages) levels. CS presents serious challenges for language technologies such as Automatic Speech Recognition, Language Modeling, Parsing, Machine Translation (MT), Information Retrieval (IR) and Extraction (IE), Keyword Search, and semantic processing. A prime example of this is acoustic modeling and language modeling in automatic speech recognition (ASR): techniques trained on one language quickly break down when there is mixed language input. The lack of basic tools such as language models, part-of-speech (POS) taggers and parsers trained on such mixed language data makes downstream tasks even more challenging. Even for problems that are largely considered solved for monolingual corpora, such as Language Identification, or POS Tagging, performance degrades at a rate proportional to the amount and level of mixed-language present in the data.</span></p>\n<p>In this project, we created a large unified repository for code switched data for 6 different languages: Arabic Standard and its dialects, Arabic English, Hindi English, Chinese English, Spanish English, and Nepali English. We created a unified set of guidelines relying on input from sociolinguists, theoretical linguists, and computational linguists. We addressed token level and sentence level annotations. We also identified challenges in subword and named entities for code switched data. We held 3 workshops for computational approaches to Linguistic code switching 2 of which were collocated with EMNLP 2014 and 2016. The third was collocated with Interspeech 2016. We are also planning a workshop at ACL 2018 and another with interspeech 2018. We also had 2 very successful shared tasks, the venue through which the public can access the labeled data that we have divided into standard data sets of training, development and test. The work results in over 15 publications.</p>\n<p>Addressing Linguistic Code Switching head on allowed us to develop more robust enabling technologies in the single language space by trying to accommodate less amounts of training data. We have also been able to note patterns in the usage of code switching that are leveraged by our machine learning models indicating that indeed linguistic code switching is not random.&nbsp;</p>\n<p>Having such large amounts of annotated data that is consistently annotated across several language pairs has also inspired other scientists to adopt our guidelines for other language pairs propelling research in this area even further.&nbsp;</p>\n<p>Large amounts of annotated data help with developing and testing various algorithms allowing for a healthy scientific exploration of that space while the challenge of data is less of an impediment.&nbsp;</p>\n<p>We also created a very nice collaborative environment for researchers to exchange data and ideas in the context of the workshops we organized.&nbsp;</p>\n<p>We established standard data sets for the community which serve as both a way for comparison of the various developed algorithms but also allowed us to expose some of the issues with the guidelines and annotation decisions taken up by our group as we iterated through the annotations as they were being tested in the context of the algorithms leveraging them hence yielding a more robust extrinsic evaluation of the quality of the annotations.&nbsp;</p>\n<p><span>This Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/18/2018<br>\n\t\t\t\t\tModified by: Mona&nbsp;T&nbsp;Diab</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nCode-switching (CS) is the phenomenon by which multilingual speakers switch back and forth between their common languages in written or spoken communication. CS may occur at the inter-utterance, intra-utterance (mixing of words from multiple languages in the same utterance) and even morphological (mixing of morphemes from different languages) levels. CS presents serious challenges for language technologies such as Automatic Speech Recognition, Language Modeling, Parsing, Machine Translation (MT), Information Retrieval (IR) and Extraction (IE), Keyword Search, and semantic processing. A prime example of this is acoustic modeling and language modeling in automatic speech recognition (ASR): techniques trained on one language quickly break down when there is mixed language input. The lack of basic tools such as language models, part-of-speech (POS) taggers and parsers trained on such mixed language data makes downstream tasks even more challenging. Even for problems that are largely considered solved for monolingual corpora, such as Language Identification, or POS Tagging, performance degrades at a rate proportional to the amount and level of mixed-language present in the data.\n\nIn this project, we created a large unified repository for code switched data for 6 different languages: Arabic Standard and its dialects, Arabic English, Hindi English, Chinese English, Spanish English, and Nepali English. We created a unified set of guidelines relying on input from sociolinguists, theoretical linguists, and computational linguists. We addressed token level and sentence level annotations. We also identified challenges in subword and named entities for code switched data. We held 3 workshops for computational approaches to Linguistic code switching 2 of which were collocated with EMNLP 2014 and 2016. The third was collocated with Interspeech 2016. We are also planning a workshop at ACL 2018 and another with interspeech 2018. We also had 2 very successful shared tasks, the venue through which the public can access the labeled data that we have divided into standard data sets of training, development and test. The work results in over 15 publications.\n\nAddressing Linguistic Code Switching head on allowed us to develop more robust enabling technologies in the single language space by trying to accommodate less amounts of training data. We have also been able to note patterns in the usage of code switching that are leveraged by our machine learning models indicating that indeed linguistic code switching is not random. \n\nHaving such large amounts of annotated data that is consistently annotated across several language pairs has also inspired other scientists to adopt our guidelines for other language pairs propelling research in this area even further. \n\nLarge amounts of annotated data help with developing and testing various algorithms allowing for a healthy scientific exploration of that space while the challenge of data is less of an impediment. \n\nWe also created a very nice collaborative environment for researchers to exchange data and ideas in the context of the workshops we organized. \n\nWe established standard data sets for the community which serve as both a way for comparison of the various developed algorithms but also allowed us to expose some of the issues with the guidelines and annotation decisions taken up by our group as we iterated through the annotations as they were being tested in the context of the algorithms leveraging them hence yielding a more robust extrinsic evaluation of the quality of the annotations. \n\nThis Project Outcomes Report for the General Public is displayed verbatim as submitted by the Principal Investigator (PI) for this award. Any opinions, findings, and conclusions or recommendations expressed in this Report are those of the PI and do not necessarily reflect the views of the National Science Foundation; NSF has not approved or endorsed its content.\n\n\t\t\t\t\tLast Modified: 03/18/2018\n\n\t\t\t\t\tSubmitted by: Mona T Diab"
 }
}