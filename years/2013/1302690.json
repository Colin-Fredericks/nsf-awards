{
 "awd_id": "1302690",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Medium: Collaborative Research: Scaling Machine Learning to Massive Datasets---A Logic Based Approach",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 333000.0,
 "awd_amount": 333000.0,
 "awd_min_amd_letter_date": "2013-07-26",
 "awd_max_amd_letter_date": "2014-07-25",
 "awd_abstract_narration": "Machine learning (ML) algorithms have become ubiquitous across applications as diverse as science, engineering, business, finance, education and healthcare. However, development of ML software that can scale to massive datasets and that are also easy-to-use remains a challenge in part due to the fact that developing an ML tool currently requires the implementation of a deep software stack, from the actual runtime (i.e., how an ML algorithm is executed) to the API exposed to the users.\r\n\r\nThis  project aims to develop DeML, a system to support the authoring and execution of ML tools. Specifically, DeML would allow ML algorithms to be formulated in the form of a declarative query over the training dataset. DeML  optimizes the execution of the query over a computing platform (e.g., Amazon EC2 or SQL Azure), taking into account the characteristics of the algorithm, the data, and the available computational resources. Adoption of DeML would greatly reduce the effort required to develop scalable implementations of ML algorithms. The project is organized around three thrusts: (i) Development of a declarative query language, based on extensions of Datalog; (ii) Analysis of runtime of DeML queries; (iii) Optimization of dataflow of DeML queries based on the characteristics of data sources and the capabilities of the underlying execution platform. The resulting open source DeML prototype implementation will be made freely available to the community through the project web page at: http://deml.cs.ucla.edu.\r\n\r\nThe availability of the DeML could greatly lower the effort needed to author scalable implementations of ML algorithms for analysis of massive datasets, which in turn would increase the availability of such tools to the broader community. Experience gained by implementing and deploying ML algorithms at scale over modern cloud-computing platforms, could help inform critical design choices in the development of future cloud computing platforms for big data analytics, and hence impact a broad range of scientific, engineering, national security, healthcare and business applications of big data analytics. The project offers enhanced opportunities for research-based advanced training of graduate and undergraduate students, including members of groups that are currently under-represented in computer science, in databases, machine learning, and cloud computing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Neoklis",
   "pi_last_name": "Polyzotis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Neoklis Polyzotis",
   "pi_email_addr": "alkis.polyzotis@gmail.com",
   "nsf_id": "000340492",
   "pi_start_date": "2013-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Cruz",
  "inst_street_address": "1156 HIGH ST",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA CRUZ",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8314595278",
  "inst_zip_code": "950641077",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "CA19",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA SANTA CRUZ",
  "org_prnt_uei_num": "",
  "org_uei_num": "VXUFPE4MCZH5"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Cruz",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "950641077",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "CA19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 255709.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 77291.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The project targeted fundamental research in the application of declarative techniques (inspired by several years of research and practice in the field of database systems) in the development of tools and algorithms for machine learning at scale. In this direction, we focused on machine-learning algorithms based on stochastic coordinate descent and in particular we investigated how we can improve their performance for big data using&nbsp;</span>techniques inspired by columnar database systems. Whereas typical parallel implementations of machine learning algorithms perform repeated parallel scans on row-organized data (basically, the features of the training examples), our approach employs a column-based organization that yields several benefits: it is much better suited for coordinate-descent algorithms; it offers for better compression of the data; and, it revealed opportunities to further optimize the algorithm by taking advantage of the column-at-a-time processing. Specifically, we first analyze the weight-update rules for parallel stochastic coordinate descent and show that the rules can be approximated with the use of split functions, which allow \"simulating\" data scans with little computational overhead. We prove that such split functions exist for specific classes of loss functions which include logistic loss. We then show how to leverage this theoretical result in a parallel implementation of coordinate descent, where each worker node processes its shard of a column to compute a set of split functions, communicates these functions to the controller, and the latter uses them to do weight updates through simulated scans over the data. We couple this design with efficient data structures in the worker and the master, and with a method to columnarize the data on-the-fly so that each worker can compute its split functions efficiently. This approach results in significant savings per worker and enables better scale-out with the number of workers. We performed extensive experimental studies to evaluate the performance of this scheme in different environments, including a deployment of the algorithm in the EC2 cloud. Our experiments show that our parallel algorithm with split functions consistently outperform common baselines, yielding 2x in total wall time and up to 10x in machine-time in the EC2 setup, without compromising on the quality of trained model.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/08/2018<br>\n\t\t\t\t\tModified by: Neoklis&nbsp;Polyzotis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project targeted fundamental research in the application of declarative techniques (inspired by several years of research and practice in the field of database systems) in the development of tools and algorithms for machine learning at scale. In this direction, we focused on machine-learning algorithms based on stochastic coordinate descent and in particular we investigated how we can improve their performance for big data using techniques inspired by columnar database systems. Whereas typical parallel implementations of machine learning algorithms perform repeated parallel scans on row-organized data (basically, the features of the training examples), our approach employs a column-based organization that yields several benefits: it is much better suited for coordinate-descent algorithms; it offers for better compression of the data; and, it revealed opportunities to further optimize the algorithm by taking advantage of the column-at-a-time processing. Specifically, we first analyze the weight-update rules for parallel stochastic coordinate descent and show that the rules can be approximated with the use of split functions, which allow \"simulating\" data scans with little computational overhead. We prove that such split functions exist for specific classes of loss functions which include logistic loss. We then show how to leverage this theoretical result in a parallel implementation of coordinate descent, where each worker node processes its shard of a column to compute a set of split functions, communicates these functions to the controller, and the latter uses them to do weight updates through simulated scans over the data. We couple this design with efficient data structures in the worker and the master, and with a method to columnarize the data on-the-fly so that each worker can compute its split functions efficiently. This approach results in significant savings per worker and enables better scale-out with the number of workers. We performed extensive experimental studies to evaluate the performance of this scheme in different environments, including a deployment of the algorithm in the EC2 cloud. Our experiments show that our parallel algorithm with split functions consistently outperform common baselines, yielding 2x in total wall time and up to 10x in machine-time in the EC2 setup, without compromising on the quality of trained model. \n\n\t\t\t\t\tLast Modified: 06/08/2018\n\n\t\t\t\t\tSubmitted by: Neoklis Polyzotis"
 }
}