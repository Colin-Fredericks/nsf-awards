{
 "awd_id": "1352950",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Investigating the Role of Discourse Context in Speech-Driven Facial Animations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2016-02-29",
 "tot_intn_awd_amt": 82552.0,
 "awd_amount": 82552.0,
 "awd_min_amd_letter_date": "2013-08-30",
 "awd_max_amd_letter_date": "2013-08-30",
 "awd_abstract_narration": "This EArly-concept Grants for Exploratory Research analyzes the role of discourse and dialog context in the generation of believable, human-like behaviors for conversational agent (CA), i.e., a virtual agent that interacts with a user. CAs aim to engage the users by displaying human-like behaviors not only through speech by also through facial gestures. One useful modality to drive facial behaviors is speech. Spoken language carries important information beyond the verbal message that a CA engine should capitalize on.  A challenge in speech-driven animation is to generate behaviors that respond to the discourse context. This proposal presents a top-down approach to explore the importance of considering contextual information in the modeling of speech-driven facial gestures. The project starts with speech-driven models, based on dynamic Bayesian networks, which do not capture the specific discourse context, responding only to the properties of the acoustic features. Then, the study considers discourse-specific models in which the intent of the gestures is known. The study defines a specific, controlled domain as testbed, recording multiple human interactions. Similar speech-driven models are trained constrained by the specific discourse function. The study evaluates the differences in the perceived naturalness, appropriateness and rapport of generated facial gestures. \r\n\r\nThe study explores which discourse aspects affect the facial animation models, and which are more domain specific or independent. By incorporating the intrinsic discourse information, the proposed models generate behaviors that respond to conversational functions, addressing one of the limitations in speech-driven facial animations. The findings have a longterm impact in variety of health care applications, such as helping hearing impaired individuals and teaching social skills to autistic children. Likewise, discourse-dependent speech-driven models can play a key role in better tutoring systems that display human-like behaviors to communicate and engage with the students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yang",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yang Liu",
   "pi_email_addr": "yangl@hlt.utdallas.edu",
   "nsf_id": "000288465",
   "pi_start_date": "2013-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Carlos",
   "pi_last_name": "Busso",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Carlos Busso",
   "pi_email_addr": "cbusso@andrew.cmu.edu",
   "nsf_id": "000544291",
   "pi_start_date": "2013-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Dallas",
  "inst_street_address": "800 WEST CAMPBELL RD.",
  "inst_street_address_2": "SP2.25",
  "inst_city_name": "RICHARDSON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9728832313",
  "inst_zip_code": "750803021",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "TX24",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT DALLAS",
  "org_prnt_uei_num": "",
  "org_uei_num": "EJCVPNN1WFS5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Dallas",
  "perf_str_addr": "800 W. Campbell Rd.",
  "perf_city_name": "Richardson",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "750803021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "TX24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 82552.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Research Objective and Significance:&nbsp;</p>\n<p>&nbsp;This EArly-concept Grants for Exploratory Research (EAGER) explored the role of discourse and dialog contextual information in the generation of believable, human-like behaviors for conversational agent (CA), i.e., a virtual agent that interacts with a user. Spoken language carries important information beyond the acoustics and the verbal message that a CA engine should capitalize on, for example, discourse and dialog context information. The research goals are: (a) build two models in the speech-driven framework: one is based on acoustic only, the other uses rich discourse and dialog context; (b) evaluate the differences in the perceived naturalness, appropriateness and rapport of facial gestures generated with the two models above; (c) identify the discourse aspects that are important in the design of facial gestures, and whether they are domain specific.</p>\n<p>&nbsp;Research Project and Findings:</p>\n<p>&nbsp;1- Speech-driven models constrained by discourse functions</p>\n<p>&nbsp;A key challenge for CA is how to create naturalistic behaviors that replicate the complex gestures observed during human interactions. Speech-driven approaches are especially appealing given the rich information conveyed on spoken language. The main limitation of these models is that they fail to capture the underlying semantic and discourse functions of the message (e.g., nodding). We proposed a speech-driven framework that explicitly model discourse functions, bridging the gap between speech-driven and rule-based models. The approach is based on dynamic Bayesian Network (DBN), where an additional node is introduced to constrain the models by specific discourse functions. We implemented the approach by synthesizing head and eyebrow motion. The statistical analysis demonstrated significant changes in head and eyebrow motion across functional classes (affirmation, negation, question, and statement). Perceptual evaluations for the functional class &ldquo;question&rdquo; showed that the animations with constrained models are perceived more preferable, natural and appropriate than the animations with the unconstrained models.</p>\n<p>&nbsp;2- Speech driven models constrained by target gestures</p>\n<p>&nbsp;We created a flexible semi-supervised framework to retrieve arbitrary number of gestures in the data. Starting from few examples of the target gestures (e.g., head nod), the framework searches for similar gestures in the database. The framework replies on temporal reduction, multi scale windows, one-class Support Vector Machines (SVMs), and Dynamic Time Alignment Kernel (DTAK) to identify similar gestures regardless of the duration. Using the retrieved examples, we synthesized novel realizations of these target gestures using data-driven models. The retrieval system was very effective, reaching precision rate over 96% for head gestures (head nods and head shakes). The approach was also effective for hand gestures. We evaluated the performance of the speech-driven models to synthesize these target gestures. We synthesized 60 segments per gestures, constraining the model with the target gesture. We annotated the accuracy per gesture by watching the resulting animations. The generated head gestures matched the intended gesture 96% of the time.</p>\n<p>3- Speech driven models using synthetic speech</p>\n<p>To have believable head movements for conversational agents (CAs), the natural coupling between speech and head movements needs to be preserved, even when the CA uses synthetic speech. Relying on prerecorded speech for every sentence that a virtual agent utters constrains the versatility and scalability of the interface, so most practical solutions for CAs use text to speech, which affect speech-driven models. We proposed strategies to leverage speech-driven models for head motion generation for cases relying on synthetic speech. We prop...",
  "por_txt_cntn": "\nResearch Objective and Significance: \n\n This EArly-concept Grants for Exploratory Research (EAGER) explored the role of discourse and dialog contextual information in the generation of believable, human-like behaviors for conversational agent (CA), i.e., a virtual agent that interacts with a user. Spoken language carries important information beyond the acoustics and the verbal message that a CA engine should capitalize on, for example, discourse and dialog context information. The research goals are: (a) build two models in the speech-driven framework: one is based on acoustic only, the other uses rich discourse and dialog context; (b) evaluate the differences in the perceived naturalness, appropriateness and rapport of facial gestures generated with the two models above; (c) identify the discourse aspects that are important in the design of facial gestures, and whether they are domain specific.\n\n Research Project and Findings:\n\n 1- Speech-driven models constrained by discourse functions\n\n A key challenge for CA is how to create naturalistic behaviors that replicate the complex gestures observed during human interactions. Speech-driven approaches are especially appealing given the rich information conveyed on spoken language. The main limitation of these models is that they fail to capture the underlying semantic and discourse functions of the message (e.g., nodding). We proposed a speech-driven framework that explicitly model discourse functions, bridging the gap between speech-driven and rule-based models. The approach is based on dynamic Bayesian Network (DBN), where an additional node is introduced to constrain the models by specific discourse functions. We implemented the approach by synthesizing head and eyebrow motion. The statistical analysis demonstrated significant changes in head and eyebrow motion across functional classes (affirmation, negation, question, and statement). Perceptual evaluations for the functional class \"question\" showed that the animations with constrained models are perceived more preferable, natural and appropriate than the animations with the unconstrained models.\n\n 2- Speech driven models constrained by target gestures\n\n We created a flexible semi-supervised framework to retrieve arbitrary number of gestures in the data. Starting from few examples of the target gestures (e.g., head nod), the framework searches for similar gestures in the database. The framework replies on temporal reduction, multi scale windows, one-class Support Vector Machines (SVMs), and Dynamic Time Alignment Kernel (DTAK) to identify similar gestures regardless of the duration. Using the retrieved examples, we synthesized novel realizations of these target gestures using data-driven models. The retrieval system was very effective, reaching precision rate over 96% for head gestures (head nods and head shakes). The approach was also effective for hand gestures. We evaluated the performance of the speech-driven models to synthesize these target gestures. We synthesized 60 segments per gestures, constraining the model with the target gesture. We annotated the accuracy per gesture by watching the resulting animations. The generated head gestures matched the intended gesture 96% of the time.\n\n3- Speech driven models using synthetic speech\n\nTo have believable head movements for conversational agents (CAs), the natural coupling between speech and head movements needs to be preserved, even when the CA uses synthetic speech. Relying on prerecorded speech for every sentence that a virtual agent utters constrains the versatility and scalability of the interface, so most practical solutions for CAs use text to speech, which affect speech-driven models. We proposed strategies to leverage speech-driven models for head motion generation for cases relying on synthetic speech. We proposed to create a parallel corpus of synthetic speech aligned with natural recordings for which we have motion capture recordings. We used this parallel corpu..."
 }
}