{
 "awd_id": "1343104",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Approximating NP-Hard Problems -Efficient Algorithms and their Limits",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rahul Shah",
 "awd_eff_date": "2012-12-01",
 "awd_exp_date": "2016-12-31",
 "tot_intn_awd_amt": 394497.0,
 "awd_amount": 394497.0,
 "awd_min_amd_letter_date": "2013-06-03",
 "awd_max_amd_letter_date": "2015-09-04",
 "awd_abstract_narration": "The vast majority of planning or design tasks involves an optimization problem, seeking to either minimize the cost of the proposed solution, or maximize its efficiency or payoff.  Often, the goal would be the identification of the optimal solution from a set of finite many discrete options (combinatorial optimization).  Unfortunately, an exact solution for the overwhelming majority of optimization problems turns out to be computationally intractable.  To cope with intractability, one often settles for algorithms that provably approximate the optimal solution. The following question stems naturally from the notion of approximation: For a given combinatorial optimization problem, what is the best approximation to the optimal solution that can be efficiently computed?\r\n\r\nThere are two facets to answering the above question: designing approximation algorithms and showing that no efficient algorithm can provide a better approximation guarantee (hardness result). The convergence of these two seemingly different facets has been one of the most exciting developments in theoretical computer science in recent years. This project would involve the design of improved approximation algorithms as well as showing that these algorithms are essentially optimal. Although the design of approximation algorithms is a vast area of research, the main tool underlying an overwhelming majority of existing approximation algorithms is a convex optimization technique such as linear or semidefinite programming. Existing algorithmic techniques have hit upon a common barrier on a large number of fundamental combinatorial optimization problems, a barrier that is encapsulated by the tantalizing \"Unique Games Conjecture (UGC).\" Therefore the study of approximability is at a very exciting juncture. On one hand, an affirmation of the UGC would resolve long standing open questions , demonstrate an underlying unity in combinatorial optimization problems, and, more importantly, show that the simplest semidefinite programs yield the best approximations. On the other hand, disproving the UGC would lead to new algorithmic techniques that will eventually lead to better approximation algorithms.\r\n\r\nThe PI proposes a set of research questions involving both design of approximation algorithms and hardness of approximation results. Broadly speaking, the project has the following four research themes:\r\n\r\n1) Understand the power of semidefinite programming hierarchies via the design of new algorithms and constructions of integrality gap examples.\r\n\r\n2) Extend the emerging framework of approximability under the UGC to a larger class of combinatorial optimization problems.\r\n\r\n3) Develop technical machinery and gadgets to show unconditionally some of the hardness results based on the UGC, making progress towards its resolution.\r\n\r\n4) Apply the analytic tools developed in hardness of approximation to other branches of theoretical computer science, such as the study of exact algorithms for constraint satisfaction problems.\r\n\r\nThis research necessarily draws upon tools from various theoretical disciplines such as coding theory, property testing, computational learning, derandomization and discrete harmonic analysis. The research has a strong potential for broader impact in terms of scientific workshops, developement of graduate courses, lecture notes and survey articles on the latest research in approximation, promoting undergraduate research, and advising Ph.D students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Prasad",
   "pi_last_name": "Raghavendra",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Prasad Raghavendra",
   "pi_email_addr": "nrprasad@gmail.com",
   "nsf_id": "000567499",
   "pi_start_date": "2013-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "",
  "perf_city_name": "berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947091050",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 74497.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 80000.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 80000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 160000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div>Optimization, perhaps the most ubiquitous computational task of all, consists of finding a solution that either maximizes or minimizes a certain function while satisfying a set of constraints. &nbsp; If the space of solutions to the problem is finite and discrete, then such problems are referred to as \"combinatorial optimization\" problems. Unfortunately, most combinatorial optimization problems are computationally intractable to solve exactly, i.e., it is widely believed that finding the optimal solution would be prohibitively time-consuming. &nbsp;A classic approach to find approximate solutions that has been studied for more than four decades now, is the use of ``convex relaxations\".&nbsp; The idea of convex relaxation is to convert the underlying optimization task into a convex optimization task, since efficient algorithms are known for convex optimization. Converting an arbitrary combinatorial optimization problem in to a convex optimization task is inherently lossy, in that we only recover an approximate solution to the original problem in the process.&nbsp; The question then is, how good is the approximation? &nbsp;and which convex relaxation yield the best approximation?</div>\n<div>Among all convex relaxation techniques, perhaps the most powerful one is the so-called ``Sum-of-Squares semidefinite program (SoS SDP)\". &nbsp; The SoS SDP is a convex relaxation technique that can be shown to underlie or subsume a majority of the algorithms in combinatorial optimization. &nbsp;This is true to the extent that, if a natural SoS SDP relaxation for an optimization problem fails to yield a good approximation, then it is likely no known algorithm will. &nbsp;Yet, the power of SoS SDP relaxations is not very well understood, in that for many fundamental computational problems, we don't know what is the approximation obtained via SoS SDP relaxation.</div>\n<div>The work carried out under this proposal made important advances in our understanding of this mysterious yet powerful algorithmic technique. &nbsp;Specifically, the work obtained the following results:</div>\n<div>1) &nbsp;Unique Games conjecture which is among the most important open questions in complexity theory, is an unproven conjecture which implies that certain simple SDP relaxations yield the best possible approximation for large classes of combinatorial optimization problems. &nbsp;While the conjecture remains wide open, this work obtained constructed hard instances (so called gadgets) of unique games against certain SDP relaxations -- yielding the strongest known evidence for the conjecture's truth. &nbsp;The \"gadgets\" so constructed also yielded the best known results on the intractability of certain basic combinatorial optimization problems like graph coloring.</div>\n<div>2) The PI and his coauthors exactly characterized the power of SoS SDPs against random constraint satisfaction problems. &nbsp;Constraint satisfaction problems (CSP) are one of the ubiquitous and basic computational problems, that capture most settings where there are a set of discrete variables and a set of discrete constraints on them. &nbsp;Random CSPs have been widely studied in theoretical computer science, statistical physics and probability theory. &nbsp; Using techniques from the theory of random matrices, we exactly characterize the power of SoS SDPs in this context. &nbsp;On the one hand, this result yields algorithms for random CSPs, and on the other hand, the result can be viewed as evidence for the computational intractability for certain random CSPs -- a widely held but unproven belief in computer science.</div>\n<div>3) &nbsp;Given a graph (say a social network), detecting subsets of the network that are isolated, i.e., not connected strongly with the rest of the network, is a basic computational problem known as sparsest cut. &nbsp; &nbsp;Representing the network as a matrix, and using the eigenvectors of the matrix to find a sparse cut, is a classic algorithm that is widely applied in practice, in areas such as image processing. &nbsp;This approach of using eigenvectors is grounded in a well-known result in spectral graph theory from 1980s, known as the Cheeger inequality. &nbsp;PI and coauthors obtained generalizations of this classic result, &nbsp;that gave a formal grounding behind the widely used algorithmic heuristic of using higher order eigenvectors in graph decomposition algorithms.</div>\n<p>&nbsp;</p>\n<p>&nbsp;PI and coauthors also proved the computational intractability of certain basic problems like approximating vertex expansion and matrix completion. &nbsp;These intractability results nicely complement existing algorithms for these problems, by demarcating the border beyond which the problems become computationally intractable.</p>\n<p>&nbsp;</p>\n<div>Broader Impacts:</div>\n<p>&nbsp;</p>\n<p>In the course of the project, the PI directly advised five graduate students many of whom either directly or indirectly contributed to it. &nbsp;</p>\n<p>The PI along with James Lee was one of the lead-organizers of a semester long program on Spectral Graph Theory at Simons Institute. &nbsp;This program included three workshops and a an expository boot camp on the topic. &nbsp;More than 100 researchers participated in the program, up to 30 of whom were graduate students or postdoctoral researchers. &nbsp;</p>\n<p>The PI created and taught graduate classes on Hardness of Approximation and Theoretical Computer Science's Greatest Hits. &nbsp;The PI has given several expository lectures at numerous venues during the course of the project.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/12/2017<br>\n\t\t\t\t\tModified by: Prasad&nbsp;Raghavendra</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nOptimization, perhaps the most ubiquitous computational task of all, consists of finding a solution that either maximizes or minimizes a certain function while satisfying a set of constraints.   If the space of solutions to the problem is finite and discrete, then such problems are referred to as \"combinatorial optimization\" problems. Unfortunately, most combinatorial optimization problems are computationally intractable to solve exactly, i.e., it is widely believed that finding the optimal solution would be prohibitively time-consuming.  A classic approach to find approximate solutions that has been studied for more than four decades now, is the use of ``convex relaxations\".  The idea of convex relaxation is to convert the underlying optimization task into a convex optimization task, since efficient algorithms are known for convex optimization. Converting an arbitrary combinatorial optimization problem in to a convex optimization task is inherently lossy, in that we only recover an approximate solution to the original problem in the process.  The question then is, how good is the approximation?  and which convex relaxation yield the best approximation?\nAmong all convex relaxation techniques, perhaps the most powerful one is the so-called ``Sum-of-Squares semidefinite program (SoS SDP)\".   The SoS SDP is a convex relaxation technique that can be shown to underlie or subsume a majority of the algorithms in combinatorial optimization.  This is true to the extent that, if a natural SoS SDP relaxation for an optimization problem fails to yield a good approximation, then it is likely no known algorithm will.  Yet, the power of SoS SDP relaxations is not very well understood, in that for many fundamental computational problems, we don't know what is the approximation obtained via SoS SDP relaxation.\nThe work carried out under this proposal made important advances in our understanding of this mysterious yet powerful algorithmic technique.  Specifically, the work obtained the following results:\n1)  Unique Games conjecture which is among the most important open questions in complexity theory, is an unproven conjecture which implies that certain simple SDP relaxations yield the best possible approximation for large classes of combinatorial optimization problems.  While the conjecture remains wide open, this work obtained constructed hard instances (so called gadgets) of unique games against certain SDP relaxations -- yielding the strongest known evidence for the conjecture's truth.  The \"gadgets\" so constructed also yielded the best known results on the intractability of certain basic combinatorial optimization problems like graph coloring.\n2) The PI and his coauthors exactly characterized the power of SoS SDPs against random constraint satisfaction problems.  Constraint satisfaction problems (CSP) are one of the ubiquitous and basic computational problems, that capture most settings where there are a set of discrete variables and a set of discrete constraints on them.  Random CSPs have been widely studied in theoretical computer science, statistical physics and probability theory.   Using techniques from the theory of random matrices, we exactly characterize the power of SoS SDPs in this context.  On the one hand, this result yields algorithms for random CSPs, and on the other hand, the result can be viewed as evidence for the computational intractability for certain random CSPs -- a widely held but unproven belief in computer science.\n3)  Given a graph (say a social network), detecting subsets of the network that are isolated, i.e., not connected strongly with the rest of the network, is a basic computational problem known as sparsest cut.    Representing the network as a matrix, and using the eigenvectors of the matrix to find a sparse cut, is a classic algorithm that is widely applied in practice, in areas such as image processing.  This approach of using eigenvectors is grounded in a well-known result in spectral graph theory from 1980s, known as the Cheeger inequality.  PI and coauthors obtained generalizations of this classic result,  that gave a formal grounding behind the widely used algorithmic heuristic of using higher order eigenvectors in graph decomposition algorithms.\n\n \n\n PI and coauthors also proved the computational intractability of certain basic problems like approximating vertex expansion and matrix completion.  These intractability results nicely complement existing algorithms for these problems, by demarcating the border beyond which the problems become computationally intractable.\n\n \nBroader Impacts:\n\n \n\nIn the course of the project, the PI directly advised five graduate students many of whom either directly or indirectly contributed to it.  \n\nThe PI along with James Lee was one of the lead-organizers of a semester long program on Spectral Graph Theory at Simons Institute.  This program included three workshops and a an expository boot camp on the topic.  More than 100 researchers participated in the program, up to 30 of whom were graduate students or postdoctoral researchers.  \n\nThe PI created and taught graduate classes on Hardness of Approximation and Theoretical Computer Science's Greatest Hits.  The PI has given several expository lectures at numerous venues during the course of the project.\n\n \n\n\t\t\t\t\tLast Modified: 05/12/2017\n\n\t\t\t\t\tSubmitted by: Prasad Raghavendra"
 }
}