{
 "awd_id": "1330072",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Breakthrough: Reinforcement Learning Algorithms for Cyber-Physical Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "David Corman",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2017-09-30",
 "tot_intn_awd_amt": 499760.0,
 "awd_amount": 499760.0,
 "awd_min_amd_letter_date": "2013-09-09",
 "awd_max_amd_letter_date": "2013-09-09",
 "awd_abstract_narration": "This project investigates new reinforcement learning algorithms to enable long-term real-time autonomous learning by cyber-physical systems (CPS). The complexity of CPS makes hand-programming safe and efficient controllers for them difficult. For CPS to meet their potential, they need methods that enable them to learn and adapt to novel situations that they were not programmed for. Reinforcement learning (RL) is a paradigm for learning sequential decision making processes with potential for solving this problem. However, existing RL algorithms do not meet all of the requirements of learning in CPS.  Efficacy of the new algorithms for CPS is evaluated in the context of smart buildings and autonomous vehicles.\r\n\r\nCyber-physical systems (CPS) have the potential to revolutionize society by enabling smart buildings, transportation, medical technology, and electric grids. Success of this project could lead to a new generation of CPS that are capable of adapting to their situation and improving their performance autonomously over time. In addition to the traditional methods of dissemination, this project will develop and release open-source code implementing the new reinforcement learning algorithms.  Education and outreach activities associated with the project include a Freshman Research Initiative course, participation in  a UT Austin annual open house that draws in many underrepresented minorities to interest the public in computer science and science in general, and the department's annual summer school for high school girls called First Bytes.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Stone",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Peter H Stone",
   "pi_email_addr": "pstone@cs.utexas.edu",
   "nsf_id": "000156504",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "101 E. 27th Street, Suite 5.300",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121532",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 499760.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Reinforcement learning (RL) is a paradigm for learning sequential <br />decision making processes and could solve the problems of learning and <br />adaptation for CPS.&nbsp; In this research, we identifie seven key <br />properties of an RL algorithm that would make it generally applicable <br />to a broad range of CPS's: <br />&nbsp; <br />1. The algorithm must learn in very few samples (which may be <br />expensive or time-consuming). <br />&nbsp; <br />2. It must learn good policies even with unknown sensor or actuator <br />delays (i.e. selecting an action may not affect the environment <br />instantaneously). <br />&nbsp; <br />3. It must learn tasks with continuous state representations. <br />&nbsp; <br />4. It must handle multi-dimensional continuous actions. <br />&nbsp; <br />5. It must be computationally efficient enough to select actions <br />continually in real time. <br />&nbsp; <br />6. It must reason about and take advantage of the hierarchical <br />structure present in many tasks. <br />&nbsp; <br />7. It must reason about the agents who may be using it as well as <br />other CPS's that it must work with. <br />&nbsp; <br />Although prior RL algorithms successfully address one or more of these <br />challenges, no existing algorithm addressed all seven of them. In this <br />project, we helped to bridge this gap by developin novel reinforcement <br />learning algorithms that enable long-term real-time autonomous <br />learning by a broad range of cyber-physical systems.</p>\n<p>Specifically, we developed these new RL algorithms around an existing <br />algorithm, called TEXPLORE, which is sample-efficient, handles <br />continuous state and actuator delays, and acts in real-time <br />(Properties 1, 2, 3 and 5). From this algorithm, we achieved theh <br />project goals by successfully executing the following four steps in <br />order to develop new RL algorithms for autonomous learning on CPS's. <br />&nbsp; <br />1. We Extended TEXPLORE to handle multi-dimensional continuous actions. <br />&nbsp; <br />2. We Extended TEXPLORE to discover and reason about hierarchies and task <br />decomposition. <br />&nbsp; <br />3. We Incorporated explicit reasoning about other agents such as other <br />users and systems that the CPS must deal with. <br />&nbsp; <br />4. We implemented and tested the algorithms on multiple domains. <br />&nbsp; <br />The research addressed two of the CPS Research Target Areas: the <br />Science of CPS's and Technology for CPS's. RL algorithms present a new <br />way to model and control CPS's that bridge the gap between computation <br />and control, while enabling CPS's to learn, inter-operate, and handle <br />uncertainty.&nbsp;&nbsp; <br />&nbsp;<br /><br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/05/2017<br>\n\t\t\t\t\tModified by: Peter&nbsp;H&nbsp;Stone</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nReinforcement learning (RL) is a paradigm for learning sequential \ndecision making processes and could solve the problems of learning and \nadaptation for CPS.  In this research, we identifie seven key \nproperties of an RL algorithm that would make it generally applicable \nto a broad range of CPS's: \n  \n1. The algorithm must learn in very few samples (which may be \nexpensive or time-consuming). \n  \n2. It must learn good policies even with unknown sensor or actuator \ndelays (i.e. selecting an action may not affect the environment \ninstantaneously). \n  \n3. It must learn tasks with continuous state representations. \n  \n4. It must handle multi-dimensional continuous actions. \n  \n5. It must be computationally efficient enough to select actions \ncontinually in real time. \n  \n6. It must reason about and take advantage of the hierarchical \nstructure present in many tasks. \n  \n7. It must reason about the agents who may be using it as well as \nother CPS's that it must work with. \n  \nAlthough prior RL algorithms successfully address one or more of these \nchallenges, no existing algorithm addressed all seven of them. In this \nproject, we helped to bridge this gap by developin novel reinforcement \nlearning algorithms that enable long-term real-time autonomous \nlearning by a broad range of cyber-physical systems.\n\nSpecifically, we developed these new RL algorithms around an existing \nalgorithm, called TEXPLORE, which is sample-efficient, handles \ncontinuous state and actuator delays, and acts in real-time \n(Properties 1, 2, 3 and 5). From this algorithm, we achieved theh \nproject goals by successfully executing the following four steps in \norder to develop new RL algorithms for autonomous learning on CPS's. \n  \n1. We Extended TEXPLORE to handle multi-dimensional continuous actions. \n  \n2. We Extended TEXPLORE to discover and reason about hierarchies and task \ndecomposition. \n  \n3. We Incorporated explicit reasoning about other agents such as other \nusers and systems that the CPS must deal with. \n  \n4. We implemented and tested the algorithms on multiple domains. \n  \nThe research addressed two of the CPS Research Target Areas: the \nScience of CPS's and Technology for CPS's. RL algorithms present a new \nway to model and control CPS's that bridge the gap between computation \nand control, while enabling CPS's to learn, inter-operate, and handle \nuncertainty.   \n \n\n\n\n\n\t\t\t\t\tLast Modified: 11/05/2017\n\n\t\t\t\t\tSubmitted by: Peter H Stone"
 }
}