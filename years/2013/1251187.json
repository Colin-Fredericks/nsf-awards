{
 "awd_id": "1251187",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Small: DA: Collaborative Research: Real Time Observation Analysis for Healthcare Applications via Automatic Adaptation to Hardware Limitations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2013-07-01",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 459901.0,
 "awd_amount": 459901.0,
 "awd_min_amd_letter_date": "2013-06-25",
 "awd_max_amd_letter_date": "2013-06-25",
 "awd_abstract_narration": "This research seeks to develop novel machine learning algorithms that enable real-time video and sensor data analysis on large data streams given limited computational resources. The work focuses on healthcare as an application domain where real-time video analysis can prevent user-errors in operating medical devices or provide immediate alerts to caregivers about dangerous situations.  The research will develop algorithms to automatically adapt data analysis approaches to maximize accuracy of analysis within a short time period despite limited available computing resources. Today's healthcare environment is significantly more technologically sophisticated than ever before. Many medical devices are now frequently used in patient's homes, ranging from simple equipment such as canes and wheelchairs to sophisticated items such as glucose meters, ambulatory infusion pumps and laptop-sized ventilators. The rapidly growing home health industry raises new safety concerns about devices being used inappropriately in the home setting. The proposed research is designed to reduce medical device related use-errors by developing computational algorithms that perform real-time video analysis and alert the patient or caregiver when medical devices are not used appropriately. The real-time video and sensor data analysis is also critical to the healthcare systems that monitor the activities of the elderly or those with disabilities in order to allow a caregiver to react immediately to an incident. \r\n\r\nNew machine learning theories and algorithms will automatically adapt to hardware limitations, with the aim to learn from a large number of training examples, a prediction function that (i) is sufficiently accurate in making effective predictions and (ii) can be run efficiently on a specified computer system to deliver time critical results. Three types of prediction models are studied to address the problem of automatic hardware adaptation, including a vector-based model, a matrix-based model, and a prediction model based on a function from a Reproducing Kernel Hilbert Space (RKHS).  A general framework and multiple optimization techniques are being developed to learn accurate prediction models that match limited memory and computational capacity. The new learning algorithms will be evaluated in several medical scenarios through real-time prediction of a patient's activities from observations in the large video archives collected by several healthcare related projects.  The intellectual merit of the proposed work is in bridging the gap between the high complexity of a prediction model and limited computational resources, a scenario that is encountered in many application domains besides healthcare. The proposed research in machine learning algorithms and theories will make it possible to run complicated prediction algorithms on big data within the limitation of a given computing infrastructure. The developed techniques for automatic hardware adaptation will be applied to a large dataset of continuous video and sensor recordings for medically-critical activity recognition.  The project's broader impacts include providing medical experts with algorithms and tools supporting novel approaches to analyzing observational data in their quest to recognize and characterize human behavior. Surveillance systems with continuous observations will be able to categorize salient events with co-located, limited hardware. Researchers with complex data from continuous streams will be able to explore their domains with greater accuracy within constrained time using their available computing resources. Similarly, large archives can be exploited as rapidly as possible with limited hardware.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Hauptmann",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander G Hauptmann",
   "pi_email_addr": "alex@cs.cmu.edu",
   "nsf_id": "000228336",
   "pi_start_date": "2013-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 459901.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Nine papers are published for event analysis. We addressed cultural event understanding by combining object / scene contents mining and Convolutional Neural Networks (CNN). In our final submission for ChaLearn LAP Challenge ICCV 2015, our method achieves the best performance among all the participants.</p>\n<p>We worked on complex event detection with different focus. We first researched the difficult zero-shot setting where no training data is supplied. We evaluate the semantic correlation of each concept w.r.t. the event of interest. After further refinement, we applied the discovered concept classifiers on all test videos and obtained multiple score vectors. These distinct score vectors were converted into pairwise comparison matrices and the nuclear norm rank aggregation framework was adopted to seek consensus. In a following work, we evaluate the semantic correlation of each concept w.r.t. the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. We further learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. Other than zero-shot event detection, we studied how to optimally learn the information from negative examples by assigning the fine-grained labels to negative examples to treat them differently for more effective exploitation.</p>\n<p>Relevant to complex event detection, we worked on how to conduct efficient video search by automatically learning detectors from the big video data on the web without any additional manual annotations. A series of algorithms have been developed. We first propose a novel method called WEbly-Labeled Learning (WELL) established on curriculum learning and self-paced learning. We then extend our WELL algorithm by integrating multi-modal information including deep learning visual, audio and speech features, to automatically learn accurate video detectors based on the user query. Furthermore, the multi-modal WELL algorithm is enhanced by incorporating meaningful prior knowledge called curriculum from the noisy web videos.&nbsp;</p>\n<p>We also worked on an interesting problem of image profiling for history events. First, we proposed to automatically construct an image profile given a one-sentence description of the historic event. We developed a full-range feature analysis module composed of several levels, each suitable for different types of image analysis tasks. Second, we add explicit semantic information to image profiling by linking images in the profile with related phrases in the event description.</p>\n<p>For noisy face searching results, we developed a novel method that is able to prune the data in an iterative way, for the models associated to a name to evolve. The idea is based on capturing discriminative and representative properties of each instance and eliminating the outliers.&nbsp;</p>\n<p>For high-dimensional data processing, we addressed the sparse recovery problem in which the feature matrix is strictly non-Restricted Isometric Property (R.I.P.). We were able to recover the sparse vector consistently when features exhibit cluster structures. The consistency comes from our proposed density correction algorithm, which removes the variance of estimated cluster centers using cluster density. On the other hand, we equivalently reformulated the maximization of variances for robust PCA, such that the optimal projection directions are learned by maximizing the sum of the projected difference between each pair of instances. Additionally, we extended the proposed robust PCA to its 2D version for image recognition. Efficient non-greedy algorithms were exploited to solve the proposed robust PCA and 2D robust PCA with fast convergence and low computational complexity.&nbsp;</p>\n<p>Three papers are published on action recognition. On one hand, several novel feature presentations are developed. In one work we introduce a new descriptor HMG (Histograms of Motion Gradients) that captures motion information without the need of computing optical flow, which obtains very competitive results while achieving a low computational complexity. In a following work, we also focus on descriptor encoding. We develop a new encoding technique that extends from VLAD which captures information regarding the distribution shape of the descriptors, providing the best trade-off between computational cost and accuracy. The third work in feature design is optimizing the traditional local feature pipeline by combining the merits of both handcrafted and Convolutional Neural Network approaches. On the other hand, we propose to exploit feature interaction to capture more information for 3D action recognition. The feature interaction is incorporated into a linear regression model, making the resulted linear classifier efficient for prediction.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/16/2017<br>\n\t\t\t\t\tModified by: Alexander&nbsp;G&nbsp;Hauptmann</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNine papers are published for event analysis. We addressed cultural event understanding by combining object / scene contents mining and Convolutional Neural Networks (CNN). In our final submission for ChaLearn LAP Challenge ICCV 2015, our method achieves the best performance among all the participants.\n\nWe worked on complex event detection with different focus. We first researched the difficult zero-shot setting where no training data is supplied. We evaluate the semantic correlation of each concept w.r.t. the event of interest. After further refinement, we applied the discovered concept classifiers on all test videos and obtained multiple score vectors. These distinct score vectors were converted into pairwise comparison matrices and the nuclear norm rank aggregation framework was adopted to seek consensus. In a following work, we evaluate the semantic correlation of each concept w.r.t. the event of interest and pick up the relevant concept classifiers, which are applied on all test videos to get multiple prediction score vectors. We further learn the optimal weights of the concept classifiers for each testing video by exploring a set of online available videos with free-form text descriptions of their content. Other than zero-shot event detection, we studied how to optimally learn the information from negative examples by assigning the fine-grained labels to negative examples to treat them differently for more effective exploitation.\n\nRelevant to complex event detection, we worked on how to conduct efficient video search by automatically learning detectors from the big video data on the web without any additional manual annotations. A series of algorithms have been developed. We first propose a novel method called WEbly-Labeled Learning (WELL) established on curriculum learning and self-paced learning. We then extend our WELL algorithm by integrating multi-modal information including deep learning visual, audio and speech features, to automatically learn accurate video detectors based on the user query. Furthermore, the multi-modal WELL algorithm is enhanced by incorporating meaningful prior knowledge called curriculum from the noisy web videos. \n\nWe also worked on an interesting problem of image profiling for history events. First, we proposed to automatically construct an image profile given a one-sentence description of the historic event. We developed a full-range feature analysis module composed of several levels, each suitable for different types of image analysis tasks. Second, we add explicit semantic information to image profiling by linking images in the profile with related phrases in the event description.\n\nFor noisy face searching results, we developed a novel method that is able to prune the data in an iterative way, for the models associated to a name to evolve. The idea is based on capturing discriminative and representative properties of each instance and eliminating the outliers. \n\nFor high-dimensional data processing, we addressed the sparse recovery problem in which the feature matrix is strictly non-Restricted Isometric Property (R.I.P.). We were able to recover the sparse vector consistently when features exhibit cluster structures. The consistency comes from our proposed density correction algorithm, which removes the variance of estimated cluster centers using cluster density. On the other hand, we equivalently reformulated the maximization of variances for robust PCA, such that the optimal projection directions are learned by maximizing the sum of the projected difference between each pair of instances. Additionally, we extended the proposed robust PCA to its 2D version for image recognition. Efficient non-greedy algorithms were exploited to solve the proposed robust PCA and 2D robust PCA with fast convergence and low computational complexity. \n\nThree papers are published on action recognition. On one hand, several novel feature presentations are developed. In one work we introduce a new descriptor HMG (Histograms of Motion Gradients) that captures motion information without the need of computing optical flow, which obtains very competitive results while achieving a low computational complexity. In a following work, we also focus on descriptor encoding. We develop a new encoding technique that extends from VLAD which captures information regarding the distribution shape of the descriptors, providing the best trade-off between computational cost and accuracy. The third work in feature design is optimizing the traditional local feature pipeline by combining the merits of both handcrafted and Convolutional Neural Network approaches. On the other hand, we propose to exploit feature interaction to capture more information for 3D action recognition. The feature interaction is incorporated into a linear regression model, making the resulted linear classifier efficient for prediction. \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 08/16/2017\n\n\t\t\t\t\tSubmitted by: Alexander G Hauptmann"
 }
}