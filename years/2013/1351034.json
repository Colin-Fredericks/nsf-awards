{
 "awd_id": "1351034",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: ATAROS: Automatic Tagging and Recognition of Stance",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 249975.0,
 "awd_amount": 257836.0,
 "awd_min_amd_letter_date": "2013-09-09",
 "awd_max_amd_letter_date": "2014-05-14",
 "awd_abstract_narration": "From activities as simple as scheduling a meeting to those as complex as balancing a national budget, people take stances in negotiations and decision making.  While the related areas of subjectivity and sentiment analysis have received significant attention, work has focused almost exclusively on text, whereas much stance-taking activity is carried out verbally.  Early experiments suggest that people alter their speaking style when engaged in stance-taking, and listeners can much more readily detect negative attitudes by listening to the original speech than by reading transcripts.  However, due to the diversity of factors that influence speech production, from individual differences to social context, isolating the signals of stance-taking in speech for automatic recognition presents substantial challenges.\r\n\r\nThis Early Grant for Exploratory Research project represents a focused exploration of spoken interactions to provide a characterization of linguistic factors associated with stance-taking and develop computational methods that exploit these features to automatically detect stance-taking behavior.  Robust linguistic markers of stance-taking are identified through analysis of both controlled elicitations and archived recordings of Congressional hearings on the financial crisis.  The former allow experimental comparisons to highlight sometimes subtle contrasts, while the latter enable validation and extension of those findings in real-world, high-stakes discussions.  The analysis includes novel acoustic-phonetic measures of dynamic patterns in speech, such as vowel space scaling and pitch/energy velocity, with sophisticated visualization techniques developed to support feature exploration.  Findings are validated via stance recognition experiments combining acoustic and lexical cues, which lay the foundation for automatic tracking of trends and shifts in attitudes.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gina-Anne",
   "pi_last_name": "Levow",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gina-Anne Levow",
   "pi_email_addr": "levow@uw.edu",
   "nsf_id": "000573802",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mari",
   "pi_last_name": "Ostendorf",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mari Ostendorf",
   "pi_email_addr": "ostendor@uw.edu",
   "nsf_id": "000109813",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Wright",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Richard A Wright",
   "pi_email_addr": "rawright@uw.edu",
   "nsf_id": "000480447",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981954340",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 249975.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 7861.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>From activities as simple as scheduling a meeting to those as complex as balancing a national budget, people take stances (express opinions) in negotiations and decision-making. While there has been substantial work on identifying sentiment in online reviews or subjectivity in printed materials such as news articles, little work has addressed how stance is expressed in spontaneous conversations. There are many differences between written and spoken communication. For example, intonation and shades of pronunciation provide a rich channel of communication. Words can take on different roles and even opposite meanings depending on when and how they are said, e.g. &lsquo;yeah&rsquo; can communicate enthusiastic agreement, skeptism, or simply &ldquo;I&rsquo;m listening.&rdquo; Understanding how stance taking is expressed in speech is important for improving systems for human-computer interaction, as well as for data mining of audio archives of speeches, hearings and debates.</p>\n<p>&nbsp;</p>\n<p>&nbsp;A main reason for the lack of research on spontaneous speech expressions of stance is the lack of availability of a corpus of speech collected to control for stance taking. To identify the lexical and acoustic markers of stance, the project created a corpus consisting of both controlled elicitations of stance-dense task-oriented spontaneous conversations and more naturalistic archived recordings of Congressional Hearings on the 2008 financial crisis. To enable the creation of the corpus, the project developed a new methodology to elicit different rates of stance-taking at different strengths. It also devised a new labeling system that captured when a stance was being taken, how strong the stance was, its polarity (positive, negative, neutral), and what kind of a speech-act it was (such as encouragement, opinion expression, or capitulation), while at the same time being abstract enough for high reliability, application to a range of controlled and natural conversational interactions, and machine learning. The corpus elicitation and stance annotation approaches were carefully validated through contrastive task analysis and evaluation of inter-annotator agreement.</p>\n<p>&nbsp;</p>\n<p>With the collected corpus, the project investigated acoustic and lexical indicators of stance taking in the speech signal, and implemented algorithms for automatically detecting stance taking. Detailed acoustic analyses highlighted a range of strategies that speakers employed to convey different stance strength and polarity, as well as more fine-grained intentions such as rapport-building, praise, reluctance, and skepticism. The results of the acoustic analysis indicate that changes in pitch and intensity are associated with increases in stance strength. Furthermore, novel measures of intonation and intensity dynamics provide finer discrimination among stance strength levels and among the various intentions expressed in stance taking. For example, different stance-related uses of the discourse marker 'yeah' were shown to differ significantly in duration, intonation and intensity. Findings on intonation in particular translate well to the naturally occurring interactions in the hearings data. This project also presents the first results demonstrating automatic recognition of stance in conversational speech, achieving accuracies of over 80% on stance detection and stance polarity recognition. The curated corpus has been made publicly available and is already being used to support further research studies as well as for course materials in computational linguistics. &nbsp;&nbsp;This work lays the foundation for the development of sophisticated techniques to track changes and trends in attitudes expressed in conversation.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/03/2017<br>\n\t\t\t\t\tModified by: Gina-Anne&nbsp;Levow</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nFrom activities as simple as scheduling a meeting to those as complex as balancing a national budget, people take stances (express opinions) in negotiations and decision-making. While there has been substantial work on identifying sentiment in online reviews or subjectivity in printed materials such as news articles, little work has addressed how stance is expressed in spontaneous conversations. There are many differences between written and spoken communication. For example, intonation and shades of pronunciation provide a rich channel of communication. Words can take on different roles and even opposite meanings depending on when and how they are said, e.g. ?yeah? can communicate enthusiastic agreement, skeptism, or simply \"I?m listening.\" Understanding how stance taking is expressed in speech is important for improving systems for human-computer interaction, as well as for data mining of audio archives of speeches, hearings and debates.\n\n \n\n A main reason for the lack of research on spontaneous speech expressions of stance is the lack of availability of a corpus of speech collected to control for stance taking. To identify the lexical and acoustic markers of stance, the project created a corpus consisting of both controlled elicitations of stance-dense task-oriented spontaneous conversations and more naturalistic archived recordings of Congressional Hearings on the 2008 financial crisis. To enable the creation of the corpus, the project developed a new methodology to elicit different rates of stance-taking at different strengths. It also devised a new labeling system that captured when a stance was being taken, how strong the stance was, its polarity (positive, negative, neutral), and what kind of a speech-act it was (such as encouragement, opinion expression, or capitulation), while at the same time being abstract enough for high reliability, application to a range of controlled and natural conversational interactions, and machine learning. The corpus elicitation and stance annotation approaches were carefully validated through contrastive task analysis and evaluation of inter-annotator agreement.\n\n \n\nWith the collected corpus, the project investigated acoustic and lexical indicators of stance taking in the speech signal, and implemented algorithms for automatically detecting stance taking. Detailed acoustic analyses highlighted a range of strategies that speakers employed to convey different stance strength and polarity, as well as more fine-grained intentions such as rapport-building, praise, reluctance, and skepticism. The results of the acoustic analysis indicate that changes in pitch and intensity are associated with increases in stance strength. Furthermore, novel measures of intonation and intensity dynamics provide finer discrimination among stance strength levels and among the various intentions expressed in stance taking. For example, different stance-related uses of the discourse marker 'yeah' were shown to differ significantly in duration, intonation and intensity. Findings on intonation in particular translate well to the naturally occurring interactions in the hearings data. This project also presents the first results demonstrating automatic recognition of stance in conversational speech, achieving accuracies of over 80% on stance detection and stance polarity recognition. The curated corpus has been made publicly available and is already being used to support further research studies as well as for course materials in computational linguistics.   This work lays the foundation for the development of sophisticated techniques to track changes and trends in attitudes expressed in conversation.\n\n\t\t\t\t\tLast Modified: 02/03/2017\n\n\t\t\t\t\tSubmitted by: Gina-Anne Levow"
 }
}