{
 "awd_id": "1302435",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: New Approaches to Robustness in High-Dimensions",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2013-07-01",
 "awd_exp_date": "2019-06-30",
 "tot_intn_awd_amt": 695369.0,
 "awd_amount": 695369.0,
 "awd_min_amd_letter_date": "2013-03-19",
 "awd_max_amd_letter_date": "2015-08-27",
 "awd_abstract_narration": "Rapid development of large-scale data collection technology has\r\nignited research into high-dimensional machine learning.  For\r\ninstance, the problem of designing recommender systems, such as those\r\nused by Amazon, Netflix and other on-line companies, involves\r\nanalyzing large matrices that describe users' behavior in past\r\nsituations.  In sociology, researchers are interested in fitting\r\nnetworks to large-scale data sets, involving hundreds or thousands of\r\nindividuals.  In medical imaging, the goal is to reconstruct\r\ncomplicated phenomena (e.g., brain images; videos of a beating heart)\r\nbased on a minimal number of incomplete and possibly corrupted\r\nmeasurements.  Motivated by such applications, the goal of this\r\nresearch is to develop and analyze models and algorithms for\r\nextracting relevant structure from such high-dimensional data sets in\r\na robust and scalable fashion.\r\n\r\n\r\nThe research leverages tools from convex optimization, signal\r\nprocessing, and robust statistics.  It consists of three main thrusts:\r\n(1) Model restrictiveness: Successful methods for high-dimensional\r\ndata exploit low-dimensional structure; however, many real-world\r\nproblems fall outside the scope of existing models.  This proposal\r\nsignificantly extends the basic set-up by allowing for multiple\r\nstructures, leading to computationally efficient algorithms while\r\neliminating negative effects of model mismatch.  (2) Non-ideal data:\r\nMissing data are prevalent in real-world problems, and can cause major\r\nbreakdowns in standard algorithms for high-dimensional data. The\r\nsecond thrust devises relaxations and greedy approaches for these\r\nnon-convex problems.  (3) Arbitrary Outliers: Gross errors can arise\r\nfor various reasons, including fault-prone sensors and manipulative\r\nagents.  The third thrust proposes efficient and randomized algorithms\r\nto address arbitrary outliers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sujay",
   "pi_last_name": "Sanghavi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sujay Sanghavi",
   "pi_email_addr": "sanghavi@mail.utexas.edu",
   "nsf_id": "000535791",
   "pi_start_date": "2013-03-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Constantine",
   "pi_last_name": "Caramanis",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Constantine Caramanis",
   "pi_email_addr": "constantine@utexas.edu",
   "nsf_id": "000102556",
   "pi_start_date": "2013-03-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "1 University Station",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787120803",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "TX",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "793600",
   "pgm_ele_name": "SIGNAL PROCESSING"
  },
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 316716.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 378653.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Increasingly, machine learning and data science are being used in security-critical applications &ndash; applications that are intrinsically linked with our financial system, or in safety critical applications such as autonomous driving, to name a few examples. Insuring the reliability of the solutions, predictions and recommendations of machine learning algorithms is of paramount importance.</p>\n<p>&nbsp;</p>\n<p>At the same time, it is well documented that machine learning-based algorithms &ndash; even those that enjoy state-of-the-art accuracy and performance on unperturbed data &ndash; are remarkably fragile to perturbations in training and testing data.</p>\n<p>&nbsp;</p>\n<p>The work funded by this grant has been focused on developing new analysis for understanding these threats, as well as computationally efficient, highly scalable algorithms, that are <em>robust</em> to such perturbations or attacks on the data. Specifically, the focus has been on modern machine learning problems in what is known as the <em>high dimensional statistical regime</em>. This is the setting where the dimensionality and complexity of the data are vast, and in fact are often have higher dimensionality &ndash; and hence degrees of freedom &ndash; than the number of data points available. This is particularly relevant as our ability to sense and also store data increases, and hence we collect and store increasingly high resolution and complex data.</p>\n<p>&nbsp;</p>\n<p>The results obtained have developed robustness for many fundamental algorithms that form the underpinning of many of the key statistical routines used in diverse areas of science and technology. This includes regression and principal component analysis. Our results include state of the art algorithms boasting the best robustness and also fastest computation times, for problems including principal component analysis with deleted data (erasures) and sparse corruption. Our algorithms have also proved ground breaking for mixture problems. Mixture problems are important as they are a key technique for building more complicated and accurate statistical models from simple building blocks.</p>\n<p>&nbsp;</p>\n<p>Finally, the funded work has also made pioneering advances in robustness for neural networks, providing simple algorithms that make neural network training robust, but at little or no additional computational expense.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/20/2020<br>\n\t\t\t\t\tModified by: Constantine&nbsp;Caramanis</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIncreasingly, machine learning and data science are being used in security-critical applications &ndash; applications that are intrinsically linked with our financial system, or in safety critical applications such as autonomous driving, to name a few examples. Insuring the reliability of the solutions, predictions and recommendations of machine learning algorithms is of paramount importance.\n\n \n\nAt the same time, it is well documented that machine learning-based algorithms &ndash; even those that enjoy state-of-the-art accuracy and performance on unperturbed data &ndash; are remarkably fragile to perturbations in training and testing data.\n\n \n\nThe work funded by this grant has been focused on developing new analysis for understanding these threats, as well as computationally efficient, highly scalable algorithms, that are robust to such perturbations or attacks on the data. Specifically, the focus has been on modern machine learning problems in what is known as the high dimensional statistical regime. This is the setting where the dimensionality and complexity of the data are vast, and in fact are often have higher dimensionality &ndash; and hence degrees of freedom &ndash; than the number of data points available. This is particularly relevant as our ability to sense and also store data increases, and hence we collect and store increasingly high resolution and complex data.\n\n \n\nThe results obtained have developed robustness for many fundamental algorithms that form the underpinning of many of the key statistical routines used in diverse areas of science and technology. This includes regression and principal component analysis. Our results include state of the art algorithms boasting the best robustness and also fastest computation times, for problems including principal component analysis with deleted data (erasures) and sparse corruption. Our algorithms have also proved ground breaking for mixture problems. Mixture problems are important as they are a key technique for building more complicated and accurate statistical models from simple building blocks.\n\n \n\nFinally, the funded work has also made pioneering advances in robustness for neural networks, providing simple algorithms that make neural network training robust, but at little or no additional computational expense.\n\n \n\n\t\t\t\t\tLast Modified: 01/20/2020\n\n\t\t\t\t\tSubmitted by: Constantine Caramanis"
 }
}