{
 "awd_id": "1254021",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Choosing Wisely: Leveraging User-Generated Doctor Ratings",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2013-02-01",
 "awd_exp_date": "2019-01-31",
 "tot_intn_awd_amt": 417108.0,
 "awd_amount": 426208.0,
 "awd_min_amd_letter_date": "2013-01-22",
 "awd_max_amd_letter_date": "2017-06-29",
 "awd_abstract_narration": "This research will advance our knowledge of user-generated doctor ratings on the Internet, thereby providing the basis for improving these systems and increasing their value for better consumer choices.  In so doing, the research will achieve a more general advance in our understanding of online reputation systems. The study has three aims: (1) to examine the supply of online doctor ratings, including their prevalence, growth trends, and the factors that affect rating posting behavior; (2) to assess the informedness of online doctor ratings, especially whether these ratings truly reflect the quality of the doctor or whether they are biased and misleading, and (3) to understand the use and impacts of online doctor ratings.\r\n\r\nDespite the growing popularity of online physician ratings, there is surprisingly little study of this emerging trend. What motivates patients to provide online ratings? To what degree do the ratings reflect a doctor's clinical quality? How do patients use these ratings, and how are their decisions affected as a result? In what situations should patients be cautious when using online doctor ratings? Can the user interface design and information display be improved to make the ratings more useful? The research represents the first systematic study of these fundamental questions.\r\n\r\nFindings from this study will have a direct impact on the policy and practice of quality transparency in healthcare. They will be particularly beneficial for senior citizens, who require more medical services, but they also will be incorporated into new college curriculum. Furthermore, upon completion of the study, the methods and non-proprietary data will be made publicly available to further enhance the research and education infrastructure.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Guodong",
   "pi_last_name": "Gao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guodong Gao",
   "pi_email_addr": "ggao@rhsmith.umd.edu",
   "nsf_id": "000568863",
   "pi_start_date": "2013-01-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "3112 Lee Building",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 81176.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 82043.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 85372.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 85987.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 91630.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"CAREER\">Finding quality doctors is a critical decision faced by every patient. In recent years, online doctor reviews by patients have emerged as a significant channel for providing such information. Just like consulting book reviews on Amazon.com or Yelp reviews for restaurants, reading through doctor reviews is becoming a common practice in doctor choice decisions. The doctor rating websites are so prevalent that when a consumer searches Google or Bing for information about a particular doctor, the chances are that the first page of search results is full of links to doctor ratings.</p>\n<p class=\"CAREER\">&nbsp;</p>\n<p class=\"CAREER\">However, given the high stake decision of choosing a doctor, we know surprisingly little how doctor ratings work. Can the online ratings be trusted to proxy &ldquo;true&rdquo; quality or not, given that only a few self-driven patients contributed the reviews? What kinds of doctors are likely to be rated online? What can we infer for doctors who have no online ratings? How have online ratings affected the decision process of choosing a doctor? And finally, how to detect the fake doctor reviews?</p>\n<p class=\"CAREER\">&nbsp;</p>\n<p class=\"CAREER\">This NSF-sponsored project strived to answer the above essential questions regarding the supply, the informedness, and the impacts of online doctor ratings. The knowledge produced has yielded much-needed knowledge regarding the safe use of doctor ratings. Below we highlight a few significant findings.</p>\n<p class=\"CAREER\">&nbsp;</p>\n<p class=\"CAREER\">First, we provide one of the first empirical insights into the opinion differences between the selected few who choose to rate online and the silent majority who do not, based on data drawn from 1,425 primary care physicians in three metropolitan areas. We find that online doctor ratings, despite being self-selected, are positively associated with the population patient satisfaction scores. Therefore, the ratings are meaningful to infer a doctor&rsquo;s bedside manner and communication. At the same time, we find that online ratings show &ldquo;hyperbole&rdquo; effect. In other words, online opinions regarding doctors tend to be exaggerated or polarized.</p>\n<p class=\"CAREER\">&nbsp;</p>\n<p class=\"CAREER\">However, we do not find that online ratings can be used to infer a doctor&rsquo;s clinical quality. Based on a sample of over one thousand internists, we find virtually no correlation between the two. Therefore, one should not use online reviews contributed by patients to judge a doctor&rsquo;s clinical competence.</p>\n<p class=\"CAREER\">&nbsp;</p>\n<p class=\"CAREER\">Second, we find that doctor reviews significantly changed how patients choose doctors. Based on detailed clickstream data of how patients browse doctor profiles, we find that when the abundance of online reviews is low, patients consider more doctors, browse for a longer duration, and consider doctors that are geographically more dispersed. In contrast, when the abundance of WOM is high, patients consider fewer doctors, browse for a shorter duration, and consider doctors in a smaller geographic area.&nbsp;</p>\n<p class=\"CAREER\">&nbsp;</p>\n<p class=\"CAREER\">Third, our results demonstrate that online reviews of doctors have a significant impact on their appointments. The findings also point to a cannibalization effect: doctors with better ratings gain demand while those with no ratings experience an adverse impact.</p>\n<p class=\"CAREER\">&nbsp;</p>\n<p class=\"CAREER\">Finally, the rising economic influence of these doctor reviews creates strong incentives for businesses to post fraudulent reviews to promote themselves. We design a deep-learning approach that takes into account all the words in the reviews and the relationships among them to detect fraudulent reviews. This approach performs significantly better than traditional Natural Language Processing (NLP) and machine learning algorithms, as well as human labeling.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/01/2019<br>\n\t\t\t\t\tModified by: Guodong&nbsp;Gao</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "Finding quality doctors is a critical decision faced by every patient. In recent years, online doctor reviews by patients have emerged as a significant channel for providing such information. Just like consulting book reviews on Amazon.com or Yelp reviews for restaurants, reading through doctor reviews is becoming a common practice in doctor choice decisions. The doctor rating websites are so prevalent that when a consumer searches Google or Bing for information about a particular doctor, the chances are that the first page of search results is full of links to doctor ratings.\n \nHowever, given the high stake decision of choosing a doctor, we know surprisingly little how doctor ratings work. Can the online ratings be trusted to proxy \"true\" quality or not, given that only a few self-driven patients contributed the reviews? What kinds of doctors are likely to be rated online? What can we infer for doctors who have no online ratings? How have online ratings affected the decision process of choosing a doctor? And finally, how to detect the fake doctor reviews?\n \nThis NSF-sponsored project strived to answer the above essential questions regarding the supply, the informedness, and the impacts of online doctor ratings. The knowledge produced has yielded much-needed knowledge regarding the safe use of doctor ratings. Below we highlight a few significant findings.\n \nFirst, we provide one of the first empirical insights into the opinion differences between the selected few who choose to rate online and the silent majority who do not, based on data drawn from 1,425 primary care physicians in three metropolitan areas. We find that online doctor ratings, despite being self-selected, are positively associated with the population patient satisfaction scores. Therefore, the ratings are meaningful to infer a doctor?s bedside manner and communication. At the same time, we find that online ratings show \"hyperbole\" effect. In other words, online opinions regarding doctors tend to be exaggerated or polarized.\n \nHowever, we do not find that online ratings can be used to infer a doctor?s clinical quality. Based on a sample of over one thousand internists, we find virtually no correlation between the two. Therefore, one should not use online reviews contributed by patients to judge a doctor?s clinical competence.\n \nSecond, we find that doctor reviews significantly changed how patients choose doctors. Based on detailed clickstream data of how patients browse doctor profiles, we find that when the abundance of online reviews is low, patients consider more doctors, browse for a longer duration, and consider doctors that are geographically more dispersed. In contrast, when the abundance of WOM is high, patients consider fewer doctors, browse for a shorter duration, and consider doctors in a smaller geographic area. \n \nThird, our results demonstrate that online reviews of doctors have a significant impact on their appointments. The findings also point to a cannibalization effect: doctors with better ratings gain demand while those with no ratings experience an adverse impact.\n \nFinally, the rising economic influence of these doctor reviews creates strong incentives for businesses to post fraudulent reviews to promote themselves. We design a deep-learning approach that takes into account all the words in the reviews and the relationships among them to detect fraudulent reviews. This approach performs significantly better than traditional Natural Language Processing (NLP) and machine learning algorithms, as well as human labeling.\n\n \n\n\t\t\t\t\tLast Modified: 06/01/2019\n\n\t\t\t\t\tSubmitted by: Guodong Gao"
 }
}