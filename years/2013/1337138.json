{
 "awd_id": "1337138",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:XPS:CLCCA: Cross-layer Thermal Reliability Management in 3D Integrated Heterogeneous Processor for Breaking the Power and Bandwidth Walls",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 208679.0,
 "awd_amount": 208679.0,
 "awd_min_amd_letter_date": "2013-08-23",
 "awd_max_amd_letter_date": "2013-08-23",
 "awd_abstract_narration": "3D stacked integration of CPU, GPU and DRAM dies vertically interconnected by TSVs (Through-Silicon Vias) is emerging as a key enabling technology for parallel and scalable computing systems of tomorrow. Such 3D Heterogeneous Processor (3DHP) is expected to deliver much higher bandwidth, lower latency and power consumption to break the power and bandwidth walls. Despite such significant benefits, 3DHP comes with new domain-specific challenges that have never been fully explored and addressed. Significantly higher power density, thinned substrate and low thermal conductivity of inter-layer dielectric material all make thermal management a serious problem that threatens overall reliability and performance of 3DHP. This project aims to address this thermal-integrity issue of 3DHP through a holistic cross-layer approach.\r\n \r\nThree major thermal integrity issues at respective target system layers including physical, architecture and runtime layers and their correlations will be extensively investigated by a team of three PIs with necessary background and expertise. The proposed novel cross-layer approach includes: 1) Self-calibrated on-chip temperature/stress co-sensor framework at physical layer, 2) Adaptive Error Detection & Correction (EDAC) and DRAM refresh engine at architecture layer for reliable storage and transfer of data among CPU, GPU and DRAM dies, and 3) Dynamic Thermal Reliability Management (DTRM) framework for fine-grained control of interaction between workloads and HW resources at runtime layer. The proposed layered techniques will be tightly interwoven to bring out the most synergistic results. The research in this project will result in a solid thermal-integrity design and simulation framework for viable 3DHP-based parallel and scalable computing systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Byunghyun",
   "pi_last_name": "Jang",
   "pi_mid_init": "",
   "pi_sufx_name": "Dr.",
   "pi_full_name": "Byunghyun Jang",
   "pi_email_addr": "bjang@cs.olemiss.edu",
   "nsf_id": "000629748",
   "pi_start_date": "2013-08-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Mississippi",
  "inst_street_address": "113 FALKNER",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY",
  "inst_state_code": "MS",
  "inst_state_name": "Mississippi",
  "inst_phone_num": "6629157482",
  "inst_zip_code": "386779704",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MS01",
  "org_lgl_bus_name": "THE UNIVERSITY OF MISSISSIPPI",
  "org_prnt_uei_num": "",
  "org_uei_num": "G1THVER8BNL4"
 },
 "perf_inst": {
  "perf_inst_name": "University of Mississippi",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MS",
  "perf_st_name": "Mississippi",
  "perf_zip_code": "386771848",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MS01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 208679.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Heterogeneous processors that merge latency-optimized multi-core CPU and throughput-optimized manycore GPU on a single-chip are emerging as a key enabling technology for scalable systems. They offer higher performance and energy efficiency per cost by eliminating data transfer overhead between CPU and GPU. However, parallelism and scalability of such systems are still severely limited by the low bandwidth, high latency and power consumption of off-chip DRAM. This off-chip DRAM memory wall should be broken to enable next level of parallelism and scalability. A promising technology that can address this issue is 3D stacked integration of CPU, GPU and DRAM dies vertically interconnected by TSVs (Through-Silicon Vias). When compared with conventional off-chip interconnects, TSVs enable a lot larger number of vertical channels among CPU, GPU and DRAM dies, while providing much shorter distance of data travel. As such, 3D Heterogeneous Processor (3DHP) delivers much higher bandwidth, lower latency and power consumption. Despite those significant benefits, 3DHPs face new challenges that have never existed and solved. Significantly higher power density, thinned substrate and low thermal conductivity of inter-layer dielectric material all make thermal management a serious problem that threatens overall reliability and performance of 3DHPs.</p>\n<p>In order to address the thermal and reliability issues of the 3D heterogeneous processors while maintaining maximum possible performance, energy efficiency, parallelism and scalability, we have developed Dynamic Thermal Reliability Management (DTRM) system for fine-grained runtime control of interaction between workload and hardware resources. To the best of our knowledge, the DTRM scheduling framework developed in this project is the first attempt to address the thermal and reliability concerns of future 3D heterogeneous processors using the emerging programming paradigm OpenCL. Extensive simulation results demonstrate that dynamic task (i.e., work-group in OpenCL terminology) scheduling is a viable, flexible, and cost-effective solution to address the thermal and reliability issues of 3D stacked processors.</p>\n<p>The other track of this research project, non-conventional heterogeneous workload developments, serves as not only workloads for the proposed DTRM system but also workloads to use for tightly coupled CPU-GPU heterogeneous processor research for years to come. Traditional GPGPU computing paradigm revealed its inherent limitations mainly caused by separate memory address spaces; for example, pointer-based data structures like trees and graphs can not benefit from GPU acceleration due to separate memory spaces even if two memories are physically merged. Shared Virtual Memory (SVM) is a key to embrace such applications, however, requires significant changes in application and hardware design. We have implemented concurrent data structures such as concurrent linked list using shared virtual memory feature of OpenCL 2.x on AMD APUs and we have disseminated the outcomes to the research community via publications.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2017<br>\n\t\t\t\t\tModified by: Byunghyun&nbsp;Jang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHeterogeneous processors that merge latency-optimized multi-core CPU and throughput-optimized manycore GPU on a single-chip are emerging as a key enabling technology for scalable systems. They offer higher performance and energy efficiency per cost by eliminating data transfer overhead between CPU and GPU. However, parallelism and scalability of such systems are still severely limited by the low bandwidth, high latency and power consumption of off-chip DRAM. This off-chip DRAM memory wall should be broken to enable next level of parallelism and scalability. A promising technology that can address this issue is 3D stacked integration of CPU, GPU and DRAM dies vertically interconnected by TSVs (Through-Silicon Vias). When compared with conventional off-chip interconnects, TSVs enable a lot larger number of vertical channels among CPU, GPU and DRAM dies, while providing much shorter distance of data travel. As such, 3D Heterogeneous Processor (3DHP) delivers much higher bandwidth, lower latency and power consumption. Despite those significant benefits, 3DHPs face new challenges that have never existed and solved. Significantly higher power density, thinned substrate and low thermal conductivity of inter-layer dielectric material all make thermal management a serious problem that threatens overall reliability and performance of 3DHPs.\n\nIn order to address the thermal and reliability issues of the 3D heterogeneous processors while maintaining maximum possible performance, energy efficiency, parallelism and scalability, we have developed Dynamic Thermal Reliability Management (DTRM) system for fine-grained runtime control of interaction between workload and hardware resources. To the best of our knowledge, the DTRM scheduling framework developed in this project is the first attempt to address the thermal and reliability concerns of future 3D heterogeneous processors using the emerging programming paradigm OpenCL. Extensive simulation results demonstrate that dynamic task (i.e., work-group in OpenCL terminology) scheduling is a viable, flexible, and cost-effective solution to address the thermal and reliability issues of 3D stacked processors.\n\nThe other track of this research project, non-conventional heterogeneous workload developments, serves as not only workloads for the proposed DTRM system but also workloads to use for tightly coupled CPU-GPU heterogeneous processor research for years to come. Traditional GPGPU computing paradigm revealed its inherent limitations mainly caused by separate memory address spaces; for example, pointer-based data structures like trees and graphs can not benefit from GPU acceleration due to separate memory spaces even if two memories are physically merged. Shared Virtual Memory (SVM) is a key to embrace such applications, however, requires significant changes in application and hardware design. We have implemented concurrent data structures such as concurrent linked list using shared virtual memory feature of OpenCL 2.x on AMD APUs and we have disseminated the outcomes to the research community via publications.\n\n\t\t\t\t\tLast Modified: 11/30/2017\n\n\t\t\t\t\tSubmitted by: Byunghyun Jang"
 }
}