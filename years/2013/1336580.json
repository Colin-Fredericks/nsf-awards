{
 "awd_id": "1336580",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS:CLCCA:LigHTS: Lagging-Hardware Tolerant Systems\" in the system.",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 749854.0,
 "awd_amount": 749854.0,
 "awd_min_amd_letter_date": "2013-09-08",
 "awd_max_amd_letter_date": "2016-07-14",
 "awd_abstract_narration": "With the advent of scalable parallel computing, thousands of devices are connected and managed collectively.\u00a0 This era is confronted with a new challenge: performance failure; systems often perform worse than expected due to large-scale management issues such as hardware failures, software bugs, and configuration mistakes.\u00a0 This project targets one overlooked cause of performance failure: \"lagging hardware\" -- hardware whose performance degrades significantly compared to its specification.\u00a0 Many reports indicate that a single lagging hardware can easily cascade and make the performance of a whole cluster collapse.\u00a0 Here, parallelism is unexploited, productivity is reduced, the system is underutilized, and energy is wasted. The goal of the LigHTS project is to transform computing systems into Lagging-Hardware Tolerant Systems.\u00a0 The LigHTS project will bring many direct benefits to the society; users from many areas (science, healthcare, business, education, military, and government) increasingly use large-scale storage and computation services.\u00a0 Here, predictable performance is a key to success, and in this context lagging-hardware tolerant computing is a critical ingredient. \r\n\r\nThe LigHTS project consists of three major objectives.\u00a0 The first is lagging-hardware data analysis and instrumentation. To improve the robustness of future parallel systems, it is crucial to study lagging characteristics exhibited by modern hardware and to devise new instrumentation methodologies that can collect cases of lagging hardware in deployment.\u00a0 The second is lagging-failure system analysis.\u00a0 It is important to rigorously analyze the impact of lagging hardware (including disk, network, processor) to currently deployed systems. The results will unearth design flaws and provide valuable reevaluations of how deployed systems should evolve.\u00a0 The last is LigHTS principles, design, and implementation.\u00a0 There is a need to establish foundational principles of lagging-hardware tolerant computing and apply the principles in building prototypes of cross-layer LigHTS systems spanning distributed storage, computing framework, operating and runtime systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Haryadi",
   "pi_last_name": "Gunawi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Haryadi Gunawi",
   "pi_email_addr": "haryadi@cs.uchicago.edu",
   "nsf_id": "000626546",
   "pi_start_date": "2013-09-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Chien",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew A Chien",
   "pi_email_addr": "achien@cs.uchicago.edu",
   "nsf_id": "000295194",
   "pi_start_date": "2013-09-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Ross",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Robert Ross",
   "pi_email_addr": "rross@mcs.anl.gov",
   "nsf_id": "000259978",
   "pi_start_date": "2013-09-08",
   "pi_end_date": "2016-07-14"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Dries",
   "pi_last_name": "Kimpe",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dries Kimpe",
   "pi_email_addr": "dries@northwestern.edu",
   "nsf_id": "000635607",
   "pi_start_date": "2013-09-08",
   "pi_end_date": "2015-07-07"
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Chicago",
  "perf_str_addr": "1100 E 58th Street",
  "perf_city_name": "Chicago",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "606375418",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "IL01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 749854.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><br />The LIGHTS (Lagging-Hardware Tolerant Systems) project has advanced storage and distributed systems to analyze, anticipate, and tolerate lagging hardware devices.&nbsp; The LIGHTS projects produces several outcomes given the execution of the sub-projects below. <br /><br />With the Limpware project [HotCloud '13], we highlight the problem of lagging hardware, providing anecdotes of lagging hardware in production systems, including lagging CPUs, network hardware, memory, and storage devices. <br /><br />With the Limplock project [SoCC '13], we unearth the fact that many modern distributed systems such as Hadoop, HDFS, ZooKeeper, Cassandra, and HBase are not fully equipped in dealing with lagging hardware. <br /><br />With the CBS project [SoCC '14], we study thousands of bugs in six popular cloud-scale distributed systems and found several cases of software bugs that incorrectly handled lagging hardware. <br /><br />With the SPV project [HotCloud '15], we show that lagging-hardware related performance bugs can be detected prior to deployment using performance model checking tool such as Colored Petri Nets. <br /><br />With the PBSE project [SoCC '17], we enhance the basic speculative execution technique in many data-parallel frameworks such as Hadoop and Spark.&nbsp; We introduce Path-Based Speculative Execution that can robustly detect and failover from degraded NICs. <br /><br />With the Tail at Store project [FAST '16], we study millions of hours of SSD and disk performance and show that disks and SSDs exhibit a lagging behavior compared to their other peers within the RAID systems. <br /><br />With the Tiny-Tail Flash project [FAST '17 and TOS '17], we achieve a near-perfect elimination of garbage collection tail latencies in NAND SSDs. <br /><br />With the tail-tolerant flash array project, we adopt techniques in the tiny-tail flash project into the context of flash array.&nbsp; The tail-tolerant flash array introduces a simple interface in which commodity SSDs can collaborate with the RAID layer to cut garbage collection tail latencies. <br /><br />Finally, with the fail-slow at scale project, we show 100 real cases of lagging hardware occurred in large-scale production systems including their detailed characteristics and chains of events. <br /><br /><br />Broader Impact: The LIGHT project places significant value on technology transfer; the outcomes of the project have led to direct industrial impact.&nbsp; For example, approaches from PBSE and Tiny-Tail Flash projects have been adopted by several industries that deploy data-parallel frameworks and SSD managements.&nbsp; In addition, Predictable performance is a key to success of multi-billion dollar computing, and we believe LIGHTS projects and the approaches introduced will be an important ingredient. Users from many areas (science, healthcare, business, education, military, and government) are increasingly use large-scale storage and computing services, and the outcomes of our project will improve the reliability and availability of these services. <br /><br /></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/10/2017<br>\n\t\t\t\t\tModified by: Haryadi&nbsp;Gunawi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nThe LIGHTS (Lagging-Hardware Tolerant Systems) project has advanced storage and distributed systems to analyze, anticipate, and tolerate lagging hardware devices.  The LIGHTS projects produces several outcomes given the execution of the sub-projects below. \n\nWith the Limpware project [HotCloud '13], we highlight the problem of lagging hardware, providing anecdotes of lagging hardware in production systems, including lagging CPUs, network hardware, memory, and storage devices. \n\nWith the Limplock project [SoCC '13], we unearth the fact that many modern distributed systems such as Hadoop, HDFS, ZooKeeper, Cassandra, and HBase are not fully equipped in dealing with lagging hardware. \n\nWith the CBS project [SoCC '14], we study thousands of bugs in six popular cloud-scale distributed systems and found several cases of software bugs that incorrectly handled lagging hardware. \n\nWith the SPV project [HotCloud '15], we show that lagging-hardware related performance bugs can be detected prior to deployment using performance model checking tool such as Colored Petri Nets. \n\nWith the PBSE project [SoCC '17], we enhance the basic speculative execution technique in many data-parallel frameworks such as Hadoop and Spark.  We introduce Path-Based Speculative Execution that can robustly detect and failover from degraded NICs. \n\nWith the Tail at Store project [FAST '16], we study millions of hours of SSD and disk performance and show that disks and SSDs exhibit a lagging behavior compared to their other peers within the RAID systems. \n\nWith the Tiny-Tail Flash project [FAST '17 and TOS '17], we achieve a near-perfect elimination of garbage collection tail latencies in NAND SSDs. \n\nWith the tail-tolerant flash array project, we adopt techniques in the tiny-tail flash project into the context of flash array.  The tail-tolerant flash array introduces a simple interface in which commodity SSDs can collaborate with the RAID layer to cut garbage collection tail latencies. \n\nFinally, with the fail-slow at scale project, we show 100 real cases of lagging hardware occurred in large-scale production systems including their detailed characteristics and chains of events. \n\n\nBroader Impact: The LIGHT project places significant value on technology transfer; the outcomes of the project have led to direct industrial impact.  For example, approaches from PBSE and Tiny-Tail Flash projects have been adopted by several industries that deploy data-parallel frameworks and SSD managements.  In addition, Predictable performance is a key to success of multi-billion dollar computing, and we believe LIGHTS projects and the approaches introduced will be an important ingredient. Users from many areas (science, healthcare, business, education, military, and government) are increasingly use large-scale storage and computing services, and the outcomes of our project will improve the reliability and availability of these services. \n\n\n\n\t\t\t\t\tLast Modified: 11/10/2017\n\n\t\t\t\t\tSubmitted by: Haryadi Gunawi"
 }
}