{
 "awd_id": "1319356",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Randomized Models for Nonlinear Optimization:  Theoretical Foundations and Practical Numerical Methods",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 200000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2013-09-06",
 "awd_max_amd_letter_date": "2013-09-06",
 "awd_abstract_narration": "This project involves the design, analysis, and implementation of numerical algorithms for the mathematical optimization of large-scale, complex systems.  In particular, the novel feature of the proposed algorithms is the use of random sampling of objective function information in the context of solving deterministic (i.e., non-random) problems.  Despite the success of randomization in, e.g., stochastic gradient techniques for machine learning, it has yet to be used actively in other settings as it has been deemed too expensive in sequential computing environments.  However, with parallel computing becoming increasingly common, and with new advancements and convergence theory for randomized algorithms, these methods have great promise.  The research in this project will focus on the use of ``accurate'' randomized models, broadening of convergence theory, and implementation of effective software.  The novelty of the approach lies in achieving a middle ground between deterministic models that have to be accurate at each algorithmic step, and stochastic models that are accurate only in expectation, by exploiting random models that need to be accurate only with sufficiently high probability. The proposed strategies will balance per-iteration cost of the optimization routine with convergence speed while utilizing parallel computation.  The priority in the project on developing practical, general-purpose numerical methods based on theoretically sound methodologies solidifies the merits of the proposed work.\r\n\r\nThis project focuses on the development of novel numerical algorithms, and their analysis, for solving problems in two related realms of engineering design.  In the first, the aim is to minimize a quantity---e.g., cost, energy, or the discrepancy between expected and observed data---that can only be determined via a computer simulation. These \"black-box\" optimization problems arise in important areas such as molecular geometry optimization, circuit design, and groundwater modeling.  The second area represents those applications in which a given design needs to be robust under various input conditions, which includes problems in, e.g., medical image registration and the optimization of control systems.  The project promises to advance the study of algorithms for solving all of these types of problems via the common thread of exploiting randomization and parallel computation.The impact of this work will clearly be cross-disciplinary, and will benefit users of optimization methods and software in academia, governmental research laboratories, and private industry.  It will also promote the use of rigorous, classical algorithms in combination with randomized models for solving cutting-edge scientific problems.Finally, the educational plan will expose undergraduate and graduate students to modern efforts and challenges in computational mathematics, improve the educational opportunities for students interested in scientific research, and encourage  faculty interaction in area schools.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Katya",
   "pi_last_name": "Scheinberg",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Katya Scheinberg",
   "pi_email_addr": "katyascheinberg@gmail.com",
   "nsf_id": "000544723",
   "pi_start_date": "2013-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Curtis",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Frank E Curtis",
   "pi_email_addr": "fec309@lehigh.edu",
   "nsf_id": "000549599",
   "pi_start_date": "2013-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Lehigh University",
  "inst_street_address": "526 BRODHEAD AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BETHLEHEM",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "6107583021",
  "inst_zip_code": "180153008",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "PA07",
  "org_lgl_bus_name": "LEHIGH UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "E13MDBKHLDB5"
 },
 "perf_inst": {
  "perf_inst_name": "Lehigh University",
  "perf_str_addr": "200 West Packer Ave., Mohler Lab",
  "perf_city_name": "Bethlehem",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "180151518",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "PA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  },
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 200000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><strong>Objectives and Methods:</strong>&nbsp; This project involved the design, analysis, and implementation of algorithms for the optimization of large-scale, complex systems with specific focus on problems for which some critical objective function information is unavailable during the optimization process.&nbsp; In particular, the methods we studied use&nbsp; randomized approximations of deterministic functions or their derivatives for solving problems in these areas. Such methods have a wide variety of applications, because such randomized approximations can often be constructed&nbsp; at much lower cost than accurate deterministic approximations. <br /><br />We focused on two classes of methods.&nbsp; The first class represents traditional methods for optimizing unconstrained smooth objective functions, which include trust region, line search, and cubic regularization methods.&nbsp; These approaches have well-known complexity and convergence analyses under the assumption that at each iteration they can use exact function and/or derivative information, or use approximations of these quantities that are sufficiently accurate. In our project, we analyzed these methods under novel assumptions that such information is random and may only be sufficiently accurate with some probability.&nbsp; The second challenging class of optimization problems we considered are those with nonconvex, nonsmooth objective functions.&nbsp; Problems of this type arise in numerous settings, particularly when the optimal solution is intended to be robust under various input conditions.&nbsp; The inability to numerically characterize the subdifferential of a nonsmooth function has been the motivating factor for the advent of subgradient, cutting plane, proximal point, and bundle methods that have been the subject of research for decades.&nbsp; These sequential methods can be effective in practice, especially for convex functions, but rely heavily on subgradients that are computed exclusively at algorithm iterates. We proposed novel techniques that combine random gradient sampling techniques to enhance the performance of methods.<br /><br /><strong>Intellectual Merits:&nbsp;</strong> We developed completely novel analyses for convergence and complexity of wide classes of optimization methods that rely on random models.&nbsp; With respect to our first class of methods, we developed a general framework which describes each algorithm as a stochastic process with certain properties.&nbsp; By analyzing this stochastic process, we were able to provide bounds on the expected number of iterations that an algorithm will take until it reaches a solution of desired accuracy. This novel technique has been already utilized by other researchers in the field and is likely to be applicable to many modern optimization methods, especially those arising in training large-scale machine learning models.&nbsp; We have also extended these results to the specific case of trust region methods for stochastic functions, which is the subject of ongoing research.&nbsp;&nbsp;&nbsp; <br />The techniques that we have developed with respect to our second class of methods, namely, for solving nonconvex, nonsmooth optimization problems, have not only been based on randomized procedures, but also include novel adaptive enhancements to gradient sampling algorithms, such as adapting sampling, the re-use of historial information, and the incorporation of quasi-Newton inverse Hessian approximations.&nbsp; Our work on these enhancements have already inspired various new developments for solving challenging classes of problems.&nbsp; All told, our research has resulted in 6 journal publications, 2 technical reports---soon to be submitted for publication---and three PhD theses. Software packages based on this research are under development and will be available within a year.<br /><br /><strong>Broader Impacts:&nbsp;</strong> The&nbsp; project is expected to have broad impact, reaching a wide variety of audiences.&nbsp; The analytical techniques are novel and combine&nbsp; optimization convergence analysis with elements of martingale theory, random matrix theory, and compressed sensing.&nbsp; The impact of the work is also cross-disciplinary, since optimization methods are now being widely used by researchers in the machine learning, statistics, and signal processing research communities.&nbsp; Our methods already inspired follow-up work published by others and have received significant numbers of references.&nbsp; Two female PhD students have been trained during the course of this project.&nbsp; Moreover, this award enabled us to start a group at Lehigh, which now contains three faculty members and a dozen students and is supported by two subsequent, substantial NSF grants, including a TRIPODS Phase 1 award.&nbsp; The work currently performed by the group has partially been enabled by the research under this completed project.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/03/2018<br>\n\t\t\t\t\tModified by: Katya&nbsp;Scheinberg</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nObjectives and Methods:  This project involved the design, analysis, and implementation of algorithms for the optimization of large-scale, complex systems with specific focus on problems for which some critical objective function information is unavailable during the optimization process.  In particular, the methods we studied use  randomized approximations of deterministic functions or their derivatives for solving problems in these areas. Such methods have a wide variety of applications, because such randomized approximations can often be constructed  at much lower cost than accurate deterministic approximations. \n\nWe focused on two classes of methods.  The first class represents traditional methods for optimizing unconstrained smooth objective functions, which include trust region, line search, and cubic regularization methods.  These approaches have well-known complexity and convergence analyses under the assumption that at each iteration they can use exact function and/or derivative information, or use approximations of these quantities that are sufficiently accurate. In our project, we analyzed these methods under novel assumptions that such information is random and may only be sufficiently accurate with some probability.  The second challenging class of optimization problems we considered are those with nonconvex, nonsmooth objective functions.  Problems of this type arise in numerous settings, particularly when the optimal solution is intended to be robust under various input conditions.  The inability to numerically characterize the subdifferential of a nonsmooth function has been the motivating factor for the advent of subgradient, cutting plane, proximal point, and bundle methods that have been the subject of research for decades.  These sequential methods can be effective in practice, especially for convex functions, but rely heavily on subgradients that are computed exclusively at algorithm iterates. We proposed novel techniques that combine random gradient sampling techniques to enhance the performance of methods.\n\nIntellectual Merits:  We developed completely novel analyses for convergence and complexity of wide classes of optimization methods that rely on random models.  With respect to our first class of methods, we developed a general framework which describes each algorithm as a stochastic process with certain properties.  By analyzing this stochastic process, we were able to provide bounds on the expected number of iterations that an algorithm will take until it reaches a solution of desired accuracy. This novel technique has been already utilized by other researchers in the field and is likely to be applicable to many modern optimization methods, especially those arising in training large-scale machine learning models.  We have also extended these results to the specific case of trust region methods for stochastic functions, which is the subject of ongoing research.    \nThe techniques that we have developed with respect to our second class of methods, namely, for solving nonconvex, nonsmooth optimization problems, have not only been based on randomized procedures, but also include novel adaptive enhancements to gradient sampling algorithms, such as adapting sampling, the re-use of historial information, and the incorporation of quasi-Newton inverse Hessian approximations.  Our work on these enhancements have already inspired various new developments for solving challenging classes of problems.  All told, our research has resulted in 6 journal publications, 2 technical reports---soon to be submitted for publication---and three PhD theses. Software packages based on this research are under development and will be available within a year.\n\nBroader Impacts:  The  project is expected to have broad impact, reaching a wide variety of audiences.  The analytical techniques are novel and combine  optimization convergence analysis with elements of martingale theory, random matrix theory, and compressed sensing.  The impact of the work is also cross-disciplinary, since optimization methods are now being widely used by researchers in the machine learning, statistics, and signal processing research communities.  Our methods already inspired follow-up work published by others and have received significant numbers of references.  Two female PhD students have been trained during the course of this project.  Moreover, this award enabled us to start a group at Lehigh, which now contains three faculty members and a dozen students and is supported by two subsequent, substantial NSF grants, including a TRIPODS Phase 1 award.  The work currently performed by the group has partially been enabled by the research under this completed project.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/03/2018\n\n\t\t\t\t\tSubmitted by: Katya Scheinberg"
 }
}