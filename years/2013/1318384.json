{
 "awd_id": "1318384",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF:Small:Scalable Memory Hierarchies with Fine-Grained QoS Guarantees",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 500000.0,
 "awd_min_amd_letter_date": "2013-07-24",
 "awd_max_amd_letter_date": "2013-07-24",
 "awd_abstract_narration": "Multicore chips are now mainstream, and increasing the number of cores per chip has become the primary way to improve performance. Current multicores rely on sophisticated cache hierarchies to mitigate the high latency, limited bandwidth, and high energy of main memory accesses, which often limit system performance. These on-chip caches consume more than half of chip area, and most of this cache space is shared among all cores. Sharing this capacity has major advantages, such as improving space utilization and accelerating core-to-core communication, but poses two fundamental problems. First, with more cores, cache accesses take longer and consume more energy, severely limiting scalability. Second, concurrently executing applications contend for this shared cache capacity, which can cause unpredictable performance degradation among them. The goal of this project is to redesign the cache hierarchy to make it both highly scalable, and to provide strict isolation among competing applications, enabling end-to-end performance guarantees. If successful, this work will improve the performance and energy efficiency of future processors, enabling systems with larger numbers of cores than previously possible. Moreover, these systems will eliminate interference and enforce quality of service guarantees among competing applications, even when those applications are latency-critical. This will enable much higher utilization of shared computing infrastructure (such as cloud computing servers), potentially saving billions of dollars in IT infrastructure and energy consumption.\r\n\r\nTo achieve the dual goals of high scalability and quality-of-service (QoS) guarantees efficiently, this project proposes an integrated hardware-software approach, where hardware exposes a small and general set of mechanisms to control cache allocations, and software uses these mechanisms to implement both partitioning and non-uniform access policies efficiently. At the hardware level, a novel cache organization provides thousands of fine-grained, spatially configurable partitions, implements lightweight monitoring and reconfiguration mechanisms to guide software policies effectively, and supports full-system scalable cache coherence cheaply. At the software level, a system-level runtime leverages this hardware to implement dynamic data classification, placement, migration, and replication mechanisms, maximizing system performance and efficiency, while at the same time enforcing the strict QoS guarantees of latency-critical workloads, transparently to applications. Combined with existing bandwidth partitioning approaches, these techniques will enforce full-system QoS guarantees by controlling all on-chip shared resources (caches, on-chip network, and memory controllers). In addition, the infrastructure and benchmarks developed as part of this project will be publicly released, allowing other researchers to build on the results of this work, and enabling the development of course projects and other educational activities in large-scale parallel computer architecture, both at MIT and elsewhere.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Sanchez Martin",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Sanchez Martin",
   "pi_email_addr": "sanchez@csail.mit.edu",
   "nsf_id": "000636815",
   "pi_start_date": "2013-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 Massachusetts Avenue",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394307",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "794100",
   "pgm_ele_name": "COMPUTER ARCHITECTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Multicore chips are now mainstream, and increasing the number of cores per chip has become the primary way to improve performance. Current multicores rely on sophisticated cache hierarchies to mitigate the high latency, limited bandwidth, and high energy of main memory accesses, which limit system performance. These caches consume more than half of chip area, and most of this cache space is shared among all cores. Sharing this capacity has major advantages, such as improving space utilization and accelerating core-to-core communication, but poses two fundamental problems. First, with more cores, cache accesses take longer and consume more energy, severely limiting scalability. Second, applications contend for this shared cache capacity, causing unpredictable performance degradation. This lack of predictability limits the utilization of shared servers, where applications much often meet strict performance targets.</p>\n<p><strong>Intellectual Merit:</strong> The key goal of this project has been to redesign the cache hierarchy to make it both highly scalable and to provide strict isolation among competing applications, enabling end-to-end performance guarantees. Our research has produced the following main outcomes:</p>\n<p>First, we have investigated and designed software-defined cache hierarchies, a new memory system organization that scales the cache hierarchy by leveraging the strengths of hardware and software. Our approach combines simple, configurable hardware mechanisms controlled by sophisticated software runtimes. Hardware exposes spatially-distributed cache banks to software, allowing an OS-level runtime to build virtual cache hierarchies tailored to the needs of each application, dynamically and transparently to applications. With a single application, our design approaches the performance of the best application-specific hierarchy; with multiple applications sharing the chip, software can control how to divide resources to satisfy system-level objectives (e.g., maximizing throughput or enforcing application priorities). To enable software-defined hierarchies, we have developed novel practical optimization algorithms that reconfigure the whole system in about 1 millisecond and perform within 1% of impractically-expensive solvers. We have demonstrated that these techniques yield large speedups in systems with spatially-distributed caches, as well as in systems with heterogeneous memory technologies (e.g., SRAM and die-stacked DRAM). Moreover, putting software in control of the cache hierarchy enables several novel optimizations throughout the system stack. We have demonstrated these capabilities through novel techniques that perform coordinated scheduling of data and computation and leverage application-level knowledge to improve data placement.</p>\n<p>Second, we have designed new management techniques to share hardware resources dynamically among applications while providing strict performance guarantees. To share resources safely, these techniques leverage simple hardware mechanisms to let software control resource allocations at high speed, as well as novel modeling techniques to account for the inherent performance inertia of each resource. As a result, these techniques allow much more efficient utilization of caches and cores, and dramatically improve utilization of shared servers in clusters and datacenters.</p>\n<p>Third, we have designed new analytical cache modeling techniques and cache replacement policies to better understand and improve cache performance. These techniques rely on a novel probabilistic framework based on absolute reuse distances. Our modeling techniques accurately predict performance for a wide range of cache configurations and policies, enabling many system optimizations. Beyond improving performance, our replacement policies yield important qualitative benefits, such as eliminating performance cliffs, which makes cache performance smooth and predictable.</p>\n<p>To prototype and evaluate these techniques, we have developed a substantial amount of infrastructure, including a state-of-the-art parallel simulator and a diverse benchmark suite. We have released this infrastructure under open-source licenses, allowing others to build on the results of our work, both in research and in the classroom.</p>\n<p><strong>Broader Impacts:&nbsp;</strong>The techniques developed in this project significantly improve the performance and energy efficiency of multicore processors, enabling systems with a larger number of cores than previously possible. Moreover, by eliminating interference and enforcing quality-of-service guarantees among competing applications, these techniques enable much higher utilization of shared computing infrastructure (such as cloud computing servers), reducing both IT infrastructure costs and energy consumption.</p>\n<p>Finally, this project has supported the training and professional development of six graduate students.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/01/2017<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Sanchez Martin</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMulticore chips are now mainstream, and increasing the number of cores per chip has become the primary way to improve performance. Current multicores rely on sophisticated cache hierarchies to mitigate the high latency, limited bandwidth, and high energy of main memory accesses, which limit system performance. These caches consume more than half of chip area, and most of this cache space is shared among all cores. Sharing this capacity has major advantages, such as improving space utilization and accelerating core-to-core communication, but poses two fundamental problems. First, with more cores, cache accesses take longer and consume more energy, severely limiting scalability. Second, applications contend for this shared cache capacity, causing unpredictable performance degradation. This lack of predictability limits the utilization of shared servers, where applications much often meet strict performance targets.\n\nIntellectual Merit: The key goal of this project has been to redesign the cache hierarchy to make it both highly scalable and to provide strict isolation among competing applications, enabling end-to-end performance guarantees. Our research has produced the following main outcomes:\n\nFirst, we have investigated and designed software-defined cache hierarchies, a new memory system organization that scales the cache hierarchy by leveraging the strengths of hardware and software. Our approach combines simple, configurable hardware mechanisms controlled by sophisticated software runtimes. Hardware exposes spatially-distributed cache banks to software, allowing an OS-level runtime to build virtual cache hierarchies tailored to the needs of each application, dynamically and transparently to applications. With a single application, our design approaches the performance of the best application-specific hierarchy; with multiple applications sharing the chip, software can control how to divide resources to satisfy system-level objectives (e.g., maximizing throughput or enforcing application priorities). To enable software-defined hierarchies, we have developed novel practical optimization algorithms that reconfigure the whole system in about 1 millisecond and perform within 1% of impractically-expensive solvers. We have demonstrated that these techniques yield large speedups in systems with spatially-distributed caches, as well as in systems with heterogeneous memory technologies (e.g., SRAM and die-stacked DRAM). Moreover, putting software in control of the cache hierarchy enables several novel optimizations throughout the system stack. We have demonstrated these capabilities through novel techniques that perform coordinated scheduling of data and computation and leverage application-level knowledge to improve data placement.\n\nSecond, we have designed new management techniques to share hardware resources dynamically among applications while providing strict performance guarantees. To share resources safely, these techniques leverage simple hardware mechanisms to let software control resource allocations at high speed, as well as novel modeling techniques to account for the inherent performance inertia of each resource. As a result, these techniques allow much more efficient utilization of caches and cores, and dramatically improve utilization of shared servers in clusters and datacenters.\n\nThird, we have designed new analytical cache modeling techniques and cache replacement policies to better understand and improve cache performance. These techniques rely on a novel probabilistic framework based on absolute reuse distances. Our modeling techniques accurately predict performance for a wide range of cache configurations and policies, enabling many system optimizations. Beyond improving performance, our replacement policies yield important qualitative benefits, such as eliminating performance cliffs, which makes cache performance smooth and predictable.\n\nTo prototype and evaluate these techniques, we have developed a substantial amount of infrastructure, including a state-of-the-art parallel simulator and a diverse benchmark suite. We have released this infrastructure under open-source licenses, allowing others to build on the results of our work, both in research and in the classroom.\n\nBroader Impacts: The techniques developed in this project significantly improve the performance and energy efficiency of multicore processors, enabling systems with a larger number of cores than previously possible. Moreover, by eliminating interference and enforcing quality-of-service guarantees among competing applications, these techniques enable much higher utilization of shared computing infrastructure (such as cloud computing servers), reducing both IT infrastructure costs and energy consumption.\n\nFinally, this project has supported the training and professional development of six graduate students.\n\n\t\t\t\t\tLast Modified: 11/01/2017\n\n\t\t\t\t\tSubmitted by: Daniel Sanchez Martin"
 }
}