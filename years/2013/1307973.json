{
 "awd_id": "1307973",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Randomization inference for contemporary problems in statistics",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2013-08-29",
 "awd_max_amd_letter_date": "2013-08-29",
 "awd_abstract_narration": "The investigators continue the development of new methodology and the accompanying mathematical theory for problems in multiple testing and inference, driven by the many burgeoning applications in the information age.  Further motivation for valid methods stems from exploratory analysis of large data sets, where the process of \"data snooping\" (or \"data mining\") often leads to challenges of multiple testing and simultaneous inference.  In such problems, the statistician is faced with the challenge of accounting for all possible errors resulting from a complex analysis of the data, so that any resulting inferences or conclusions can reliably be viewed as \"real\" rather than spurious findings or artifacts of the data.  It is safe to say that the mathematical justification of sound statistical methods is not keeping pace with the demand for valid new tools.  In particular, the investigators develop randomization tests as  inferential methods for semi-parametric and nonparametric models that do not rely on unverifiable assumptions.  To a great extent, resampling methods, such as the bootstrap and subsampling, are successful in many problems, at least in an asymptotic sense, but for many problems they are unsatisfactory.  Examples of such problems in contemporary statistics include \"high\" dimensional problems, where the \"curse of dimensionality\" may cause resampling methods to break down, and \"non-regular\" problems, where a lack of convergence of the approximation that is not at least locally uniform in the underlying data generating process may cause resampling methods to break down.   Some specific problems addressed include Tobit regression and linear regression with weak instruments.  Moreover, resampling methods do not enjoy exact finite-sample validity, which is perhaps the main reason permutation and rank tests are so commonly used in many fields, such as medical studies.  The investigators apply randomization tests to many new problems that statisticians face, despite issues of high dimensionality, simultaneous inference, unknown dependence structures, non-Gaussianity, etc.  An exciting feature of the approach is that, properly constructed, randomization tests enjoy good robustness properties in situations where the assumptions guaranteeing finite-sample validity may fail.  Mathematical theory is developed as well as feasible computational constructs.\r\n\r\nUseful statistical methodology is the key tool to analyzing any study or scientific experiment.  Recently, the demand for efficient and reliable confirmatory statistical methods has grown rapidly, driven by problems arising in the analysis of DNA microarray biotechnology, econometrics, finance, educational evaluation, global warming, and astronomy, as well as many others.  In general, the philosophical approach is to develop practical methods that have both robustness of validity and robustness of efficiency so that they may be applied in increasingly complex situations as the scope of modern data analysis continues to grow.  The broader impact of this work is potentially quite large because the resulting inferential tools can be applied to such diverse fields as genetics, bioengineering, image processing and neuroimaging, clinical trials, education, astronomy, finance and econometrics. The results will be widely disseminated, and public software of new statistical tools made accessible whenever possible. The many thriving fields of applications demand new statistical methods, creating challenging and exciting opportunities for young scholars under the direction of the investigators.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Romano",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph P Romano",
   "pi_email_addr": "romano@stanford.edu",
   "nsf_id": "000197385",
   "pi_start_date": "2013-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "CA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main goal of my research is to develop new inferenetial methods for analyzing data which do not rely on unrealistic or unverifiable assumptions, so that any reported conclusions have both statistical validity and speak to the reproducibility of any scientific claims. &nbsp;Virtually any scientific experiment sets out to&nbsp;answer &nbsp;questions about the process under investigation, which&nbsp;often can &nbsp;be translated formally into a set of statistical &nbsp;hypotheses.It is the exception that a single hypothesis is considered. For example, in clinical trials, even a &nbsp;single treatment &nbsp;may be evaluated using multiple outcome measures, &nbsp;multiple time points, multiple doses, and multiple subgroups (depending on covariates). Moreover, due to effects of ``data snooping'' (or ``data mining''), additional hypotheses &nbsp;arise as well. The statistician is then faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any resulting inferences or interesting conclusions can reliably be viewed as real structure rather than artifacts of random data. &nbsp;Driven by the availability of larger data sets in the information age,there is a growing demand for more reliable and efficient methods for multiple testing. For example, current methods in biotechnology and genomics generate DNA microarray experiments, where expression levels in cells for thousands of genes must be analyzed simultaneously.I t is now not uncommon to encounter data consisting of many megabytes of information. Thus, the statistician is faced with new challenges of devising techniques that are not based on strong assumptions and can effectively deal with problems of multiplicity, complex dependencies, unknown model structure, heteroskedasticity and more. &nbsp;In general, the philosophical approach is the development of practical methods that may be applied in increasingly complex situations as the scope of modern data analysis continues to grow. The broader impact of this work is potentially quite large because the resulting inferential tools can be applied to such diverse fields as genetics, bioengineering, image processing and neuroimaging, clinical trials, education, climate change and climatology, &nbsp;finance and econometrics. &nbsp;The many burgeoning fields of applications demand new statistical methods, and many new methods have been developed.</p>\n<p>In order to accomplish these goals, computer-intensive methods are exploited as these methods are seen to be more reliable and do not depend on unrealistic assumptions about how the data was generated. &nbsp;In particular, the main tools are resampling methods, including randomization and permutation methods, the bootstrap and subsampling. &nbsp;In modern high dimensional \"big\" data settings, randomizations are perhaps the only choice that offer guaranteed error control, but it is also important to understand when their use may not be warranted. This research has developed new tools in a mathematically rigorous way, but also provides methods which are computationally feasible. &nbsp; &nbsp;The methods have been specifically applied to important scientific questions, such as whether or not there has been a hiatus in global warming, which of interest to scientists and the public as well. &nbsp;Further application areas include clinical trials and drug development, as well as inference in education studies.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/03/2016<br>\n\t\t\t\t\tModified by: Joseph&nbsp;P&nbsp;Romano</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe main goal of my research is to develop new inferenetial methods for analyzing data which do not rely on unrealistic or unverifiable assumptions, so that any reported conclusions have both statistical validity and speak to the reproducibility of any scientific claims.  Virtually any scientific experiment sets out to answer  questions about the process under investigation, which often can  be translated formally into a set of statistical  hypotheses.It is the exception that a single hypothesis is considered. For example, in clinical trials, even a  single treatment  may be evaluated using multiple outcome measures,  multiple time points, multiple doses, and multiple subgroups (depending on covariates). Moreover, due to effects of ``data snooping'' (or ``data mining''), additional hypotheses  arise as well. The statistician is then faced with the challenge of accounting for all possible errors resulting from a complex data analysis, so that any resulting inferences or interesting conclusions can reliably be viewed as real structure rather than artifacts of random data.  Driven by the availability of larger data sets in the information age,there is a growing demand for more reliable and efficient methods for multiple testing. For example, current methods in biotechnology and genomics generate DNA microarray experiments, where expression levels in cells for thousands of genes must be analyzed simultaneously.I t is now not uncommon to encounter data consisting of many megabytes of information. Thus, the statistician is faced with new challenges of devising techniques that are not based on strong assumptions and can effectively deal with problems of multiplicity, complex dependencies, unknown model structure, heteroskedasticity and more.  In general, the philosophical approach is the development of practical methods that may be applied in increasingly complex situations as the scope of modern data analysis continues to grow. The broader impact of this work is potentially quite large because the resulting inferential tools can be applied to such diverse fields as genetics, bioengineering, image processing and neuroimaging, clinical trials, education, climate change and climatology,  finance and econometrics.  The many burgeoning fields of applications demand new statistical methods, and many new methods have been developed.\n\nIn order to accomplish these goals, computer-intensive methods are exploited as these methods are seen to be more reliable and do not depend on unrealistic assumptions about how the data was generated.  In particular, the main tools are resampling methods, including randomization and permutation methods, the bootstrap and subsampling.  In modern high dimensional \"big\" data settings, randomizations are perhaps the only choice that offer guaranteed error control, but it is also important to understand when their use may not be warranted. This research has developed new tools in a mathematically rigorous way, but also provides methods which are computationally feasible.    The methods have been specifically applied to important scientific questions, such as whether or not there has been a hiatus in global warming, which of interest to scientists and the public as well.  Further application areas include clinical trials and drug development, as well as inference in education studies.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/03/2016\n\n\t\t\t\t\tSubmitted by: Joseph P Romano"
 }
}