{
 "awd_id": "1319412",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Speeding Up Learning through Modeling the Pragmatics of Training",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Weng-keen Wong",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 135000.0,
 "awd_amount": 135000.0,
 "awd_min_amd_letter_date": "2013-09-09",
 "awd_max_amd_letter_date": "2014-07-25",
 "awd_abstract_narration": "Years of effort to develop algorithms capable of learning from reward signals have resulted in a plethora of techniques that can leverage numerical signals that vary in value based on performance. Recent efforts to use these techniques to learn from humans providing rewards have been slower to progress, in part, because humans give feedback discretely rather than numerically. This project contributes new learning algorithms designed specifically to leverage the information contained in the choices humans make to provide such discrete feedbacks. The algorithms are inspired by the human-canine partnership, and the incredible things that humans are able to teach dogs using only discrete feedback and carefully constructed sequences of tasks. The Bayesian learning framework being developed in this project will leverage the pragmatic implicatures contained in the feedbacks and tasks sequences to learn more quickly from human feedback. \r\n\r\nThe ultimate goal of this work is to provide a more natural paradigm for humans to tell computers what they would like for them to do. To that end, project efforts will result in a teaching module for Brown University?s Learning Exchange (LE). The LE involves undergraduates working with underserved minority middle school students to engage them in STEM. They are a perfect audience to demonstrate the broader impacts of this work. LE participants learn to instruct computers using a combination of programming with the Scratch environment and the feedback paradigm, which shows how powerful the algorithms are.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Taylor",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew Taylor",
   "pi_email_addr": "taylorm@eecs.wsu.edu",
   "nsf_id": "000560224",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Washington State University",
  "inst_street_address": "240 FRENCH ADMINISTRATION BLDG",
  "inst_street_address_2": "",
  "inst_city_name": "PULLMAN",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "5093359661",
  "inst_zip_code": "991640001",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "WA05",
  "org_lgl_bus_name": "WASHINGTON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "XRJSGX384TD6"
 },
 "perf_inst": {
  "perf_inst_name": "Washington State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "991643140",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "WA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 83715.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 51285.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Despite years of work developing more intuitive programming languages, many people are unable to code even simple programs on today's computers. While there has been some advances in this direction, such as creating AI-techniques like programming by demonstration and learning from demonstration, novices are generally unable to create novel computer behaviors. While the problem of learning from human feedback has been studied previously, developing a formalism for computers to learn from humans which views feedback as explicit, intentional, discrete communication is novel.</p>\n<p>Our paradigm is a unique take on the problem of learning from human feedback, which contrasts with existing approaches heavily rooted in the field of reinforcement learning (RL). RL-styled learning algorithms consider training feedback as numerically valued, which has been shown to be fundamentally limiting in this paradigm. Acknowledging, modeling, and explicitly incorporating these insights into learning algorithms will enable techniques to learn with fewer examples and in way&nbsp;that human trainers can intuitively leverage. Expert and non-expert humans regularly train dogs to perform complex tasks, such as guiding the blind, herding livestock, and locating narcotics. The core contribution of this work is operationalizing the concept that training is an intentional communicative act, and by exploiting the implied communication within training techniques, novel algorithms for learning from human teachers can increase the ability of humans to shape computer behavior without relying on programming. Accordingly, the specific contributions of this project include:</p>\n<ul>\n<li>An algorithm, called Strategy Aware Behavior Learning (SABL), for learning and leveraging the implied communication in training feedback. In particular, SABL is able to learn in the absence of feedback when context enables it to infer whether no feedback is an implied positive or negative reward.&nbsp;</li>\n<li>An empirical analysis of human trainer performance teaching SABL in a simulated environment comparing the performance of trainers with different skills at animal training. The results of these studies challenged existing assumptions about the relationships of agent action selection and human feedback.&nbsp;</li>\n<li>Identifying that human feedbacks can change based on the learning agent's current behavior in addition to the behavior being taught resulted in the COnvergent Actor-Critic with Humans (COACH) algorithm; prior work has always assumed that human feedback is independent of the learner's current behaviors, and is based solely on the behavior being taught. By challenging this assumption and implementing COACH, we have drastically increased the capabilities for machine learnings to benefit from information implied by the training curriculum design.</li>\n</ul>\n<p>Creating novel computational tools for humans to intuitively and quickly teach computers what they expect of them will enable non-programmers to contribute to our increasingly-computational culture. Beyond technology development, this project supported the education of multiple graduate students. It also supported undergraduate outreach. Results were disseminated broadly, published in top peer-reviewed publications in Artificial Intelligence and Machine Learning (e.g., AAAI, ICML, and NIPS).&nbsp;Overall the project resulted in 13 publications across a variety of top-quality peer-reviewed journal, conference, and workshop publication venues.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/10/2017<br>\n\t\t\t\t\tModified by: Matthew&nbsp;Taylor</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDespite years of work developing more intuitive programming languages, many people are unable to code even simple programs on today's computers. While there has been some advances in this direction, such as creating AI-techniques like programming by demonstration and learning from demonstration, novices are generally unable to create novel computer behaviors. While the problem of learning from human feedback has been studied previously, developing a formalism for computers to learn from humans which views feedback as explicit, intentional, discrete communication is novel.\n\nOur paradigm is a unique take on the problem of learning from human feedback, which contrasts with existing approaches heavily rooted in the field of reinforcement learning (RL). RL-styled learning algorithms consider training feedback as numerically valued, which has been shown to be fundamentally limiting in this paradigm. Acknowledging, modeling, and explicitly incorporating these insights into learning algorithms will enable techniques to learn with fewer examples and in way that human trainers can intuitively leverage. Expert and non-expert humans regularly train dogs to perform complex tasks, such as guiding the blind, herding livestock, and locating narcotics. The core contribution of this work is operationalizing the concept that training is an intentional communicative act, and by exploiting the implied communication within training techniques, novel algorithms for learning from human teachers can increase the ability of humans to shape computer behavior without relying on programming. Accordingly, the specific contributions of this project include:\n\nAn algorithm, called Strategy Aware Behavior Learning (SABL), for learning and leveraging the implied communication in training feedback. In particular, SABL is able to learn in the absence of feedback when context enables it to infer whether no feedback is an implied positive or negative reward. \nAn empirical analysis of human trainer performance teaching SABL in a simulated environment comparing the performance of trainers with different skills at animal training. The results of these studies challenged existing assumptions about the relationships of agent action selection and human feedback. \nIdentifying that human feedbacks can change based on the learning agent's current behavior in addition to the behavior being taught resulted in the COnvergent Actor-Critic with Humans (COACH) algorithm; prior work has always assumed that human feedback is independent of the learner's current behaviors, and is based solely on the behavior being taught. By challenging this assumption and implementing COACH, we have drastically increased the capabilities for machine learnings to benefit from information implied by the training curriculum design.\n\n\nCreating novel computational tools for humans to intuitively and quickly teach computers what they expect of them will enable non-programmers to contribute to our increasingly-computational culture. Beyond technology development, this project supported the education of multiple graduate students. It also supported undergraduate outreach. Results were disseminated broadly, published in top peer-reviewed publications in Artificial Intelligence and Machine Learning (e.g., AAAI, ICML, and NIPS). Overall the project resulted in 13 publications across a variety of top-quality peer-reviewed journal, conference, and workshop publication venues.\n\n \n\n\t\t\t\t\tLast Modified: 04/10/2017\n\n\t\t\t\t\tSubmitted by: Matthew Taylor"
 }
}