{
 "awd_id": "1318798",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Efficient Techniques for Modular Past State Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2013-09-09",
 "awd_max_amd_letter_date": "2013-09-09",
 "awd_abstract_narration": "Retrospection is the ability of a data store to run ad-hoc programs over consistent past states of a data store as if they were the current state. Retrospection makes it easier to analyze past states providing  a valuable tool for auditors, historians, economists, social scientists and others with a need to investigate  historical data. Retrospection is also valuable to those who want to analyze past states to predict the future, an increasingly in-demand feature in modern data management applications.\r\n\r\nMost light-weight data stores today do not support  retrospection. The key reason is that existing retrospection techniques, for performance, require invasive hard-to-adopt modifications to data store internals. Without support for retrospection, it may be hard for application developers to reconstruct the consistent states corresponding to past events of interest.\r\n\r\nThis project will develop an easy-to-adopt modular method and a set of associated techniques for supporting retrospection in light-weight transactional data stores using an\r\nembedded persistent consistent past-state system. The technical challenges are:\r\n1)\tHow to provide consistent past states without harming data store performance? The past state system needs to be tightly integrated for efficiency but extensive modifications to the internal data store components are infeasible, requiring new modular  techniques that operate at a low-level in the data store software stack.\r\n2)\tHow to run programs efficiently over past state that spans large time intervals? past state needs to be created  incrementally to avoid disrupting the data store but running a program over incremental state can be slow, requiring new clustering and caching  techniques optimized for incremental data.\r\n3)\tCan one avoid slowing down programs that do not use retrospection? To evaluate any additional overhead, the project will develop an experimental prototype in an industrial strength data store, and conduct studies to answer this question experimentally and analytically.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Liuba",
   "pi_last_name": "Shrira",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Liuba Shrira",
   "pi_email_addr": "Liuba@cs.brandeis.edu",
   "nsf_id": "000100143",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brandeis University",
  "inst_street_address": "415 SOUTH ST",
  "inst_street_address_2": "",
  "inst_city_name": "WALTHAM",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "7817362121",
  "inst_zip_code": "024532728",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "BRANDEIS UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MXLZGAMFEKN5"
 },
 "perf_inst": {
  "perf_inst_name": "Brandeis University",
  "perf_str_addr": "415 South Street",
  "perf_city_name": "Waltham",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "024549110",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Now more than ever, auditing and fact-checking are essential for applications that manage persistent state. It is not difficult to support queries known in advance to be of long- term interest, but it is difficult to support unanticipated, ad-hoc queries on long-ago states.&nbsp;</span></p>\n<p><span>Retrospection is the ability of a data store to run ad-hoc programs over consistent past states as if they were the current state.&nbsp;</span>Retrospection makes is easier for developers to provide auditing and fact checking but in current light-weight data stores the support for retrospection is limited. Without adequate support it is hard for developers to reconstruct the consistent past states corresponding to the events of interest. Today retrospection is available in&nbsp; temporal data bases used by specialized applications but if regular stores could support retrospection, auditing could become easily available to all applications.&nbsp;</p>\n<p><span>This project developed an easy-to-adopt method and a set of associated techniques for supporting efficient retrospection in light-weight transactional data stores using an embeded persistent consistent snapshot system.&nbsp;</span>In such a data store, an ad-hoc query to reconstruct the context of a past event, or a request to check claims about series of past events, can be computed from the stored snapshots as easily as computing over the current state.</p>\n<p><span>Adding a consistent snapshot system needed for retrospection can disrupt data store performance. Current techniques for avoiding performance disruption </span><span>require invasive modifications of the data store internals, a significant challenge for today?s systems. A key novelty of the approach in Retro, the system developed by this project for supporting retrospection&nbsp; in Berkeley DB (BDB), a&nbsp; widely used data store,&nbsp; is an efficient yet simple and robust implementation method based on a low-level snapshot system.&nbsp;</span>Unlike prior approaches, Retro protocols, based on a formal specification,&nbsp; extend standard transaction protocols in a modular way, requiring minimal data store modification (about 250 lines of BDB code). Importantly, they impose minimal 4% worst-case overhead for in-production programs running in the data store.</p>\n<p>An attractive retrospection feature for developers is that application code base can be used to analyze any snapshot. Auditing however, requires to analyze multiple snapshots. Current snapshot systems offer no satisfactory support for computations that analyze multiple snapshots, requiring developers to write low-level scripts combining snapshot programs. Retro provides a new Retrospective Query Language (RQL), a declarative extension to SQL, that allows to specify and run multi-snapshot computations conveniently, using a small number of&nbsp; mechanisms defined in terms of relational constructs familiar to developers. The mechanisms support both common analysis patterns and general computations and are translated into SQL &nbsp;using SQLite UDF framework.&nbsp;</p>\n<p>RQL programs running over low-level snapshots bring up new performance issues that have not been studied before. &nbsp;An important new feature in RQL&nbsp; is its analytical performance model that characterizes the performance of a computation over&nbsp; multiple low-level snapshots, in terms familiar to developers, relative to the current state program performance.</p>\n<p>RQL programs&nbsp; can be costly. Standard SQL optimizer can be used to improve RQL program performance within each snapshot but it can not detect redundancies that occur between snapshots. These redundancies occur when data of interest to audit remains unchanged between snapshots causing exact same duplicate computations to be repeated in multiple snapshots. The problem in&nbsp; prominent in common application workloads where parts of data change slowly, leading to waste of resources and energy.&nbsp;</p>\n<p><span>The project developed RID, a novel optimizer&nbsp; that eliminates duplicate snapshot computations. RID composes with standard SQL optimizer, boosting its effectiveness for multi-snapshot RQL programs substantially. RID operates at two levels.&nbsp;</span>A low-level detector, implemented in the snapshot system, exploits snapshot system metadata to identify duplicate computations. A high-level eliminator, avoids duplicate computations, instead reusing duplicate results efficiently, taking advantage of RQL query semantics.&nbsp;</p>\n<p><span>The low-level duplicate detector algorithm is remarkably fast but coarse, it can miss some duplicates, reporting false negatives.&nbsp;</span>A new contribution of RID&nbsp; is an analytical model that explains how application update workload determines the amount of false negatives. Measurements of RQL programs optimized with RID, using synthetic TPC-H workloads corresponding to expected use cases, validate the model, and show that, when an audit programs inspect infrequently modified data, RID provides substantial, sometimes more than 90% reduction in cost, despite coarse detection. &nbsp;</p>\n<p>The retrospection method and techniques developed by the project were implemented in a SQL-based transactional data store but the approach&nbsp; is more general. The modular snapshot implementation techniques in Retro, the general structure of RQL mechanisms, and RID duplicate detection are language-independent and support other programming languages. Moreover, while RQL implementation and RID result reuse are specialized to SQL, similar approaches are applicable to other languages.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/28/2019<br>\n\t\t\t\t\tModified by: Liuba&nbsp;Shrira</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nNow more than ever, auditing and fact-checking are essential for applications that manage persistent state. It is not difficult to support queries known in advance to be of long- term interest, but it is difficult to support unanticipated, ad-hoc queries on long-ago states. \n\nRetrospection is the ability of a data store to run ad-hoc programs over consistent past states as if they were the current state. Retrospection makes is easier for developers to provide auditing and fact checking but in current light-weight data stores the support for retrospection is limited. Without adequate support it is hard for developers to reconstruct the consistent past states corresponding to the events of interest. Today retrospection is available in  temporal data bases used by specialized applications but if regular stores could support retrospection, auditing could become easily available to all applications. \n\nThis project developed an easy-to-adopt method and a set of associated techniques for supporting efficient retrospection in light-weight transactional data stores using an embeded persistent consistent snapshot system. In such a data store, an ad-hoc query to reconstruct the context of a past event, or a request to check claims about series of past events, can be computed from the stored snapshots as easily as computing over the current state.\n\nAdding a consistent snapshot system needed for retrospection can disrupt data store performance. Current techniques for avoiding performance disruption require invasive modifications of the data store internals, a significant challenge for today?s systems. A key novelty of the approach in Retro, the system developed by this project for supporting retrospection  in Berkeley DB (BDB), a  widely used data store,  is an efficient yet simple and robust implementation method based on a low-level snapshot system. Unlike prior approaches, Retro protocols, based on a formal specification,  extend standard transaction protocols in a modular way, requiring minimal data store modification (about 250 lines of BDB code). Importantly, they impose minimal 4% worst-case overhead for in-production programs running in the data store.\n\nAn attractive retrospection feature for developers is that application code base can be used to analyze any snapshot. Auditing however, requires to analyze multiple snapshots. Current snapshot systems offer no satisfactory support for computations that analyze multiple snapshots, requiring developers to write low-level scripts combining snapshot programs. Retro provides a new Retrospective Query Language (RQL), a declarative extension to SQL, that allows to specify and run multi-snapshot computations conveniently, using a small number of  mechanisms defined in terms of relational constructs familiar to developers. The mechanisms support both common analysis patterns and general computations and are translated into SQL  using SQLite UDF framework. \n\nRQL programs running over low-level snapshots bring up new performance issues that have not been studied before.  An important new feature in RQL  is its analytical performance model that characterizes the performance of a computation over  multiple low-level snapshots, in terms familiar to developers, relative to the current state program performance.\n\nRQL programs  can be costly. Standard SQL optimizer can be used to improve RQL program performance within each snapshot but it can not detect redundancies that occur between snapshots. These redundancies occur when data of interest to audit remains unchanged between snapshots causing exact same duplicate computations to be repeated in multiple snapshots. The problem in  prominent in common application workloads where parts of data change slowly, leading to waste of resources and energy. \n\nThe project developed RID, a novel optimizer  that eliminates duplicate snapshot computations. RID composes with standard SQL optimizer, boosting its effectiveness for multi-snapshot RQL programs substantially. RID operates at two levels. A low-level detector, implemented in the snapshot system, exploits snapshot system metadata to identify duplicate computations. A high-level eliminator, avoids duplicate computations, instead reusing duplicate results efficiently, taking advantage of RQL query semantics. \n\nThe low-level duplicate detector algorithm is remarkably fast but coarse, it can miss some duplicates, reporting false negatives. A new contribution of RID  is an analytical model that explains how application update workload determines the amount of false negatives. Measurements of RQL programs optimized with RID, using synthetic TPC-H workloads corresponding to expected use cases, validate the model, and show that, when an audit programs inspect infrequently modified data, RID provides substantial, sometimes more than 90% reduction in cost, despite coarse detection.  \n\nThe retrospection method and techniques developed by the project were implemented in a SQL-based transactional data store but the approach  is more general. The modular snapshot implementation techniques in Retro, the general structure of RQL mechanisms, and RID duplicate detection are language-independent and support other programming languages. Moreover, while RQL implementation and RID result reuse are specialized to SQL, similar approaches are applicable to other languages.\n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/28/2019\n\n\t\t\t\t\tSubmitted by: Liuba Shrira"
 }
}