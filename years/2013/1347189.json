{
 "awd_id": "1347189",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Eager:  HPC Virtualization with SR-IOV",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2014-09-30",
 "tot_intn_awd_amt": 98291.0,
 "awd_amount": 98291.0,
 "awd_min_amd_letter_date": "2013-09-05",
 "awd_max_amd_letter_date": "2013-09-05",
 "awd_abstract_narration": "The recently introduced Single Root I/O Virtualization (SR-IOV)\r\ntechnique for InfiniBand and High Speed Ethernet provides native I/O\r\nvirtualization capabilities and enables us to provision the internal\r\nPCI bus interface between multiple Virtual Machines (VMs). However,\r\nachieving near native throughput for HPC applications that use both\r\npoint-to-point and collective operations on virtualized multi-core\r\nsystems with SR-IOV presents a new set of challenges for the designers\r\nof high performance middleware, such as MPI.  In order to solve this\r\nproblem, this project aims to address the following set of challenges:\r\n1) How to redesign MPI communication library to achieve efficient\r\nlocality-aware communication and facilitate fair resource sharing on\r\nmodern virtualized high performance clusters, with SR-IOV? 2) Can\r\ncommunication libraries be designed to deliver the best communication\r\nperformance across different VM subscription policies and network\r\ncommunication modes? 3) What are the the challenges involved in\r\ndesigning support for advanced features such as, live migration,\r\nQuality of Service, and I/O storage virtualization?  and 4) What kind\r\nof benefits, in terms of performance and scalability, can be achieved\r\nby the proposed approach for HPC applications? A synergistic and\r\ncomprehensive research plan is proposed to address the above\r\nchallenges for HPC Virtualization on clusters with SR-IOV and study\r\nits impact for a set of HPC applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dhabaleswar",
   "pi_last_name": "Panda",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Dhabaleswar K Panda",
   "pi_email_addr": "panda.2@osu.edu",
   "nsf_id": "000487085",
   "pi_start_date": "2013-09-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101206",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 98291.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Virtualized clusters with multi-core processors, high-performance<br />interconnects/protocols (such as InfiniBand, RDMA over Converged Enhanced Ethernet (RoCE) and 10-Gigabit Ethernet with iWARP), and Solid State Drives (SSDs) have been widely used in the enterprise computing domain for easy system management and efficient resource utilization. But in the High Performance Computing (HPC) domain, the adoption of virtualization still remains lower. One of the biggest hurdles in realizing this objective comes from lower performance of virtualized I/O devices, offered by virtualized computing environments. The recently introduced Single Root I/O Virtualization (SR-IOV) technique for InfiniBand and High Speed Ethernet provides native I/O virtualization capabilities and enables us to provision the internal PCI bus interfaces among multiple virtual machines.&nbsp; However, achieving near native throughput for HPC applications that use both point-to-point and collective operations in MPI on the upcoming virtualized multi-/many-core systems presents a new set of challenges for the designers of high performance middleware.</p>\n<p>To address the challenges for this project, we have investigated and designed novel approaches to utilize the underlying system capabilities to improve MPI communication and application performance and scalability with SR-IOV. The main goals we have achieved for this project are categorized as: 1) Efficient network virtualization with SR-IOV for MPI; 2) Locality aware communication for point-to-point and collective operations; 3) Integrated evaluation with HPC applications; and 4) Exploratory researches on storage virtualization and I/O scheduling, end-to-end Quality of Service, and live migration.&nbsp; We have designed and implemented locality aware point-to-point/collective communication for high-performance MVAPICH2 library over SR-IOV enabled InfiniBand clusters. We have systematically evaluated our design with MPI point-to-point, RMA,<br />collectives operations and typical HPC end applications or benchmarks. Compared with MPI over default SR-IOV, our design can significantly improve MPI communication performance in virtualized environments. The improvement for point-to-point can be up to 84% and 158% in terms of latency and bandwidth. In the meanwhile, our design can also deliver near native performance with only 3%-8% overhead in virtualized environments. For different message sizes, our design can improve the latency of various collectives, such as MPI_Bcast, MPI_Allgather, MPI_Allreduce, and MPI_Alltoall, by up to 68%, 76%, 61%, 29%, respectively. For NAS and P3DFFT, the benefits of our design brings can be up to 43% and 33%, respectively.&nbsp; We have also done many studies by technical survey and experiments with our new design for exploratory research tasks.</p>\n<p>The new SR-IOV-based designs are planned to be integrated into the MVAPICH2 library and made available to the HPC Virtualization community in the near future.&nbsp; In addition to the software distribution, the results have been presented at various conferences and events, such as Euro-Par &rsquo;14, HiPC &rsquo;14, and HP-CAST &rsquo;14.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/07/2015<br>\n\t\t\t\t\tModified by: Dhabaleswar&nbsp;K&nbsp;Panda</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nVirtualized clusters with multi-core processors, high-performance\ninterconnects/protocols (such as InfiniBand, RDMA over Converged Enhanced Ethernet (RoCE) and 10-Gigabit Ethernet with iWARP), and Solid State Drives (SSDs) have been widely used in the enterprise computing domain for easy system management and efficient resource utilization. But in the High Performance Computing (HPC) domain, the adoption of virtualization still remains lower. One of the biggest hurdles in realizing this objective comes from lower performance of virtualized I/O devices, offered by virtualized computing environments. The recently introduced Single Root I/O Virtualization (SR-IOV) technique for InfiniBand and High Speed Ethernet provides native I/O virtualization capabilities and enables us to provision the internal PCI bus interfaces among multiple virtual machines.  However, achieving near native throughput for HPC applications that use both point-to-point and collective operations in MPI on the upcoming virtualized multi-/many-core systems presents a new set of challenges for the designers of high performance middleware.\n\nTo address the challenges for this project, we have investigated and designed novel approaches to utilize the underlying system capabilities to improve MPI communication and application performance and scalability with SR-IOV. The main goals we have achieved for this project are categorized as: 1) Efficient network virtualization with SR-IOV for MPI; 2) Locality aware communication for point-to-point and collective operations; 3) Integrated evaluation with HPC applications; and 4) Exploratory researches on storage virtualization and I/O scheduling, end-to-end Quality of Service, and live migration.  We have designed and implemented locality aware point-to-point/collective communication for high-performance MVAPICH2 library over SR-IOV enabled InfiniBand clusters. We have systematically evaluated our design with MPI point-to-point, RMA,\ncollectives operations and typical HPC end applications or benchmarks. Compared with MPI over default SR-IOV, our design can significantly improve MPI communication performance in virtualized environments. The improvement for point-to-point can be up to 84% and 158% in terms of latency and bandwidth. In the meanwhile, our design can also deliver near native performance with only 3%-8% overhead in virtualized environments. For different message sizes, our design can improve the latency of various collectives, such as MPI_Bcast, MPI_Allgather, MPI_Allreduce, and MPI_Alltoall, by up to 68%, 76%, 61%, 29%, respectively. For NAS and P3DFFT, the benefits of our design brings can be up to 43% and 33%, respectively.  We have also done many studies by technical survey and experiments with our new design for exploratory research tasks.\n\nThe new SR-IOV-based designs are planned to be integrated into the MVAPICH2 library and made available to the HPC Virtualization community in the near future.  In addition to the software distribution, the results have been presented at various conferences and events, such as Euro-Par \u00c614, HiPC \u00c614, and HP-CAST \u00c614.\n\n\t\t\t\t\tLast Modified: 01/07/2015\n\n\t\t\t\t\tSubmitted by: Dhabaleswar K Panda"
 }
}