{
 "awd_id": "1309174",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Advancing Theory and Computation in Statistical Learning Problems",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2013-07-01",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2013-06-27",
 "awd_max_amd_letter_date": "2015-05-19",
 "awd_abstract_narration": "This research is composed of four related statistical learning projects. The first two projects are theoretical. In the first, the investigator will study of degrees of freedom (i.e., the effective number of parameters) of adaptive modeling techniques. It has been shown that variable selection procedures based on the L1 norm, such as the lasso, exhibit control over their effective number of parameters, since adaptivity here is counterbalanced by shrinkage in coefficient estimation. This project instead considers adaptive procedures that do not employ shrinkage, such as best subset selection, in which the effective number of parameters is (comparatively) greatly inflated. In the second project, the investigator will examine trend filtering, a recently proposed nonparametric regression estimator fit by penalizing the L1 norm of discrete derivatives. Trend filtering estimates can be computed efficiently (e.g., using the work of the third project), but their theoretical properties are not well-understood. The goal is to study the rate of convergence of trend filtering estimates over broad function classes, and make detailed comparisons to existing nonparametric regression estimators (such as smoothing splines, locally adaptive regression splines, etc.). The last two projects are computational. The third project is focused on efficient computations for the generalized lasso path algorithm. The generalized lasso is an estimator that encourages specific structural properties, as opposed to pure sparsity itself, using the L1 norm; one such example is the trend filtering estimator mentioned above. The fourth and final project is an extension of the idea behind stagewise regression to general convex regularization problems. Forward stagewise regression is a simple, scalable algorithm whose estimates can be seen as an approximation to the lasso regularization path. The stagewise extension to general problems produces efficient approximation algorithms for the group lasso, matrix completion, and more; approximation guarantees are unknown and will be studied.  \r\n\r\nStatistical modeling, estimation, and inference are becoming integral aspects of problems in many scientific disciplines. As a result, the field of statistical learning---which broadly encapsulates these three statistical tasks---has witnessed a recent explosion of research. Arguably, current research in this field focuses on creating new methods or extending methods to new domains, and much less so on understanding existing methods. Instead, the investigator will pursue four projects aimed at (i) deepening our understanding of a few well-known (but not as well-understood) statistical learning techniques, and (ii) developing algorithms so that we can employ these techniques efficiently at a larger scale, and hence evaluate their performance.  Code for such algorithms will be made freely available through open-source software.  Potential applications of this work include the forecasting of medical diagnoses, the modeling of brain signals in neuroscience, and the development of recommender systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ryan",
   "pi_last_name": "Tibshirani",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ryan Tibshirani",
   "pi_email_addr": "ryantibs@berkeley.edu",
   "nsf_id": "000622235",
   "pi_start_date": "2013-06-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 36871.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 56006.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 57123.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Statistical modeling, estimation, and inference are becoming integral aspects of problems in many scientific disciplines, for example, genetics, epidemiology, finance, language processing, automation and robotics,&nbsp;among many other areas. There are no signs of the progress and interest in statistical learning (a term that broadly encapsulates the three statistical areas---modeling, estimation, and inference---described above) slowing down, and if anything, it appears that progress is only accelerating and that statistics will play an increasingly large role in the development of science in the future.</p>\n<p>In this research, I have developed new statistical theories to help inform practitioners of the properties of key statistical methods, and the tradeoff involved in choosing one method over another. I have also developed new computational methods to enable the application of statistical techniques in problems of large size. This work was completed with Ph.D. students, who I trained in the process. The developed research could have impact on the course of study, both theoretically and computationally, within the fields of statistics and machine learning. It could also have impact in several other areas of science and industry, such as epidemiological forecasting, financial modeling, and online advertising. All of the&nbsp;developed research has been disseminated through journal or conference papers, and when appropriate, freely available software packages.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/11/2017<br>\n\t\t\t\t\tModified by: Ryan&nbsp;Tibshirani</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nStatistical modeling, estimation, and inference are becoming integral aspects of problems in many scientific disciplines, for example, genetics, epidemiology, finance, language processing, automation and robotics, among many other areas. There are no signs of the progress and interest in statistical learning (a term that broadly encapsulates the three statistical areas---modeling, estimation, and inference---described above) slowing down, and if anything, it appears that progress is only accelerating and that statistics will play an increasingly large role in the development of science in the future.\n\nIn this research, I have developed new statistical theories to help inform practitioners of the properties of key statistical methods, and the tradeoff involved in choosing one method over another. I have also developed new computational methods to enable the application of statistical techniques in problems of large size. This work was completed with Ph.D. students, who I trained in the process. The developed research could have impact on the course of study, both theoretically and computationally, within the fields of statistics and machine learning. It could also have impact in several other areas of science and industry, such as epidemiological forecasting, financial modeling, and online advertising. All of the developed research has been disseminated through journal or conference papers, and when appropriate, freely available software packages.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 09/11/2017\n\n\t\t\t\t\tSubmitted by: Ryan Tibshirani"
 }
}