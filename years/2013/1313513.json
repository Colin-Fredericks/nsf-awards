{
 "awd_id": "1313513",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Contextual Research-Empirical Research:Psychometric and Growth Modeling of Complex Patterns of Learning Resulting From the Interrelationships Between Multiple Learning Progressions",
 "cfda_num": "47.076",
 "org_code": "11090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Finbarr Sloane",
 "awd_eff_date": "2012-08-31",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 707775.0,
 "awd_amount": 707775.0,
 "awd_min_amd_letter_date": "2013-01-11",
 "awd_max_amd_letter_date": "2013-01-11",
 "awd_abstract_narration": "The project aims is to identify a set of psychometric and growth models that will permit researchers and curriculum developers to describe and test the interrelationships between students' progress along two or more learning progressions. Unlike the current thinking in the field, these psychometric and growth models will not assume linearity, continuity, or unidimensionality of student learning. The project proposes three goals: gathering evidence for the existence of non-linear patterns of learning that demonstrate branching of the learning progressions; gathering evidence for the existence of non-linear patterns of learning that demonstrate parallel learning progressions, and proposing the psychometric and growth models for these types of learning progressions. \r\n\r\nThe psychometric models will be developed to estimate the cognitive models underlying complex learning patterns. The work will progress in four phases: (1) exploring, statistically, existing datasets from NSF-funded projects containing simultaneous measures of learning on more than one learning progression; (2) testing the psychometric and growth models using the existing data; (3) collecting and analyzing new data; and (4) testing the psychometric and growth models using new data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DRL",
 "org_div_long_name": "Division of Research on Learning in Formal and Informal Settings (DRL)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nathaniel",
   "pi_last_name": "Brown",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nathaniel Brown",
   "pi_email_addr": "nathaniel.js.brown@bc.edu",
   "nsf_id": "000522107",
   "pi_start_date": "2013-01-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Boston College",
  "inst_street_address": "140 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHESTNUT HILL",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6175528000",
  "inst_zip_code": "024673800",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MA04",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "MJ3JH8CRJBZ7"
 },
 "perf_inst": {
  "perf_inst_name": "Boston College",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "024673804",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MA04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "762500",
   "pgm_ele_name": "REAL"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9177",
   "pgm_ref_txt": "ELEMENTARY/SECONDARY EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0411",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001112DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0412",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001213DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 107153.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 600622.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"standard\"><span style=\"font-family: 'Times New Roman'; font-size: small;\">The overarching goal of this project was to develop methods for better assessing and measuring science learning. To this end, our research produced three primary outcomes: (1) documentation of complex patterns of science learning that arise from interrelationships between multiple skills; (2) identification of a psychometric model that can describe and account for these patterns of learning; and (3) development of <span style=\"color: black;\">an analytic procedure for coding students&rsquo; scientific explanations that provides a rich picture of the complexities of student understanding.</span></span></p>\n<p class=\"standard\"><span style=\"font-family: 'Times New Roman'; font-size: small;\">As described in the National Research Council report <em>A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas</em> (2012), and as embodied by the <em>Next Generation Science Standards</em> (2013), student learning in science involves the development of multiple skills over many years. Learning progressions (LPs) are descriptions of how these skills develop over time and represent a new appreciation of the multidimensionality of student science learning. However, cognitive research suggests that learning in science is unlikely to be as linear, continuous, or independent across dimensions as some formulations of LPs imply.</span></p>\n<p class=\"standard\"><span style=\"font-family: 'Times New Roman'; font-size: small;\">To document complex patterns of science learning, we analyzed existing data sets from previous NSF-funded projects. <span style=\"color: black;\">These data were from science assessments that measured performance on holistic tasks that require multiple skills described by multiple LPs. For example, an assessment task might require both conceptual understanding and scientific reasoning. We demonstrated that, at higher levels of proficiency, student progress on these multiple LPs could be determined. At lower levels of proficiency, however, it was difficult if not impossible to disentangle the contributions of the different LPs. This is a complex pattern of learning called <em>branching</em>, in which </span>two or more skills can be highly dependent if not entirely conflated at lower levels of understanding, only to diverge and become increasingly independent as students become more proficient.</span></p>\n<p class=\"standard\"><span style=\"color: black; font-family: 'Times New Roman'; font-size: small;\">To account for this pattern of learning, we identified a novel method for modeling multidimensional assessment data. Traditional modeling of such data treats each LP as independent and ignores dependencies between scores on different LPs beyond simple correlation. This negatively impacts ability estimation, validity, and reliability. Our method involves a novel construct definition and the application of an existing psychometric model (within-item multidimensionality) in a novel way.</span></p>\n<p class=\"standard\"><span style=\"color: black; font-family: 'Times New Roman'; font-size: small;\">Using this method, we tested different psychometric models that represent different forms of branching and therefore imply different cognitive models of low-level responses. According to these models, low-level responses may indicate: (1) primarily poor understanding, holding back reasoning, (2) primarily poor reasoning, holding back understanding, or (3) a lack of a third skill holding back both understanding and reasoning. Our results indicate that the third model best fits the data, with this third skill only very weakly correlated with the other two (<em>r</em> &lt; 0.1). This third skill represents a minimum competence that must be achieved before the assessment tasks can begin to be performed well, or perhaps even understood.</span></p>\n<p class=\"standard\"><span style=\"font-family: 'Times New Roman'; font-size: small;\">Finally, to better describe and understand these complex patterns of learning, we developed an analytic procedure for coding students&rsquo; scientific explanations. Supporting this effort, we designed a novel assessment of chemical equilibrium and administered it to elementary, middle, and high school students in a cross-sectional and longitudinal design.</span></p>\n<p class=\"standard\"><span style=\"color: black; font-family: 'Times New Roman'; font-size: small;\">Typically, students&rsquo; scientific explanations are analyzed using a holistic rubric that identifies overall levels of quality. Such rubrics have achieved sufficient validity and reliability but have suffered from several drawbacks. For one, they capture only an overall sense of the student's understanding and do not provide very useful diagnostic information for teachers and students. For another, they have been quite difficult to instantiate in an automated scoring engine. In contrast, an analytic coding procedure is an explicit set of rules for identifying and classifying the individual elements of a student's response and how they are connected. By focusing on the parts that constitute the whole, a richer picture of student understanding can emerge, identifying both strengths and weaknesses in a student's understanding. In addition, by codifying the scoring procedure as a set of rules, the analysis has the potential to be automated by a scoring engine.</span></p>\n<p class=\"standard\"><span style=\"color: black; font-family: 'Times New Roman'; font-size: small;\">The procedure we developed processes written responses in three stages. First, the text is divided into sections that correspond to individual ideas using grammatical analysis. Second, the idea units are situated within a flowchart that represents the structure of the explanation using narrative analysis. Third, the idea units are categorized by content using keyword analysis. Together, these three stages deconstruct written responses and characterize their sophistication, complexity, and accuracy. This analytic coding procedure is an important step forward in the automated coding of written responses that takes into account the structure of and relationships between ideas, rather than just the content of those ideas.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/28/2016<br>\n\t\t\t\t\tModified by: Nathaniel&nbsp;Brown</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "The overarching goal of this project was to develop methods for better assessing and measuring science learning. To this end, our research produced three primary outcomes: (1) documentation of complex patterns of science learning that arise from interrelationships between multiple skills; (2) identification of a psychometric model that can describe and account for these patterns of learning; and (3) development of an analytic procedure for coding students? scientific explanations that provides a rich picture of the complexities of student understanding.\nAs described in the National Research Council report A Framework for K-12 Science Education: Practices, Crosscutting Concepts, and Core Ideas (2012), and as embodied by the Next Generation Science Standards (2013), student learning in science involves the development of multiple skills over many years. Learning progressions (LPs) are descriptions of how these skills develop over time and represent a new appreciation of the multidimensionality of student science learning. However, cognitive research suggests that learning in science is unlikely to be as linear, continuous, or independent across dimensions as some formulations of LPs imply.\nTo document complex patterns of science learning, we analyzed existing data sets from previous NSF-funded projects. These data were from science assessments that measured performance on holistic tasks that require multiple skills described by multiple LPs. For example, an assessment task might require both conceptual understanding and scientific reasoning. We demonstrated that, at higher levels of proficiency, student progress on these multiple LPs could be determined. At lower levels of proficiency, however, it was difficult if not impossible to disentangle the contributions of the different LPs. This is a complex pattern of learning called branching, in which two or more skills can be highly dependent if not entirely conflated at lower levels of understanding, only to diverge and become increasingly independent as students become more proficient.\nTo account for this pattern of learning, we identified a novel method for modeling multidimensional assessment data. Traditional modeling of such data treats each LP as independent and ignores dependencies between scores on different LPs beyond simple correlation. This negatively impacts ability estimation, validity, and reliability. Our method involves a novel construct definition and the application of an existing psychometric model (within-item multidimensionality) in a novel way.\nUsing this method, we tested different psychometric models that represent different forms of branching and therefore imply different cognitive models of low-level responses. According to these models, low-level responses may indicate: (1) primarily poor understanding, holding back reasoning, (2) primarily poor reasoning, holding back understanding, or (3) a lack of a third skill holding back both understanding and reasoning. Our results indicate that the third model best fits the data, with this third skill only very weakly correlated with the other two (r &lt; 0.1). This third skill represents a minimum competence that must be achieved before the assessment tasks can begin to be performed well, or perhaps even understood.\nFinally, to better describe and understand these complex patterns of learning, we developed an analytic procedure for coding students? scientific explanations. Supporting this effort, we designed a novel assessment of chemical equilibrium and administered it to elementary, middle, and high school students in a cross-sectional and longitudinal design.\nTypically, students? scientific explanations are analyzed using a holistic rubric that identifies overall levels of quality. Such rubrics have achieved sufficient validity and reliability but have suffered from several drawbacks. For one, they capture only an overall sense of the student's understanding and do not provide very useful diagnostic information for teachers and students. For another, they have been quite difficult to instantiate in an automated scoring engine. In contrast, an analytic coding procedure is an explicit set of rules for identifying and classifying the individual elements of a student's response and how they are connected. By focusing on the parts that constitute the whole, a richer picture of student understanding can emerge, identifying both strengths and weaknesses in a student's understanding. In addition, by codifying the scoring procedure as a set of rules, the analysis has the potential to be automated by a scoring engine.\nThe procedure we developed processes written responses in three stages. First, the text is divided into sections that correspond to individual ideas using grammatical analysis. Second, the idea units are situated within a flowchart that represents the structure of the explanation using narrative analysis. Third, the idea units are categorized by content using keyword analysis. Together, these three stages deconstruct written responses and characterize their sophistication, complexity, and accuracy. This analytic coding procedure is an important step forward in the automated coding of written responses that takes into account the structure of and relationships between ideas, rather than just the content of those ideas.\n\n\t\t\t\t\tLast Modified: 11/28/2016\n\n\t\t\t\t\tSubmitted by: Nathaniel Brown"
 }
}