{
 "awd_id": "1344269",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "INSPIRE Track 1:  Gradient Symbolic Computation",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Joan Maling",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2020-02-29",
 "tot_intn_awd_amt": 999997.0,
 "awd_amount": 1017997.0,
 "awd_min_amd_letter_date": "2013-09-12",
 "awd_max_amd_letter_date": "2015-05-07",
 "awd_abstract_narration": "This INSPIRE award is partially funded by the Linguistics Program and the Perception, Action & Cognition Program in the Division of Behavioral and Cognitive Sciences in the Directorate for Social, Behavioral & Economic Sciences; by the Robust Intelligence Program in the Division of Information & Intelligent Systems in the Directorate for Computer & Information Science & Engineering; and by the Algorithmic Foundations Program in the Division of Computer and Network Systems in the Directorate for Computer & Information Science & Engineering.\r\n\r\nDiscrete, combinatorial systems of structured symbols permeate human cognition in domains such as language, motor control, complex action planning, learning, and higher-level vision.  Nonetheless, the computational apparatus that the brain exploits is based on continuous, activation-based propagation of information through complex networks of neurons. A fundamental problem of the cognitive sciences is how to integrate gradient, continuous neural computation with the discrete combinatorial dimension of cognition. The solution to this puzzle will provide a deeper understanding of the mind and may also serve as the basis of a new generation of computing systems capable of authentically brain-like behavior.\r\n\r\nUnder the direction of Dr. Smolensky, the research team will develop an approach to this puzzle by exploring and testing the predictions of their theory of Gradient Symbolic Computation (GSC) in the domain of language. Their efforts will include the development of the formal, mathematical foundations of GSC. In parallel, the PIs will develop a framework for modeling Gradient Symbolic Processing. To that end, the PIs will use computational modeling and experimental psycholinguistic studies of phenomena that typify the morpho-phonological, syntactic, and semantic characteristics of language and language processing. \r\n\r\nThe broader impacts of the work include the potential to transform general computing for future approaches to computer design, to provide innovations in computer language processing, and to empower major advances in our understanding of human language, its impairment in disease, and its learning and remediation. The project also strongly engages STEM education. Undergraduate, graduate, and post-doctoral researchers will all play key roles in highly interdisciplinary STEM research integrating experimental, theoretical, and computational methods. The new type of computation created will provide an integrative framework for developing courses bridging computation theory, psychology, and linguistics. Pedagogical materials developed in these courses will be made publicly available to facilitate undergraduate and graduate program development at other institutions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "Smolensky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Paul Smolensky",
   "pi_email_addr": "smolensky@jhu.edu",
   "nsf_id": "000221798",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Geraldine",
   "pi_last_name": "Legendre",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Geraldine Legendre",
   "pi_email_addr": "legendre@jhu.edu",
   "nsf_id": "000370840",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Kyle",
   "pi_last_name": "Rawlins",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Kyle Rawlins",
   "pi_email_addr": "rawlins@cogsci.jhu.edu",
   "nsf_id": "000586455",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Van Durme",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin Van Durme",
   "pi_email_addr": "vandurme@cs.jhu.edu",
   "nsf_id": "000611034",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Akira",
   "pi_last_name": "Omaki",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Akira Omaki",
   "pi_email_addr": "omaki@uw.edu",
   "nsf_id": "000646527",
   "pi_start_date": "2013-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N Charles Street",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182685",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  },
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "792600",
   "pgm_ele_name": "ALGORITHMS"
  },
  {
   "pgm_ele_code": "807800",
   "pgm_ele_name": "INSPIRE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "7252",
   "pgm_ref_txt": "Perception, Action and Cognition"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "8089",
   "pgm_ref_txt": "Understanding the Brain/Cognitive Scienc"
  },
  {
   "pgm_ref_code": "8653",
   "pgm_ref_txt": "INSPIRE Track-1 Creative"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 833997.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 166000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 18000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 1\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p>Two seemingly contradictory fundamental ideas underpin work in the cognitive sciences. Linguistics and artificial intelligence traditionally rely on discrete <em>symbols</em>: hard-edged categories, with precise rules specifying how symbols are combined in complex arrangements. In contrast, neural computation -- as understood by neuroscience, psychology, and machine learning -- describes the mind, as well as the brain, in terms of continuous <em>activation levels</em>.&nbsp; This NSF INSPIRE project defines an amalgam of these two views: <strong>Gradient Symbolic Computation</strong> (<strong>GSC</strong>). In GSC, data consists of structures built of symbols that have continuously varying activation values. Computations over these Gradient Symbolic Representations are defined by continuous, dynamical neural network algorithms.</p>\n<p>We have used GSC's partially-activated symbols to advance the understanding of patterns in sound, word, and sentence structure, as well as sentence meaning. GSC provides traction on a frequently-occurring problem in the language sciences: apparent contradictions between two theories remain unresolved because each theory explains data the other cannot. In many cases, GSC allows such theories to be <em>blended together,</em> creating a single analysis that covers a wide range of data not previously explicable within a single theory.&nbsp;For example, in French, certain consonants appear only at the juncture of two special types of words. Theories of French phonology have long debated whether these consonants are at the end of Word1 or the beginning of Word2: each theory accounts for data that the other does not. GSC allows both possibilities to be partially true. Partially-active consonants appear both at the end of Word1 <em>and</em> the beginning of Word2. When the two words are brought together, these partial activations combine to form a single fully-active consonant. Algorithms for automatically learning the partial-activation values of these consonants from data have been developed and demonstrated to be effective. Other blended analyses we have developed address long-standing unresolved theoretical disputes concerning sentence structure (e.g., how event representations are built when verbs combine with phrases describing participants and properties, or small 'particles' like <em>up</em>) and bilingual code-mixing (where multiple linguistic systems are blended together within a single sentence). This work has appeared in prominent journals, and the general framework has been adopted by a growing number of researchers in the international scientific community: more than two dozen GSC analyses in formal linguistics were developed over the course of the grant period, and the number continues to grow.</p>\n<p>GSC has also substantially advanced our ability to computationally model human linguistic abilities. Using dynamic neural network algorithms, we have developed models that exploit Gradient Symbolic Representations to gradually build up complex representations of sentence structure. These models provide new insight into how brain-like systems can maintain partially activated sentence structures until the reader or listener has received enough information to commit to a particular sentence representation. GSC dynamic models have been proven to achieve their computational goals in principle, and many of the practical challenges arising in particular instantiations of these models have been overcome by innovative computational mechanisms.</p>\n<p>Models have also been developed, some in collaboration with researchers at Microsoft Research AI, that use Deep Learning in specially-designed neural networks to learn Gradient Symbolic Representations that are optimized for the needs of specific Natural Language Processing tasks in Artificial Intelligence. These tasks include question answering, image captioning, and problem-solving in mathematics and computer programming, and inference of missing information in knowledge bases. This new generation of model exhibits performance that sets a new state of the art in several problem domains. More importantly, the use of Gradient Symbolic Representations yields models that are more <strong>interpretable</strong> than standard Deep Learning models, which are notoriously opaque. New methods have also been developed to use GSC to interpret the learned encodings of information in key <em>standard</em> Deep Learning models, achieving a groundbreaking level of understanding of how particular deep models actually work.</p>\n<p>The substantial intellectual impacts of the award have gone hand-in-hand with development of technical &nbsp;and human resources. Publicly-available software for GSC modeling has been released. Training and research experience has been provided to undergraduate students, graduate students and postdoctoral fellows at multiple institutions across the United States and Europe. This occurred both in the context of conducting research within the project and through courses and tutorial workshops we held at both national and international venues. Students have received in-depth, hands-on training in linguistic analysis, computational modeling, experimental design, manuscript preparation, and statistical analysis techniques. The project has deepened institutional collaboration at Johns Hopkins between the schools of Engineering (Computer Science) and Arts and Sciences (Cognitive Science), and, via the PI's work with Microsoft Research, has allowed concepts from Cognitive Science to impact work in Natural Language Processing and AI.</p>\n<p>&nbsp;</p>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/30/2020<br>\n\t\t\t\t\tModified by: Paul&nbsp;Smolensky</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588259506246_Fig14fig-GP3-S2-actC-trace--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588259506246_Fig14fig-GP3-S2-actC-trace--rgov-800width.jpg\" title=\"GSC dynamical model state trajectory (all symbols)\"><img src=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588259506246_Fig14fig-GP3-S2-actC-trace--rgov-66x44.jpg\" alt=\"GSC dynamical model state trajectory (all symbols)\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Trajectories of the activation levels of all symbols in each tree position as the model processed the sentence 'N Vi P N' (e.g, 'Kay left at noon') one word at a time.</div>\n<div class=\"imageCredit\">Pyeong Whan Cho</div>\n<div class=\"imageSubmitted\">Paul&nbsp;Smolensky</div>\n<div class=\"imageTitle\">GSC dynamical model state trajectory (all symbols)</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588259102453_Fig17fig-stability-with-noise--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588259102453_Fig17fig-stability-with-noise--rgov-800width.jpg\" title=\"GSC dynamical model state trajectory\"><img src=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588259102453_Fig17fig-stability-with-noise--rgov-66x44.jpg\" alt=\"GSC dynamical model state trajectory\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Trajectories of the activation levels of the two most active symbols in each tree position as the model processed the sentence 'N Vi P N' (e.g, 'Kay left at noon') one word at a time (slowed time scale).</div>\n<div class=\"imageCredit\">Pyeong Whan Cho</div>\n<div class=\"imageSubmitted\">Paul&nbsp;Smolensky</div>\n<div class=\"imageTitle\">GSC dynamical model state trajectory</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588260211452_fig-GP3-S2-act-trace-both--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588260211452_fig-GP3-S2-act-trace-both--rgov-800width.jpg\" title=\"GSC dynamical model state trajectory in 2 coordinate systems\"><img src=\"/por/images/Reports/POR/2020/1344269/1344269_10279025_1588260211452_fig-GP3-S2-act-trace-both--rgov-66x44.jpg\" alt=\"GSC dynamical model state trajectory in 2 coordinate systems\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Trajectories of activation values of all symbols, during processing of sentence 'N Vi P N' (e.g., 'Kay left at noon'), in two different coordinate systems. Top: neural coordinates, in which the computer simulation is carried out. Bottom: conceptual coordinates, used solely for interpretation.</div>\n<div class=\"imageCredit\">Pyeong Whan Cho</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Paul&nbsp;Smolensky</div>\n<div class=\"imageTitle\">GSC dynamical model state trajectory in 2 coordinate systems</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n\n\n\n\nTwo seemingly contradictory fundamental ideas underpin work in the cognitive sciences. Linguistics and artificial intelligence traditionally rely on discrete symbols: hard-edged categories, with precise rules specifying how symbols are combined in complex arrangements. In contrast, neural computation -- as understood by neuroscience, psychology, and machine learning -- describes the mind, as well as the brain, in terms of continuous activation levels.  This NSF INSPIRE project defines an amalgam of these two views: Gradient Symbolic Computation (GSC). In GSC, data consists of structures built of symbols that have continuously varying activation values. Computations over these Gradient Symbolic Representations are defined by continuous, dynamical neural network algorithms.\n\nWe have used GSC's partially-activated symbols to advance the understanding of patterns in sound, word, and sentence structure, as well as sentence meaning. GSC provides traction on a frequently-occurring problem in the language sciences: apparent contradictions between two theories remain unresolved because each theory explains data the other cannot. In many cases, GSC allows such theories to be blended together, creating a single analysis that covers a wide range of data not previously explicable within a single theory. For example, in French, certain consonants appear only at the juncture of two special types of words. Theories of French phonology have long debated whether these consonants are at the end of Word1 or the beginning of Word2: each theory accounts for data that the other does not. GSC allows both possibilities to be partially true. Partially-active consonants appear both at the end of Word1 and the beginning of Word2. When the two words are brought together, these partial activations combine to form a single fully-active consonant. Algorithms for automatically learning the partial-activation values of these consonants from data have been developed and demonstrated to be effective. Other blended analyses we have developed address long-standing unresolved theoretical disputes concerning sentence structure (e.g., how event representations are built when verbs combine with phrases describing participants and properties, or small 'particles' like up) and bilingual code-mixing (where multiple linguistic systems are blended together within a single sentence). This work has appeared in prominent journals, and the general framework has been adopted by a growing number of researchers in the international scientific community: more than two dozen GSC analyses in formal linguistics were developed over the course of the grant period, and the number continues to grow.\n\nGSC has also substantially advanced our ability to computationally model human linguistic abilities. Using dynamic neural network algorithms, we have developed models that exploit Gradient Symbolic Representations to gradually build up complex representations of sentence structure. These models provide new insight into how brain-like systems can maintain partially activated sentence structures until the reader or listener has received enough information to commit to a particular sentence representation. GSC dynamic models have been proven to achieve their computational goals in principle, and many of the practical challenges arising in particular instantiations of these models have been overcome by innovative computational mechanisms.\n\nModels have also been developed, some in collaboration with researchers at Microsoft Research AI, that use Deep Learning in specially-designed neural networks to learn Gradient Symbolic Representations that are optimized for the needs of specific Natural Language Processing tasks in Artificial Intelligence. These tasks include question answering, image captioning, and problem-solving in mathematics and computer programming, and inference of missing information in knowledge bases. This new generation of model exhibits performance that sets a new state of the art in several problem domains. More importantly, the use of Gradient Symbolic Representations yields models that are more interpretable than standard Deep Learning models, which are notoriously opaque. New methods have also been developed to use GSC to interpret the learned encodings of information in key standard Deep Learning models, achieving a groundbreaking level of understanding of how particular deep models actually work.\n\nThe substantial intellectual impacts of the award have gone hand-in-hand with development of technical  and human resources. Publicly-available software for GSC modeling has been released. Training and research experience has been provided to undergraduate students, graduate students and postdoctoral fellows at multiple institutions across the United States and Europe. This occurred both in the context of conducting research within the project and through courses and tutorial workshops we held at both national and international venues. Students have received in-depth, hands-on training in linguistic analysis, computational modeling, experimental design, manuscript preparation, and statistical analysis techniques. The project has deepened institutional collaboration at Johns Hopkins between the schools of Engineering (Computer Science) and Arts and Sciences (Cognitive Science), and, via the PI's work with Microsoft Research, has allowed concepts from Cognitive Science to impact work in Natural Language Processing and AI.\n\n \n\n\n\n\n\n\t\t\t\t\tLast Modified: 04/30/2020\n\n\t\t\t\t\tSubmitted by: Paul Smolensky"
 }
}