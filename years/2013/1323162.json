{
 "awd_id": "1323162",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research:  Expanding a National Network for Automated Analysis of Constructed Response Assessments to Reveal Student Thinking in STEM",
 "cfda_num": "47.076",
 "org_code": "11040200",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ellen Carpenter",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2021-08-31",
 "tot_intn_awd_amt": 2885732.0,
 "awd_amount": 3279439.0,
 "awd_min_amd_letter_date": "2013-09-07",
 "awd_max_amd_letter_date": "2020-11-19",
 "awd_abstract_narration": "This project is being conducted by a large team across 6 institutions that is building on already developed open-ended constructed response versions of well-established concept inventories that can be accurately assessed with already created computer automated analysis resources.  The computer-automated analyses are able to predict human ratings of students' work on these topics and have demonstrated higher inter-rater reliability than a group of trained expert human graders. Constructed response assessments reveal more about student thinking and the persistence of misconceptions than do multiple-choice questions, but require more analysis on the part of the educator. In past work, items designed to identify important disciplinary constructs were created based on prior research. The items were then administered via online course management systems where students entered responses. Lexical and statistical analysis software was used to predict expert ratings of student responses. To date, the work has focused primarily in the fields of biology and chemistry in biological contexts. \r\n\r\nThe current project is leveraging the previous research on Automated Assessment of Constructed Response (AACR), and extending the work to other institutions and other STEM disciplines. The specific goals of this project are to: 1. Create a community web portal for the Automated Assessment of Constructed Response (AACR) assessments to expand and deepen collaborations among STEM education researchers, thus providing the infrastructure for expanding the community of researchers and supporting the adoption and implementation of the innovative instructional materials by instructors at other institutions. 2. Propagate the innovations by providing instructors with professional development and long-term, ongoing support to use the assessments. This includes information about common student conceptions revealed by the questions, instructional materials for addressing conceptual barriers, and the opportunity to join a community of practitioners who are using the AACR questions and exchanging materials. 3. Expand the basic research to create and validate AACR questions in introductory chemistry, chemical engineering, and statistics. 4. Engage in ongoing project evaluation for continuous quality improvement and to document the challenges and successes the project encounters. 5. Lay the foundation for sustainability by providing interfaces for e-text publishers, Learning Management System vendors, and Massively Open Online Courses as potential revenue streams to operate and maintain the online infrastructure.\r\n\r\nIntellectual Merit:\r\nImproving STEM education requires valid and reliable instruments that provide insight into student thinking. The automated analysis of constructed response assessments have the potential to assess \"big ideas\" in STEM in a richer, more multi-faceted manner than multiple-choice instruments. This project is extending the number of these items and provide an online community where instructors may obtain, score, and contribute to the library of items and resources necessary for their analyses. \r\n\r\nBroader Impacts:\r\nThe web portal is extending the use of the products created in this project to instructors nationwide. In addition it is providing the foundation for a national collaboration of science and engineering educators interested in developing deeper conceptual assessment tools and supports and mentors postdoctoral research fellows, and graduate research assistants in STEM education research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Urban-Lurain",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Mark Urban-Lurain",
   "pi_email_addr": "urban@msu.edu",
   "nsf_id": "000320484",
   "pi_start_date": "2013-09-07",
   "pi_end_date": "2018-08-30"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Kevin",
   "pi_last_name": "Haudek",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Kevin C Haudek",
   "pi_email_addr": "haudekke@msu.edu",
   "nsf_id": "000505381",
   "pi_start_date": "2018-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Melanie",
   "pi_last_name": "Cooper",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Melanie M Cooper",
   "pi_email_addr": "mmc@msu.edu",
   "nsf_id": "000257059",
   "pi_start_date": "2013-09-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Merrill",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "John E Merrill",
   "pi_email_addr": "merrill3@msu.edu",
   "nsf_id": "000433249",
   "pi_start_date": "2013-09-07",
   "pi_end_date": "2020-11-19"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Carl",
   "pi_last_name": "Lira",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "Carl T Lira",
   "pi_email_addr": "lira@egr.msu.edu",
   "nsf_id": "000407843",
   "pi_start_date": "2013-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Michigan State University",
  "inst_street_address": "426 AUDITORIUM RD RM 2",
  "inst_street_address_2": "",
  "inst_city_name": "EAST LANSING",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "5173555040",
  "inst_zip_code": "488242600",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MI07",
  "org_lgl_bus_name": "MICHIGAN STATE UNIVERSITY",
  "org_prnt_uei_num": "VJKZC4D1JN36",
  "org_uei_num": "R28EKN92ZTZ9"
 },
 "perf_inst": {
  "perf_inst_name": "Michigan State University",
  "perf_str_addr": "428 S. Shaw Lane, Room 3410",
  "perf_city_name": "East Lansing",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "488241226",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MI07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "199800",
   "pgm_ele_name": "IUSE"
  },
  {
   "pgm_ele_code": "726100",
   "pgm_ele_name": "Project & Program Evaluation"
  },
  {
   "pgm_ele_code": "751200",
   "pgm_ele_name": "TUES-Type 3 Project"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0413",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001314DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0415",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001516DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0416",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001617DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0417",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001718DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "0418",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001819DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 1463031.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 459974.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 340582.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 413538.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 602314.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-7ca76bde-7fff-2086-6f4a-08811469a6fe\"> </span></p>\n<p dir=\"ltr\"><span>Many national calls for science, technology and mathematics (STEM) education reform over the last decade have encouraged focusing on the teaching and learning of key disciplinary concepts (e.g. evolution). Instructors need more information about the ideas students struggle to learn in order to adapt their teaching and support student learning. Written assessments, or constructed responses, often reveal student ideas that are difficult to uncover using only using multiple choice tests.&nbsp; Another issue instructors face when teaching for conceptual understanding is efficiently evaluating written responses, especially for large-enrollment introductory level college courses.</span></p>\n<p dir=\"ltr\"><span>The Automated Analysis of Constructed Response project has supported and extended a national network of science education researchers beginning with five universities: Michigan State University, State University of New York - Stony Brook, University of Colorado, University of Georgia, and the University of Maine. The work expanded to include teams at the University of South Florida, South Dakota State University, and Cornell University. This project focused on: 1) exploring student thinking about key ideas in STEM using student writing; 2) developing written, college-level assessment items that target key ideas within multiple STEM disciplines which can be evaluated using computer-assisted scoring models (CSMs); 3) making the assessments and CSMs freely and publicly available on project websites; and 4) supporting a network of college faculty to use the items and the scored student writing. Since faculty are often hesitant to adopt new technology, and require support to implement such technology in teaching, this project (along with NSF DUE 1347740) supported professional development efforts using faculty learning communities (FLCs) at seven research-intensive universities. Participants in these FLCs used the developed assessment items and classroom-level reports from CSMs as part of their STEM courses, then discussed as a group how to use this information to better engage students and improve student learning.&nbsp;</span></p>\n<p dir=\"ltr\"><span>The intellectual merit of this project addresses the application of new technology, research into student thinking, and science faculty engagement in professional development. First, the research team applied methods from artificial intelligence as part of the development of CSMs to help identify and categorize ideas from student writing in STEM. These tools can evaluate student responses with the same level of accuracy as scientific experts, evaluate any number of new student responses in near real time, and return analysis reports to faculty. These reports can assist faculty in evaluating student work and adjusting instruction in the classroom to address identified student difficulties. Second, the team advanced knowledge about student difficulties in learning key STEM disciplinary concepts by developing a set of assessment items on key disciplinary concepts. The answers to these items showed that college students often exhibit a mix of both scientific and non-scientific ideas about scientific phenomena, which can be identified using the CSMs. Third, faculty participants in our FLCs were motivated to persist in multi-year teaching professional development because it connected them to a national effort targeting college science teaching. We also found that our participants varied in their level of student-centered teaching and held different perspectives about teaching and learning. Even though all faculty valued student thinking, most of their teaching remained stable over time. This outcome is significant in that science faculty at several research-intensive universities were engaged in a long-term effort to employ technology and improve their instruction. This outcome also shows additional work is necessary to help faculty connect their engagement in student thinking and technology with changes in instructional practices.</span></p>\n<p><span>The broader impacts of this project are demonstrated through the number of faculty users and students assessed, as well as impacts on related science education initiatives. First, over the life of the project a total of 35 instructors at seven institutions participated in in-person professional development programs in FLCs. </span><span>Second, </span><span>the project extended the network of users of assessment items and CSMs outside of the research team and FLCs by making these tools freely available on project websites, reaching at least an additional 60 faculty users. Over the life of the project, faculty have administered the developed questions to over 22,000 undergraduate students and collected more than 158,000 written responses. On average, about 160 student written responses are processed per upload to the CSM, demonstrating the utility of CSMs for evaluating large numbers of written responses. Third, web portals supported during this project make the CSMs available to any interested instructor, help connect a community of interested users, and remain available for free use (www.beyondmultiplechoice.org). Fourth, </span><span>the findings from our research on professional development are guiding the design of future college-level professional development initiatives. </span><span>Finally, the methods from this project are being used in other efforts in science education assessment aligned with the Next Generation Science Standards for K-12 education and for assessing undergraduate learning in other STEM disciplines over 2- and 4- year college programs. </span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2021<br>\n\t\t\t\t\tModified by: Kevin&nbsp;Haudek</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640794749380_Fig2_TUESIII_projoutcomes_BMCwebpage--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640794749380_Fig2_TUESIII_projoutcomes_BMCwebpage--rgov-800width.jpg\" title=\"A screenshot of the project webpage\"><img src=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640794749380_Fig2_TUESIII_projoutcomes_BMCwebpage--rgov-66x44.jpg\" alt=\"A screenshot of the project webpage\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The project webpage delivers basic information about the research funded by this and other grants. Registered users can browse assessment questions created by this project and upload student responses to be analyzed by computerized assisted scoring models.</div>\n<div class=\"imageCredit\">Automated Analysis of Constructed Response research project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Kevin&nbsp;Haudek</div>\n<div class=\"imageTitle\">A screenshot of the project webpage</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640794675773_Fig1_TUESIII_projectoutcomes_FLCs--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640794675773_Fig1_TUESIII_projectoutcomes_FLCs--rgov-800width.jpg\" title=\"Supported network of faculty learning communities (FLCs) during this project\"><img src=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640794675773_Fig1_TUESIII_projectoutcomes_FLCs--rgov-66x44.jpg\" alt=\"Supported network of faculty learning communities (FLCs) during this project\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Each institution, denoted by large blue circles, had a group of faculty participants (small red dots) led by a project team member (small blue dots) as a facilitator. This project attempted to connect local FLCs, at a total of 7 institutions, to a larger community of practice.</div>\n<div class=\"imageCredit\">McCourt, J., Andrews, T. C., Knight, J. K., Merrill, J., Nehm, R., Prevost, L. B., Smith, M. K., Urban-Lurain, M., and Lemons, P.P. (2017) CBE-Life Sciences Education. 16: ar54 doi: 10.1187/cbe.16-08-0241.</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Kevin&nbsp;Haudek</div>\n<div class=\"imageTitle\">Supported network of faculty learning communities (FLCs) during this project</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640874945466_Fig3_TUESIII_projoutcomes_BMCreport--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640874945466_Fig3_TUESIII_projoutcomes_BMCreport--rgov-800width.jpg\" title=\"A sample screenshot from an instructor feedback report\"><img src=\"/por/images/Reports/POR/2021/1323162/1323162_10276567_1640874945466_Fig3_TUESIII_projoutcomes_BMCreport--rgov-66x44.jpg\" alt=\"A sample screenshot from an instructor feedback report\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Interactive reports for instructors contain a variety of representations of predicted scores and ideas included in student responses. Reports also allow instructors to view individual student responses and connections between phrases, ideas and predicted scores.</div>\n<div class=\"imageCredit\">Automated Analysis of Constructed Response research project</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Kevin&nbsp;Haudek</div>\n<div class=\"imageTitle\">A sample screenshot from an instructor feedback report</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nMany national calls for science, technology and mathematics (STEM) education reform over the last decade have encouraged focusing on the teaching and learning of key disciplinary concepts (e.g. evolution). Instructors need more information about the ideas students struggle to learn in order to adapt their teaching and support student learning. Written assessments, or constructed responses, often reveal student ideas that are difficult to uncover using only using multiple choice tests.  Another issue instructors face when teaching for conceptual understanding is efficiently evaluating written responses, especially for large-enrollment introductory level college courses.\nThe Automated Analysis of Constructed Response project has supported and extended a national network of science education researchers beginning with five universities: Michigan State University, State University of New York - Stony Brook, University of Colorado, University of Georgia, and the University of Maine. The work expanded to include teams at the University of South Florida, South Dakota State University, and Cornell University. This project focused on: 1) exploring student thinking about key ideas in STEM using student writing; 2) developing written, college-level assessment items that target key ideas within multiple STEM disciplines which can be evaluated using computer-assisted scoring models (CSMs); 3) making the assessments and CSMs freely and publicly available on project websites; and 4) supporting a network of college faculty to use the items and the scored student writing. Since faculty are often hesitant to adopt new technology, and require support to implement such technology in teaching, this project (along with NSF DUE 1347740) supported professional development efforts using faculty learning communities (FLCs) at seven research-intensive universities. Participants in these FLCs used the developed assessment items and classroom-level reports from CSMs as part of their STEM courses, then discussed as a group how to use this information to better engage students and improve student learning. \nThe intellectual merit of this project addresses the application of new technology, research into student thinking, and science faculty engagement in professional development. First, the research team applied methods from artificial intelligence as part of the development of CSMs to help identify and categorize ideas from student writing in STEM. These tools can evaluate student responses with the same level of accuracy as scientific experts, evaluate any number of new student responses in near real time, and return analysis reports to faculty. These reports can assist faculty in evaluating student work and adjusting instruction in the classroom to address identified student difficulties. Second, the team advanced knowledge about student difficulties in learning key STEM disciplinary concepts by developing a set of assessment items on key disciplinary concepts. The answers to these items showed that college students often exhibit a mix of both scientific and non-scientific ideas about scientific phenomena, which can be identified using the CSMs. Third, faculty participants in our FLCs were motivated to persist in multi-year teaching professional development because it connected them to a national effort targeting college science teaching. We also found that our participants varied in their level of student-centered teaching and held different perspectives about teaching and learning. Even though all faculty valued student thinking, most of their teaching remained stable over time. This outcome is significant in that science faculty at several research-intensive universities were engaged in a long-term effort to employ technology and improve their instruction. This outcome also shows additional work is necessary to help faculty connect their engagement in student thinking and technology with changes in instructional practices.\n\nThe broader impacts of this project are demonstrated through the number of faculty users and students assessed, as well as impacts on related science education initiatives. First, over the life of the project a total of 35 instructors at seven institutions participated in in-person professional development programs in FLCs. Second, the project extended the network of users of assessment items and CSMs outside of the research team and FLCs by making these tools freely available on project websites, reaching at least an additional 60 faculty users. Over the life of the project, faculty have administered the developed questions to over 22,000 undergraduate students and collected more than 158,000 written responses. On average, about 160 student written responses are processed per upload to the CSM, demonstrating the utility of CSMs for evaluating large numbers of written responses. Third, web portals supported during this project make the CSMs available to any interested instructor, help connect a community of interested users, and remain available for free use (www.beyondmultiplechoice.org). Fourth, the findings from our research on professional development are guiding the design of future college-level professional development initiatives. Finally, the methods from this project are being used in other efforts in science education assessment aligned with the Next Generation Science Standards for K-12 education and for assessing undergraduate learning in other STEM disciplines over 2- and 4- year college programs. \n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/30/2021\n\n\t\t\t\t\tSubmitted by: Kevin Haudek"
 }
}