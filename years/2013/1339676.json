{
 "awd_id": "1339676",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SI2 SSI: Collaborative Research: Sustained Innovations for Linear Algebra Software (SILAS)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rajiv Ramnath",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 510000.0,
 "awd_amount": 611954.0,
 "awd_min_amd_letter_date": "2013-08-29",
 "awd_max_amd_letter_date": "2015-09-02",
 "awd_abstract_narration": "As the era of computer architectures dominated by serial processors comes to a close, the convergence of several unprecedented changes in processor design has produced a broad consensus that much of the essential software infrastructure of computational science and engineering is utterly obsolete. Math libraries have historically been in the vanguard of software that must be quickly adapted to such design revolutions because they are the common, low-level software workhorses that do all the most basic mathematical calculations for many different types of applications. The Sustained Innovation for Linear Algebra Software (SILAS) project updates two of the most widely used numerical libraries in the history of Computational Science and Engineering---LAPACK and ScaLAPACK, (abbreviated Sca/LAPACK)---enhancing and hardening them for this ongoing revolution in processor architecture and system design. SILAS creates a layered package of software components, capable of running at every level of the platform deployment pyramid, from the desktop to the largest supercomputers in the world. It achieves three complementary objectives: 1) Wherever possible, SILAS delivers seamless access to the most up-to-date algorithms, numerical implementations, and performance, by way of Sca/LAPACK programming interfaces that are familiar to many computational scientists; 2) Wherever necessary, SILAS makes advanced algorithms, numerical implementations and performance capabilities available through new interface extensions; and 3) SILAS provides a well engineered conduit through which new discoveries at the frontiers of research in these areas can be channeled as quickly as possible to all the application communities that depend on high performance linear algebra. The improvements and innovations included in SILAS derive from a variety of sources. They represent the results (including designs and well tested prototypes) of the PIs' own algorithmic and software research agenda, which has targeted multicore, hybrid and extreme scale system architectures. They are an outcome of extensive and on-going interactions with users, vendors, and the management of large NSF and DOE supercomputing facilities. They flow from cross-disciplinary engagement with other areas of computer science and engineering, anticipating the demands and opportunities of new architectures and programming models. And finally, they come from the enthusiastic participation of the research community in developing and offering enhanced versions of existing Sca/LAPACK codes.\r\n\r\nThe primary impact of SILAS is a direct function of the importance of the Sca/LAPACK libraries to many branches of computational science. The Sca/LAPACK libraries are the community standard for dense linear algebra and have been adopted and/or supported by a large community of users, computing centers, and HPC vendors. Learning to use them is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. Application domains where Sca/LAPACK have historically been heavily used include (among a host of other examples) airplane wing design, radar cross-section studies, flow around ships and other off-shore constructions, diffusion of solid bodies in a liquid, noise reduction, and diffusion of light through small particles. Moreover, the list of application partners working with SILAS to enhance and transform these libraries for next generation platforms expands this traditional list to include quantum chemistry, adaptive mesh refinement schemes, computational materials science, geophysical flows, stochastic simulation and database research for \"big data\". No other numerical library can claim this breadth of integration with the community. Thus, there is every reason to believe that enhancing these libraries with state of the art methods and algorithms and adapting them for new and emerging platforms (reaching up to extreme scale), will have a correspondingly large impact on the research and education community, government laboratories, and private industry.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Demmel",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "James W Demmel",
   "pi_email_addr": "demmel@cs.berkeley.edu",
   "nsf_id": "000207659",
   "pi_start_date": "2013-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "564 Soda Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947201776",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  },
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8009",
   "pgm_ref_txt": "Scientifc Software Integration"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 400000.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 161954.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Outcomes Report for SILAS &ndash; NSF 1339822</p>\n<p>The convergence of several unprecedented changes in processor design, including formidable new system constraints and revolutionary levels of heterogeneity, has produced a broad consensus that much of the essential software infrastructure of computational science and engineering (CS&amp;E) is becoming obsolete. Math libraries are historically in the vanguard of software that needs to adapt to such changes, both because they are the low level workhorses critical to the accuracy and performance of so many different types of applications, and because they have proved to be outstanding vehicles for finding and implementing solutions to the problems that new types of architectures pose. Under the <em>Sustained Innovation for Linear Algebra Software</em> (SILAS) project, the principal designers of LAPACK and ScaLAPACK&mdash;two of the most widely used numerical libraries in the history of CS&amp;E, have maintained, enhanced, hardened, and updated these libraries (abbreviated Sca/LAPACK), as well as widely used research libraries that exploit the new processor innovations &mdash; PLASMA (multicore), MAGMA (accelerators), and DPLASMA (distributed memory) &mdash; for the ongoing revolution in processor architecture and system design.</p>\n<p>&nbsp;</p>\n<p><strong>Intellectual merit:</strong></p>\n<p>The advanced research in dense linear algebra (DLA) performed under SILAS generated fundamental new knowledge and understanding for numerical algorithms on emerging architectures, including the following: (1) Successful research results on how to transition DLA libraries to multicore and accelerator enabled versions; (2) Vastly improved numerical methods and algorithms, such as communication-avoiding algorithms; (3) Autotuning methods, methodology, and tools; (4) New solver designs requested by users and stakeholder communities; (5) DLA programming models and software engineering to keep a substantial code base efficient and maintainable at reasonable cost in the future.</p>\n<p>&nbsp;</p>\n<p><strong>Broader Impacts:</strong></p>\n<p>Under SILAS, the Sca/LAPACK libraries were maintained, enhanced, hardened, and updated. Improved versions were regularly released as follows: (1) Releases targeting multicore and accelerators included MAGMA 2.1 (Aug. 2016), MAGMA 2.0 (May 2016), MAGMA 1.7 (Sept. 2015), MAGMA 1.6 (Nov. 2014), MAGMA 1.5 (Sept. 2014). PLASMA releases followed similar schedule of two major releases per year. In addition, there were separate MAGMA releases targeting Intel Xeon Phi and AMD GPU architectures; (2) The reference implementation for Sca/LAPACK was maintained by incorporating vastly improved numerical methods and algorithms that resulted from SILAS research and community algorithmic developments. For example, LAPACK 3.6.1 (June 2016) included blocked back-transformation for the non-symmetric eigenvalue problem, LAPACK 3.6.0 (Nov. 2015): BLAS3 routines for generalized SVD, blocked Hessenberg-triangular reductions, removed deprecated subroutines, complex Jacobi SVD, recursive Cholesky, and Subset SVD, LAPACK 3.5.0 (Nov. 2013): 2-by-1 CS decomposition, Symmetric LDLT factorization routines with rook pivoting algorithm; (3) Ease of use and deployment were improved, and the software engineering approach was updated to keep the substantial code base efficient, maintainable, and testable at reasonable cost in the future.</p>\n<p>&nbsp;</p>\n<p><strong>Sustainability of Sca/LAPACK, and impact on industry and education:</strong></p>\n<p>The enhancements under SILAS, including modified software engineering, improved the sustainability of Sca/LAPACK and enhanced openness to contributions by a larger community of developers. Consequently, despite the ongoing revolution in processor architectures, the Sca/LAPACK libraries continued to be the community standard for dense linear algebra and to be adopted and/or supported by a large community of users, computing centers, and HPC vendors. Sca/LAPACK is used now in libraries like Armadillo, Blaze, MATLAB, NumPy and R, solvers like Hypre, MUMPS, PETSc, SuperLU and Trilinos. The SILAS team collaborates with groups and organizations who provide improved versions of different library components, including Intel MKL, AMD ACML, Cray LibSci, Debian, Cygwin, Red Hat/Fedora, and Apple Accelerate Framework. Sca/LAPACK is used in many applications, e.g., for electronic-structure calculations in packages like ABINIT, Quantum ESPRESSO and VASP, finite element packages DEAL II (2007 Wilkinson Prize for Numerical Software) and OpenSees. Moreover, learning to use Sca/LAPACK is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. No other numerical library can claim this breadth of integration with the community. The SILAS team provided one-month Summer REUs for undergraduate students from various ethnic groups and genders, taught recurring graduate level Scientific Computing for Engineers class, gave recurring tutorials at major conferences, including SC and ISC. Along with the numerous tutorials, classes, workshops, and User Group meetings,&nbsp; the SILAS team published project results and techniques in a tutorial manner and provided responsive assistance to a large user base via email and web-based systems.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/03/2017<br>\n\t\t\t\t\tModified by: James&nbsp;W&nbsp;Demmel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOutcomes Report for SILAS &ndash; NSF 1339822\n\nThe convergence of several unprecedented changes in processor design, including formidable new system constraints and revolutionary levels of heterogeneity, has produced a broad consensus that much of the essential software infrastructure of computational science and engineering (CS&amp;E) is becoming obsolete. Math libraries are historically in the vanguard of software that needs to adapt to such changes, both because they are the low level workhorses critical to the accuracy and performance of so many different types of applications, and because they have proved to be outstanding vehicles for finding and implementing solutions to the problems that new types of architectures pose. Under the Sustained Innovation for Linear Algebra Software (SILAS) project, the principal designers of LAPACK and ScaLAPACK&mdash;two of the most widely used numerical libraries in the history of CS&amp;E, have maintained, enhanced, hardened, and updated these libraries (abbreviated Sca/LAPACK), as well as widely used research libraries that exploit the new processor innovations &mdash; PLASMA (multicore), MAGMA (accelerators), and DPLASMA (distributed memory) &mdash; for the ongoing revolution in processor architecture and system design.\n\n \n\nIntellectual merit:\n\nThe advanced research in dense linear algebra (DLA) performed under SILAS generated fundamental new knowledge and understanding for numerical algorithms on emerging architectures, including the following: (1) Successful research results on how to transition DLA libraries to multicore and accelerator enabled versions; (2) Vastly improved numerical methods and algorithms, such as communication-avoiding algorithms; (3) Autotuning methods, methodology, and tools; (4) New solver designs requested by users and stakeholder communities; (5) DLA programming models and software engineering to keep a substantial code base efficient and maintainable at reasonable cost in the future.\n\n \n\nBroader Impacts:\n\nUnder SILAS, the Sca/LAPACK libraries were maintained, enhanced, hardened, and updated. Improved versions were regularly released as follows: (1) Releases targeting multicore and accelerators included MAGMA 2.1 (Aug. 2016), MAGMA 2.0 (May 2016), MAGMA 1.7 (Sept. 2015), MAGMA 1.6 (Nov. 2014), MAGMA 1.5 (Sept. 2014). PLASMA releases followed similar schedule of two major releases per year. In addition, there were separate MAGMA releases targeting Intel Xeon Phi and AMD GPU architectures; (2) The reference implementation for Sca/LAPACK was maintained by incorporating vastly improved numerical methods and algorithms that resulted from SILAS research and community algorithmic developments. For example, LAPACK 3.6.1 (June 2016) included blocked back-transformation for the non-symmetric eigenvalue problem, LAPACK 3.6.0 (Nov. 2015): BLAS3 routines for generalized SVD, blocked Hessenberg-triangular reductions, removed deprecated subroutines, complex Jacobi SVD, recursive Cholesky, and Subset SVD, LAPACK 3.5.0 (Nov. 2013): 2-by-1 CS decomposition, Symmetric LDLT factorization routines with rook pivoting algorithm; (3) Ease of use and deployment were improved, and the software engineering approach was updated to keep the substantial code base efficient, maintainable, and testable at reasonable cost in the future.\n\n \n\nSustainability of Sca/LAPACK, and impact on industry and education:\n\nThe enhancements under SILAS, including modified software engineering, improved the sustainability of Sca/LAPACK and enhanced openness to contributions by a larger community of developers. Consequently, despite the ongoing revolution in processor architectures, the Sca/LAPACK libraries continued to be the community standard for dense linear algebra and to be adopted and/or supported by a large community of users, computing centers, and HPC vendors. Sca/LAPACK is used now in libraries like Armadillo, Blaze, MATLAB, NumPy and R, solvers like Hypre, MUMPS, PETSc, SuperLU and Trilinos. The SILAS team collaborates with groups and organizations who provide improved versions of different library components, including Intel MKL, AMD ACML, Cray LibSci, Debian, Cygwin, Red Hat/Fedora, and Apple Accelerate Framework. Sca/LAPACK is used in many applications, e.g., for electronic-structure calculations in packages like ABINIT, Quantum ESPRESSO and VASP, finite element packages DEAL II (2007 Wilkinson Prize for Numerical Software) and OpenSees. Moreover, learning to use Sca/LAPACK is a basic part of the education of a computational scientist or engineer in many fields and at many academic institutions. No other numerical library can claim this breadth of integration with the community. The SILAS team provided one-month Summer REUs for undergraduate students from various ethnic groups and genders, taught recurring graduate level Scientific Computing for Engineers class, gave recurring tutorials at major conferences, including SC and ISC. Along with the numerous tutorials, classes, workshops, and User Group meetings,  the SILAS team published project results and techniques in a tutorial manner and provided responsive assistance to a large user base via email and web-based systems.\n\n\t\t\t\t\tLast Modified: 01/03/2017\n\n\t\t\t\t\tSubmitted by: James W Demmel"
 }
}