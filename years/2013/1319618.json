{
 "awd_id": "1319618",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Speeding Up Learning through Modeling the Pragmatics of Training",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Weng-keen Wong",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 147999.0,
 "awd_amount": 155999.0,
 "awd_min_amd_letter_date": "2013-09-09",
 "awd_max_amd_letter_date": "2016-06-10",
 "awd_abstract_narration": "Years of effort to develop algorithms capable of learning from reward signals have resulted in a plethora of techniques that can leverage numerical signals that vary in value based on performance. Recent efforts to use these techniques to learn from humans providing rewards have been slower to progress, in part, because humans give feedback discretely rather than numerically. This project contributes new learning algorithms designed specifically to leverage the information contained in the choices humans make to provide such discrete feedbacks. The algorithms are inspired by the human-canine partnership, and the incredible things that humans are able to teach dogs using only discrete feedback and carefully constructed sequences of tasks. The Bayesian learning framework being developed in this project will leverage the pragmatic implicatures contained in the feedbacks and tasks sequences to learn more quickly from human feedback. \r\n\r\nThe ultimate goal of this work is to provide a more natural paradigm for humans to tell computers what they would like for them to do. To that end, project efforts will result in a teaching module for Brown University?s Learning Exchange (LE). The LE involves undergraduates working with underserved minority middle school students to engage them in STEM. They are a perfect audience to demonstrate the broader impacts of this work. LE participants learn to instruct computers using a combination of programming with the Scratch environment and the feedback paradigm, which shows how powerful the algorithms are.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Littman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Littman",
   "pi_email_addr": "mlittman@cs.brown.edu",
   "nsf_id": "000210482",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brown University",
  "inst_street_address": "1 PROSPECT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PROVIDENCE",
  "inst_state_code": "RI",
  "inst_state_name": "Rhode Island",
  "inst_phone_num": "4018632777",
  "inst_zip_code": "029129100",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "RI01",
  "org_lgl_bus_name": "BROWN UNIVERSITY",
  "org_prnt_uei_num": "E3FDXZ6TBHW3",
  "org_uei_num": "E3FDXZ6TBHW3"
 },
 "perf_inst": {
  "perf_inst_name": "Brown University",
  "perf_str_addr": "115 Waterman Street",
  "perf_city_name": "Providence",
  "perf_st_code": "RI",
  "perf_st_name": "Rhode Island",
  "perf_zip_code": "029129002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "RI01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 72845.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 75154.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>For real-world applications, virtual agents must be able to learn new behaviors from non-technical users. Positive and negative feedback are an intuitive way to train new behaviors, and existing work has presented algorithms for learning from such feedback. These algorithms, however, are based on assumptions about human training strategies that turn out not to be true. This project found that feedback is often not a numeric reward to be maximized and different trainers use feedback in different ways. Users may not always give explicit feedback in response to an action, and may be more likely to provide explicit reward than explicit punishment, or vice versa, such that the lack of feedback itself conveys information about the behavior. In addition to empirical demonstrations of the training strategies adopted by users, the project developed a probabilistic model of trainer feedback that describes how a trainer chooses to provide explicit reward and/or explicit punishment. Based on this formal model, the project team developed novel learning algorithms (SABL, I-SABL, I-learning, and COACH) which take trainer strategy into account, can learn from cases where no feedback is provided, can handle diminishing feedback, and avoid causing learners to get stuck in \"positive reward cycles\". Online user studies demonstrated that these algorithms can learn desired behavior more accurately and with less feedback than algorithms based on a numerical interpretation of feedback. </span></p>\n<p>The project also showed that there are advantages to designing learning algorithms that assume that training is being provided from a beneficent teacher instead of classical IID (independent and identically distributed) data. The work extends existing results on learning from others' demonstrations---classic inverse reinforcement learning (IRL) algorithms---by developing a novel Bayesian model for the problem of explicit teaching by demonstration. Stark differences arise when demonstrators are intentionally teaching a task versus simply performing a task. Two experiments in this work showed that human participants systematically modify their teaching behavior consistent with the predictions of a teaching-by-demonstration model. Further work showed that even standard IRL algorithms benefit when learning from behaviors that are intentionally pedagogical.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/13/2017<br>\n\t\t\t\t\tModified by: Michael&nbsp;L&nbsp;Littman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nFor real-world applications, virtual agents must be able to learn new behaviors from non-technical users. Positive and negative feedback are an intuitive way to train new behaviors, and existing work has presented algorithms for learning from such feedback. These algorithms, however, are based on assumptions about human training strategies that turn out not to be true. This project found that feedback is often not a numeric reward to be maximized and different trainers use feedback in different ways. Users may not always give explicit feedback in response to an action, and may be more likely to provide explicit reward than explicit punishment, or vice versa, such that the lack of feedback itself conveys information about the behavior. In addition to empirical demonstrations of the training strategies adopted by users, the project developed a probabilistic model of trainer feedback that describes how a trainer chooses to provide explicit reward and/or explicit punishment. Based on this formal model, the project team developed novel learning algorithms (SABL, I-SABL, I-learning, and COACH) which take trainer strategy into account, can learn from cases where no feedback is provided, can handle diminishing feedback, and avoid causing learners to get stuck in \"positive reward cycles\". Online user studies demonstrated that these algorithms can learn desired behavior more accurately and with less feedback than algorithms based on a numerical interpretation of feedback. \n\nThe project also showed that there are advantages to designing learning algorithms that assume that training is being provided from a beneficent teacher instead of classical IID (independent and identically distributed) data. The work extends existing results on learning from others' demonstrations---classic inverse reinforcement learning (IRL) algorithms---by developing a novel Bayesian model for the problem of explicit teaching by demonstration. Stark differences arise when demonstrators are intentionally teaching a task versus simply performing a task. Two experiments in this work showed that human participants systematically modify their teaching behavior consistent with the predictions of a teaching-by-demonstration model. Further work showed that even standard IRL algorithms benefit when learning from behaviors that are intentionally pedagogical.\n\n \n\n\t\t\t\t\tLast Modified: 02/13/2017\n\n\t\t\t\t\tSubmitted by: Michael L Littman"
 }
}