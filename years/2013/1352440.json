{
 "awd_id": "1352440",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: PARTIAL: An Exploratory Study on Practical Approaches for Robust NLP Tools with Integrated Annotation Languages",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2013-08-19",
 "awd_max_amd_letter_date": "2013-08-19",
 "awd_abstract_narration": "In order to develop natural language processing (NLP) technologies for text in a wider range of languages, dialects, genres, and styles, this Early Grant for Exploratory Research investigates a novel methodological approach.  Conventionally, linguistic experts are employed to create gold-standard linguistically annotated datasets to which supervised machine learning algorithms are applied.  This project frees annotators from the requirement that annotations be complete by moving more of the burden to learning algorithms. Algorithms are developed that are robust to partial evidence, annotator variation, and noise due to errors.  As a result, any language enthusiast (not just trained experts) can provide annotations so that NLP can be developed for more kinds of text in more languages for less money.  In this exploration, the focus is on dependency parsing, a fundamental NLP component that predicts the grammatical relationships between words in sentences, with experimentation on data in English (two genres), Chinese, and Farsi.  The formal basis for the approach is a framework called Graph Fragment Language (GFL).   The project assesses the quality of parsers learned from GFL and the productivity of annotators accorded this new flexibility.\r\n\r\nBeyond documentation and assessment of the new methodology, this project produces open-source software tools for gathering annotated data and constructing NLP tools using the data.  It emphasizes the usability of these tools in classrooms, contributing exercises that can be used in NLP and linguistics courses to allow students to engage directly with data, with the models that make use of the data, and with the technological goals that data annotation supports.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Noah",
   "pi_last_name": "Smith",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Noah A Smith",
   "pi_email_addr": "noah@allenai.org",
   "nsf_id": "000228357",
   "pi_start_date": "2013-08-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Dyer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chris Dyer",
   "pi_email_addr": "cdyer@cs.cmu.edu",
   "nsf_id": "000622961",
   "pi_start_date": "2013-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"column\">\n<p><span>Driven by the goal of enabling language technologies for more of the world&rsquo;s languages, in more dialects, genres, and styles, we reconsider the predominant paradigm of &ldquo;annotate-and-learn,&rdquo; in which experts create gold-standard linguistically annotated datasets to which supervised machine learning algorithms are applied. Simply put, such resources are too expensive to produce, there aren&rsquo;t enough experts, and the cost of developing guidelines that allow complete annotation of every text instance in a sample is prohibitive. </span></p>\n<p><span>This exploratory project focused on dependency parsing. In the lightweight dependency annotation paradigm we have developed, annotators are not required to annotate every token: they can instead focus on interesting or important linguistic phenomena, or they can provide provide just high-level constraints on what the underlying analysis of a sentence should be. We are able to adopt this relaxed methodological stance because we view annotation as partial evidence, not complete truth. Thus, annotators are freed from the need to have a great deal of annotation expertise (such as having learned a style guide), and annotation can become an engaging task. We therefore use students and language enthusiasts as annotators.&nbsp;</span></p>\n</div>\n<p>The results of this project include:</p>\n<ol>\n<li>An empirical study on algorithms for obtaining Stanford dependencies, a popular representation widely used in downstream applications. &nbsp;We found that direct parsing to dependencies can attain much better performance than had been found previously, due to advances in dependency parsing.</li>\n<li>A new dataset annotated by non-experts, consisting of dependency-parsed tweets.</li>\n<li>A new annotation interface that can be used to extend the dataset and create more datasets. &nbsp;This interface has also been tested in an undergraduate class.</li>\n<li>A new dependency parser for tweets, built using that dataset, and achieving 80% unlabeled attachment accuracy.</li>\n</ol>\n<div class=\"column\">\n<p><span>Intellectual merit:&nbsp;</span><span>This project advanced methodology for developing language-general NLP, a goal closely related to developing computational theories of human language. In particular, it developed a new annotation framework that can support a wide range of partial linguistic annotation styles (Graph Fragment Language); it advanced statistical modeling to accommodate partial and noisy annotation from non-experts. </span></p>\n<p><span>Broader impact:&nbsp;</span><span>This project produced a broad set of open-source software tools for gathering annotated data and constructing NLP tools using the data. It emphasized the usability of these tools in classrooms, contributing an exercise that can be used in NLP and linguistics courses to allow students to engage directly with data, with the models that make use of the data, and with the technological goals that data annotation supports. The PIs incorporated the exercises into a course offered in their department, demonstrating how pedagogy and annotation can be tightly integrated. One graduate student researcher and one recently graduated linguistics student (acting as a research assistant) were supported by this project. Research results were reported in high-profile, open-access conference and arXiv papers.&nbsp;</span></p>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2014<br>\n\t\t\t\t\tModified by: Noah&nbsp;A&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\nDriven by the goal of enabling language technologies for more of the world\u00c6s languages, in more dialects, genres, and styles, we reconsider the predominant paradigm of \"annotate-and-learn,\" in which experts create gold-standard linguistically annotated datasets to which supervised machine learning algorithms are applied. Simply put, such resources are too expensive to produce, there aren\u00c6t enough experts, and the cost of developing guidelines that allow complete annotation of every text instance in a sample is prohibitive. \n\nThis exploratory project focused on dependency parsing. In the lightweight dependency annotation paradigm we have developed, annotators are not required to annotate every token: they can instead focus on interesting or important linguistic phenomena, or they can provide provide just high-level constraints on what the underlying analysis of a sentence should be. We are able to adopt this relaxed methodological stance because we view annotation as partial evidence, not complete truth. Thus, annotators are freed from the need to have a great deal of annotation expertise (such as having learned a style guide), and annotation can become an engaging task. We therefore use students and language enthusiasts as annotators. \n\n\nThe results of this project include:\n\nAn empirical study on algorithms for obtaining Stanford dependencies, a popular representation widely used in downstream applications.  We found that direct parsing to dependencies can attain much better performance than had been found previously, due to advances in dependency parsing.\nA new dataset annotated by non-experts, consisting of dependency-parsed tweets.\nA new annotation interface that can be used to extend the dataset and create more datasets.  This interface has also been tested in an undergraduate class.\nA new dependency parser for tweets, built using that dataset, and achieving 80% unlabeled attachment accuracy.\n\n\n\nIntellectual merit: This project advanced methodology for developing language-general NLP, a goal closely related to developing computational theories of human language. In particular, it developed a new annotation framework that can support a wide range of partial linguistic annotation styles (Graph Fragment Language); it advanced statistical modeling to accommodate partial and noisy annotation from non-experts. \n\nBroader impact: This project produced a broad set of open-source software tools for gathering annotated data and constructing NLP tools using the data. It emphasized the usability of these tools in classrooms, contributing an exercise that can be used in NLP and linguistics courses to allow students to engage directly with data, with the models that make use of the data, and with the technological goals that data annotation supports. The PIs incorporated the exercises into a course offered in their department, demonstrating how pedagogy and annotation can be tightly integrated. One graduate student researcher and one recently graduated linguistics student (acting as a research assistant) were supported by this project. Research results were reported in high-profile, open-access conference and arXiv papers. \n\n\n \n\n\t\t\t\t\tLast Modified: 11/30/2014\n\n\t\t\t\t\tSubmitted by: Noah A Smith"
 }
}