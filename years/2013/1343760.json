{
 "awd_id": "1343760",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EarthCube Building Blocks:  A Cognitive Computer Infrastructure for Geoscience",
 "cfda_num": "47.050",
 "org_code": "06010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Eva Zanzerkia",
 "awd_eff_date": "2013-09-15",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 1497798.0,
 "awd_amount": 1497798.0,
 "awd_min_amd_letter_date": "2013-08-26",
 "awd_max_amd_letter_date": "2017-09-18",
 "awd_abstract_narration": "This is an era when access to information and data is often less of a problem than the ability\r\nto efficiently process and use it. In some cases, these problems are caused by massive, monolithic\r\ndatasets that are difficult to store, transfer, and/or analyze. In other cases, the first-order\r\nproblem is discovering and then aggregating relevant data that are widely disseminated in\r\nmany different locations and formats, such as in the tables, text, and figures of published\r\npapers, government agency reports, spreadsheets, and websites. Geosciences currently lacks\r\na cyberinfrastructure that can efficiently, cheaply, and with high precision and accuracy\r\nfind, extract, and organize many different types of data that are critical to advancing science\r\nand leveraging current and past investments in data acquisition. Instead, there are dozens\r\nof isolated, sometimes redundant, geosciences data mining efforts that use humans as the primary\r\nmechanism for finding data and then keystroking them into structured databases. This mode\r\nof operation is not only costly and slow, but it is also an inefficient use of human resources\r\nand scientific expertise. This project develops a geoscience-oriented trained computing\r\nsystem that can serve as a cross-disciplinary tool for rapidly finding, extracting, and organizing\r\ngeosciences data. Unlike traditional data processing systems, trained systems use statistical, or machine learning, techniques to provide rich answers to complex queries of data that are much less structured. \r\n\r\nThe longer-term vision is to establish an EarthCube trained computing system that can aid in finding, extracting, and aggregating data, as well as in processing, summarizing, and synthesizing them in a way that helps geoscientists to tackle new problems\r\nand better understand and model Earth systems. This project brings together a unique interdisciplinary team that is committed to building, testing, and operating an EarthCube Building Block that will bring the power of trained computing systems\r\ntechnologies to the broader geoscience community. Trained computing systems offer an entirely new breed of tools for data processing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "GEO",
 "org_dir_long_name": "Directorate for Geosciences",
 "div_abbr": "RISE",
 "org_div_long_name": "Integrative and Collaborative Education and Research (ICER)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shanan",
   "pi_last_name": "Peters",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Shanan E Peters",
   "pi_email_addr": "peters@geology.wisc.edu",
   "nsf_id": "000270093",
   "pi_start_date": "2017-09-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Re",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Re",
   "pi_email_addr": "chrismre@cs.stanford.edu",
   "nsf_id": "000555316",
   "pi_start_date": "2013-08-26",
   "pi_end_date": "2015-07-24"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Miron",
   "pi_last_name": "Livny",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Miron Livny",
   "pi_email_addr": "miron@cs.wisc.edu",
   "nsf_id": "000340383",
   "pi_start_date": "2013-08-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Shanan",
   "pi_last_name": "Peters",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Shanan E Peters",
   "pi_email_addr": "peters@geology.wisc.edu",
   "nsf_id": "000270093",
   "pi_start_date": "2013-08-26",
   "pi_end_date": "2015-07-24"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Christopher",
   "pi_last_name": "Re",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Christopher Re",
   "pi_email_addr": "chrismre@cs.stanford.edu",
   "nsf_id": "000555316",
   "pi_start_date": "2017-09-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 N Park Street Suite 6401",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "807400",
   "pgm_ele_name": "EarthCube"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 1497798.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>The published literature contains much of the world's accumulated scientific knowledge and has served as a primary outlet for observations, data, model results, and scientific information for over 100 years. In the normal mode of operation, scholars wishing to aggregate published information in order to characterize a large-scale phenomenon or state of the natural world (e.g., the total volume of coal on Earth) needed to locate relevant documents and then manually find and extract specific information from them (e.g., the geographic location and thickness of coal beds described in scientific publications). Millions of dollars in trained personnel time in multiple different scientific domains have been devoted to these types of literature-derived knowledge base construction tasks. Online access alone does not allow satisfactory progress to be made because relevant information can be distributed across multiple different publishers and millions of individual documents, making any manual efforts prohibitively costly and impossible to reproduce. As rates of publication rise, it is only become more difficult to fully leverage past investments in data acquisition.</span></p>\n<p><span>Our NSF EarthCube-supported project has overcome several of the barriers that limit the ability of researchers to aggregate and reuse published information, thereby providing a mechanism to improve the rates at which new scientific and societally-relevant discovery and data reuse can proceed. Specifically, GeoDeepDive has established the first end-to-end pipeline that solves the challenges that are presented by document access, discovery, as well as the computational demands that are imposed by the need to process, analyze, and synthesize results from across millions of documents. We have released new, publicly-accessible online functionality that advances the ability of individuals in academia, industry, and government to harness scholarly publications in their local workflows. One of the simplest, but most readily deployable and powerful of these capabilities is the GeoDeepDive API, which offers an open web platform for building locally-hosted applications that are tailored to specific research, education, or policy needs. The API is open and can be used by anyone to build applications that connect directly to relevant publications and specific terms and phrases within the full text of publications. GeoDeepDive also provides a first-of-its-kind mechanism for users to automatically pre-label hundreds of thousands of terms across millions of publications and to then summarize the results quantitatively or build and deploy custom applications that locate and extract information from the set of documents containing those terms. For example, the output of document software tools, like Natural Language Processing (NLP), produced at scale by GeoDeepDive, can be used as the input for user applications that find and extract specific relationships between terms, thereby saving hundreds of hours or more of manual effort and reducing cost and time-to-science. A simple example illustrating both scientific impact and methodology have been published by PI Peters, along with example code.</span></p>\n<p><span>In addition to building a scalable, dependable infrastructure to host an ever-growing digital library that is coupled to high throughput computing infrastructure, work supported in part by this NSF project stimulated key developments in the DeepDive machine reading and learning system for knowledge base construction. This work, led by co-PI Chris Re, has had numerous spinoffs, including contributions to fundamental components to the startup company Lattice Data (Re was CTO), which was acquired by Apple Computer in 2017. Continued work by Re's group on the next-generation systems designed to address the multi-modal complexities of publications (i.e., information distributed across text, tables, and figures) has also spilled over into many different domains, ranging from industry to law enforcement, with far-reaching improvements to the state-of-the-art. Re's work is intersecting with GeoDeepDive once again as the document acquisition and processing system surpasses the 8 million document mark. This new phase in the project promises to rapidly advance our ability to better find, synthesize, and make actionable the data and information now locked as \"dark data\" within scientific publications.</span></p>\n<p>In summary, this NSF EarthCube Building Block project was designed and implemented to provide a first-of-its-kind integrative platform for harnessing publications and the information and the data they contain within academic, industry, and governmental workflows. In all of these respects, this Building Block has achieved its primary goal: it provides first-of-its kind capabilities in several different dimensions, ranging from automated document access, storage and metadata management that is still acquiring approximately 10,000 new and archival scientific publications per day, to associated ground-breaking text and data mining usage rights, to computational horsepower required to take advantage of more than 8.2 million documents. GeoDeepDive infrastructure will continue to support today's and tomorrow's machine reading, learning and automated knowledge base construction applications, providing a platform for reducing time-to-science and maximizing the return on investment from data and knowledge that is disseminated within scientific publications.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/25/2018<br>\n\t\t\t\t\tModified by: Shanan&nbsp;E&nbsp;Peters</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542223006509_lattice--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542223006509_lattice--rgov-800width.jpg\" title=\"Lattice Data\"><img src=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542223006509_lattice--rgov-66x44.jpg\" alt=\"Lattice Data\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Lattice Data was a startup company created by co-PI Chris R?'s group. The startup was ultimately acquired by Apple Computer in 2017. This NSF project contributed substantively to several components in the development of key tech, including the Ph.D. work of Ce Zhang.</div>\n<div class=\"imageCredit\">Lattice Data</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Shanan&nbsp;E&nbsp;Peters</div>\n<div class=\"imageTitle\">Lattice Data</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542226686685_GDD--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542226686685_GDD--rgov-800width.jpg\" title=\"GDD status\"><img src=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542226686685_GDD--rgov-66x44.jpg\" alt=\"GDD status\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Snaptshot of GDD total document count and acquisition status as of mid-November 2018. GDD has produced one the worlds largest sources for newly-published and archival full-text content that can be used for text and data mining.</div>\n<div class=\"imageCredit\">geodeepdive.org</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Shanan&nbsp;E&nbsp;Peters</div>\n<div class=\"imageTitle\">GDD status</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542997439702_gdd--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542997439702_gdd--rgov-800width.jpg\" title=\"Schematic GDD Infrastructure\"><img src=\"/por/images/Reports/POR/2018/1343760/1343760_10270939_1542997439702_gdd--rgov-66x44.jpg\" alt=\"Schematic GDD Infrastructure\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">General overview of GDD infrastructure produced as a primary outcome of this project. The infrastructure is automated and functionality, including document acquisition, continues past the end-date of this award.</div>\n<div class=\"imageCredit\">Shanan Peters</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Shanan&nbsp;E&nbsp;Peters</div>\n<div class=\"imageTitle\">Schematic GDD Infrastructure</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe published literature contains much of the world's accumulated scientific knowledge and has served as a primary outlet for observations, data, model results, and scientific information for over 100 years. In the normal mode of operation, scholars wishing to aggregate published information in order to characterize a large-scale phenomenon or state of the natural world (e.g., the total volume of coal on Earth) needed to locate relevant documents and then manually find and extract specific information from them (e.g., the geographic location and thickness of coal beds described in scientific publications). Millions of dollars in trained personnel time in multiple different scientific domains have been devoted to these types of literature-derived knowledge base construction tasks. Online access alone does not allow satisfactory progress to be made because relevant information can be distributed across multiple different publishers and millions of individual documents, making any manual efforts prohibitively costly and impossible to reproduce. As rates of publication rise, it is only become more difficult to fully leverage past investments in data acquisition.\n\nOur NSF EarthCube-supported project has overcome several of the barriers that limit the ability of researchers to aggregate and reuse published information, thereby providing a mechanism to improve the rates at which new scientific and societally-relevant discovery and data reuse can proceed. Specifically, GeoDeepDive has established the first end-to-end pipeline that solves the challenges that are presented by document access, discovery, as well as the computational demands that are imposed by the need to process, analyze, and synthesize results from across millions of documents. We have released new, publicly-accessible online functionality that advances the ability of individuals in academia, industry, and government to harness scholarly publications in their local workflows. One of the simplest, but most readily deployable and powerful of these capabilities is the GeoDeepDive API, which offers an open web platform for building locally-hosted applications that are tailored to specific research, education, or policy needs. The API is open and can be used by anyone to build applications that connect directly to relevant publications and specific terms and phrases within the full text of publications. GeoDeepDive also provides a first-of-its-kind mechanism for users to automatically pre-label hundreds of thousands of terms across millions of publications and to then summarize the results quantitatively or build and deploy custom applications that locate and extract information from the set of documents containing those terms. For example, the output of document software tools, like Natural Language Processing (NLP), produced at scale by GeoDeepDive, can be used as the input for user applications that find and extract specific relationships between terms, thereby saving hundreds of hours or more of manual effort and reducing cost and time-to-science. A simple example illustrating both scientific impact and methodology have been published by PI Peters, along with example code.\n\nIn addition to building a scalable, dependable infrastructure to host an ever-growing digital library that is coupled to high throughput computing infrastructure, work supported in part by this NSF project stimulated key developments in the DeepDive machine reading and learning system for knowledge base construction. This work, led by co-PI Chris Re, has had numerous spinoffs, including contributions to fundamental components to the startup company Lattice Data (Re was CTO), which was acquired by Apple Computer in 2017. Continued work by Re's group on the next-generation systems designed to address the multi-modal complexities of publications (i.e., information distributed across text, tables, and figures) has also spilled over into many different domains, ranging from industry to law enforcement, with far-reaching improvements to the state-of-the-art. Re's work is intersecting with GeoDeepDive once again as the document acquisition and processing system surpasses the 8 million document mark. This new phase in the project promises to rapidly advance our ability to better find, synthesize, and make actionable the data and information now locked as \"dark data\" within scientific publications.\n\nIn summary, this NSF EarthCube Building Block project was designed and implemented to provide a first-of-its-kind integrative platform for harnessing publications and the information and the data they contain within academic, industry, and governmental workflows. In all of these respects, this Building Block has achieved its primary goal: it provides first-of-its kind capabilities in several different dimensions, ranging from automated document access, storage and metadata management that is still acquiring approximately 10,000 new and archival scientific publications per day, to associated ground-breaking text and data mining usage rights, to computational horsepower required to take advantage of more than 8.2 million documents. GeoDeepDive infrastructure will continue to support today's and tomorrow's machine reading, learning and automated knowledge base construction applications, providing a platform for reducing time-to-science and maximizing the return on investment from data and knowledge that is disseminated within scientific publications.\n\n\t\t\t\t\tLast Modified: 11/25/2018\n\n\t\t\t\t\tSubmitted by: Shanan E Peters"
 }
}