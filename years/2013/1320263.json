{
 "awd_id": "1320263",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Light-weight Architectural Schemes for Resilient  High-performance Microprocessors",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2013-07-01",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 492844.0,
 "awd_amount": 492844.0,
 "awd_min_amd_letter_date": "2013-06-27",
 "awd_max_amd_letter_date": "2013-06-27",
 "awd_abstract_narration": "In future technology generations, smaller and more transistors  \r\noperating at low supply voltages and high clock speeds will be  \r\nincreasingly susceptible to many different resiliency problems, such  \r\nas soft errors, wear-out issues, hard errors, and off- and on-chip bus  \r\nbit errors. These errors may cause silent data corruption, application  \r\naborts, or system crashes in high-performance microprocessors and  \r\ncomputer systems. Previous techniques for addressing these errors  \r\nincur significant performance and power overheads despite  \r\noptimizations, and often require invasive changes that incur high  \r\nimplementation complexity.\r\n\r\nIn this research project, the investigators propose a novel,  \r\nlight-weight, yet highly-effective architectural approach to processor  \r\nreliability that incurs much lower overheads than existing approaches  \r\nby leveraging key architectural observations about the problems.\r\n\r\nThis project's innovative approach for the detection of soft errors,  \r\nwear-out, and hard errors is based on detecting execution anomalies  \r\nthat are triggered by errors, without using redundant execution. By  \r\nexploiting the notion of value locality, this project generalizes  \r\nanomalies to include unexpected values as well as conditions (e.g.,  \r\nmemory access exceptions) and provides significant coverage which  \r\nincludes the most problematic cases of silent data corruption. For  \r\nrecovery from soft errors, the project's investigators propose a  \r\nretry-based scheme that avoids adding any hardware overhead to achieve  \r\nrecovery by using existing spare speculative resources in the  \r\nprocessor. For off-chip bus bit errors, the investigators propose a  \r\nnovel bit interleaving scheme that reduces the chances of multiple  \r\nbits in a single error correcting code (ECC)-protected data unit being \r\ncorrupted undetectably or uncorrectably. Like the other schemes, this\r\ninterleaving imposes minimal power, performance, and complexity overhead.\r\n\r\nThis project targets achieving reliability while keeping power,  \r\nperformance, and hardware overheads low, an important goal for the  \r\nU.S. microprocessor and computer hardware industry.  The project's  \r\ninvestigators are committed to releasing the research artifacts as  \r\nopen-source software to be used by the research community. The  \r\ngraduate students working on this project will be trained in  \r\narchitecture and reliability issues and will be well-positioned to  \r\njoin the U.S. computer hardware industry. This project will also  \r\nsupport educational activities such as homework and term projects in  \r\nundergraduate and graduate courses as well as outreach activities of  \r\nvarious centers at Purdue with which the investigators are involved.  \r\nWith a woman as one of the investigators, the project will act as a  \r\nbasis for encouraging women to join graduate programs in electrical  \r\nand computer engineering.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Terani",
   "pi_last_name": "Vijaykumar",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Terani N Vijaykumar",
   "pi_email_addr": "vijay@ecn.purdue.edu",
   "nsf_id": "000337724",
   "pi_start_date": "2013-06-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Irith",
   "pi_last_name": "Pomeranz",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Irith Pomeranz",
   "pi_email_addr": "pomeranz@ecn.purdue.edu",
   "nsf_id": "000387349",
   "pi_start_date": "2013-06-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072017",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "794100",
   "pgm_ele_name": "COMPUTER ARCHITECTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 492844.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Soft error susceptibility is a growing concern with continued CMOS scaling in modern, high-performance, general-purpose microprocessors. Smaller transistors and lower voltages achieve by contiued scaling&nbsp; exacerbate the soft error problem. Previous work explores full- and partial-redundancy schemes in hardware and software for soft-fault tolerance. However, full-redundancy schemes incur high performance and energy overheads whereas partial-redundancy schemes achieve low coverage.</p>\n<p>Value locality is well-known where values generated&nbsp; in a computation fall within small neighborhoods in the value space, rather than be&nbsp; spread arbitrarily &nbsp; over the entire value space. Previous value-locality efforts have attempted to improve performance by&nbsp; exploiting&nbsp; value localityto predict values faster than they can be&nbsp; computed due to inherent long latencies such as cache misses. This project&nbsp; exploits  value locaity to detect soft errors which  would usually perturb a value  making it fall outside value locality  neighborhoods. Such detection does not require redundancy and therefore can avoid the corresponding overheads. To this end, the authors&nbsp;  employ hardware filters that capture value locality  neighborhoods by  capturing which bit positions are unchanging &nbsp;0's or  1's and which  positions change so that a new value matching in the  unchanging  positions does not flag an error and does otherwise. Upon  detection, the authors&nbsp;  leverage modern processor pipeline's in-built abiltiy to  roll-back  computation to try to correct the error. Because the  roll-back (i.e.,  redundant execution) occurs only when an error is  flagged or on a false  postiive (which are about 2% of all  instructions), the proposed&nbsp; method acheives  lower power and performance  overheads than previous full-redundancy  approaches.</p>\n<p>In contrast to value prediction for performance where the prediciton has to exactly match the actual value, exploiting value locality to detect soft errors has to&nbsp; be accurate enough to detect value perturbation without &nbsp; prediction the value with 100% accuracy.&nbsp; An initial study, called Perturbation Based Fault Screening (PBFS), explored exploiting value locality to provide hints<br />of soft faults whenever a value falls outside its neighborhood.&nbsp; However, PBFS achieves low coverage; straightforwardly improving the coverage results in high false-positive rates, and performance and energy overheads.</p>\n<p>The authors propose FaultHound, a value-locality-based soft-fault tolerance scheme, which employs five novel mechanisms to address PBFS&rsquo;s limitations: (1) a scheme to cluster the filters via an inverted organization of the filter tables to reinforce learning and reduce the false-positive rates; (2) a learning scheme for ignoring the delinquent bit positions that raise repeated false alarms, to reduce further the false- positive rate; (3) a light-weight predecessor replay scheme instead of a full rollback to reduce the performance and energy penalty of the remaining false positives; (4) a simple scheme to distinguish rename faults, which require rollback instead of replay for recovery, from false positives to avoid unnecessary rollback penalty; and (5) a detection scheme, which avoids rollback, for the load-store queue which is not covered by the&nbsp; replay. Using simulations, the authors&nbsp; show that while PBFS achieves either low coverage (30%), or high false-positive rates (8%) with high performance overheads (97%), FaultHound achieves higher coverage (75%) and lower false-positive rates (3%) with lower performance and energy overheads (10% and 25%). Further, full-redundancy schemes scaled to achieve equal coverage of 75% result in&nbsp;  13% performance loss and 57% energy overheads. These results are for a  broad range of benchmarks which include commercial workloads (TPC-C-ilke online transaction processing, SPECjbb, and apache webserver), SPECint,  SPECFP, and SPLASH.</p>\n<p>Considering all the three metrics of coverage, performance and energy overheads, FaultHound performs better than previous redundancy-based and value-locality-based schemes which perform well against only one or two of these metrics. Because of this attractive combination of features, FaultHound will likely be important in the path of continued CMOS&nbsp; scaling in modern microprocessors.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/28/2017<br>\n\t\t\t\t\tModified by: T.&nbsp;N&nbsp;Vijaykumar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSoft error susceptibility is a growing concern with continued CMOS scaling in modern, high-performance, general-purpose microprocessors. Smaller transistors and lower voltages achieve by contiued scaling  exacerbate the soft error problem. Previous work explores full- and partial-redundancy schemes in hardware and software for soft-fault tolerance. However, full-redundancy schemes incur high performance and energy overheads whereas partial-redundancy schemes achieve low coverage.\n\nValue locality is well-known where values generated  in a computation fall within small neighborhoods in the value space, rather than be  spread arbitrarily   over the entire value space. Previous value-locality efforts have attempted to improve performance by  exploiting  value localityto predict values faster than they can be  computed due to inherent long latencies such as cache misses. This project  exploits  value locaity to detect soft errors which  would usually perturb a value  making it fall outside value locality  neighborhoods. Such detection does not require redundancy and therefore can avoid the corresponding overheads. To this end, the authors   employ hardware filters that capture value locality  neighborhoods by  capturing which bit positions are unchanging  0's or  1's and which  positions change so that a new value matching in the  unchanging  positions does not flag an error and does otherwise. Upon  detection, the authors   leverage modern processor pipeline's in-built abiltiy to  roll-back  computation to try to correct the error. Because the  roll-back (i.e.,  redundant execution) occurs only when an error is  flagged or on a false  postiive (which are about 2% of all  instructions), the proposed  method acheives  lower power and performance  overheads than previous full-redundancy  approaches.\n\nIn contrast to value prediction for performance where the prediciton has to exactly match the actual value, exploiting value locality to detect soft errors has to  be accurate enough to detect value perturbation without   prediction the value with 100% accuracy.  An initial study, called Perturbation Based Fault Screening (PBFS), explored exploiting value locality to provide hints\nof soft faults whenever a value falls outside its neighborhood.  However, PBFS achieves low coverage; straightforwardly improving the coverage results in high false-positive rates, and performance and energy overheads.\n\nThe authors propose FaultHound, a value-locality-based soft-fault tolerance scheme, which employs five novel mechanisms to address PBFS?s limitations: (1) a scheme to cluster the filters via an inverted organization of the filter tables to reinforce learning and reduce the false-positive rates; (2) a learning scheme for ignoring the delinquent bit positions that raise repeated false alarms, to reduce further the false- positive rate; (3) a light-weight predecessor replay scheme instead of a full rollback to reduce the performance and energy penalty of the remaining false positives; (4) a simple scheme to distinguish rename faults, which require rollback instead of replay for recovery, from false positives to avoid unnecessary rollback penalty; and (5) a detection scheme, which avoids rollback, for the load-store queue which is not covered by the  replay. Using simulations, the authors  show that while PBFS achieves either low coverage (30%), or high false-positive rates (8%) with high performance overheads (97%), FaultHound achieves higher coverage (75%) and lower false-positive rates (3%) with lower performance and energy overheads (10% and 25%). Further, full-redundancy schemes scaled to achieve equal coverage of 75% result in   13% performance loss and 57% energy overheads. These results are for a  broad range of benchmarks which include commercial workloads (TPC-C-ilke online transaction processing, SPECjbb, and apache webserver), SPECint,  SPECFP, and SPLASH.\n\nConsidering all the three metrics of coverage, performance and energy overheads, FaultHound performs better than previous redundancy-based and value-locality-based schemes which perform well against only one or two of these metrics. Because of this attractive combination of features, FaultHound will likely be important in the path of continued CMOS  scaling in modern microprocessors.\n\n\t\t\t\t\tLast Modified: 09/28/2017\n\n\t\t\t\t\tSubmitted by: T. N Vijaykumar"
 }
}