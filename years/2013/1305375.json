{
 "awd_id": "1305375",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "II-NEW: An Experimental Platform for Investigating Energy-Performance Tradeoffs for Systems with Deep Memory Hierarchies",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2013-08-30",
 "awd_max_amd_letter_date": "2013-08-30",
 "awd_abstract_narration": "As the scale and complexity of computing and data infrastructures supporting science and engineering grow, power costs are becoming important concerns in terms of costs, reliability and overall sustainability. As a result, it is becoming increasingly important to understand power/performance behaviors and tradeoffs from an application perspective for emerging system configuration, i.e., those with multiple cores, deep memory hierarchies and accelerators. This project builds an instrumented experimental platform that supports such an understanding, and enables research and training activities in this area. Specifically, the proposed experimental platform is composed of nodes with a deep memory architecture that contains four different levels: DRAM, PCIe-based non-volatile memory, solid-state drive and spinning hard disk, in addition to accelerators. Power metering is deployed as part of the infrastructure.\r\n\r\nThe experimental platform enables the experimental exploration of the power/performance behaviors of large scale computing systems and datacenters as well as compute and data intensive application they support, and uniquely supports research toward understanding the management and optimization of these systems and applications. It also enables research in multiple areas, including: application-aware cross-layer management, power-performance tradeoffs for data-intensive scientific workflows and thermal implications of deep memory hierarchies in virtualized Cloud environments.\r\n \r\nData and compute intensive applications are becoming increasingly critical to a wide range of domains, and the ability to develop large-scale and sustainable platforms and software infrastructure to support these applications will have significant impact in driving research and innovations in these domains. The developed experimental platform enables key research activities to support this. It provides important insights that will impact the realization and sustainability of very large-scale infrastructures necessary for current and emerging data and compute intensive applications. The infrastructure also provides an important infrastructure for education and training in different areas related to power management, energy efficiency, data management, memory management, and virtualization.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Manish",
   "pi_last_name": "Parashar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Manish Parashar",
   "pi_email_addr": "manish.parashar@utah.edu",
   "nsf_id": "000148826",
   "pi_start_date": "2013-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dario",
   "pi_last_name": "Pompili",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dario Pompili",
   "pi_email_addr": "pompili@rutgers.edu",
   "nsf_id": "000501685",
   "pi_start_date": "2013-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ivan",
   "pi_last_name": "Rodero",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ivan Rodero",
   "pi_email_addr": "ivan.rodero@utah.edu",
   "nsf_id": "000631333",
   "pi_start_date": "2013-08-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers Discovery Informatics Institute",
  "perf_str_addr": "94 Brett Road",
  "perf_city_name": "Piscataway",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "088548058",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "NJ06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Understanding power/performance behaviors and tradeoffs from an application perspective for emerging system configuration, i.e., those with multiple cores, deep memory hierarchies and accelerators has become a critical concern. The major goal of this project was to develop an instrumented experimental platform that can enable such an understanding and can fundamentally enable research and training activities in this area. This experimental infrastructure enables the experimental exploration of the power/performance behaviors of large scale computing systems and datacenters as well as compute and data intensive application they support and supports research toward the understanding of the management and optimization of these systems and applications. Central to these efforts is understanding energy/performance tradeoffs for data-intensive scientific workflows and develop smarter application-aware cross-layer power management at different levels.&nbsp;</p>\n<p>There are many research accomplishments under these goals. A list of some of the outcomes of the project is provided below.</p>\n<ul>\n<li>We designed and constructed CAPER (Computational and      dAta&nbsp;Platform for&nbsp;Energy efficiency&nbsp;Research), which is a      unique and flexible experimental platform composed of nodes with a deep      memory architecture with four different levels: DRAM, PCIe-based NVRAM,      solid-state drive (SSD) and spinning hard disk. CAPER combines high      performance Intel Xeon processors with a complete deep memory hierarchy,      latest generation coprocessors (i.e., Intel Xeon Phi), high-performance      network interconnects, and comprehensive system power instrumentation.</li>\n<li>We implemented a comprehensive and scalable monitoring      platform using big data technologies such as HDFS and Apache Hadoop.</li>\n<li>We studied the costs incurred by the different in-situ      analysis strategies in terms of execution time, scalability and power      consumption, which is a fundamental issue of a large-scale workflows.</li>\n<li>We characterized the performance and energy behaviors      of each level of the memory hierarchy and evaluated the performance and      energy consumption of different data management strategies and data      exchange patterns, as well as the energy/performance tradeoffs associated      with data placement, data movement and data processing.</li>\n<li>We studied performance and power/energy tradeoffs of      different data processing configurations and data movement strategies, and      how to balance these tradeoffs with the quality of solution.</li>\n<li>We evaluated the costs incurred by the different      in-situ data analysis strategies for large-scale systems (e.g., combustion      simulations) in terms of execution time, scalability and power consumption.</li>\n<li>We explored the local recovery for stencil-based      parallel applications, which represent a significant set of physical      simulations.</li>\n<li>We explored how to manage power budgets dynamically      leveraging the properties of AMR (Adaptive Mesh Refinement) algorithms,      which is a new approach for power and workload management.</li>\n<li>We supported other research activities that required      the use of CAPER power instrumentation and/or the use of deep memory      hierarchies such as the study of energy-efficient autonomic cyber-security      strategies, data analytics at large scale, imaging-based medical research,</li>\n</ul>\n<p>In addition, this project provided training to postdocs and graduate students. The work resulted in a plethora of publications including referenced journal and conference papers, and numerous presentations. Keynotes and invited talks were given at many esteemed forums such as the ACM/IEEE International Conferences for High Performance Computing, Networking, Storage and Analysis, IEEE International Parallel &amp; Distributed Processing Symposium, IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, in many countries throughout the world (e.g., Japan, Colombia, Italy, Germany, Spain).</p>\n<p>Some of the research findings were also disseminated to the general public via the educational outreach programs such as the Aresty Research Center (Division of Undergraduate Academic Affairs at Rutgers University) and the New Jersey Governor&rsquo;s School of Engineering and Technology.</p>\n<p>An experimental instrument platform has been developed, which provides an instrument for conducting research in different areas related to the study of power/energy efficiency, deep memory hierarchies and data analysis. Further, this research has enabled the development of other NSF-funded research projects and has influenced the design of large-scale computing platforms such as Caliburn at Rutgers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/27/2017<br>\n\t\t\t\t\tModified by: Manish&nbsp;Parashar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nUnderstanding power/performance behaviors and tradeoffs from an application perspective for emerging system configuration, i.e., those with multiple cores, deep memory hierarchies and accelerators has become a critical concern. The major goal of this project was to develop an instrumented experimental platform that can enable such an understanding and can fundamentally enable research and training activities in this area. This experimental infrastructure enables the experimental exploration of the power/performance behaviors of large scale computing systems and datacenters as well as compute and data intensive application they support and supports research toward the understanding of the management and optimization of these systems and applications. Central to these efforts is understanding energy/performance tradeoffs for data-intensive scientific workflows and develop smarter application-aware cross-layer power management at different levels. \n\nThere are many research accomplishments under these goals. A list of some of the outcomes of the project is provided below.\n\nWe designed and constructed CAPER (Computational and      dAta Platform for Energy efficiency Research), which is a      unique and flexible experimental platform composed of nodes with a deep      memory architecture with four different levels: DRAM, PCIe-based NVRAM,      solid-state drive (SSD) and spinning hard disk. CAPER combines high      performance Intel Xeon processors with a complete deep memory hierarchy,      latest generation coprocessors (i.e., Intel Xeon Phi), high-performance      network interconnects, and comprehensive system power instrumentation.\nWe implemented a comprehensive and scalable monitoring      platform using big data technologies such as HDFS and Apache Hadoop.\nWe studied the costs incurred by the different in-situ      analysis strategies in terms of execution time, scalability and power      consumption, which is a fundamental issue of a large-scale workflows.\nWe characterized the performance and energy behaviors      of each level of the memory hierarchy and evaluated the performance and      energy consumption of different data management strategies and data      exchange patterns, as well as the energy/performance tradeoffs associated      with data placement, data movement and data processing.\nWe studied performance and power/energy tradeoffs of      different data processing configurations and data movement strategies, and      how to balance these tradeoffs with the quality of solution.\nWe evaluated the costs incurred by the different      in-situ data analysis strategies for large-scale systems (e.g., combustion      simulations) in terms of execution time, scalability and power consumption.\nWe explored the local recovery for stencil-based      parallel applications, which represent a significant set of physical      simulations.\nWe explored how to manage power budgets dynamically      leveraging the properties of AMR (Adaptive Mesh Refinement) algorithms,      which is a new approach for power and workload management.\nWe supported other research activities that required      the use of CAPER power instrumentation and/or the use of deep memory      hierarchies such as the study of energy-efficient autonomic cyber-security      strategies, data analytics at large scale, imaging-based medical research,\n\n\nIn addition, this project provided training to postdocs and graduate students. The work resulted in a plethora of publications including referenced journal and conference papers, and numerous presentations. Keynotes and invited talks were given at many esteemed forums such as the ACM/IEEE International Conferences for High Performance Computing, Networking, Storage and Analysis, IEEE International Parallel &amp; Distributed Processing Symposium, IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, in many countries throughout the world (e.g., Japan, Colombia, Italy, Germany, Spain).\n\nSome of the research findings were also disseminated to the general public via the educational outreach programs such as the Aresty Research Center (Division of Undergraduate Academic Affairs at Rutgers University) and the New Jersey Governor?s School of Engineering and Technology.\n\nAn experimental instrument platform has been developed, which provides an instrument for conducting research in different areas related to the study of power/energy efficiency, deep memory hierarchies and data analysis. Further, this research has enabled the development of other NSF-funded research projects and has influenced the design of large-scale computing platforms such as Caliburn at Rutgers.\n\n\t\t\t\t\tLast Modified: 01/27/2017\n\n\t\t\t\t\tSubmitted by: Manish Parashar"
 }
}