{
 "awd_id": "1258213",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Small: A Physical Vocabulary for Human-Robot Interaction",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2012-06-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 63498.0,
 "awd_amount": 97098.0,
 "awd_min_amd_letter_date": "2012-10-22",
 "awd_max_amd_letter_date": "2013-05-09",
 "awd_abstract_narration": "We are the victims of our own success.  We can now deploy mobile robots in real-world environments and have them operate completely autonomously for extended periods of time.  We no longer have to surround our robots with graduate student wranglers to keep them functional, and to keep the general public at a safe distance.  These technical successes mean that members of the general public must now interact directly with robots, without the aid of an interpreter.  But members of the public are poorly equipped for such interactions, since they are unfamiliar with real robots and how they work.  Thus, the interactions often go poorly; the robot is hindered in performing its task, and the human is unhappy.  For people to be comfortable interacting with a robot, they must feel that they understand what it's thinking, what it's trying to do, and the actions that it will take.  Moreover, people must be able to deduce this information from observing the robot for a short period of time, just as we do with other humans that we encounter.  The fundamental problem here is that humans communicate a wealth of information by means of a non-verbal \"vocabulary\" in which body language (how we stand, how we hold our arms, etc.), eye contact, nods, and other subtle cues ostensibly not essential to the task at hand play significant roles.  We do this naturally, and without conscious effort.  Taken in context, this information allows us to infer another person's state of mind, goals, and intentions with surprising accuracy; this, in turn, allows us to predict how a given interaction will unfold, and gives us some control over it.  Because people take this ability for granted, they suffer when it is absent, as is currently often the case when interacting with a mobile robot.  The PI intends to address this deficiency in the current project.  He argues that to make human-robot interactions as natural as possible, we must equip robots with our physical vocabulary and ensure that they use it appropriately, following social norms.  To achieve this goal the PI will turn to the performing arts, where actors are trained to express themselves physically.  A good actor can convey a vast amount of information about a character's state of mind, goals, and intentions by simply walking across the stage in a particular way.  The actions may be styled, larger-than-life, or subtle, but they are intended to convey information about the character's internal mental state.  The techniques that actors employ have been honed and refined for hundreds of years and tested for effectiveness on the general public.  In this research, the PI will exploit such insights and skills to develop a physical vocabulary that can communicate beliefs, intentions, and goals to humans interacting with a robot, thereby enabling people to better predict the robot's actions.  Finally, the PI will rigorously evaluate these actions to verify that they are actually useful.  \r\n\r\nBroader Impacts:  Robots are becoming more and more a part of our lives, and members of the public will be forced to deal with them sooner or later.  If we have an understanding of the physical aspects of these interactions, the integration of robots into our everyday lives will be made much less painful and distressing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Smart",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "William D Smart",
   "pi_email_addr": "smartw@oregonstate.edu",
   "nsf_id": "000111047",
   "pi_start_date": "2012-10-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Oregon State University",
  "inst_street_address": "1500 SW JEFFERSON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CORVALLIS",
  "inst_state_code": "OR",
  "inst_state_name": "Oregon",
  "inst_phone_num": "5417374933",
  "inst_zip_code": "973318655",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "OR04",
  "org_lgl_bus_name": "OREGON STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MZ4DYXE1SL98"
 },
 "perf_inst": {
  "perf_inst_name": "Oregon State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OR",
  "perf_st_name": "Oregon",
  "perf_zip_code": "973318507",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "OR04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7218",
   "pgm_ref_txt": "RET SUPP-Res Exp for Tchr Supp"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0109",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000910DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2009,
   "fund_oblg_amt": 63498.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 8000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 25600.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this work is to understand how robots can use \"body language\" to let them work better with people. &nbsp;When we interact with other people, we use a variety of (often subtle) body language to show our intentions to others. &nbsp;For example, when we meet someone in a narrow corridor, we often look at them to indicate that we're giving them the right of way, and then move to the right hand side of the corridor. &nbsp;These sorts of actions, which aren't really necessary to accomplish the tasks, and the social rules that they follow, make our interactions with other people more efficient and effective. &nbsp;The central question of this work is whether or not the same sorts of thing will help human-robot interactions.</p>\n<p><br />The main things that we learned from the work are that it is important &nbsp;for the robots to treat people differently from the way they treat obstacles. &nbsp;For example, a table does not care how close the robot comes to it, while a human has a very well-defined \"comfort zone\", and gets uneasy if the robot (or another human, for that matter) gets too close. &nbsp;Recognizing people, figuring our where they're going, and respecting this comfort zone is going to be very important if we are ever going to see robots and people work together in the real world.</p>\n<p><br />We also learned that the gestures that people make when interacting with others can often mean different things when done by a robot. When a human nods or makes eye contact, there is a lot of subtlety in that gesture. &nbsp;Most robots are not capable of this level of subtle movement, so their gestures are more easily misunderstood.</p>\n<p><br />One of the biggest practical things to come out of this work was a new set of software that lets robots navigate around the world, and interact appropriately with people. &nbsp;This is now part of a very widely-used set of software for all sorts of robots, and there is a good chance that it will start to make it into commercial products at some point in the future. &nbsp;The things that we found out as part of this work will make robots interact more efficiently and effectively with people in the real world, using the new software that we wrote.</p>\n<p>A number of graduate students and undergraduates from all over the United States contributed to this work as part of their degree programs. &nbsp;All of the software that was written during this project has been made freely-available to both the academic and industrial robotics communities, in the hopes that it will prove useful, and that others will build on the work we have started.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/09/2013<br>\n\t\t\t\t\tModified by: William&nbsp;D&nbsp;Smart</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this work is to understand how robots can use \"body language\" to let them work better with people.  When we interact with other people, we use a variety of (often subtle) body language to show our intentions to others.  For example, when we meet someone in a narrow corridor, we often look at them to indicate that we're giving them the right of way, and then move to the right hand side of the corridor.  These sorts of actions, which aren't really necessary to accomplish the tasks, and the social rules that they follow, make our interactions with other people more efficient and effective.  The central question of this work is whether or not the same sorts of thing will help human-robot interactions.\n\n\nThe main things that we learned from the work are that it is important  for the robots to treat people differently from the way they treat obstacles.  For example, a table does not care how close the robot comes to it, while a human has a very well-defined \"comfort zone\", and gets uneasy if the robot (or another human, for that matter) gets too close.  Recognizing people, figuring our where they're going, and respecting this comfort zone is going to be very important if we are ever going to see robots and people work together in the real world.\n\n\nWe also learned that the gestures that people make when interacting with others can often mean different things when done by a robot. When a human nods or makes eye contact, there is a lot of subtlety in that gesture.  Most robots are not capable of this level of subtle movement, so their gestures are more easily misunderstood.\n\n\nOne of the biggest practical things to come out of this work was a new set of software that lets robots navigate around the world, and interact appropriately with people.  This is now part of a very widely-used set of software for all sorts of robots, and there is a good chance that it will start to make it into commercial products at some point in the future.  The things that we found out as part of this work will make robots interact more efficiently and effectively with people in the real world, using the new software that we wrote.\n\nA number of graduate students and undergraduates from all over the United States contributed to this work as part of their degree programs.  All of the software that was written during this project has been made freely-available to both the academic and industrial robotics communities, in the hopes that it will prove useful, and that others will build on the work we have started.\n\n \n\n\t\t\t\t\tLast Modified: 12/09/2013\n\n\t\t\t\t\tSubmitted by: William D Smart"
 }
}