{
 "awd_id": "1353248",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Workshop: Robust Research in the Social, Behavioral, and Economic Sciences, February, 2014, Arlington, VA",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Deborah Olster",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2014-09-30",
 "tot_intn_awd_amt": 49553.0,
 "awd_amount": 49553.0,
 "awd_min_amd_letter_date": "2013-09-13",
 "awd_max_amd_letter_date": "2013-09-13",
 "awd_abstract_narration": "This award supports a two-day workshop to investigate integrative policies and procedures that would promote robust and reliable published findings in science. Questions have been raised in recent years about the replicability of various findings in the biological, medical, cognitive, and social sciences. Making sure that scientists understand and are rewarded for using methods that yield the most robust results  and incentivizing replication of results are among the many possible actions that could help ensure that published results are highly reliable. This workshop will explore these and other steps that might be taken to improve the validity of scientific findings.\r\n\r\nScientists build upon published scientific knowledge in both basic and applied research. When those published results are not reliable, scientific discoveries can be set back and time and money spent on non-productive avenues of research. Additionally, science is often used as a basis for decisions in policy and industry, which also necessitate reliability. Social and behavioral scientists have contributed to improvements in experimental and quasi-experimental designs, methodological procedures (e.g., random assignment & blind procedures), and statistical (e.g., meta-analytic) techniques. The proposed workshop will convene social, behavioral, and statistical scientists to consider: (i) the state of the science, (ii) possible improvements in scientific practice and procedures, (iii) the implications for science education and training, (iv) the implications for editorial policies and procedures, (v) the implications for research university policies and evaluation criteria, and (vi) the implications for federal funding policies and evaluation criteria. This workshop will bring together experts in each of these domains to develop a coherent set of recommendations for ensuring that published empirical findings in the social, behavioral, and economic sciences are robust and replicable and to identify research needs on this topic. A written report from this workshop will be produced and presentation materials shared widely. The effort will be led by members of the Subcommittee on Replication, a part of the Advisory Committee of the Directorate for Social, Behavioral, and Economic Sciences at the National Science Foundation.\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Cacioppo",
   "pi_mid_init": "T",
   "pi_sufx_name": "",
   "pi_full_name": "John T Cacioppo",
   "pi_email_addr": "Cacioppo@uchicago.edu",
   "nsf_id": "000396890",
   "pi_start_date": "2013-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Chicago",
  "inst_street_address": "5801 S ELLIS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CHICAGO",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "7737028669",
  "inst_zip_code": "606375418",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "IL01",
  "org_lgl_bus_name": "UNIVERSITY OF CHICAGO",
  "org_prnt_uei_num": "ZUE9HKT2CLC9",
  "org_uei_num": "ZUE9HKT2CLC9"
 },
 "perf_inst": {
  "perf_inst_name": "NSF",
  "perf_str_addr": "4201 Wilson Blvd.",
  "perf_city_name": "Arlington",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "222300001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "VA08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "139700",
   "pgm_ele_name": "Cross-Directorate  Activities"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 49553.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Scientists build upon published scientific<br />knowledge in both basic and applied research, and decision makers in industry<br />and public policy depend on reliable scientific evidence. When published<br />scientific results are not reliable, scientific advances can be stalled, time<br />and money can be spent on non-productive avenues of research, and economic and<br />government policies can go awry. Science is a human endeavor, with the<br />attendant potential for human biases, both unintentional and intentional.&nbsp; Social and behavioral scientists have<br />contributed to improvements in experimental and quasi-experimental designs,<br />methodological procedures (e.g., random assignment &amp; blind procedures), and<br />statistical (e.g., meta-analytic) techniques.&nbsp;<br />Given the questions that have been raised in recent years about the<br />replicability of various findings in the physical, biological, medical,<br />cognitive, and social sciences, this grant supported a workshop of social,<br />behavioral, and statistical scientists to consider:&nbsp; (i) the extent and cause of the problem<br />across the sciences, (ii) possible improvements in scientific practice and<br />procedures, (iii) the implications for science education and training, (iv) the<br />implications for editorial policies and procedures, (v) the implications for<br />research university policies and evaluation criteria, and (vi) the implications<br />for federal funding policies and evaluation criteria. The Workshop was held at<br />the National Science Foundation in Arlington, Virginia on February 20-21, 2014,<br />and included 6 panels, 31 participants, and 16 hours of plenary talks and<br />discussions.&nbsp; The grant also provided<br />support for the preparation of a report and workshop summaries by the<br />Subcommittee on Replication, a part of the Advisory Committee of the<br />Directorate for Social, Behavioral, and Economic Sciences at the National<br />Science Foundation (NSF SBE AC). A preliminary draft of this report, which<br />included twelve specific recommendations for improving empirical research<br />reproducibility, was shared with the Advisory Committee of the Directorate for<br />Social, Behavioral, and Economic Sciences at the National Science Foundation on<br />October 30, 2014.&nbsp; The final report and<br />summaries will be made available once they have been completed and approved by<br />the NSF SBE AC.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/13/2014<br>\n\t\t\t\t\tModified by: John&nbsp;T&nbsp;Cacioppo</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nScientists build upon published scientific\nknowledge in both basic and applied research, and decision makers in industry\nand public policy depend on reliable scientific evidence. When published\nscientific results are not reliable, scientific advances can be stalled, time\nand money can be spent on non-productive avenues of research, and economic and\ngovernment policies can go awry. Science is a human endeavor, with the\nattendant potential for human biases, both unintentional and intentional.  Social and behavioral scientists have\ncontributed to improvements in experimental and quasi-experimental designs,\nmethodological procedures (e.g., random assignment &amp; blind procedures), and\nstatistical (e.g., meta-analytic) techniques. \nGiven the questions that have been raised in recent years about the\nreplicability of various findings in the physical, biological, medical,\ncognitive, and social sciences, this grant supported a workshop of social,\nbehavioral, and statistical scientists to consider:  (i) the extent and cause of the problem\nacross the sciences, (ii) possible improvements in scientific practice and\nprocedures, (iii) the implications for science education and training, (iv) the\nimplications for editorial policies and procedures, (v) the implications for\nresearch university policies and evaluation criteria, and (vi) the implications\nfor federal funding policies and evaluation criteria. The Workshop was held at\nthe National Science Foundation in Arlington, Virginia on February 20-21, 2014,\nand included 6 panels, 31 participants, and 16 hours of plenary talks and\ndiscussions.  The grant also provided\nsupport for the preparation of a report and workshop summaries by the\nSubcommittee on Replication, a part of the Advisory Committee of the\nDirectorate for Social, Behavioral, and Economic Sciences at the National\nScience Foundation (NSF SBE AC). A preliminary draft of this report, which\nincluded twelve specific recommendations for improving empirical research\nreproducibility, was shared with the Advisory Committee of the Directorate for\nSocial, Behavioral, and Economic Sciences at the National Science Foundation on\nOctober 30, 2014.  The final report and\nsummaries will be made available once they have been completed and approved by\nthe NSF SBE AC.\n\n\t\t\t\t\tLast Modified: 11/13/2014\n\n\t\t\t\t\tSubmitted by: John T Cacioppo"
 }
}