{
 "awd_id": "1254106",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Modeling Dependencies via Graphs: Scalable Inference Methods for Massive Datasets",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2013-02-01",
 "awd_exp_date": "2019-01-31",
 "tot_intn_awd_amt": 560646.0,
 "awd_amount": 560646.0,
 "awd_min_amd_letter_date": "2013-01-18",
 "awd_max_amd_letter_date": "2017-01-11",
 "awd_abstract_narration": "This research centers on theoretical and applied research on learning and representation of high-dimensional data. The term high dimensionality refers to the property that the number of variables or unknowns is typically much larger than the number of observations available at hand. A key challenge is being able to represent and learn such phenomena with sample and computational requirements scaling favorably in the number of dimensions. This project addresses these challenges through a graphical approach by exploiting the inherent graphical structure present in many large data-sets.\r\n\r\nThis research considers modeling high-dimensional data through probabilistic graphical models, also known as Markov random fields. An important research thrust of this proposal is to develop novel algorithms for learning and inference under the framework of graphical models. Another important thrust of this proposal is to develop efficient scalable models for representing high-dimensional data beyond the traditional framework of graphical models. This research establishes strong theoretical guarantees for the developed methods, as well as applies them to real data in various domains, including genetic and financial data, and data from large online social networks such as Facebook and Twitter.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Animashree",
   "pi_last_name": "Anandkumar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Animashree Anandkumar",
   "pi_email_addr": "anima@caltech.edu",
   "nsf_id": "000542748",
   "pi_start_date": "2013-01-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "",
  "perf_city_name": "Irvine",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926173213",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "793600",
   "pgm_ele_name": "SIGNAL PROCESSING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7936",
   "pgm_ref_txt": "SIGNAL PROCESSING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 208142.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 227019.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 125485.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project involved machine learning in high dimensions using structures such as graphs and tensors. Probabilistic graphical models are a succinct representation of probability distributions in high dimensions, where dependencies among the variables are represented with a graph. It is especially challenging to learn in the presence of hidden variables since there is no supervision possible at the time of training. In a seminal work, supported by this grant, Prof. Anandkumar proposed a novel algorithm involving tensors. Prof. Anandkumar established the first theoretical guarantees for unsupervised learning of a broad class of latent variable models using tensor methods. She showed that these methods enjoy low computational and sample complexity, meaning that they require a limited amount of data and computation for learning.&nbsp; In her theory, she characterized conditions under which these methods succeed in finding the globally optimal solution, and they turn out to be mild and reasonable for most ML problems. Hence, these are the first methods to guarantee unsupervised learning of latent variable models. The tensor methods she developed utilize factorization of data moments (typically only third or fourth order).&nbsp;&nbsp; They can learn a broad class of models such as Gaussian mixtures (that can represent clusters), latent Dirichlet allocation (for document categorization), hidden Markov models (for time series), and network community models (for social networks). The tensor algorithms are also computationally efficient. In fact, they retain the computational efficiency of matrix methods, and are embarrassingly parallel. They present even more opportunities for parallelism compared to matrix methods. Thus, these methods enjoy &ldquo;best of both the worlds&rdquo;, <em>viz., </em>low computational and sample complexity, and also theoretical guarantees. She also proposed building tensor computations into neural network architectures. Current neural networks are built on matrix computations with simple non-linearities between the layers.&nbsp; Prof. Anandkumar found a natural opportunity to extend these architectures to use richer computational units involving tensors. These tensorized neural networks can better encode the data dimensions and higher-order correlations. They are more compact and generalize better in multiple domains, especially on challenging tasks such as long-term forecasting. Prof. Anandkumar has made these methods widely available through open-source frameworks such as Tensorly as well as cloud AI services such as Amazon SageMaker.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/03/2019<br>\n\t\t\t\t\tModified by: Animashree&nbsp;Anandkumar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project involved machine learning in high dimensions using structures such as graphs and tensors. Probabilistic graphical models are a succinct representation of probability distributions in high dimensions, where dependencies among the variables are represented with a graph. It is especially challenging to learn in the presence of hidden variables since there is no supervision possible at the time of training. In a seminal work, supported by this grant, Prof. Anandkumar proposed a novel algorithm involving tensors. Prof. Anandkumar established the first theoretical guarantees for unsupervised learning of a broad class of latent variable models using tensor methods. She showed that these methods enjoy low computational and sample complexity, meaning that they require a limited amount of data and computation for learning.  In her theory, she characterized conditions under which these methods succeed in finding the globally optimal solution, and they turn out to be mild and reasonable for most ML problems. Hence, these are the first methods to guarantee unsupervised learning of latent variable models. The tensor methods she developed utilize factorization of data moments (typically only third or fourth order).   They can learn a broad class of models such as Gaussian mixtures (that can represent clusters), latent Dirichlet allocation (for document categorization), hidden Markov models (for time series), and network community models (for social networks). The tensor algorithms are also computationally efficient. In fact, they retain the computational efficiency of matrix methods, and are embarrassingly parallel. They present even more opportunities for parallelism compared to matrix methods. Thus, these methods enjoy \"best of both the worlds\", viz., low computational and sample complexity, and also theoretical guarantees. She also proposed building tensor computations into neural network architectures. Current neural networks are built on matrix computations with simple non-linearities between the layers.  Prof. Anandkumar found a natural opportunity to extend these architectures to use richer computational units involving tensors. These tensorized neural networks can better encode the data dimensions and higher-order correlations. They are more compact and generalize better in multiple domains, especially on challenging tasks such as long-term forecasting. Prof. Anandkumar has made these methods widely available through open-source frameworks such as Tensorly as well as cloud AI services such as Amazon SageMaker.\n\n \n\n\t\t\t\t\tLast Modified: 06/03/2019\n\n\t\t\t\t\tSubmitted by: Animashree Anandkumar"
 }
}