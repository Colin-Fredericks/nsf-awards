{
 "awd_id": "1341698",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "Gateways to Discovery: Cyberinfrastructure for the Long Tail of Science",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924863",
 "po_email": "edwalker@nsf.gov",
 "po_sign_block_name": "Edward Walker",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2021-07-31",
 "tot_intn_awd_amt": 12000000.0,
 "awd_amount": 27313477.0,
 "awd_min_amd_letter_date": "2013-09-27",
 "awd_max_amd_letter_date": "2021-02-09",
 "awd_abstract_narration": "The University of California at San Diego will provide a ground-breaking new computing facility, Wildfire, that will be made available to the research community to both well established users of high end computing (HEC) and especially to new user communities that are less familiar with how HEC can advance their scientific and engineering goals.  \r\nThe distinguishing features of Wildfire are:\r\n(i)\tDeliver 1.8-2.0 Petaflop/s of long sought capacity for the 98% of XSEDE jobs (50% of XSEDE core hours) that use fewer than 1,000 cores and also support larger jobs.  The exact number will depend on the speed of the processor being delivered by Intel but cannot be less that 1.8 Petaflop/s.\r\n(ii)\tProvide 7 PB of Lustre-based Performance Storage at 200 GB/s bandwidth for both scratch and allocated storage as well as 6 PB of Durable Storage\r\n(iii)\tEnsure high throughput and responsiveness using allocation/scheduling using proven policies on earlier deployed systems such as Trestles and Gordon\r\n(iv)\tEstablish a rapid-access queue to provide new accounts within one day of the request\r\n(v)\tEnable community-supported custom software stacks via virtualization for communities that are unfamiliar with HPC environments.  These virtual clusters will be able to perform at or near native InfinBand bandwidth/latency\r\n\r\nWildfire will provide novel approaches for resource allocation, scheduling, and user support, queues with quicker response for high-throughput computing, medium-term storage allocations, virtualized environments with customized software stack, dedicated allocations of physical/virtual machines, support for Science Gateways and bandwidth reservations on high-speed networks.  Wildfire has been designed to efficiently serve the 98% of XSEDE jobs that need fewer than 1,000 cores, while also supporting larger jobs. The award leverages but also enhances the services available through the XSEDE project. \r\n\r\nThe Wildfire acquisition will work to increase the diversity of researchers able to effectively make use of advanced computational resources and establish a pipeline of potential users through virtualization, science gateways and educational activities focused on the undergraduate, graduate and post-graduate levels.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Norman",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Michael L Norman",
   "pi_email_addr": "mlnorman@ucsd.edu",
   "nsf_id": "000235680",
   "pi_start_date": "2013-09-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Chaitanya",
   "pi_last_name": "Baru",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Chaitanya K Baru",
   "pi_email_addr": "baru@sdsc.edu",
   "nsf_id": "000483394",
   "pi_start_date": "2013-09-27",
   "pi_end_date": "2014-08-14"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Philip",
   "pi_last_name": "Papadopoulos",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Philip M Papadopoulos",
   "pi_email_addr": "ppapadopoulos@ucsd.edu",
   "nsf_id": "000462781",
   "pi_start_date": "2013-09-27",
   "pi_end_date": "2018-08-30"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Amitava",
   "pi_last_name": "Majumdar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Amitava Majumdar",
   "pi_email_addr": "majumdar@sdsc.edu",
   "nsf_id": "000413199",
   "pi_start_date": "2018-08-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Moore",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Richard L Moore",
   "pi_email_addr": "rlm@sdsc.edu",
   "nsf_id": "000292321",
   "pi_start_date": "2013-09-27",
   "pi_end_date": "2015-09-25"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Nancy",
   "pi_last_name": "Wilkins-Diehr",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Nancy R Wilkins-Diehr",
   "pi_email_addr": "wilkinsn@sdsc.edu",
   "nsf_id": "000210022",
   "pi_start_date": "2013-09-27",
   "pi_end_date": "2019-08-07"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Sinkovits",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Robert Sinkovits",
   "pi_email_addr": "rssinkovits@ucsd.edu",
   "nsf_id": "000616724",
   "pi_start_date": "2015-11-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shawn",
   "pi_last_name": "Strande",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Shawn M Strande",
   "pi_email_addr": "sstrande@sdsu.edu",
   "nsf_id": "000451928",
   "pi_start_date": "2015-09-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 Gilman Drive",
  "perf_city_name": "San Diego",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930934",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "747600",
   "pgm_ele_name": "XD-Extreme Digital"
  },
  {
   "pgm_ele_code": "761900",
   "pgm_ele_name": "Innovative HPC"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7619",
   "pgm_ref_txt": "EQUIPMENT ACQUISITIONS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0118",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001819DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 12000000.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 9599963.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 2399881.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 21000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 906388.0
  },
  {
   "fund_oblg_fiscal_yr": 2018,
   "fund_oblg_amt": 2386245.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The San Diego Supercomputer Center at the University of California, San Diego deployed the Comet supercomputer as a natioanl resource in 2015. It was operated for allocated access by academic researchers, educators, and students through the NSF XSEDE project from May 2015 to July 2021. Following its decommissioning as an NSF-funded resource, Comet transitioned to a resource for the Center for Western Weather and Water Extremes (CW3E), a research and service project of the Scripps Institution of Oceanography. During its 75 months of operation as an XSEDE resource, Comet ran over 28 million jobs; provided over 2 billion core-hours and 13 million GPU-hours of compute time; served over 100,000 unique users, most of whom gained access via a science gateway rather than the command line; enabled publication of over 2,000 scientific papers; and supported research across virtually every domain of science and engineering.</p>\n<p>Comet has a peak speed of 2.8 Pflop/s delivered by 48,784 cores in 1,944 compute nodes, each with two, 12-core Intel Haswell processors and 128 GB DRAM. It also has 72 GPU nodes, half with 4 NVIDIA K80s and half with 4 NVIDIA P100s, plus 4 large-memory (1.5 TB) nodes. Like its Gordon predecessor, Comet features a large amount of flash memory via solid-state discs on every compute and GPU node. SDSC designed Comet in collaboration with vendor partners Dell, Intel, NVIDIA, Mellanox, and Aeon Computing.</p>\n<p>Comet was designed explicitly to serve the long tail of science, defined as the large number of researchers who require only modest numbers of cores or GPUs. Such users also benefited from Comet's optimized scheduling and allocations policies that lowered the barrier for accessing a complex high-performance computer. The design incorporated several significant technology and policy innovations:</p>\n<p>-A heterogenous architecture of CPUs, GPUs, large-memory nodes, along with a rich storage hierarchy, supported a broad range of science and engineering research.</p>\n<p>-One compute rack of 2,016 cores, connected by an FDR InfiniBand, non-blocking fat tree, supported a wide range of job sizes, from single-core to modest-scale, fully-coupled applications.</p>\n<p>-Virtual Cluster (VC) software, developed by SDSC in partnership with collaborators at Indiana University, provided a low-overhead virtualization layer that allowed customized software to run side-by-side with the standard cluster software stack.</p>\n<p>-Restricting the allocation limit of an individual PI to 10M core-hours allowed Comet to support more projects. A higher limit of 20M core-hours for science gateways provided access for many more users without the overhead of requesting their own allocation.</p>\n<p>Comet's Virtual Cluster interface was used by researchers from the Laser Interferometer Gravitational-Wave Observatory (LIGO) in support of the confirmation of the landmark detection of gravitational waves as hypothesized by Albert Einstein over 100 years ago. LIGO researchers consumed nearly 630,000 hours of computational time on Comet via the Open Science Grid (OSG) using a VC that supported the direct integration of OSG's high-throughput scheduler into Comet's batch scheduler. Comet also became one of the first NSF national resources to use Singularity, which allowed users to run containerized application software that would otherwise not be feasible with a standard cluster software stack.</p>\n<p>Comet set out to reach 10,000 unique users over its lifetime, a goal that was achieved within the first year of operations. Notable science gateways included CIPRES, I-TASSER, and the Neuroscience Gateway. Between these and the other gateways on Comet, over 100,000 unique users accessed Comet to study a wide range of physical, chemical, and biological systems.</p>\n<p>During its lifetime, Comet became a primary source of GPUs for the community. A research team led by UCSD's Rommie Amaro and Arvind Ramanathan, a computational biologist at Argonne National Laboratory, explored the movement of SARS-CoV-2's spike protein to understand how it behaves and gains access to the human cell. Using Comet's GPU resources as part of the scaling work, the team built a workflow based on artificial intelligence (AI) to more efficiently simulate the spike protein. The work led to a special Gordon Bell Award at the Annual Supercomputing Conference. In 2020, Comet joined the COVID-19 HPC Consortium, adding resources to help understand the spread of COVID-19 and help search for treatments and vaccines.&nbsp;</p>\n<p>Outreach activities exposed thousands of researchers, educators, and students to the benefits of Comet's unique features and ease of use. SDSC staff hosted tutorials at scientific meetings, workshops at SDSC and on other university campuses, and annual summer institutes. Following travel constraints due to COVID-19, programs were conducted virtually. SDSC used that opportunity to improve remote training processes and tools, ultimately increasing participation rates above those seen before the pandemic. In the final years of service, there was a marked increase in the interest in machine learning and AI. Targeted outreach to meet this demand resulted in a body of training materials now being used with SDSC's Expanse system.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/26/2021<br>\n\t\t\t\t\tModified by: Michael&nbsp;L&nbsp;Norman</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe San Diego Supercomputer Center at the University of California, San Diego deployed the Comet supercomputer as a natioanl resource in 2015. It was operated for allocated access by academic researchers, educators, and students through the NSF XSEDE project from May 2015 to July 2021. Following its decommissioning as an NSF-funded resource, Comet transitioned to a resource for the Center for Western Weather and Water Extremes (CW3E), a research and service project of the Scripps Institution of Oceanography. During its 75 months of operation as an XSEDE resource, Comet ran over 28 million jobs; provided over 2 billion core-hours and 13 million GPU-hours of compute time; served over 100,000 unique users, most of whom gained access via a science gateway rather than the command line; enabled publication of over 2,000 scientific papers; and supported research across virtually every domain of science and engineering.\n\nComet has a peak speed of 2.8 Pflop/s delivered by 48,784 cores in 1,944 compute nodes, each with two, 12-core Intel Haswell processors and 128 GB DRAM. It also has 72 GPU nodes, half with 4 NVIDIA K80s and half with 4 NVIDIA P100s, plus 4 large-memory (1.5 TB) nodes. Like its Gordon predecessor, Comet features a large amount of flash memory via solid-state discs on every compute and GPU node. SDSC designed Comet in collaboration with vendor partners Dell, Intel, NVIDIA, Mellanox, and Aeon Computing.\n\nComet was designed explicitly to serve the long tail of science, defined as the large number of researchers who require only modest numbers of cores or GPUs. Such users also benefited from Comet's optimized scheduling and allocations policies that lowered the barrier for accessing a complex high-performance computer. The design incorporated several significant technology and policy innovations:\n\n-A heterogenous architecture of CPUs, GPUs, large-memory nodes, along with a rich storage hierarchy, supported a broad range of science and engineering research.\n\n-One compute rack of 2,016 cores, connected by an FDR InfiniBand, non-blocking fat tree, supported a wide range of job sizes, from single-core to modest-scale, fully-coupled applications.\n\n-Virtual Cluster (VC) software, developed by SDSC in partnership with collaborators at Indiana University, provided a low-overhead virtualization layer that allowed customized software to run side-by-side with the standard cluster software stack.\n\n-Restricting the allocation limit of an individual PI to 10M core-hours allowed Comet to support more projects. A higher limit of 20M core-hours for science gateways provided access for many more users without the overhead of requesting their own allocation.\n\nComet's Virtual Cluster interface was used by researchers from the Laser Interferometer Gravitational-Wave Observatory (LIGO) in support of the confirmation of the landmark detection of gravitational waves as hypothesized by Albert Einstein over 100 years ago. LIGO researchers consumed nearly 630,000 hours of computational time on Comet via the Open Science Grid (OSG) using a VC that supported the direct integration of OSG's high-throughput scheduler into Comet's batch scheduler. Comet also became one of the first NSF national resources to use Singularity, which allowed users to run containerized application software that would otherwise not be feasible with a standard cluster software stack.\n\nComet set out to reach 10,000 unique users over its lifetime, a goal that was achieved within the first year of operations. Notable science gateways included CIPRES, I-TASSER, and the Neuroscience Gateway. Between these and the other gateways on Comet, over 100,000 unique users accessed Comet to study a wide range of physical, chemical, and biological systems.\n\nDuring its lifetime, Comet became a primary source of GPUs for the community. A research team led by UCSD's Rommie Amaro and Arvind Ramanathan, a computational biologist at Argonne National Laboratory, explored the movement of SARS-CoV-2's spike protein to understand how it behaves and gains access to the human cell. Using Comet's GPU resources as part of the scaling work, the team built a workflow based on artificial intelligence (AI) to more efficiently simulate the spike protein. The work led to a special Gordon Bell Award at the Annual Supercomputing Conference. In 2020, Comet joined the COVID-19 HPC Consortium, adding resources to help understand the spread of COVID-19 and help search for treatments and vaccines. \n\nOutreach activities exposed thousands of researchers, educators, and students to the benefits of Comet's unique features and ease of use. SDSC staff hosted tutorials at scientific meetings, workshops at SDSC and on other university campuses, and annual summer institutes. Following travel constraints due to COVID-19, programs were conducted virtually. SDSC used that opportunity to improve remote training processes and tools, ultimately increasing participation rates above those seen before the pandemic. In the final years of service, there was a marked increase in the interest in machine learning and AI. Targeted outreach to meet this demand resulted in a body of training materials now being used with SDSC's Expanse system.\n\n\t\t\t\t\tLast Modified: 11/26/2021\n\n\t\t\t\t\tSubmitted by: Michael L Norman"
 }
}