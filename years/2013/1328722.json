{
 "awd_id": "1328722",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "NRI: Large: Collaborative Research: Human-robot Coordinated Manipulation and Transportation of Large Objects",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2021-09-30",
 "tot_intn_awd_amt": 1499987.0,
 "awd_amount": 1499987.0,
 "awd_min_amd_letter_date": "2013-09-25",
 "awd_max_amd_letter_date": "2020-06-22",
 "awd_abstract_narration": "Motivated by the complementary abilities of humans and humanoids, the objective of this proposal is to develop the science and technology necessary for realizing human-robot cooperative object manipulation and transportation. The key concepts that this research seeks to promote are adaptability to human activity under minimal communication, and robustness to variability and uncertainty in the environment, achieved through a layered representation and deliberate processing of the available information. Moreover, this project aims to make maximum use of a minimal set of sensors to plan and control the actions of the robot, while ensuring safe and efficient cooperative transportation.  The embodiment of this research is a humanoid co-worker that bears most of the load, when helping a person to carry an object, without requiring excessive communication, or prior training on the part of the human.\r\n\r\nBy introducing concrete methods for human-robot physical collaboration in semi-structured environments, this project enables a unique synergy between robots and humans that has the potential to increase productivity, and reduce accidents and injuries. In doing so, it also promotes the advancement of new practical applications of robots in construction, manufacturing, logistics, and home services. By developing open-source, portable algorithms for humanoid robots and mobile manipulators, this effort results in cost and time savings for researchers, developers, educators, and end-users in robotics. Finally, through an aggressive educational and community outreach plan, and by actively engaging K-12 students in an exciting RoboTech Fellows program, this project seeks to increase diversity and attract underrepresented groups to STEM.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Stergios",
   "pi_last_name": "Roumeliotis",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Stergios I Roumeliotis",
   "pi_email_addr": "stergios@cs.umn.edu",
   "nsf_id": "000487045",
   "pi_start_date": "2013-09-25",
   "pi_end_date": "2020-06-17"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hyun Soo",
   "pi_last_name": "Park",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hyun Soo Park",
   "pi_email_addr": "hspark@umn.edu",
   "nsf_id": "000741915",
   "pi_start_date": "2020-06-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Demoz",
   "pi_last_name": "Gebre-Egziabher",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Demoz Gebre-Egziabher",
   "pi_email_addr": "gebre@umn.edu",
   "nsf_id": "000465118",
   "pi_start_date": "2013-09-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota-Twin Cities",
  "inst_street_address": "2221 UNIVERSITY AVE SE STE 100",
  "inst_street_address_2": "",
  "inst_city_name": "MINNEAPOLIS",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "6126245599",
  "inst_zip_code": "554143074",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MN05",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "KABJZBBJ4B54"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota-Twin Cities",
  "perf_str_addr": "207 Pleasant Street SE",
  "perf_city_name": "Minneapolis",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "554552070",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MN05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7298",
   "pgm_ref_txt": "COLLABORATIVE RESEARCH"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 676943.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 165465.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 657579.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>This research program aims to develop a computer vision model that can parse 3D geometry about a person, including objects, scenes, and people, which will allow robots to interact with humans. We focus on (1) reconstructing 3D scenes from egocentric videos, (2) understanding underlying physics from visual observations, and (3) leveraging these principles to formulate a self-supervised learning.&nbsp;</span></p>\n<p><span>Intellectual Merit: We develop a new computer vision theories and algorithms to learn geometry and physics of objects, people, and scenes. We make use of egocentric videos that can closely measure human activities. For geometry, we develop a spatial rectifier that can stabilize the egocentric image with respect to gravity, which allows learning a geometrically coherent representation given a limited amount of labeled data. For physics, we develop an optimal control theory that can fit physics simulation into the videos, which provides understanding of physical interactions, e.g., contact force, inertia, and friction.&nbsp;</span>With this high-risk-high-payoff project, we have published in top conferences in computer vision including CVPR, ECCV, and ICCV and we have been conducting the follow-up research. Notable papers include:</p>\n<p><span>+ T. Do, K. Vuong, S. Roumeliotis, H.S. Park \"Surface Normal Estimation of Tilted Images via Spatial Rectifier\", ECCV 2020, Spotlight presentation---</span>We present a spatial rectifier to estimate surface normals of tilted (egocentric) images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion.</p>\n<p>+&nbsp;J. Guo, J. Li, R. Narain, H. S. Park \"Inverse Simulation: Reconstructing Dynamic Geometry of Clothed Humans via Optimal Control\", CVPR 2021---We study the problem of inverse cloth simulation?to estimate shape and time-varying poses of the underlying body that generates physically plausible cloth motion, which matches to the point cloud measurements on the clothed humans. A key innovation is to represent the dynamics of the cloth geometry using a dynamical system that is controlled by the body states (shape and pose). This allows us to express the cloth motion as a resultant of external (skin friction and gravity) and internal (elasticity) forces. Inspired by the theory of optimal control, we optimize the body states such that the simulated cloth motion is matched to the point cloud measurements, and the analytic gradient of the simulator is back-propagated to update the body states. We propose a cloth relaxation scheme to initialize the cloth state, which ensures the physical validity. Our method produces physically plausible and temporally smooth cloth and body movements that are faithful to the measurements, and shows superior performance compared to the existing methods. As a byproduct, the stress and strain that are applied to the body and clothes can be recovered.</p>\n<p>+ J. S. Yoon, T. Shiratori, S.-I. Yu, H. S. Park, \"Self-supervised Adaptation of High-fidelity Face Models for Monocular Performance Tracking\", CVPR 2019 Oral presentation---W<span>e propose a self-supervised domain adaptation approach to enable the animation of high-fidelity face models from a commodity (egocentric) camera. Our approach first circumvents the requirement for special input data by training a new network that can directly drive a face model just from a single 2D image. Then, we overcome the domain mismatch between lab and uncontrolled environments by performing self-supervised domain adaptation based on \"consecutive frame texture consistency\" based on the assumption that the appearance of the face is consistent over consecutive frames, avoiding the necessity of modeling the new environment such as lighting or background. Experiments show that we are able to drive a high-fidelity face model to perform complex facial motion from a cellphone camera without requiring any labeled data from the new domain.</span></p>\n<p>Broader Impact: The outcomes of the project are integrated in computer vision courses in the University of Minnesota: CSCI 5561 Introduction to Computer Vision and CSCI 5563 Multiview 3D Computer Vision. We used the datasets and codes generated from this project as course materials. PI has given talks in various settings including conferences, universities and industrial partners for dissemination. PI also disseminated the results in public settings in local communities such as Minnesota State Fair.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/06/2021<br>\n\t\t\t\t\tModified by: Hyun Soo&nbsp;Park</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2021/1328722/1328722_10283725_1638819552242_Clipboard01--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2021/1328722/1328722_10283725_1638819552242_Clipboard01--rgov-800width.jpg\" title=\"Inverse Simulation\"><img src=\"/por/images/Reports/POR/2021/1328722/1328722_10283725_1638819552242_Clipboard01--rgov-66x44.jpg\" alt=\"Inverse Simulation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">We present an inverse simulation method to reconstruct time-varying geometry of the clothes and the underlyingbody.</div>\n<div class=\"imageCredit\">Jingfan Guo</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Hyun Soo&nbsp;Park</div>\n<div class=\"imageTitle\">Inverse Simulation</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis research program aims to develop a computer vision model that can parse 3D geometry about a person, including objects, scenes, and people, which will allow robots to interact with humans. We focus on (1) reconstructing 3D scenes from egocentric videos, (2) understanding underlying physics from visual observations, and (3) leveraging these principles to formulate a self-supervised learning. \n\nIntellectual Merit: We develop a new computer vision theories and algorithms to learn geometry and physics of objects, people, and scenes. We make use of egocentric videos that can closely measure human activities. For geometry, we develop a spatial rectifier that can stabilize the egocentric image with respect to gravity, which allows learning a geometrically coherent representation given a limited amount of labeled data. For physics, we develop an optimal control theory that can fit physics simulation into the videos, which provides understanding of physical interactions, e.g., contact force, inertia, and friction. With this high-risk-high-payoff project, we have published in top conferences in computer vision including CVPR, ECCV, and ICCV and we have been conducting the follow-up research. Notable papers include:\n\n+ T. Do, K. Vuong, S. Roumeliotis, H.S. Park \"Surface Normal Estimation of Tilted Images via Spatial Rectifier\", ECCV 2020, Spotlight presentation---We present a spatial rectifier to estimate surface normals of tilted (egocentric) images. Tilted images are of particular interest as more visual data are captured by arbitrarily oriented sensors such as body-/robot-mounted cameras. Existing approaches exhibit bounded performance on predicting surface normals because they were trained using gravity-aligned images. We design a spatial rectifier that is learned to transform the surface normal distribution of a tilted image to the rectified one that matches the gravity-aligned training data distribution. The resulting estimator outperforms the state-of-the-art methods including data augmentation baselines not only on ScanNet and NYUv2 but also on a new dataset called Tilt-RGBD that includes considerable roll and pitch camera motion.\n\n+ J. Guo, J. Li, R. Narain, H. S. Park \"Inverse Simulation: Reconstructing Dynamic Geometry of Clothed Humans via Optimal Control\", CVPR 2021---We study the problem of inverse cloth simulation?to estimate shape and time-varying poses of the underlying body that generates physically plausible cloth motion, which matches to the point cloud measurements on the clothed humans. A key innovation is to represent the dynamics of the cloth geometry using a dynamical system that is controlled by the body states (shape and pose). This allows us to express the cloth motion as a resultant of external (skin friction and gravity) and internal (elasticity) forces. Inspired by the theory of optimal control, we optimize the body states such that the simulated cloth motion is matched to the point cloud measurements, and the analytic gradient of the simulator is back-propagated to update the body states. We propose a cloth relaxation scheme to initialize the cloth state, which ensures the physical validity. Our method produces physically plausible and temporally smooth cloth and body movements that are faithful to the measurements, and shows superior performance compared to the existing methods. As a byproduct, the stress and strain that are applied to the body and clothes can be recovered.\n\n+ J. S. Yoon, T. Shiratori, S.-I. Yu, H. S. Park, \"Self-supervised Adaptation of High-fidelity Face Models for Monocular Performance Tracking\", CVPR 2019 Oral presentation---We propose a self-supervised domain adaptation approach to enable the animation of high-fidelity face models from a commodity (egocentric) camera. Our approach first circumvents the requirement for special input data by training a new network that can directly drive a face model just from a single 2D image. Then, we overcome the domain mismatch between lab and uncontrolled environments by performing self-supervised domain adaptation based on \"consecutive frame texture consistency\" based on the assumption that the appearance of the face is consistent over consecutive frames, avoiding the necessity of modeling the new environment such as lighting or background. Experiments show that we are able to drive a high-fidelity face model to perform complex facial motion from a cellphone camera without requiring any labeled data from the new domain.\n\nBroader Impact: The outcomes of the project are integrated in computer vision courses in the University of Minnesota: CSCI 5561 Introduction to Computer Vision and CSCI 5563 Multiview 3D Computer Vision. We used the datasets and codes generated from this project as course materials. PI has given talks in various settings including conferences, universities and industrial partners for dissemination. PI also disseminated the results in public settings in local communities such as Minnesota State Fair. \n\n \n\n\t\t\t\t\tLast Modified: 12/06/2021\n\n\t\t\t\t\tSubmitted by: Hyun Soo Park"
 }
}