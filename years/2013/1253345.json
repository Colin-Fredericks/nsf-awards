{
 "awd_id": "1253345",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: The Algorithmic Foundations of Data Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2013-06-01",
 "awd_exp_date": "2020-05-31",
 "tot_intn_awd_amt": 484175.0,
 "awd_amount": 484175.0,
 "awd_min_amd_letter_date": "2013-01-14",
 "awd_max_amd_letter_date": "2017-05-31",
 "awd_abstract_narration": "The past decade has seen a growing reliance on data driven technologies, including recommendation systems, targeted advertising, and search personalization. This growth in big data has made data privacy into a central concern. The central question raised is: how can we continue to extract useful information from large datasets, while provably protecting some measure of privacy for the individuals contained in these datasets?\r\n\r\nThis research centers around advancing the state of the art in privacy preserving data analysis. It specifically has several themes: (1) Exploiting structure in the private data being analyzed, as well as the classes of queries used in the analysis to give computationally efficient algorithms for private data analysis. (2) Deepening the connections between private data analysis and machine learning theory. (3) Relaxing the adversarial collusion model implicit in most work on the foundations of data privacy, and (4) applying the tools of differential privacy to usefully exploit and analyze noise in other algorithmic settings. To ensure the broad impact of this research, this project includes substantial outreach activities, including workshop organization, course development, and the development of a textbook and other educational materials.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "AARON",
   "pi_last_name": "ROTH",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "AARON ROTH",
   "pi_email_addr": "aaroth@cis.upenn.edu",
   "nsf_id": "000624673",
   "pi_start_date": "2013-01-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  },
  {
   "pgm_ele_code": "792700",
   "pgm_ele_name": "COMPLEXITY & CRYPTOGRAPHY"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 92200.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 94750.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 97350.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 99975.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 99900.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Over the course of this project, we have helped develop the foundations of Differential Privacy and to popularize its understanding. Differential privacy is a semantic guarantee of privacy that promises that outcomes of computations that are performed in differentially private way cannot be used to reliably distinguish whether a particular individual was included in a dataset or not. It differs from past approaches to data privacy exactly by enunciating a semantic guarantee, rather than relying on ad-hoc syntactic constraints that historically have not stood the test of time.</p>\n<p>The main scientific question related to differential privacy is what kinds of data analyses are compatible with differential privacy, and what the costs are of imposing differential privacy in terms of accuracy and running time. My work has helped answer these basic questions, from tasks ranging from synthetic data generation (creating a fake dataset that is safe to share, but nevertheless is true to the original dataset on some large class of statistics) to machine learning to combinatorial optimization. Many of these problems become computationally hard in the worst case. One major theme of my work has been to build a framework through which these problems can nevertheless be solved in practice, by developing algorithms that can leverage powerful heuristic optimization procedures like integer program solvers. The important aspect of this work is that privacy guarantees must always hold in the worst case, because differential privacy cannot be empirically verified, but utility guarantees --- which can be empirically checked --- can be relaxed to hold only when the heuristic optimization procedures succeed (which they often do). &nbsp;</p>\n<p>Another important thrust of my work over the course of this project has been finding applications of differential privacy to other problems that are not outwardly related. I have developed two such applications:</p>\n<p>1)&nbsp;&nbsp;&nbsp;&nbsp; The first it to game theory and mechanism design. Differential privacy implies a notion of stability to unilateral changes of data that closely corresponds to the notion of equilibrium in game theory. If an individual cannot substantially change the results of an outcome by misreporting their data, then they also cannot substantially improve their payout in a mechanism that allocates goods or makes decisions as a function of their reported data. This allowed us to derive a number of new results in mechanism design, including for pricing goods when buyers have complicated valuations, and matching students to schools when both the students and the schools have preferences over each other.</p>\n<p>2)&nbsp;&nbsp;&nbsp;&nbsp; The second is to false discovery control in adaptive statistical analysis. An epidemic of incorrect and non-reproducible results in the empirical sciences has been blamed in part on researchers&rsquo; failure to adhere to the ideal of &ldquo;pre-registration&rdquo; whereby a data analysis is fixed before the data is gathered. Instead, the practice of scientific research is &ldquo;adaptive&rdquo; in that data exploration is combined with confirmatory data analysis on the same datasets, which invalidates most traditional tests of statistical validity. However, as we showed, the results of differentially private data analyses are free from these concerns, even when data is analyzed adaptively.</p>\n<p>&nbsp;</p>\n<p>Finally, an important thrust of this project has been educational. In addition to organizing a number of academic workshops and conferences organized around the themes of this projects&rsquo; research, I wrote a textbook together with Cynthia Dwork that became the first (and still the standard) reference text for differential privacy and became widely used in teaching university courses across the world. I have also made high quality lecture notes available for courses in differential privacy, as well as its applications to game theory and to adaptive data analysis.I have written for a popular audience. This includes a popular science book called &ldquo;The Ethical Algorithm&rdquo; that has a chapter devoted to explaining differential privacy to a general audience. I have spoken at a number of companies, policy thinktanks, and radio, TV, and podcast audiences as part of this effort.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/29/2020<br>\n\t\t\t\t\tModified by: Aaron&nbsp;Roth</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOver the course of this project, we have helped develop the foundations of Differential Privacy and to popularize its understanding. Differential privacy is a semantic guarantee of privacy that promises that outcomes of computations that are performed in differentially private way cannot be used to reliably distinguish whether a particular individual was included in a dataset or not. It differs from past approaches to data privacy exactly by enunciating a semantic guarantee, rather than relying on ad-hoc syntactic constraints that historically have not stood the test of time.\n\nThe main scientific question related to differential privacy is what kinds of data analyses are compatible with differential privacy, and what the costs are of imposing differential privacy in terms of accuracy and running time. My work has helped answer these basic questions, from tasks ranging from synthetic data generation (creating a fake dataset that is safe to share, but nevertheless is true to the original dataset on some large class of statistics) to machine learning to combinatorial optimization. Many of these problems become computationally hard in the worst case. One major theme of my work has been to build a framework through which these problems can nevertheless be solved in practice, by developing algorithms that can leverage powerful heuristic optimization procedures like integer program solvers. The important aspect of this work is that privacy guarantees must always hold in the worst case, because differential privacy cannot be empirically verified, but utility guarantees --- which can be empirically checked --- can be relaxed to hold only when the heuristic optimization procedures succeed (which they often do).  \n\nAnother important thrust of my work over the course of this project has been finding applications of differential privacy to other problems that are not outwardly related. I have developed two such applications:\n\n1)     The first it to game theory and mechanism design. Differential privacy implies a notion of stability to unilateral changes of data that closely corresponds to the notion of equilibrium in game theory. If an individual cannot substantially change the results of an outcome by misreporting their data, then they also cannot substantially improve their payout in a mechanism that allocates goods or makes decisions as a function of their reported data. This allowed us to derive a number of new results in mechanism design, including for pricing goods when buyers have complicated valuations, and matching students to schools when both the students and the schools have preferences over each other.\n\n2)     The second is to false discovery control in adaptive statistical analysis. An epidemic of incorrect and non-reproducible results in the empirical sciences has been blamed in part on researchers\u2019 failure to adhere to the ideal of \"pre-registration\" whereby a data analysis is fixed before the data is gathered. Instead, the practice of scientific research is \"adaptive\" in that data exploration is combined with confirmatory data analysis on the same datasets, which invalidates most traditional tests of statistical validity. However, as we showed, the results of differentially private data analyses are free from these concerns, even when data is analyzed adaptively.\n\n \n\nFinally, an important thrust of this project has been educational. In addition to organizing a number of academic workshops and conferences organized around the themes of this projects\u2019 research, I wrote a textbook together with Cynthia Dwork that became the first (and still the standard) reference text for differential privacy and became widely used in teaching university courses across the world. I have also made high quality lecture notes available for courses in differential privacy, as well as its applications to game theory and to adaptive data analysis.I have written for a popular audience. This includes a popular science book called \"The Ethical Algorithm\" that has a chapter devoted to explaining differential privacy to a general audience. I have spoken at a number of companies, policy thinktanks, and radio, TV, and podcast audiences as part of this effort.\n\n \n\n\t\t\t\t\tLast Modified: 09/29/2020\n\n\t\t\t\t\tSubmitted by: Aaron Roth"
 }
}