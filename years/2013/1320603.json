{
 "awd_id": "1320603",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Bench-testing Environment for Automated Software Tuning (BEAST)",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 499995.0,
 "awd_amount": 499995.0,
 "awd_min_amd_letter_date": "2013-07-26",
 "awd_max_amd_letter_date": "2013-07-26",
 "awd_abstract_narration": "In the world of high-performance scientific computing, the rapid emergence of hybrid processors that  make heavy use of accelerator technologies, such as Graphics Processing Units (GPUs) or the Intel Xeon  Phi (a.k.a., Many Integrated Cores, MIC), raises critical new challenges for computational scientists. Their research applications typically depend on computational kernels (i.e., software implementations of one or more of the basic patterns of scientific computing) that are optimized for speed. Such programs spend most of their computing time executing one or more of these kernels, and long experience has taught developers that tuning their kernels for the architecture of a given processor is absolutely essential to achieving excellent performance at the level of the individual computing node. Since scientists want to run these applications on supercomputers with thousands of such nodes, high performance at the node  level is essential to high productivity for the application at large. Unfortunately, for the vast majority of computational kernels, the three classic approaches to performance tuning?compiler-driven code transformations, low-level manual programming, or empirical autotuning?have always been very difficult, often producing mixed results; and the emerging era of hybrid processors makes all three techniques less effective still. The Bench-testing Environment for Automated Software Tuning (BEAST) makes a substantial contribution to solving this important problem.   \r\nBEAST creates a framework for exploring and optimizing the performance of computational kernels on hybrid processors that 1) applies to a diverse range of computational kernels, 2) (semi)automatically generates better performing implementations on various hybrid processor architectures, and 3) increases developer insight into why given kernel/processor combinations have the performance profiles they do. To achieve this three-fold goal, it applies the model used for traditional application benchmarking in a completely novel way: it combines an abstract kernel specification and corresponding verification test, similar to standard benchmarking, with an automated testing engine and data analysis and machine learning tools, called the BEAST workbench. Using a new method for specifying language-neutral code stencils and a prototype BEAST workbench, the project explores alternative tuning methods and strategies for a diverse range of computational kernels. \r\nExperiments carried out under this project are expected to show that the BEAST framework can dramatically improve the performance of many computational kernels that are of fundamental importance to scientific computing. As this software and the techniques for using it are made widely available to the science and engineering community, they will help to ensure the timely delivery of performance- optimized kernels for many domains and many types of hybrid processors, making the impact of the BEAST bench-tuning software infrastructure very broad indeed. Scientists and engineers, across a vast array of intellectually, economically and socially important domains, will be able to rapidly tune the underlying kernels in their applications to the characteristics of the latest platform, and thereby quickly gain the productivity benefits of each successive generation of accelerator technology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jack",
   "pi_last_name": "Dongarra",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Jack J Dongarra",
   "pi_email_addr": "dongarra@icl.utk.edu",
   "nsf_id": "000299281",
   "pi_start_date": "2013-07-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "689200",
   "pgm_ele_name": "CI REUSE"
  },
  {
   "pgm_ele_code": "794200",
   "pgm_ele_name": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6892",
   "pgm_ref_txt": "CI REUSE"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 499995.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-1e037249-a0e0-9e9e-ffa2-9c7572c7f73c\"> </span></p>\n<p dir=\"ltr\"><span>The main outcome of the BEAST project is producing a methodology for the development of fast computational kernels for accelerators. Here, a kernel means a small piece of code implementing a well-defined computational task, such as multiplying matrices, and an accelerator means a device geared toward high computational throughput, and is mostly synonymous with a graphics processing unit (GPU).</span></p>\n<p dir=\"ltr\"><span>In the BEAST approach, the programmer implements a parametrized kernel, meaning a single piece of source code, which can be compiled to produce a large number of alternative implementations. BEAST provides an intuitive language for expressing the parameter search space along with a set of pruning constraints that eliminate invalid or deficient kernels. The kernels passing the elimination process are then compiled and run, and performance metrics are collected, to identify the fastest or the most energy-efficient kernels. Also, a trove of information can be collected from the hardware performance counters, which can then be analyzed and visualized for gaining insights into the hardware, the algorithm, and their mutual interactions.</span></p>\n<p dir=\"ltr\"><span>In the course of the project, numerous GPU kernels were implemented, tuned, and analyzed, leading to many valuable insights about the hardware, the algorithms, and the implementation tradeoffs. The kernels ranged from basic matrix operations to much more complex machine-learning workloads. Two prime examples of the potential of the BEAST methodology are the batched linear system solver kernel &mdash; which outperformed NVIDIA cuBLAS implementations by an order of magnitude &mdash; and the alternating least squares kernel &mdash; which outperformed the implementation in the mainstream Spark MLLib package, also by an order of magnitude. (See Figure.)</span></p>\n<p dir=\"ltr\">The most important aspect of the autotuning methodology is that, when a new generation of hardware replaces an old generation, the codes do not have to be rewritten, only re-tuned, in an automated manner. In other words, autotuning leads to performance portability.</p>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/11/2016<br>\n\t\t\t\t\tModified by: Jack&nbsp;J&nbsp;Dongarra</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1320603/1320603_10261361_1476115666646_BEAST-figures--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1320603/1320603_10261361_1476115666646_BEAST-figures--rgov-800width.jpg\" title=\"BEAST Methodology Potential\"><img src=\"/por/images/Reports/POR/2016/1320603/1320603_10261361_1476115666646_BEAST-figures--rgov-66x44.jpg\" alt=\"BEAST Methodology Potential\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Left chart: the batched linear system solver kernel, which outperformed NVIDIA cuBLAS implementations by an order of magnitude. Right chart: the alternating least squares kernel, which outperformed the implementation in the mainstream Spark MLLib package, also by an order of magnitude.</div>\n<div class=\"imageCredit\">Jakub Kurzak et al., Innovative Computing Laboratory</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Jack&nbsp;J&nbsp;Dongarra</div>\n<div class=\"imageTitle\">BEAST Methodology Potential</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThe main outcome of the BEAST project is producing a methodology for the development of fast computational kernels for accelerators. Here, a kernel means a small piece of code implementing a well-defined computational task, such as multiplying matrices, and an accelerator means a device geared toward high computational throughput, and is mostly synonymous with a graphics processing unit (GPU).\nIn the BEAST approach, the programmer implements a parametrized kernel, meaning a single piece of source code, which can be compiled to produce a large number of alternative implementations. BEAST provides an intuitive language for expressing the parameter search space along with a set of pruning constraints that eliminate invalid or deficient kernels. The kernels passing the elimination process are then compiled and run, and performance metrics are collected, to identify the fastest or the most energy-efficient kernels. Also, a trove of information can be collected from the hardware performance counters, which can then be analyzed and visualized for gaining insights into the hardware, the algorithm, and their mutual interactions.\nIn the course of the project, numerous GPU kernels were implemented, tuned, and analyzed, leading to many valuable insights about the hardware, the algorithms, and the implementation tradeoffs. The kernels ranged from basic matrix operations to much more complex machine-learning workloads. Two prime examples of the potential of the BEAST methodology are the batched linear system solver kernel &mdash; which outperformed NVIDIA cuBLAS implementations by an order of magnitude &mdash; and the alternating least squares kernel &mdash; which outperformed the implementation in the mainstream Spark MLLib package, also by an order of magnitude. (See Figure.)\nThe most important aspect of the autotuning methodology is that, when a new generation of hardware replaces an old generation, the codes do not have to be rewritten, only re-tuned, in an automated manner. In other words, autotuning leads to performance portability.\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/11/2016\n\n\t\t\t\t\tSubmitted by: Jack J Dongarra"
 }
}