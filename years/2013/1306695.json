{
 "awd_id": "1306695",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Fellowship Award",
 "awd_titl_txt": "NSF Postdoctoral Fellowship in Biology FY 2013",
 "cfda_num": "47.074",
 "org_code": "08080000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Michael Vanni",
 "awd_eff_date": "2013-06-01",
 "awd_exp_date": "2015-05-31",
 "tot_intn_awd_amt": 138000.0,
 "awd_amount": 138000.0,
 "awd_min_amd_letter_date": "2013-05-14",
 "awd_max_amd_letter_date": "2013-05-14",
 "awd_abstract_narration": "Neural mechanisms of mobile prey detection during locomotion\r\n\r\nThe visual world of animals is always in motion. A video camera mounted directly onto the eyeball would record a blur of perpetual movement. Visual motion can signal the presence of behaviorally important objects such as prey or predators, yet each time an animal shifts its gaze or engages in locomotion, the visual surround also moves. How does the brain distinguish between visual motion caused by movement of the eyes, head or body, versus visual motion caused by mobile objects in the environment? To tackle this question, this fellowship will support the development of innovative, virtual reality methods that will be combined with genetic techniques to record the activity of specific circuit elements in the brain of larval zebrafish while they attempt to visually track and capture \"virtual\" prey items. In zebrafish and many other vertebrates, the ability to capture prey is reliant on a midbrain structure called the optic tectum; however, very little is known about identity or function of neural circuits in the tectum or elsewhere in the brain that function to detect and direct attention towards mobile prey. Although current neural recording techniques require that fish be immobilized, a virtual reality system overcomes this limitation by simulating the motion of the visual world that fish would experience while freely swimming. Critically, a virtual reality system will make it possible to experimentally decouple changes in the sensory surround from motor actions, allowing for a precise dissection of feedback mechanisms between sensory and motor processing in the nervous system.\r\n\r\nTraining goals include learning to implement real-time, closed-loop controllers for visual displays, and learning to use genetic methods for identifying and imaging neurons in zebrafish. This fellowship will also support the development of a human eye-tracking, virtual reality system, to be used in the classroom or in a public venue, such as a museum. This eye-tracking system will allow students or the public to interactively investigate relationships between their movement and their perception of motion.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "DBI",
 "org_div_long_name": "Division of Biological Infrastructure",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthew",
   "pi_last_name": "Green",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Matthew H Green",
   "pi_email_addr": "",
   "nsf_id": "000631137",
   "pi_start_date": "2013-05-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Green                   Matthew        H",
  "inst_street_address": "",
  "inst_street_address_2": "",
  "inst_city_name": "Evanston",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "",
  "inst_zip_code": "602083111",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "",
  "org_prnt_uei_num": "",
  "org_uei_num": ""
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": null,
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083111",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "805400",
   "pgm_ele_name": "Inters Biol and Math and Phys"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7137",
   "pgm_ref_txt": "POSTDOCTORAL FELLOWSHIPS"
  },
  {
   "pgm_ref_code": "7174",
   "pgm_ref_txt": "POSTDOC FELLOW IN SCI, MATH EN"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 138000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Animals use signals from the sensory surround to guide their movement, and in turn, movement changes the sensory experience of an animal. A major challenge for the brain is to distinguish between sensory signals arising from external sources (exafference) versus sensory signals arising from the movement of the body (reafference). In the context of vision, motion can reveal the presence of behaviorally important objects such as prey or predators, yet this exafferent visual motion must be distinguished from reafferent visual motion that occurs as a consequence of self-movement during locomotion or orienting maneuvers. The goal of this project was to understand the organization and function of neural circuits that distinguish between exafferent and reafferent visual signals. Towards this goal, we built virtual reality instrumentation that allows experimental control of the relationship between movement and sensory input in the larval zebrafish, a relatively simple, accessible and genetically malleable model organism.</p>\n<p>The effects of self-movement on vision in larval zebrafish have not been investigated previously. Our first aim was to determine whether fish are able to see during swimming, or if instead they are insensitive to visual stimuli during locomotion. We performed an experiment using virtual reality that reversed the natural relationship between visual motion and self-motion. Under normal conditions, the visual world moves in the opposite direction of a turn. For example, when you rotate your head left, the visual scene shifts to the right. We compared this natural condition to a reversal condition, in which the visual scene rotates in the same direction as a turning maneuver. We found that fish performed a series of spontaneous turns that were unbiased to the left or right in the normal condition; however, in the reversed condition, fish performed a series of highly biased turns, effectively swimming in circles. Because the stimulus only moved when the fish moved, and because the behavioral response to the normal and reversed condition were quantitatively different, these data rule out the hypothesis that fish cannot see during swimming. Instead, fish appear to be integrating visual information while on the move.</p>\n<p>Our second aim was to investigate the neural mechanisms by which the visual system compensates for self-motion. Each movement we make is generated by our motor system, which is located in the spinal cord and in several regions of the brain. The motor system issues motor commands, which are neural signals that ultimately contract our muscles and make our body move. One theory is that the motor system sends a copy of each motor command to the visual system, and this allows the visual system to predict what kind of visual motion it will see as a consequence of the motor command. In this way, the visual system can ignore visual motion caused by self-movement. This motor command copy may arise at any level of the motor system; however, as a start, we examined the activity of neurons in the spinal cord during visually guided turning maneuvers. Some of these spinal neurons have the capacity to send information back to the brain, and could potentially relay motor command copies back to the visual system. This work is ongoing, but we have preliminarily found a class of spinal neurons that is involved in generating turning maneuvers. We hope that continued study of this neuron population reveals whether it serves to relay motor command copies back to the visual system.</p>\n<p>Our cognitive visual abilities, such as perception of colors and visual recognition of faces and objects, are easily appreciated in daily life experiences. By contrast, we do not often think about the fact that our eyes and body are constantly moving, and that our brains have evolved to contend with the ceaseless visual motion caused by these movements. A goal of our broader...",
  "por_txt_cntn": "\nAnimals use signals from the sensory surround to guide their movement, and in turn, movement changes the sensory experience of an animal. A major challenge for the brain is to distinguish between sensory signals arising from external sources (exafference) versus sensory signals arising from the movement of the body (reafference). In the context of vision, motion can reveal the presence of behaviorally important objects such as prey or predators, yet this exafferent visual motion must be distinguished from reafferent visual motion that occurs as a consequence of self-movement during locomotion or orienting maneuvers. The goal of this project was to understand the organization and function of neural circuits that distinguish between exafferent and reafferent visual signals. Towards this goal, we built virtual reality instrumentation that allows experimental control of the relationship between movement and sensory input in the larval zebrafish, a relatively simple, accessible and genetically malleable model organism.\n\nThe effects of self-movement on vision in larval zebrafish have not been investigated previously. Our first aim was to determine whether fish are able to see during swimming, or if instead they are insensitive to visual stimuli during locomotion. We performed an experiment using virtual reality that reversed the natural relationship between visual motion and self-motion. Under normal conditions, the visual world moves in the opposite direction of a turn. For example, when you rotate your head left, the visual scene shifts to the right. We compared this natural condition to a reversal condition, in which the visual scene rotates in the same direction as a turning maneuver. We found that fish performed a series of spontaneous turns that were unbiased to the left or right in the normal condition; however, in the reversed condition, fish performed a series of highly biased turns, effectively swimming in circles. Because the stimulus only moved when the fish moved, and because the behavioral response to the normal and reversed condition were quantitatively different, these data rule out the hypothesis that fish cannot see during swimming. Instead, fish appear to be integrating visual information while on the move.\n\nOur second aim was to investigate the neural mechanisms by which the visual system compensates for self-motion. Each movement we make is generated by our motor system, which is located in the spinal cord and in several regions of the brain. The motor system issues motor commands, which are neural signals that ultimately contract our muscles and make our body move. One theory is that the motor system sends a copy of each motor command to the visual system, and this allows the visual system to predict what kind of visual motion it will see as a consequence of the motor command. In this way, the visual system can ignore visual motion caused by self-movement. This motor command copy may arise at any level of the motor system; however, as a start, we examined the activity of neurons in the spinal cord during visually guided turning maneuvers. Some of these spinal neurons have the capacity to send information back to the brain, and could potentially relay motor command copies back to the visual system. This work is ongoing, but we have preliminarily found a class of spinal neurons that is involved in generating turning maneuvers. We hope that continued study of this neuron population reveals whether it serves to relay motor command copies back to the visual system.\n\nOur cognitive visual abilities, such as perception of colors and visual recognition of faces and objects, are easily appreciated in daily life experiences. By contrast, we do not often think about the fact that our eyes and body are constantly moving, and that our brains have evolved to contend with the ceaseless visual motion caused by these movements. A goal of our broader impacts plan is to bring public awareness to the motion processing capabilities..."
 }
}