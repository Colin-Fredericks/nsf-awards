{
 "awd_id": "1319984",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Yesterday's News: Theory of Staleness under Data Churn",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 473420.0,
 "awd_amount": 473420.0,
 "awd_min_amd_letter_date": "2013-08-26",
 "awd_max_amd_letter_date": "2013-08-26",
 "awd_abstract_narration": "Many distributed applications in the current Internet are massively replicated to ensure unsurpassed data robustness and scalability; however, constant data churn (i.e., update of the source) and delayed synchronization lead to staleness and thus lower performance in these systems. The goal of this project is to pioneer a stochastic theory of data replication that can tackle non-trivial dependency issues in synchronization of general non-Poisson point processes, design more accurate sampling and prediction algorithms for measuring data churn, solve novel multi-source and multi-replica staleness-optimization problems, establish new fundamental understanding of cooperative and multi-hop replication, and model non-stationary update processes of real sources.\r\n \r\nThe now omnipresent cloud technology has become a vast consumer and generator of data that must be stored, replicated, and streamed to a variety of clients. This project focuses on understanding theoretical and experimental properties of data evolution and staleness in such systems, whose outcomes are likely to impact Internet computing through creation of insight that leads to better content-distribution mechanisms, more accurate search results, and ultimately higher satisfaction among everyday users. Furthermore, this project blends a variety of inter-disciplinary scientific areas, reaches out to the student population at Texas A&M to engage them in research activities from early stages of their careers, trains well-rounded PhD students knowledgeable in both theoretical and experimental aspects of large-scale networked systems, engages under-represented student groups in STEM fields, disseminates information through two new seminars at Texas A&M, and shares data models and experimental results with the public.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dmitri",
   "pi_last_name": "Loguinov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dmitri Loguinov",
   "pi_email_addr": "dmitri@cs.tamu.edu",
   "nsf_id": "000410005",
   "pi_start_date": "2013-08-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Daren",
   "pi_last_name": "Cline",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Daren B Cline",
   "pi_email_addr": "dcline@stat.tamu.edu",
   "nsf_id": "000432650",
   "pi_start_date": "2013-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas Engineering Experiment Station",
  "perf_str_addr": "MS 3112",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433112",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 473420.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many distributed applications in the current&nbsp;Internet (e.g., cloud computing, world-scale web search, online banking,&nbsp;content-distribution systems, social networks) are now massively&nbsp;replicated, which ensures unsurpassed data robustness and scalability.&nbsp;However, constant data churn (i.e., update of the source) and delayed&nbsp;synchronization lead to&nbsp;<em>staleness</em>, which refers to deviation of&nbsp;the information being served to clients and/or processed by the system&nbsp;from that at the source. This project studied replication under&nbsp;non-Poisson update processes, general cost functions, and multi-hop operation. Until now, these topics have remained virtually unexplored, which prevented&nbsp;accurate characterization of extreme-scale distributed systems, creation&nbsp;of alternative replication paradigms, and new theoretical insight into&nbsp;data churn. In this work, we addressed these issues by&nbsp;pioneering a stochastic theory of data replication that could tackle&nbsp;non-trivial dependency issues in synchronization of general non-Poisson&nbsp;point processes, designed more accurate sampling and prediction&nbsp;algorithms for measuring data churn, solved novel multi-source and&nbsp;multi-replica joint staleness-optimization problems, established new&nbsp;fundamental understanding of cooperative and multi-hop replication, and&nbsp;modeled non-stationary update processes of real&nbsp;sources.&nbsp;</p>\n<p>Accurate modeling of data churn has remained&nbsp;one of the most elusive topics in the literature, while being of&nbsp;paramount importance to improving the design and scalability of existing&nbsp;networks, databases, cloud applications, and large-scale distributed&nbsp;systems. The theory component of this work formalized data&nbsp;evolution using random process theory and achieved novel results in the&nbsp;understanding of staleness, our ability to sample network sources through&nbsp;random observation, and recovery of interval-censored data using non-parametric estimation algorithms. By studying the effect of various data-update&nbsp;distributions and refresh techniques on staleness, this work also produced insight into how to better design future replication solutions and achieve higher resilience to&nbsp;failure in existing systems. The experimental part of this work&nbsp;measured existing Internet sources (such as Wikipedia and Yelp) to verify&nbsp;the assumptions and performance of the proposed models and created novel&nbsp;data-churn characterization that achieved higher fidelity in practice than prior techniques. This, coupled with&nbsp;our unifying modeling framework, has increased the body of practical and theoretical knowledge about caching&nbsp;networks, their performance, optimality, and various avenues for achieving more scalable operation.&nbsp;</p>\n<p>This project blended a variety of cross-disciplinary&nbsp;scientific areas including random process theory, stochastic modeling,&nbsp;renewal theory, databases, content retrieval,&nbsp;networking, distributed systems, and experimental Internet data sampling&nbsp;and measurement. The educational component of this project&nbsp;reached out to the student population at Texas A&amp;M and engaged them in&nbsp;research activities they would not be otherwise exposed to. Outcomes include attraction of&nbsp;students to cross-disciplinary research programs, training of well-rounded PhD&nbsp;students knowledgeable in both theoretical and experimental aspects of&nbsp;large-scale networked systems, engagement of under-represented student&nbsp;groups in STEM fields, and information dissemination through publications/presentations.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2017<br>\n\t\t\t\t\tModified by: Dmitri&nbsp;Loguinov</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany distributed applications in the current Internet (e.g., cloud computing, world-scale web search, online banking, content-distribution systems, social networks) are now massively replicated, which ensures unsurpassed data robustness and scalability. However, constant data churn (i.e., update of the source) and delayed synchronization lead to staleness, which refers to deviation of the information being served to clients and/or processed by the system from that at the source. This project studied replication under non-Poisson update processes, general cost functions, and multi-hop operation. Until now, these topics have remained virtually unexplored, which prevented accurate characterization of extreme-scale distributed systems, creation of alternative replication paradigms, and new theoretical insight into data churn. In this work, we addressed these issues by pioneering a stochastic theory of data replication that could tackle non-trivial dependency issues in synchronization of general non-Poisson point processes, designed more accurate sampling and prediction algorithms for measuring data churn, solved novel multi-source and multi-replica joint staleness-optimization problems, established new fundamental understanding of cooperative and multi-hop replication, and modeled non-stationary update processes of real sources. \n\nAccurate modeling of data churn has remained one of the most elusive topics in the literature, while being of paramount importance to improving the design and scalability of existing networks, databases, cloud applications, and large-scale distributed systems. The theory component of this work formalized data evolution using random process theory and achieved novel results in the understanding of staleness, our ability to sample network sources through random observation, and recovery of interval-censored data using non-parametric estimation algorithms. By studying the effect of various data-update distributions and refresh techniques on staleness, this work also produced insight into how to better design future replication solutions and achieve higher resilience to failure in existing systems. The experimental part of this work measured existing Internet sources (such as Wikipedia and Yelp) to verify the assumptions and performance of the proposed models and created novel data-churn characterization that achieved higher fidelity in practice than prior techniques. This, coupled with our unifying modeling framework, has increased the body of practical and theoretical knowledge about caching networks, their performance, optimality, and various avenues for achieving more scalable operation. \n\nThis project blended a variety of cross-disciplinary scientific areas including random process theory, stochastic modeling, renewal theory, databases, content retrieval, networking, distributed systems, and experimental Internet data sampling and measurement. The educational component of this project reached out to the student population at Texas A&amp;M and engaged them in research activities they would not be otherwise exposed to. Outcomes include attraction of students to cross-disciplinary research programs, training of well-rounded PhD students knowledgeable in both theoretical and experimental aspects of large-scale networked systems, engagement of under-represented student groups in STEM fields, and information dissemination through publications/presentations.\n\n\t\t\t\t\tLast Modified: 11/30/2017\n\n\t\t\t\t\tSubmitted by: Dmitri Loguinov"
 }
}