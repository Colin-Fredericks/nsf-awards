{
 "awd_id": "1319365",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Reinforcement Learning with Predictive State Representations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rebecca Hwa",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2013-08-05",
 "awd_max_amd_letter_date": "2014-07-24",
 "awd_abstract_narration": "Like animals and humans, artificial autonomous agents that are able to predict short-term and long-term consequences of their actions can then plan their behavior, act more intelligently, and achieve greater reward. Agents that can learn such predictive models from experience can be more robust in their intelligence than agents that rely on pre-built models. The PI and graduate students are focused on the particularly challenging but natural case where observations from the agent's sensors far in the past can continue to influence the predictions of consequences of actions long into the future. (For example, the observation of where you park the car in the morning will help predict where you will see the car later in the day.) There are two broad classes of approaches to learning predictive models in such 'partially observable' settings. Finite-history models use short-term history of observations to predict future observations conditioned on actions; these are fast to learn but are limited because they cannot capture the effects of long-term history. Latent-variable models can capture the effects of long-term history by positing hidden or latent variables that capture the true state of the environment (e.g., the location of the car), but such models are difficult to learn because the latent variables have to be inferred from data. \r\n\r\nThis project builds on previous work by the PI and others on a third approach, called Predictive State Representations (or PSRs), in which the agent maintains predictions of future observations conditioned on future actions as a summary-representation of history; these models can both be fast to learn and capture the effect of long-term history. This project develops new PSR-based methods and algorithms for hierarchical models, rich-feature-based models, and local and modular models.  The project applies the new methods to challenging applications from active perception and robotics. In addition, theoretical understanding of these richer and newer methods will be developed. Altogether the project significantly expands the applicability of PSR-methods as well as their theoretical foundations and algorithms. \r\n\r\nBroader Impacts: New methods that allow artificial agents to robustly build predictive models would advance the state of knowledge across the fields of artificial intelligence, reinforcement learning, control, operations research, psychology, and neuroscience. The PI is co-leading an effort to create a new undergraduate degree in Data Sciences at the University of Michigan to be jointly managed by Computer Science & Engineering and Statistics. This future degree as well as other current undergraduate research programs will be targeted to recruit, mentor, and train students for this project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Satinder",
   "pi_last_name": "Baveja",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Satinder S Baveja",
   "pi_email_addr": "baveja@umich.edu",
   "nsf_id": "000100173",
   "pi_start_date": "2013-08-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, BBB Building, 3749",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 126866.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 323134.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The field of Reinforcement Learning (RL) addresses the following challenging problem within Artificial Intelligence (AI): how should an artificial agent learn to act so as to improve its performance over time. In RL, agents observe their environment through sensors, take actions, and obtain reward in a repeated loop that results in improved accumulated reward over time through a kind of trial-and-error learning process. There has been a lot of progress in developing RL algorithms and theory; nonetheless, some advanced questions remain open.</p>\n<p><br />In this project, we focus on the fundamental question of how the artificial agent should represent state, i.e., how should an agent summarize its history in order to condition future behavior on this fixed-length summary rather than the ever-growing history. This is a widely studied question in control theory, operations research, and AI. There are at least three broad classes of methods for summarizing history: (1) remembering recent history, (2) estimating latent representations of aspects of the environment from history, and (3) making predictions of the future from current history as a summary of history. This project focuses on the third class of methods, called Predictive State Representations (or PSRs), which are relatively new and underdeveloped thus far. PSRs offer the exciting possibility of being just as powerful as but being far more efficiently learnable than the other two, more widely studied, classes of methods.</p>\n<p><br />In this project we develop a set of theoretical and empirical results that significantly extend our understand of PSRs for RL problems. We rigorously study multiple settings with properties we expect to hold in real world problems. (1) Settings in which we do not know in advance how many predictions of the future we would need to adequately summarize history, where we provide a formal notion of the difficulties and how to algorithmically ameliorate them. (2) Settings in which we do not have a lot of data to learn PSRs and so we must approximate, where we provide algorithms and theoretical analyses to show how to select the nature of the approximations that do best. (3) Settings in which the we are in advance given a set of possible hierarchy of features to use to summarize history, where we show how to best select from such given abstractions using small amounts of data. (4) Settings in which the agent must plan with an approximate model built from approximate states, where in a Best-paper-winning-work we showed that the less data that goes into building the approximate model the less deeply the agent should plan, for otherwise it risks overfitting to the error/noise in the model. (5)&nbsp;Settings in which data becomes available online (rather than in batches as in previous work), where we provide a gradient descent based approach to incorporating new data as it arrives to improve the PSR based models.</p>\n<p><br />Taken together the contributions of the work funded by this project significantly advance our understanding of and our capabilities in building artificial agents that can learn how to represent history in state features that are useful for future decision-making and planning.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/26/2019<br>\n\t\t\t\t\tModified by: Satinder&nbsp;S&nbsp;Baveja</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe field of Reinforcement Learning (RL) addresses the following challenging problem within Artificial Intelligence (AI): how should an artificial agent learn to act so as to improve its performance over time. In RL, agents observe their environment through sensors, take actions, and obtain reward in a repeated loop that results in improved accumulated reward over time through a kind of trial-and-error learning process. There has been a lot of progress in developing RL algorithms and theory; nonetheless, some advanced questions remain open.\n\n\nIn this project, we focus on the fundamental question of how the artificial agent should represent state, i.e., how should an agent summarize its history in order to condition future behavior on this fixed-length summary rather than the ever-growing history. This is a widely studied question in control theory, operations research, and AI. There are at least three broad classes of methods for summarizing history: (1) remembering recent history, (2) estimating latent representations of aspects of the environment from history, and (3) making predictions of the future from current history as a summary of history. This project focuses on the third class of methods, called Predictive State Representations (or PSRs), which are relatively new and underdeveloped thus far. PSRs offer the exciting possibility of being just as powerful as but being far more efficiently learnable than the other two, more widely studied, classes of methods.\n\n\nIn this project we develop a set of theoretical and empirical results that significantly extend our understand of PSRs for RL problems. We rigorously study multiple settings with properties we expect to hold in real world problems. (1) Settings in which we do not know in advance how many predictions of the future we would need to adequately summarize history, where we provide a formal notion of the difficulties and how to algorithmically ameliorate them. (2) Settings in which we do not have a lot of data to learn PSRs and so we must approximate, where we provide algorithms and theoretical analyses to show how to select the nature of the approximations that do best. (3) Settings in which the we are in advance given a set of possible hierarchy of features to use to summarize history, where we show how to best select from such given abstractions using small amounts of data. (4) Settings in which the agent must plan with an approximate model built from approximate states, where in a Best-paper-winning-work we showed that the less data that goes into building the approximate model the less deeply the agent should plan, for otherwise it risks overfitting to the error/noise in the model. (5) Settings in which data becomes available online (rather than in batches as in previous work), where we provide a gradient descent based approach to incorporating new data as it arrives to improve the PSR based models.\n\n\nTaken together the contributions of the work funded by this project significantly advance our understanding of and our capabilities in building artificial agents that can learn how to represent history in state features that are useful for future decision-making and planning. \n\n\t\t\t\t\tLast Modified: 07/26/2019\n\n\t\t\t\t\tSubmitted by: Satinder S Baveja"
 }
}