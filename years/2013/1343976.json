{
 "awd_id": "1343976",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SCH: EXP:  Collaborative Research: Privacy-Preserving Framework for Publishing Electronic Healthcare Records",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2014-01-01",
 "awd_exp_date": "2018-12-31",
 "tot_intn_awd_amt": 269505.0,
 "awd_amount": 269505.0,
 "awd_min_amd_letter_date": "2013-09-13",
 "awd_max_amd_letter_date": "2017-09-08",
 "awd_abstract_narration": "This project builds a novel privacy-preserving framework with both new algorithms and software tools to: 1) evaluate the effectiveness of current identifier-suppression techniques for Electronic Healthcare Record (EHR) data; 2) de-identify and anonymize EHR data to protect personal information without significantly reducing the utility of data for secondary data analysis. The proposed techniques eliminate the violation of privacy through re-identification, and facilitate the secondary usage, sharing, publishing and exchange of healthcare data without the risk of breaching protected health information (PHI). This new privacy-preserving framework injects the ICD-9-CM-aware constraint-based privacy-preserving techniques into EHRs to eliminate the threat of identifying an individual in the secondary use of research data. The proposed technique and development can be readily adapted to other types of healthcare databases in order to ensure privacy and prevent re-identification of published data. The project produces groundbreaking algorithms and tools for identifying privacy leakages and protecting personal privacy information in EHRs to improve healthcare data publishing. New privacy-preserving techniques developed in this project lead towards a new type of healthcare science for EHRs. The project also delivers fundamental advancements to engineering by showing how to integrate biomedical domain knowledge with a computationally advanced quantitative framework for preserving the privacy of published EHRs. HIPAA has established protocols and industry standards to protect the confidentiality of PHI. However, our results demonstrate that, even with regard to health data that meets HIPAA requirements, the risk of re-identification is not completely eliminated. By identifying the security vulnerabilities inherent in the HIPAA standards, our research develops a more rigorous security standard that greatly improves privacy protections by applying state-of-the-art algorithms.\r\n \r\nThe developed data privacy-preserving framework has significant implications for the future of US healthcare data publishing and related applications. Specifically, the transition from paper records to EHRs has accelerated significantly since the passage of the HITECH Act of 2009. The Act provides monetary incentives for the \"meaningful use\" of EHRs. As a result, the quality and quantity of healthcare databases has risen sharply, which has renewed the public's fear of a breach of privacy of their medical information. This research work is innovative and crucial not only for facilitating EHR data publishing, but also for enhancing the development and promotion of EHRs. At the educational front, this project facilitates the development of novel educational tools to construct entirely new courses and laboratory classes for healthcare, data privacy, data mining, and a wide range of applications. As a result, it enhances the current instructional methods for teaching data privacy and data mining, and has compelling biomedical and healthcare applications that can facilitate learning of computational algorithms. This project involves both undergraduate and graduate students in the three participating institutions. The PIs make a strong effort to engage minority graduate and undergraduate students in research activities in order to increase their exposure to cutting-edge research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Xiuzhen",
   "pi_last_name": "Cheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiuzhen Cheng",
   "pi_email_addr": "cheng@gwu.edu",
   "nsf_id": "000381089",
   "pi_start_date": "2015-12-04",
   "pi_end_date": "2017-09-08"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nan",
   "pi_last_name": "Zhang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nan Zhang",
   "pi_email_addr": "zhang.nan@ufl.edu",
   "nsf_id": "000491454",
   "pi_start_date": "2017-09-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Xiuzhen",
   "pi_last_name": "Cheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiuzhen Cheng",
   "pi_email_addr": "cheng@gwu.edu",
   "nsf_id": "000381089",
   "pi_start_date": "2013-09-13",
   "pi_end_date": "2015-12-04"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Xiuzhen",
   "pi_last_name": "Cheng",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Xiuzhen Cheng",
   "pi_email_addr": "cheng@gwu.edu",
   "nsf_id": "000381089",
   "pi_start_date": "2017-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "George Washington University",
  "inst_street_address": "1918 F ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "WASHINGTON",
  "inst_state_code": "DC",
  "inst_state_name": "District of Columbia",
  "inst_phone_num": "2029940728",
  "inst_zip_code": "200520042",
  "inst_country_name": "United States",
  "cong_dist_code": "00",
  "st_cong_dist_code": "DC00",
  "org_lgl_bus_name": "GEORGE WASHINGTON UNIVERSITY (THE)",
  "org_prnt_uei_num": "",
  "org_uei_num": "ECR5E2LU5BL6"
 },
 "perf_inst": {
  "perf_inst_name": "George Washington University",
  "perf_str_addr": "801 22nd St NW",
  "perf_city_name": "Washington",
  "perf_st_code": "DC",
  "perf_st_name": "District of Columbia",
  "perf_zip_code": "200520001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "00",
  "perf_st_cong_dist": "DC00",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801800",
   "pgm_ele_name": "Smart and Connected Health"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8018",
   "pgm_ref_txt": "Smart and Connected Health"
  },
  {
   "pgm_ref_code": "8061",
   "pgm_ref_txt": "SCH Type I:  EXP"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 269505.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our research findings are highly relevant to the current debate over health information privacy. Our research calls into question the efficacy of the state-of-the-practice anonymization framework.&nbsp; For example, we demonstrated how the publication of public health data, even after applying state-of-the-practice data anonymization techniques, could lead to breaches of patient privacy. Based on the findings, in an article published at the <em>Anesthesia and Analgesia</em> journal, we proposed an editorial policy for anesthesia journals to significantly reduce the likelihood of a privacy breach while supporting the goal of transparency of the research process.</p>\n<p>Another interesting finding from our research is related to the linking of healthcare data with publicly available information, such as data from social media.&nbsp; Contrary to the conventional wisdom that privacy leakage in this case is mostly caused by a user's own posts, we found that a significant amount of privacy disclosure on social media, especially on attributes that are often linkable to healthcare data, is caused by activities of a user's social ties. Our research not only demonstrated the existence of such \"peer-disclosure\", but also identified the intriguing differences between the identity elements revealed through self- and peer-disclosure.</p>\n<p>Our research also examines the inherent tradeoff between privacy protection and data utility.&nbsp; For example, we studied how the application of a state-of-the-practice data anonymization technique and a state-of-the-art differential privacy technique affects the ability for researchers to detect evidence of health disparity from the privacy-preserved data.&nbsp; Our results demonstrated the complex challenges facing privacy protection in health disparity research, as the essential data elements for enabling health disparity research might lead to privacy disparity and cause further harm to underserved populations if not carefully treated.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/22/2019<br>\n\t\t\t\t\tModified by: Nan&nbsp;Zhang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur research findings are highly relevant to the current debate over health information privacy. Our research calls into question the efficacy of the state-of-the-practice anonymization framework.  For example, we demonstrated how the publication of public health data, even after applying state-of-the-practice data anonymization techniques, could lead to breaches of patient privacy. Based on the findings, in an article published at the Anesthesia and Analgesia journal, we proposed an editorial policy for anesthesia journals to significantly reduce the likelihood of a privacy breach while supporting the goal of transparency of the research process.\n\nAnother interesting finding from our research is related to the linking of healthcare data with publicly available information, such as data from social media.  Contrary to the conventional wisdom that privacy leakage in this case is mostly caused by a user's own posts, we found that a significant amount of privacy disclosure on social media, especially on attributes that are often linkable to healthcare data, is caused by activities of a user's social ties. Our research not only demonstrated the existence of such \"peer-disclosure\", but also identified the intriguing differences between the identity elements revealed through self- and peer-disclosure.\n\nOur research also examines the inherent tradeoff between privacy protection and data utility.  For example, we studied how the application of a state-of-the-practice data anonymization technique and a state-of-the-art differential privacy technique affects the ability for researchers to detect evidence of health disparity from the privacy-preserved data.  Our results demonstrated the complex challenges facing privacy protection in health disparity research, as the essential data elements for enabling health disparity research might lead to privacy disparity and cause further harm to underserved populations if not carefully treated.\n\n \n\n\t\t\t\t\tLast Modified: 03/22/2019\n\n\t\t\t\t\tSubmitted by: Nan Zhang"
 }
}