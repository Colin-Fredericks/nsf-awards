{
 "awd_id": "1251131",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "BIGDATA: Small: DA: Big Multilinguality for Data-Driven Lexical Semantics",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 249994.0,
 "awd_amount": 249994.0,
 "awd_min_amd_letter_date": "2013-06-20",
 "awd_max_amd_letter_date": "2013-06-20",
 "awd_abstract_narration": "A key challenge in natural language processing is defining the computational representation of words.  Data-driven distributional approaches use corpora to induce vector-space representations for words, based on the contexts they occur in.  This project goes beyond traditional approaches (e.g., latent semantic analysis; Deerwester et al., 1990), which use words that tend to occur near a word in corpora to define the context, by extending the types of contexts used in constructing semantic vectors.  First, this project incorporates translation contexts, i.e., words readily available in multilingual parallel corpora, alongside traditional monolingual corpora.  This allows evidence-sharing across languages, most importantly from resource-rich languages with large corpora to more resource-poor languages.  Second, this project incorporates social context inferable from social network platforms, captured through author, time, geographic, and social connection metadata.  Taken together, these additional features give a broader definition of a word's context and lead to a more unified approach to the distributional approach to modeling human language, moving in the direction of a language-independent semantics.  The project focuses on ten typologically diverse languages representing several major language families (English, Arabic, Chinese, Spanish, Russian, German, Portuguese, Swahili, Malagasy, and Farsi). A key emphasis is scaling up algorithms for inferring distributional representations to web-scale corpora and dealing with much larger contextual vectors representing the expanded notion of context.  The approach also leverages noisy syntactic processing to enable syntactic information, rather than just information about neighboring words, to be considered when defining context.\r\n\r\nIn addition to improving the quality of the learned lexico-semantic representations by including richer contextual information, this project creates lexical semantic representations that link word types across languages. These have direct use in text processing applications such as text categorization, machine translation, information extraction, and semantic analysis of text, and they will enable the construction of robust lexical semantic resources in lower-resource languages that benefit from the richness of resources in languages they are paired with.  The multilingual vector representations produced will be released to the research community and will be used in undergraduate class projects.  The project provides integrated educational and research experience for two graduate students in a dynamic research environment.  The project website (http://www.ark.cs.cmu.edu/BigMultilinguality) will be used for dissemination of results.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Noah",
   "pi_last_name": "Smith",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Noah A Smith",
   "pi_email_addr": "noah@allenai.org",
   "nsf_id": "000228357",
   "pi_start_date": "2013-06-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Chris",
   "pi_last_name": "Dyer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chris Dyer",
   "pi_email_addr": "cdyer@cs.cmu.edu",
   "nsf_id": "000622961",
   "pi_start_date": "2013-06-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808300",
   "pgm_ele_name": "Big Data Science &Engineering"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8083",
   "pgm_ref_txt": "Big Data Science &Engineering"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 249994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>What is a word? &nbsp;In a computer program that deals with human language, words are often represented as vectors of real numbers. &nbsp;This allows us to encode the idea that words can be more or less similar to each other (e.g., \"apple\" and \"pear\" are, in some sense, closer than \"apple\" and \"saxophone\").</p>\n<p>This project contributed new methods that use data to define such word vectors. &nbsp;The first kind of data is parallel text, which consists of pairs of sentences in two different languages. &nbsp;We developed a new method that uses alignments of words in parallel text to estimate vectors for words in both languages. &nbsp;A second kind of data is social media messages, which include various non-textual elements about the context of the message, such as time and geographical location of posting. &nbsp;We developed a new method that differentiates a word's vector depending on where it is used (e.g., \"wicked\" has different meanings in Massachusetts and Kansas).</p>\n<p>Other methods developed in this project use existing lexical resources (e.g., the WordNet lexicon) to improve the quality of word vectors, or new learning algorithms to estimate word vectors with special properties like sparsity or hierarchical sparsity. &nbsp;These properties were found to improve word vector quality and arguably move word vectors closer to theories of lexical semantics.</p>\n<p>The project also contributes a website, wordvectors.org, which offers a suite of evaluation benchmarks, allowing other researchers and practitioners to compare their word vectors to existing ones (including ours).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/16/2015<br>\n\t\t\t\t\tModified by: Noah&nbsp;A&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWhat is a word?  In a computer program that deals with human language, words are often represented as vectors of real numbers.  This allows us to encode the idea that words can be more or less similar to each other (e.g., \"apple\" and \"pear\" are, in some sense, closer than \"apple\" and \"saxophone\").\n\nThis project contributed new methods that use data to define such word vectors.  The first kind of data is parallel text, which consists of pairs of sentences in two different languages.  We developed a new method that uses alignments of words in parallel text to estimate vectors for words in both languages.  A second kind of data is social media messages, which include various non-textual elements about the context of the message, such as time and geographical location of posting.  We developed a new method that differentiates a word's vector depending on where it is used (e.g., \"wicked\" has different meanings in Massachusetts and Kansas).\n\nOther methods developed in this project use existing lexical resources (e.g., the WordNet lexicon) to improve the quality of word vectors, or new learning algorithms to estimate word vectors with special properties like sparsity or hierarchical sparsity.  These properties were found to improve word vector quality and arguably move word vectors closer to theories of lexical semantics.\n\nThe project also contributes a website, wordvectors.org, which offers a suite of evaluation benchmarks, allowing other researchers and practitioners to compare their word vectors to existing ones (including ours).\n\n\t\t\t\t\tLast Modified: 12/16/2015\n\n\t\t\t\t\tSubmitted by: Noah A Smith"
 }
}