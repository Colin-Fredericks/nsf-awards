{
 "awd_id": "1337217",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "XPS: FP: Collaborative Research: Parallel Irregular Programs: From High-Level Specifications to Run-time Optimizations",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927885",
 "po_email": "abanerje@nsf.gov",
 "po_sign_block_name": "Anindya Banerjee",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 374867.0,
 "awd_amount": 374867.0,
 "awd_min_amd_letter_date": "2013-08-29",
 "awd_max_amd_letter_date": "2013-08-29",
 "awd_abstract_narration": "The high performance super-computers of today and the ordinary computers of tomorrow have an ever-increasing number of cores.  Utilizing these computers efficiently will allow research advancements in every field of science, from understanding the brain to understanding the fundamental particles to understanding the cosmos.  These new computers are increasingly complex and difficult to program.  At the same time, standard algorithms used in science and engineering are evolving and are increasingly hard to map to these machines.  Advances in programming models, tools, and implementations which make implementing complex algorithms simpler, while achieving high performance, are essential to making high performance computing a standard tool of all scientists.\r\n\r\nRegular algorithms, usually expressed with matrices, have driven high performance computing.  Increasingly there is considerable interest in using large-scale computers for irregular algorithms. Irregular algorithms arise in manipulating graphs, sparse-matrices, trees, adaptive meshes, etc and are increasingly a standard tool used by computational scientists.  Expressing such algorithms at a high-level has allowed high-performance run-times to achieve performance comparable to the best hand-coded implementations of these algorithms on shared-memory machines.  A high level description frees the programmer from the complexities of parallel programming.  The PIs are building run-times and compilers to allow the execution of complex, irregular algorithms on distributed-memory, large-scale computers.  A high-level representation allows the system to exploit considerable knowledge about the semantics of the algorithm to optimize communication, mask latency, and achieve high-performance.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marc",
   "pi_last_name": "Snir",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marc Snir",
   "pi_email_addr": "snir@illinois.edu",
   "nsf_id": "000165753",
   "pi_start_date": "2013-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "Dept. of Computer Science, U. of Illinois",
  "perf_str_addr": "201 N. Goodwin Av.",
  "perf_city_name": "Urbana",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618012302",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "828300",
   "pgm_ele_name": "Exploiting Parallel&Scalabilty"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 374867.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Supercomputers are constructed by connecting&nbsp; tens of thousand of individual machines into one \"clusters,\" using specialized, high-performance networks. Communication libraries are used by those machines to talk to each other over the network.<br /><br />Until a decade ago, such supercomputers were used mainly for the simulation of complex physical systems, such as the earth atmosphere, for weather prediction, or a flying plane, for aircraft design. MPI, the main communication library used on supercomputers, was designed to match the needs of such applications.<br /><br />Today, supercomputing clusters are used more and more for the analysis of large data sets. In many of these applications, the data is organized as a structure known as a graph in Computer Science. The computing and communication requirements of these applications are quite different from those of traditional engineering and science applications. It is necessary to revisit the design and implementation of communication software in supercomputers to&nbsp; optimize this software for such big-data, graph analytics applications.<br /><br />This project developed communication software called the Lightweight Communication Interface (LCI) to better address the needs of graph analytics applications. It maps more&nbsp; directly to the underlying hardware and better matches the requirements of graph analytics applications, thus eliminating software overheads and improving performance. It enables more light-weight interaction with computation and communication,&nbsp; by eliminating semantic features in libraries such as MPI that are not required for graph analytics applications; and by providing directly functionality&nbsp; that require additional layers of software&nbsp; when built atop MPI.<br /><br />The LCI communication software was integrated into a system called D-Galois, developed at the University of Texas at Austin, for programming graph analytics applications. The&nbsp; goal of D-Galois is to permit application developers to write graph analytics applications using high-level programming abstractions that promote programmer productivity.</p>\n<p>Experimental studies on big clusters at the Texas Advanced Computing Center (TACC) showed&nbsp; that the integration of D-Galois and LCI resulted in a powerful system that was roughly ten times faster than previous state-of-the-art cluster-based graph analytics systems.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/14/2018<br>\n\t\t\t\t\tModified by: Marc&nbsp;Snir</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSupercomputers are constructed by connecting  tens of thousand of individual machines into one \"clusters,\" using specialized, high-performance networks. Communication libraries are used by those machines to talk to each other over the network.\n\nUntil a decade ago, such supercomputers were used mainly for the simulation of complex physical systems, such as the earth atmosphere, for weather prediction, or a flying plane, for aircraft design. MPI, the main communication library used on supercomputers, was designed to match the needs of such applications.\n\nToday, supercomputing clusters are used more and more for the analysis of large data sets. In many of these applications, the data is organized as a structure known as a graph in Computer Science. The computing and communication requirements of these applications are quite different from those of traditional engineering and science applications. It is necessary to revisit the design and implementation of communication software in supercomputers to  optimize this software for such big-data, graph analytics applications.\n\nThis project developed communication software called the Lightweight Communication Interface (LCI) to better address the needs of graph analytics applications. It maps more  directly to the underlying hardware and better matches the requirements of graph analytics applications, thus eliminating software overheads and improving performance. It enables more light-weight interaction with computation and communication,  by eliminating semantic features in libraries such as MPI that are not required for graph analytics applications; and by providing directly functionality  that require additional layers of software  when built atop MPI.\n\nThe LCI communication software was integrated into a system called D-Galois, developed at the University of Texas at Austin, for programming graph analytics applications. The  goal of D-Galois is to permit application developers to write graph analytics applications using high-level programming abstractions that promote programmer productivity.\n\nExperimental studies on big clusters at the Texas Advanced Computing Center (TACC) showed  that the integration of D-Galois and LCI resulted in a powerful system that was roughly ten times faster than previous state-of-the-art cluster-based graph analytics systems.\n\n\t\t\t\t\tLast Modified: 12/14/2018\n\n\t\t\t\t\tSubmitted by: Marc Snir"
 }
}