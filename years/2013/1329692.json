{
 "awd_id": "1329692",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Collaborative Research: High-Level Perception and Control for Autonomous Reconfigurable Modular Robots",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Bruce Kramer",
 "awd_eff_date": "2013-10-01",
 "awd_exp_date": "2017-09-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2013-09-09",
 "awd_max_amd_letter_date": "2013-09-09",
 "awd_abstract_narration": "The goal of the project is the development of the theory, hardware and computational infrastructure that will enable automatically transforming user-defined, high-level tasks such as inspection of hazardous environments and object retrieval, into provably-correct control for modular robots. Modular robots are composed of simple individual modules; while a single module has limited capabilities, connecting multiple modules in different configurations allows the system to perform complex actions such as climbing, manipulating objects, traveling in unstructured environments and self-reconfiguring (breaking into multiple independent robots and reassembling into larger structures).  The project includes (i) defining and populating a large library of perception and actuation building blocks both manually through educational activities and automatically through novel algorithms, (ii) creating automated tools to assign values to probabilistic metrics associated with the performance of library components, (iii) developing a grammar and automated tools for control synthesis that sequence different components of the library to accomplish higher level tasks, if possible, or provide feedback to the user if the task cannot be accomplished and (iv) designing and building a novel modular robot platform capable of rapid and robust self-reconfiguration.\r\n \r\nThis research will have several outcomes. First, it will lay the foundations for making modular robots easily controlled by anyone. This will enrich the robotic industry with new types of robots with unique capabilities. Second, the research will create novel algorithms that tightly combine perception, control and hardware capabilities. Finally, this project will create an open-source infrastructure that will allow the public to contribute basic controllers to the library thus promoting general research and social interest in robotics and engineering.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hadas",
   "pi_last_name": "Kress Gazit",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hadas Kress Gazit",
   "pi_email_addr": "hadaskg@cornell.edu",
   "nsf_id": "000521463",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Campbell",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Mark E Campbell",
   "pi_email_addr": "mc288@cornell.edu",
   "nsf_id": "000214501",
   "pi_start_date": "2013-09-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 600000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Modular robots are composed of individual modules with limited sensing and actuation; while each module can move in the environment, connecting multiple modules in different configurations allows modular robots to perform complex actions such as climbing, manipulating objects and traveling in unstructured environments. This project addressed the challenge of getting modular robots to perform different tasks, fully autonomously, in unknown environments. More specifically, it designed novel modular robot hardware that can self-reconfigure, i.e. attach and detach modules to change the robot&rsquo;s shape without human intervention, and developed perception and control algorithms to enable the robots to understand their surrounding and choose their control actions to achieve a high-level task.</p>\n<p>The following describes the different outcomes of the project:</p>\n<p>&nbsp;</p>\n<p><strong>Hardware design</strong>: The project developed a novel modular robot platform, SMORES-EP, together with a sensor module that enables the modular robot to sense its environment and reason about it. Each SMORES-EP module is the size of an 80mm cube and has four actuated joints, including two wheels that can be used for driving on flat ground. The modules are equipped with electro-permanent magnets that can be turned on and off, allowing any face of one module to connect to or disconnect from any face of another, allowing the robot to self-reconfigure. The magnetic faces can also be used to attach to objects made of ferromagnetic materials, for example steel.</p>\n<p>&nbsp;</p>\n<p><strong>Library of controllers:</strong> The approach the project took to dealing with the large space of control actions for a modular robot was to develop a library of controllers that can be used to compose more complex behaviors. The library consisted of configuration-gait pairs; each library entry describes the shape of the robot, i.e. the number on modules it is composed of and their connectivity, and the control actions (joint angle sequence) used to create a simple behavior for that shape.</p>\n<p>&nbsp;</p>\n<p><strong>Online simulator and design tool: </strong>To facilitate creating different robot shapes (configurations) and basic behaviors (gaits), the project developed an online simulator, the Verification, Simulation, Programming And Robot Construction (VSPARC) simulator, that allows anyone to create library entries that can then be used for high-level control. This simulator was used by a group of undergraduate students to populate the library of controllers used for the end-to-end system demonstrations.</p>\n<p>&nbsp;</p>\n<p><strong>High-level control</strong>: Given the library of controllers, this project created a specification language that allows people to describe the task they want the robot to do, and control synthesis algorithms that convert the task description into actions the robot autonomously takes to achieve its task. In addition, if the robot realizes it does not have the appropriate actions for the task, it informs the person that the task in not achievable and provides a description of the missing control action.</p>\n<p>&nbsp;</p>\n<p><strong>Perception algorithms: </strong>To enable the robot to understand its surroundings and perform a task in an unknown environment, the project developed &ldquo;next best view&rdquo; perception and control algorithms that moved the robot to positions in which it would collect the most useful sensor data. In addition, the project developed environment characterization algorithms that are used to detect distinct structures in the environment such as tunnels and gaps. These structures are then used by the high-level controller to determine the correct next configuration and action the robot must take. &nbsp;<strong>&nbsp;</strong></p>\n<p>&nbsp;</p>\n<p><strong>Environment augmentation: </strong>Sometimes the environment and available controllers prevent the robot from achieving its task, however by augmenting the environment, for example by building bridges or ramps, the task may be completed. This project designed novel passive modules that can be composed autonomously by the modular robots into different environment augmenting elements and developed algorithms to autonomously assemble and place the appropriate structure needed for continuing the task.</p>\n<p>&nbsp;</p>\n<p><strong>End-to-end demonstrations: </strong>The hardware and algorithms were integrated into an end-to-end system used to perform different high-level tasks in<strong> </strong>different environments. Videos of these demonstrations can be seen at:</p>\n<p><a href=\"https://www.youtube.com/watch?v=eJsnG9DZjgM&amp;feature=youtu.be\">https://www.youtube.com/watch?v=eJsnG9DZjgM&amp;feature=youtu.be</a></p>\n<p><a href=\"https://www.youtube.com/watch?v=NKj-xulsxco&amp;feature=youtu.be\">https://www.youtube.com/watch?v=NKj-xulsxco&amp;feature=youtu.be</a></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/04/2018<br>\n\t\t\t\t\tModified by: Hadas&nbsp;Kress Gazit</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080356746_drawer_open_top--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080356746_drawer_open_top--rgov-800width.jpg\" title=\"Robot opening drawer\"><img src=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080356746_drawer_open_top--rgov-66x44.jpg\" alt=\"Robot opening drawer\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The task was to gather information about what is inside the second drawer. The robot autonomously detected the environment, built a ramp, placed it in the correct place, climbed it, and used its magnets to open the second drawer</div>\n<div class=\"imageCredit\">Tarik Tosun</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Hadas&nbsp;Kress Gazit</div>\n<div class=\"imageTitle\">Robot opening drawer</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080096445_reconf_drive--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080096445_reconf_drive--rgov-800width.jpg\" title=\"Self-reconfiguration\"><img src=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080096445_reconf_drive--rgov-66x44.jpg\" alt=\"Self-reconfiguration\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A view from the sensor module's webcam used for self-reconfiguration. The robot is autonomously self-reconfiguring from a car-like shape to a proboscis</div>\n<div class=\"imageCredit\">Tarik Tosun</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Hadas&nbsp;Kress Gazit</div>\n<div class=\"imageTitle\">Self-reconfiguration</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080200227_bridge_place_bridge--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080200227_bridge_place_bridge--rgov-800width.jpg\" title=\"Placing a bridge\"><img src=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515080200227_bridge_place_bridge--rgov-66x44.jpg\" alt=\"Placing a bridge\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The modular robot detected there is a gap preventing it from reaching its destination. It then assembled a bridge and is autonomously placing it in the correct spot</div>\n<div class=\"imageCredit\">Tarik Tosun</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Hadas&nbsp;Kress Gazit</div>\n<div class=\"imageTitle\">Placing a bridge</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515079905604_stamp_placing--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515079905604_stamp_placing--rgov-800width.jpg\" title=\"Modular robot placing a stamp on a box\"><img src=\"/por/images/Reports/POR/2018/1329692/1329692_10277182_1515079905604_stamp_placing--rgov-66x44.jpg\" alt=\"Modular robot placing a stamp on a box\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Modular robot placing a stamp on a box</div>\n<div class=\"imageCredit\">Tarik Tosun</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Hadas&nbsp;Kress Gazit</div>\n<div class=\"imageTitle\">Modular robot placing a stamp on a box</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nModular robots are composed of individual modules with limited sensing and actuation; while each module can move in the environment, connecting multiple modules in different configurations allows modular robots to perform complex actions such as climbing, manipulating objects and traveling in unstructured environments. This project addressed the challenge of getting modular robots to perform different tasks, fully autonomously, in unknown environments. More specifically, it designed novel modular robot hardware that can self-reconfigure, i.e. attach and detach modules to change the robot?s shape without human intervention, and developed perception and control algorithms to enable the robots to understand their surrounding and choose their control actions to achieve a high-level task.\n\nThe following describes the different outcomes of the project:\n\n \n\nHardware design: The project developed a novel modular robot platform, SMORES-EP, together with a sensor module that enables the modular robot to sense its environment and reason about it. Each SMORES-EP module is the size of an 80mm cube and has four actuated joints, including two wheels that can be used for driving on flat ground. The modules are equipped with electro-permanent magnets that can be turned on and off, allowing any face of one module to connect to or disconnect from any face of another, allowing the robot to self-reconfigure. The magnetic faces can also be used to attach to objects made of ferromagnetic materials, for example steel.\n\n \n\nLibrary of controllers: The approach the project took to dealing with the large space of control actions for a modular robot was to develop a library of controllers that can be used to compose more complex behaviors. The library consisted of configuration-gait pairs; each library entry describes the shape of the robot, i.e. the number on modules it is composed of and their connectivity, and the control actions (joint angle sequence) used to create a simple behavior for that shape.\n\n \n\nOnline simulator and design tool: To facilitate creating different robot shapes (configurations) and basic behaviors (gaits), the project developed an online simulator, the Verification, Simulation, Programming And Robot Construction (VSPARC) simulator, that allows anyone to create library entries that can then be used for high-level control. This simulator was used by a group of undergraduate students to populate the library of controllers used for the end-to-end system demonstrations.\n\n \n\nHigh-level control: Given the library of controllers, this project created a specification language that allows people to describe the task they want the robot to do, and control synthesis algorithms that convert the task description into actions the robot autonomously takes to achieve its task. In addition, if the robot realizes it does not have the appropriate actions for the task, it informs the person that the task in not achievable and provides a description of the missing control action.\n\n \n\nPerception algorithms: To enable the robot to understand its surroundings and perform a task in an unknown environment, the project developed \"next best view\" perception and control algorithms that moved the robot to positions in which it would collect the most useful sensor data. In addition, the project developed environment characterization algorithms that are used to detect distinct structures in the environment such as tunnels and gaps. These structures are then used by the high-level controller to determine the correct next configuration and action the robot must take.   \n\n \n\nEnvironment augmentation: Sometimes the environment and available controllers prevent the robot from achieving its task, however by augmenting the environment, for example by building bridges or ramps, the task may be completed. This project designed novel passive modules that can be composed autonomously by the modular robots into different environment augmenting elements and developed algorithms to autonomously assemble and place the appropriate structure needed for continuing the task.\n\n \n\nEnd-to-end demonstrations: The hardware and algorithms were integrated into an end-to-end system used to perform different high-level tasks in different environments. Videos of these demonstrations can be seen at:\n\nhttps://www.youtube.com/watch?v=eJsnG9DZjgM&amp;feature=youtu.be\n\nhttps://www.youtube.com/watch?v=NKj-xulsxco&amp;feature=youtu.be\n\n \n\n\t\t\t\t\tLast Modified: 01/04/2018\n\n\t\t\t\t\tSubmitted by: Hadas Kress Gazit"
 }
}