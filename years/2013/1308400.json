{
 "awd_id": "1308400",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "A Comprehensive Framework for Fully Efficient Robust Estimation and Variable Selection, with Application to High-Dimensional and Complex Data",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 149997.0,
 "awd_amount": 149997.0,
 "awd_min_amd_letter_date": "2013-07-24",
 "awd_max_amd_letter_date": "2015-08-28",
 "awd_abstract_narration": "This research addresses robustness in both estimation and variable selection within the context of today's complex data structures. The overarching theme of the research is that carefully specified moment restrictions combined with appropriate weighting of the data will lead to the ideal goals of full efficiency in estimation and variable selection which remains stable in the presence of atypical observations. The methodology is developed via generalized empirical likelihood, which yields estimated weights for each observation. In the process, this automatically downweights observations that may deviate from the model, thus reducing their influence. Meanwhile, the estimators have no loss of efficiency compared with the fully efficient model-based estimator if the model were correctly specified, even in finite samples. Taking this point of view allows a unified framework to the construction of robust and efficient procedures that can be developed for a variety of models. The foundation of efficiency and robustness allows variable selection to be built into the methods to handle, not only the moderate, but also the high-dimensional setting. Due to the performance of the baseline approach, the variable selection consistency under contamination and misspecification can improve on existing selection methods that rest on a starting point that may be already non-robust or less than fully efficient. \r\n\r\nModern scientific data is characterized by a wealth of information. The data explosion has arisen in diverse areas running the gamut from drug discovery to the financial markets and even homeland security. While the massive influx of data has led to breakthroughs in these fields, it brings many statistical issues to the forefront. In particular, it can be an overwhelming task to determine the relevant predictor variables that provide a suitable model. Meanwhile, with today's complex data, this postulated model will surely be only a simplification of reality. Thus it is inevitable that some of the data will deviate, perhaps significantly, from the model, although it is still useful for the bulk of the data and can provide meaningful insight. This research targets the essential task of developing techniques to perform estimation and variable selection, while also allowing for some of the data to deviate from the model without greatly affecting the results. The methods developed from this research are robust to outliers and model misspecification, while still maintaining efficiency for both estimation and variable selection even in the presence of this contamination. Thus it will be a key component to enable meaningful results in the face of complex data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Howard",
   "pi_last_name": "Bondell",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Howard D Bondell",
   "pi_email_addr": "bondell@stat.ncsu.edu",
   "nsf_id": "000132094",
   "pi_start_date": "2013-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "North Carolina State University",
  "inst_street_address": "2601 WOLF VILLAGE WAY",
  "inst_street_address_2": "",
  "inst_city_name": "RALEIGH",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9195152444",
  "inst_zip_code": "276950001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "NC02",
  "org_lgl_bus_name": "NORTH CAROLINA STATE UNIVERSITY",
  "org_prnt_uei_num": "U3NVH931QJJ3",
  "org_uei_num": "U3NVH931QJJ3"
 },
 "perf_inst": {
  "perf_inst_name": "North Carolina State University",
  "perf_str_addr": "",
  "perf_city_name": "Raleigh",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "276958203",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "NC02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 48051.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 49968.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 51978.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Complex data naturally brings with it observations that do not fit the assumptions of the model, either systematically, or simply due to the fact that the model is only an approximation to reality. This project addressed the difficulties in both estimation and variable selection to remain robust even in the presence of these atypical data points.</p>\n<p><br />The research developed a thorough and general framework for robust and fully efficient estimation and variable selection that can be adapted to a wide variety of model scenarios. These methods were first developed in moderate dimensions in order to provide a rigorous and firm foundation for the new framework. They were then further developed to aid in the growing analysis that is necessary for the high-dimensional and even ultra-high dimensional settings.</p>\n<p>Two main results have been obtained.</p>\n<p><br />1) Large- and finite-sample efficiency and resistance to outliers are the key goals of robust statistics. Although often not simultaneously attainable, we developed and studied a linear regression estimator that comes close. Efficiency obtains from the estimator&rsquo;s close connection to generalized empirical likelihood, and its favorable robustness properties are obtained by constraining the associated sum of (weighted) squared residuals. We proved maximum attainable finite-sample replacement breakdown point, and full asymptotic efficiency for normal errors.</p>\n<p><br />2) For the outlier detection and variable selection problem in linear regression, a mean shift parameter was added to the linear model to reflect the effect of the outlier, where an outlier has a nonzero shift parameter. We then applied an adaptive regularization on these shift parameters to shrink most of them to zero. For those observations with nonzero mean shift parameter estimates, they are regarded as outliers. Meanwhile, an L1 penalty is added to the regression parameters to select important predictors. We proposed an efficient algorithm to solve this jointly penalized optimization problem. Theoretical results were developed in terms of high breakdown point, full efficiency as well as outlier detection consistency.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/27/2019<br>\n\t\t\t\t\tModified by: Howard&nbsp;D&nbsp;Bondell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nComplex data naturally brings with it observations that do not fit the assumptions of the model, either systematically, or simply due to the fact that the model is only an approximation to reality. This project addressed the difficulties in both estimation and variable selection to remain robust even in the presence of these atypical data points.\n\n\nThe research developed a thorough and general framework for robust and fully efficient estimation and variable selection that can be adapted to a wide variety of model scenarios. These methods were first developed in moderate dimensions in order to provide a rigorous and firm foundation for the new framework. They were then further developed to aid in the growing analysis that is necessary for the high-dimensional and even ultra-high dimensional settings.\n\nTwo main results have been obtained.\n\n\n1) Large- and finite-sample efficiency and resistance to outliers are the key goals of robust statistics. Although often not simultaneously attainable, we developed and studied a linear regression estimator that comes close. Efficiency obtains from the estimator?s close connection to generalized empirical likelihood, and its favorable robustness properties are obtained by constraining the associated sum of (weighted) squared residuals. We proved maximum attainable finite-sample replacement breakdown point, and full asymptotic efficiency for normal errors.\n\n\n2) For the outlier detection and variable selection problem in linear regression, a mean shift parameter was added to the linear model to reflect the effect of the outlier, where an outlier has a nonzero shift parameter. We then applied an adaptive regularization on these shift parameters to shrink most of them to zero. For those observations with nonzero mean shift parameter estimates, they are regarded as outliers. Meanwhile, an L1 penalty is added to the regression parameters to select important predictors. We proposed an efficient algorithm to solve this jointly penalized optimization problem. Theoretical results were developed in terms of high breakdown point, full efficiency as well as outlier detection consistency.\n\n\t\t\t\t\tLast Modified: 01/27/2019\n\n\t\t\t\t\tSubmitted by: Howard D Bondell"
 }
}