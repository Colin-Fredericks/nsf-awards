{
 "awd_id": "1318392",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Robust and Long-Term Visual Mapping and Localization",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2013-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 373293.0,
 "awd_amount": 373293.0,
 "awd_min_amd_letter_date": "2013-08-26",
 "awd_max_amd_letter_date": "2014-06-21",
 "awd_abstract_narration": "This project develops robust and persistent algorithms for mapping and localization using low-cost visual/depth cameras and inertial sensors. New map representations and algorithms are developed to provide computationally efficient long-term 3D mapping and navigation. Topics of investigation include incremental non-Gaussian inference techniques, dense mapping, change detection in dynamic environments, and semantic understanding.  A lack of robustness has been a key shortcoming of previous techniques for localization, and thwarted the development of persistently autonomous mobile robot systems. Extension to multimodal distributions poses significant intellectual difficulties.  Dense methods are transforming robotic perception, enabling sophisticated physical interaction with objects, traversal of stairs, and safe maneuvering in cluttered and confined spaces. Whereas most past research in robotic mapping has assumed a static world, the approach being developed in this grant exploits the dynamics of the world to discover information about objects and places.  These advances are being tested for robotic and man-portable sensing systems operating in indoor, outdoor, and underwater environments.  The expected impacts span a broad range of applications, from robotic manufacturing, medical robotics, agriculture, and space and underwater exploration, in which perception is a key requirement.  Other potential spin-offs include human-portable mapping applications in real estate, construction, and facility maintenance, health and safety.  MIT Online Robotics Education provides a set of online course materials for core topics in robotics, targeted to a broad audience for high school and college education.  Open source software modules provide positioning capabilities for low-cost robots for education and service robotics applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Leonard",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "John J Leonard",
   "pi_email_addr": "jleonard@mit.edu",
   "nsf_id": "000212797",
   "pi_start_date": "2013-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "",
  "perf_city_name": "Cambridge",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 120691.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 252602.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research project developed new robust algorithms for improving the abilities of mobile robots to build maps of unknown environments, while concurrently using those maps to navigate.&nbsp; This capability, known as Simultaneous Localization and Mapping (SLAM) is a cornerstone of robust perception and autonomy for a wide range of mobile robots, such as self-driving cars, unmanned air vehicles, and undersea robots. Our work made advances in the fundamental underlying techniques for SLAM in a number of ways, producing several products with a high intellectual merit in relation to the mobile robotics literature.&nbsp; A key focus was increasing robustness, thereby improving long-term autonomy. For example, we developed a novel solution for performing \"loop closing\", in which a robot corrects its map by revisiting a place that it has been before, to apply a geometric correction to its map, using rich dense 3D representations extracted from RGB-D camera data [Whelan et al., \"Real-time Large Scale Dense RGB-D SLAM with Volumetric Fusion\", IJRR, Vol 34, Issue 4-5, 2015]. Another advance was the development of a new approach to enable a robot to maintain multimodal beliefs about the locations of entities in the world; this is helpful for highly ambiguous situations in which sensor measurements and location estimates are not well represented with a Gaussian distribution [Fourie et al., \"A nonparametric belief solution to the Bayes tree\", IROS 2016].&nbsp; This problem is challenging due to computational complexity, but important because it can improve the ability of mobile robots to avoid making mistakes and to more easily recover from failures.&nbsp; In addition, we developed some of the first \"certifiably correct\" algorithms for SLAM [Rosen et al., WAFR 2016, Best Paper Award.]&nbsp; Finally, our work also lead to some of the first SLAM algorithms that uses a Deep Learning architecture to perform visual odometry and loop closing, with self-supervised techniques for motion estimation and map correction [Pillai and Leonard, \"Towards visual ego-motion learning in robots\", IROS 2017.]<br />The broader impacts of our project include a number of contributions to mentoring of young roboticists and the development of educational materials for widespread use.&nbsp; Perhaps the most notable broader impacts outcome of our project was the creation of the highly successful \"Duckietown\" http://duckietown.mit.edu/ which was developed and taught at MIT in Spring, 2016 by the PIs junior colleagues Andrea Censi and Liam Paull, under partial support from this NSF grant.&nbsp; Duckietown is an open, reproducible, in-expensive, and friendly platform for autonomy education and research.&nbsp; Duckietown is explicitly designed to make robotics more approachable, enhancing diversity and inclusion. The system comprises small autonomous vehicles (&ldquo;Duckiebots&rdquo;) built from off-the-shelf components, and a city (&ldquo;Duckietown&rdquo;) complete of roads, signage, traffic lights, obstacles, and citizens (duckies) in need of transportation. The Duckietown platform is designed to be &ldquo;minimal&rdquo; in terms of cost, while providing a large range of autonomy behaviors. The Duckiebots are able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots; localize within a global map; navigate a city; and coordinate at intersections to avoid collisions. A Duckiebot senses the world with only one monocular camera and performs all processing onboard a Raspberri Pi 2. To perform all the functionality required (compensate for variable illumination, detect lane markings, signage, traffic lights, obstacles, etc.), we implemented explicit resource management for computation, memory, and bandwidth. Since all of the code and instructions are fully available as open source and the platform is low-cost, the hope is that others in the community will adopt the platform for education and research.&nbsp; Duckietown has now grown into a global effort, involving approximately ten universities on four continents.&nbsp; See http://duckietown.org/ for more information.<br />Overall, this project supported and involved a wide of MIT graduate and undergraduate students, as well as MIT postdocs, who contributed both to fundamental research and educational curriculum developed, generating a considerable number of papers that were presented at leading international conferences and workshops in Robotics.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/16/2017<br>\n\t\t\t\t\tModified by: John&nbsp;J&nbsp;Leonard</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research project developed new robust algorithms for improving the abilities of mobile robots to build maps of unknown environments, while concurrently using those maps to navigate.  This capability, known as Simultaneous Localization and Mapping (SLAM) is a cornerstone of robust perception and autonomy for a wide range of mobile robots, such as self-driving cars, unmanned air vehicles, and undersea robots. Our work made advances in the fundamental underlying techniques for SLAM in a number of ways, producing several products with a high intellectual merit in relation to the mobile robotics literature.  A key focus was increasing robustness, thereby improving long-term autonomy. For example, we developed a novel solution for performing \"loop closing\", in which a robot corrects its map by revisiting a place that it has been before, to apply a geometric correction to its map, using rich dense 3D representations extracted from RGB-D camera data [Whelan et al., \"Real-time Large Scale Dense RGB-D SLAM with Volumetric Fusion\", IJRR, Vol 34, Issue 4-5, 2015]. Another advance was the development of a new approach to enable a robot to maintain multimodal beliefs about the locations of entities in the world; this is helpful for highly ambiguous situations in which sensor measurements and location estimates are not well represented with a Gaussian distribution [Fourie et al., \"A nonparametric belief solution to the Bayes tree\", IROS 2016].  This problem is challenging due to computational complexity, but important because it can improve the ability of mobile robots to avoid making mistakes and to more easily recover from failures.  In addition, we developed some of the first \"certifiably correct\" algorithms for SLAM [Rosen et al., WAFR 2016, Best Paper Award.]  Finally, our work also lead to some of the first SLAM algorithms that uses a Deep Learning architecture to perform visual odometry and loop closing, with self-supervised techniques for motion estimation and map correction [Pillai and Leonard, \"Towards visual ego-motion learning in robots\", IROS 2017.]\nThe broader impacts of our project include a number of contributions to mentoring of young roboticists and the development of educational materials for widespread use.  Perhaps the most notable broader impacts outcome of our project was the creation of the highly successful \"Duckietown\" http://duckietown.mit.edu/ which was developed and taught at MIT in Spring, 2016 by the PIs junior colleagues Andrea Censi and Liam Paull, under partial support from this NSF grant.  Duckietown is an open, reproducible, in-expensive, and friendly platform for autonomy education and research.  Duckietown is explicitly designed to make robotics more approachable, enhancing diversity and inclusion. The system comprises small autonomous vehicles (\"Duckiebots\") built from off-the-shelf components, and a city (\"Duckietown\") complete of roads, signage, traffic lights, obstacles, and citizens (duckies) in need of transportation. The Duckietown platform is designed to be \"minimal\" in terms of cost, while providing a large range of autonomy behaviors. The Duckiebots are able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots; localize within a global map; navigate a city; and coordinate at intersections to avoid collisions. A Duckiebot senses the world with only one monocular camera and performs all processing onboard a Raspberri Pi 2. To perform all the functionality required (compensate for variable illumination, detect lane markings, signage, traffic lights, obstacles, etc.), we implemented explicit resource management for computation, memory, and bandwidth. Since all of the code and instructions are fully available as open source and the platform is low-cost, the hope is that others in the community will adopt the platform for education and research.  Duckietown has now grown into a global effort, involving approximately ten universities on four continents.  See http://duckietown.org/ for more information.\nOverall, this project supported and involved a wide of MIT graduate and undergraduate students, as well as MIT postdocs, who contributed both to fundamental research and educational curriculum developed, generating a considerable number of papers that were presented at leading international conferences and workshops in Robotics. \n\n\t\t\t\t\tLast Modified: 12/16/2017\n\n\t\t\t\t\tSubmitted by: John J Leonard"
 }
}