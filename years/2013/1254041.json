{
 "awd_id": "1254041",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: An Information-Theoretic Approach to Communication-Constrained Statistical Learning",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2013-02-01",
 "awd_exp_date": "2022-01-31",
 "tot_intn_awd_amt": 518435.0,
 "awd_amount": 518435.0,
 "awd_min_amd_letter_date": "2012-12-21",
 "awd_max_amd_letter_date": "2021-02-02",
 "awd_abstract_narration": "This project aims to develop an information-theoretic approach to communication-constrained statistical learning problems involving multiple learning agents located at the nodes of a large network. This approach will build on the recently introduced coordination paradigm within network information theory, which looks at multiterminal problems in terms of optimal use of communication resources in order to establish some desired statistical correlations between the nodes of a network. The main theoretical goal is to explicitly identify the effect of bandwidth limitations, losses, delays, and lack of central coordination on the performance of statistical learning algorithms over networks. The project will systematically explore the fundamental limits of learning in multiterminal settings and design efficiently implementable and robust coding/decoding schemes. The theory developed under this project will be a novel synthesis of probabilistic techniques from machine learning (such as empirical process theory) and of multiterminal information theory (such as distributed lossy source coding).\r\n\r\nAs a broader impact, this project will provide key enabling technologies for large-scale, distributed applications of machine learning in such domains as smart grids, health-care informatics, transportation networks, and cybersecurity. Statistical machine learning is emerging as a dominant paradigm for making accurate predictions on the basis of empirical observations in the presence of significant model uncertainty. Most of the research activity in this field, however, has taken place in isolation from the realities of complex networks and all the attendant limitations on information transmission and processing: it is frequently assumed that the data needed for learning are available instantly, with arbitrary precision, and at a single location. However, given the fact that most data fed to machine learning algorithms are increasingly generated, exchanged, stored and processed over large-scale networks, there is a pressing need to dispense with this assumption and thus take network effects into consideration. The theory and the algorithms developed as part of this project will ensure that the relevant data are delivered over the network to the right decision-makers, while securing accurate decisions made on the basis of the received information. The research component of the project is tightly integrated with an education and outreach plan, including development and teaching of new courses on machine learning aimed specifically at engineering students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Maxim",
   "pi_last_name": "Raginsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Maxim Raginsky",
   "pi_email_addr": "maxim@illinois.edu",
   "nsf_id": "000551233",
   "pi_start_date": "2012-12-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618207473",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  },
  {
   "pgm_ele_code": "793500",
   "pgm_ele_name": "COMM & INFORMATION THEORY"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 200045.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 209720.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 108670.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project developed theoretical and algorithmic foundations for statistical machine learning in situations where the learning agent(s) are constrained in their communication capabilities. Such situations may arise, for instance, when the training data are collected by sensing devices that must communicate with a global processor over wireless channels. Another example is latency and memory constraints in a distributed cloud-based architecture. These scenarios call for a principled synthesis of statistical learning theory (which studies fundamental statistical limits of learning) and information theory (which studies fundamental limits of communication systems). As part of the research program articulated in this project, upper and lower bounds on the generalization error of learning algorithms were obtained that account for both the statistical effects (such as sample size or model complexity) and the information-theoretic limitations (such as the number of bits that can be communicated to the global processor). These bounds allow system designers to make informed and principled trade-offs between communication, computation, and statistical performance. In addition, this project initiated a new line of work that uses information-theoretic quantities, such as the mutual information between the training data and the outcome of the learning algorithm, to analyze the generalization performance of learning algorithms. The operational meaning of mutual information in these bounds pertains to the sensitivity of the learning algorithm to local modifications of the training examples, which can be connected to other important operational characteristics, such as differential privacy.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/01/2022<br>\n\t\t\t\t\tModified by: Maxim&nbsp;Raginsky</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project developed theoretical and algorithmic foundations for statistical machine learning in situations where the learning agent(s) are constrained in their communication capabilities. Such situations may arise, for instance, when the training data are collected by sensing devices that must communicate with a global processor over wireless channels. Another example is latency and memory constraints in a distributed cloud-based architecture. These scenarios call for a principled synthesis of statistical learning theory (which studies fundamental statistical limits of learning) and information theory (which studies fundamental limits of communication systems). As part of the research program articulated in this project, upper and lower bounds on the generalization error of learning algorithms were obtained that account for both the statistical effects (such as sample size or model complexity) and the information-theoretic limitations (such as the number of bits that can be communicated to the global processor). These bounds allow system designers to make informed and principled trade-offs between communication, computation, and statistical performance. In addition, this project initiated a new line of work that uses information-theoretic quantities, such as the mutual information between the training data and the outcome of the learning algorithm, to analyze the generalization performance of learning algorithms. The operational meaning of mutual information in these bounds pertains to the sensitivity of the learning algorithm to local modifications of the training examples, which can be connected to other important operational characteristics, such as differential privacy.\n\n\t\t\t\t\tLast Modified: 06/01/2022\n\n\t\t\t\t\tSubmitted by: Maxim Raginsky"
 }
}