{
 "awd_id": "1343906",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Generalized Network Slicing for NSF OpenCloud",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "John Brassil",
 "awd_eff_date": "2013-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 296143.0,
 "awd_amount": 296143.0,
 "awd_min_amd_letter_date": "2013-08-12",
 "awd_max_amd_letter_date": "2013-08-12",
 "awd_abstract_narration": "This project will demonstrate the feasibility of generalized network slicing for Software Defined Networks (SDN) that would enable multiple multi-tenant virtual clouds on the same physical infrastructure.  The generalized network slicing is aimed at network operators as opposed to the end users or tenants of a cloud. The network operators require network slicing with the ability to specify performance, topology, network programmability, and independent address space and require performance isolation. In this regard the proposed network slicing is distinct compared to other efforts in the broader area of network virtualization and can be used independent of network virtualization aimed at supporting 100s and 1000s of cloud tenants.\r\n\r\nThe project will demonstrate generalized network slicing system by focusing on four fundamental challenges:\r\n       -\tTopology Mapping. Goal is to allow an experimenter or a tenant to specify topology of her network slice and map the (virtual) slice topology on the physical network and maintain this mapping as both physical and slice topologies change over time.\r\n       -\tAddress Space Mapping. Goal is to allow a tenant to use her own private address space and translate the private address space to the physical address space while exploiting packet forwarding capabilities of the existing devices.\r\n       -\tControl Function Mapping. Goal is to allow each tenant to have SDN style programming capability of her virtual network slice and map control functions on to the corresponding physical instantiation of the network.\r\n       -\tPerformance Isolation. Goal is to allow each tenant to specify resource requirements for her network slice and allocate and police resource usage for each virtual slice to ensure performance isolation.\r\n\r\nThe project will focus on demonstrating the functionality and gaining valuable experience with experimentation. This will help the community to subsequently focus on performance, scale, robustness, and other attributes of the solution that are needed for a real world deployment.\r\n\r\nThe cloud is rapidly changing the face of computing, and as such, is having an increasing effect on experimental systems research, teaching and research in the broader Computer Science community, and how research is done across the breadth of science and engineering. By demonstrating the feasibility of network slicing as an important building block of a research cloud this project will inform the the on-going discussion about the cloud?s evolution.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nick",
   "pi_last_name": "McKeown",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Nick W McKeown",
   "pi_email_addr": "nickm@ee.stanford.edu",
   "nsf_id": "000487607",
   "pi_start_date": "2013-08-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "353 Serra Mall",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943059025",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7363",
   "pgm_ref_txt": "RES IN NETWORKING TECH & SYS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "8002",
   "pgm_ref_txt": "CISE Research Resources"
  }
 ],
 "app_fund": [
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 296143.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><h1>NSF EAGER Generalized Network Slicing: Outcomes Report</h1>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">We proposed an exploratory project to demonstrate the feasibility of generalized network slicing?for SDN that would enable multiple virtual tenant networks on the same physical network. We focused on four fundamental challenges:</p>\n<p class=\"Default\">&nbsp;</p>\n<ul>\n<li>Topology Mapping. Allow a tenant to specify topology of her?virtual network (a slice) and map the (virtual) topology on the physical network and maintain this mapping as both physical and virtual topologies change over time. </li>\n</ul>\n<p class=\"Default\">&nbsp;</p>\n<ul>\n<li>Address Space Mapping. Allow a tenant to use her own private address space and translate the private address space to the physical address space while exploiting packet forwarding capabilities of the existing devices. </li>\n</ul>\n<p class=\"Default\">&nbsp;</p>\n<ul>\n<li>Control Function Mapping. Allow each tenant to have SDN style programming capability of her virtual network and map control functions on to the corresponding physical instantiation of the network. </li>\n</ul>\n<p class=\"Default\">&nbsp;</p>\n<ul>\n<li>Performance Isolation. Allow each tenant to specify resource requirements for?her network slice and allocate and police resource usage for each virtual slice to ensure performance isolation.</li>\n</ul>\n<p class=\"Default\">&nbsp;</p>\n<ul>\n<li>Control of Programmable Forwarding: Give more control to a virtual network operator using programmable data plane capabilities made possible by P4 (<a href=\"http://p4.org/\">http://p4.org/</a>) and a new class of programmable switches that are starting to become available. </li>\n</ul>\n<p class=\"Default\">&nbsp;</p>\n<p>In collaboration with Open Networking Laboratory (ON.Lab) we developed OpenVirteX (OVX), a network virtualization platform that enables operators to create and manage virtual Software Defined Networks (vSDNs) on behalf of their tenants. Tenants are free to specify the topology and addressing scheme of their vSDN, and run their own Network Operating System (NOS) to control it. Since OpenVirteX logically decouples vSDNs from the infrastructure, it also enables the introduction of features such as link and switch resiliency, and network snapshotting and migration of these tenant networks. OVX functions as an OpenFlow controller proxy between an operator&rsquo;s physical network and the tenants&rsquo; network OSes. Our evaluations of OVX demonstrated it is capable of presenting tenants with configurable vSDNs while incurring a modest overhead to the control channel.</p>\n<p class=\"Default\">&nbsp;</p>\n<p>We deployed OVX on Internet 2 NDDI, a nation-wide research network, to demonstrate its capabilities in a live physical WAN. We ran two vSDNs, one a copy of the I2 topology and the other, a single giant switch. Each vSDN was controlled by its network OS, ONOS and Floodlight, respectively, which ran unique applications on I2 WAN.</p>\n<p class=\"Default\">We open sourced OVX and it is available on github with an Apache 2.0 license and many R&amp;E networks deployed and experimented with OVX.&nbsp;</p>\n<p class=\"Default\">&nbsp;</p>\n<p class=\"Default\">In collaboration with Princeton University, VMWare, and Barefoot Networks, we developed PISCES, a software switch derived from Open vSwitch (OVS), a hard-wired hypervisor switch, whose behavior is customized using P4. PISCES is not hard-wired to specific protocols; this independence makes it easy to add new features. We demonstrated how the compiler can analyze the high-level specification to optimize forwarding performance. Our evaluation shows that PISCES performs comparably to OVS and that PISCES programs are about 40 times shorter than equivalent changes to OVS source code.</p>\n<p class=\"Default\">&nbsp;</p>\n<p>We have made PISCES available as open-source on github at <a href=\"https://github.com/P4-vSwitch\">https://github.com/P4-vSwitch</a> with significant interest in the community.</p>\n<p class=\"Default\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/14/2016<br>\n\t\t\t\t\tModified by: Nick&nbsp;W&nbsp;Mckeown</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "NSF EAGER Generalized Network Slicing: Outcomes Report\n \nWe proposed an exploratory project to demonstrate the feasibility of generalized network slicing?for SDN that would enable multiple virtual tenant networks on the same physical network. We focused on four fundamental challenges:\n \n\nTopology Mapping. Allow a tenant to specify topology of her?virtual network (a slice) and map the (virtual) topology on the physical network and maintain this mapping as both physical and virtual topologies change over time. \n\n \n\nAddress Space Mapping. Allow a tenant to use her own private address space and translate the private address space to the physical address space while exploiting packet forwarding capabilities of the existing devices. \n\n \n\nControl Function Mapping. Allow each tenant to have SDN style programming capability of her virtual network and map control functions on to the corresponding physical instantiation of the network. \n\n \n\nPerformance Isolation. Allow each tenant to specify resource requirements for?her network slice and allocate and police resource usage for each virtual slice to ensure performance isolation.\n\n \n\nControl of Programmable Forwarding: Give more control to a virtual network operator using programmable data plane capabilities made possible by P4 (http://p4.org/) and a new class of programmable switches that are starting to become available. \n\n \n\nIn collaboration with Open Networking Laboratory (ON.Lab) we developed OpenVirteX (OVX), a network virtualization platform that enables operators to create and manage virtual Software Defined Networks (vSDNs) on behalf of their tenants. Tenants are free to specify the topology and addressing scheme of their vSDN, and run their own Network Operating System (NOS) to control it. Since OpenVirteX logically decouples vSDNs from the infrastructure, it also enables the introduction of features such as link and switch resiliency, and network snapshotting and migration of these tenant networks. OVX functions as an OpenFlow controller proxy between an operator?s physical network and the tenants? network OSes. Our evaluations of OVX demonstrated it is capable of presenting tenants with configurable vSDNs while incurring a modest overhead to the control channel.\n \n\nWe deployed OVX on Internet 2 NDDI, a nation-wide research network, to demonstrate its capabilities in a live physical WAN. We ran two vSDNs, one a copy of the I2 topology and the other, a single giant switch. Each vSDN was controlled by its network OS, ONOS and Floodlight, respectively, which ran unique applications on I2 WAN.\nWe open sourced OVX and it is available on github with an Apache 2.0 license and many R&amp;E networks deployed and experimented with OVX. \n \nIn collaboration with Princeton University, VMWare, and Barefoot Networks, we developed PISCES, a software switch derived from Open vSwitch (OVS), a hard-wired hypervisor switch, whose behavior is customized using P4. PISCES is not hard-wired to specific protocols; this independence makes it easy to add new features. We demonstrated how the compiler can analyze the high-level specification to optimize forwarding performance. Our evaluation shows that PISCES performs comparably to OVS and that PISCES programs are about 40 times shorter than equivalent changes to OVS source code.\n \n\nWe have made PISCES available as open-source on github at https://github.com/P4-vSwitch with significant interest in the community.\n \n\n\t\t\t\t\tLast Modified: 11/14/2016\n\n\t\t\t\t\tSubmitted by: Nick W Mckeown"
 }
}