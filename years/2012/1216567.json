{
 "awd_id": "1216567",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Methods for Stochastic and Nonlinear Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "rosemary renaut",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2012-07-24",
 "awd_max_amd_letter_date": "2012-07-24",
 "awd_abstract_narration": "The projects described in this proposal are designed to advance the capabilities of optimization methods for a class of stochastic and deterministic optimization problems. The first project focuses on  problems where the objective function is given by an expectation or a loss function. We propose dynamic sample algorithms that attempt to bridge the gap between stochastic and batch  methods. Their essential characteristic is that they adapt the sample size during the progression of the optimization in a manner that leads to low computational effort and high accuracy in the solution, when so desired. The second project deals with the design of new active-set methods for solving constrained optimization and convex regularized L1 problems.  Our work builds on two algorithms recently proposed in the literature: the block active-set method (also called the primal-dual active-set method), and the orthant-wise method  for solving L1 regularized problems. Our new algorithms are provably convergent and applicable to a wider class of applications. The third project addresses the need to improve the robustness of nonlinear optimization methods in the presence of infeasibility. Our first goal is to design an interior point method endowed with infeasibility detection capabilities, and to show how its main mechanism can be extended to other interior point methods. The second goal is to develop a convergence theory that is applicable to both active set and interior point methods consisting of three components:  an optimization phase, a feasibility phase, and a mechanism for transitioning between the two phases.\r\n\r\nThe methods developed in this project are useful in big data analysis, which is playing a vital role in genomics, materials science, meteorology, climate modeling and  information science. In all these disciplines, vast amounts of data have become available in the last decade, with the rate of  generation  accelerating exponentially.  The challenge is to process this large amount of information to make inferences and predictions, thereby accelerating our basic understanding of physical and social systems. For example, the complex physics simulations employed in the design of advanced materials, meteorology and climate modeling, require the use of detailed information obtained over a large set of scenarios. The optimization and machine learning methods developed in this project can be integrated in support of such simulations, thereby obviating the need for  extremely complex models that are difficult to study and generalize. Our work has direct impact in genomics and other areas of biology. For example, we plan to investigate its use in metagenomics, specifically de novo assembly of next generation DNA sequencing data.  Sequences  can be tagged with markers, or found in reference data sets like transcriptomes.  A goal is to use this new information to enable faster and more accurate de novo assembly.  In computer science and information technology, our new algorithms will be useful in the development of a new generation of speech recognition and computer vision systems. Speech recognition, which will play an increasingly important role in many technological applications, can only advance by incorporating more data more intelligently, and the algorithms described in this proposal are designed precisely for that purpose.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jorge",
   "pi_last_name": "Nocedal",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jorge Nocedal",
   "pi_email_addr": "j-nocedal@northwestern.edu",
   "nsf_id": "000375872",
   "pi_start_date": "2012-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2145 Sheridan Road",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  },
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The promise of artificial intelligence has been a topic of public and private interest for decades. &nbsp;Starting in the 1990s many researchers started to doubt classical AI approaches, choosing instead to focus their efforts on the design of systems based on statistical techniques, such as in the rapidly evolving and expanding field of machine learning.&nbsp;Machine learning and the intelligent systems that have been borne out of it have become an indispensable part of modern society. &nbsp;Rooted in statistics and relying heavily on the efficiency of numerical algorithms, machine learning techniques capitalize on the world's increasingly powerful computing platforms and the availability of datasets of great size. &nbsp;In addition, as the fruits of its efforts have become so easily accessible to the public, interest in machine learning is bound to continue its dramatic rise.</p>\n<p>One of the pillars of machine learning is mathematical optimization, which, in this context, involves the numerical computation of parameters for a system designed to make decisions based on yet unseen data. &nbsp;That is, based on currently available data, these parameters are chosen to be optimal with respect to a given learning problem. &nbsp;The success of optimization methods for machine learning has inspired researchers to study new methods that are more efficient and reliable.</p>\n<p>This project focused on the development of new optimization algorithms for large-scale machine learning applications. Our work differs from mainstream research in that, contrary to popular views, we believe that second-order methods are not only viable but also promise to yield much faster solution times. This research was complemented by another nonstandard approach. Rather than focusing on the stochastic gradient method that employs a small amount of gradient information at every iteration, we proposed an optimization algorithm that gathers progressively more information during the solution process. This strategy builds on the strengths of the classical stochastic gradient method in the early iterations, but allows for more accurate gradient (and thus solutions)--- and at the same time opens the door for the incorporation of second-order information. The contributions of this project were in the form of new algorithms, their supporting theory, and software.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/30/2016<br>\n\t\t\t\t\tModified by: Jorge&nbsp;Nocedal</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe promise of artificial intelligence has been a topic of public and private interest for decades.  Starting in the 1990s many researchers started to doubt classical AI approaches, choosing instead to focus their efforts on the design of systems based on statistical techniques, such as in the rapidly evolving and expanding field of machine learning. Machine learning and the intelligent systems that have been borne out of it have become an indispensable part of modern society.  Rooted in statistics and relying heavily on the efficiency of numerical algorithms, machine learning techniques capitalize on the world's increasingly powerful computing platforms and the availability of datasets of great size.  In addition, as the fruits of its efforts have become so easily accessible to the public, interest in machine learning is bound to continue its dramatic rise.\n\nOne of the pillars of machine learning is mathematical optimization, which, in this context, involves the numerical computation of parameters for a system designed to make decisions based on yet unseen data.  That is, based on currently available data, these parameters are chosen to be optimal with respect to a given learning problem.  The success of optimization methods for machine learning has inspired researchers to study new methods that are more efficient and reliable.\n\nThis project focused on the development of new optimization algorithms for large-scale machine learning applications. Our work differs from mainstream research in that, contrary to popular views, we believe that second-order methods are not only viable but also promise to yield much faster solution times. This research was complemented by another nonstandard approach. Rather than focusing on the stochastic gradient method that employs a small amount of gradient information at every iteration, we proposed an optimization algorithm that gathers progressively more information during the solution process. This strategy builds on the strengths of the classical stochastic gradient method in the early iterations, but allows for more accurate gradient (and thus solutions)--- and at the same time opens the door for the incorporation of second-order information. The contributions of this project were in the form of new algorithms, their supporting theory, and software.\n\n \n\n\t\t\t\t\tLast Modified: 10/30/2016\n\n\t\t\t\t\tSubmitted by: Jorge Nocedal"
 }
}