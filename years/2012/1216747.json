{
 "awd_id": "1216747",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research - SI2-S2I2: High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Daniel Katz",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 62035.0,
 "awd_amount": 62035.0,
 "awd_min_amd_letter_date": "2012-08-28",
 "awd_max_amd_letter_date": "2012-08-28",
 "awd_abstract_narration": "The starting point for this proposal is a view of scientific simulation articulated in the conclusions of the 2008 National Academy of Sciences Study, The Potential Impact of High-End Capability Computing on Four Illustrative Fields of Science and Engineering: \"Advanced computational science and engineering is a complex enterprise that requires models, algorithms, software, hardware, facilities, education and training, and a community of researchers attuned to its special needs.\" (p. 122)\r\n\r\nOver the last few years, the design of computer and software systems, particularly as they relate to simulation in the physical sciences, has been organized around a collection of algorithmic patterns / motifs. These patterns have been very productive because they are a natural \"common language\" in which application scientists can express their computations, and for which computer scientists can provide optimized libraries, domain specific languages, compilers, and other software tools.\r\n\r\nThis project will design an institute focused on a subset of these patterns --- structured grid discretizations of partial differential equations and particle methods, along with the linear and nonlinear solvers that enable their effective use --- with the specific goals of providing simulation capabilities for a set of scientific domains that make heavy use of these patterns. Two major components are envisioned to this proposed institute, called the Institute for High-Performance Computational Science with Structured Meshes and Particles (HPCS-SMP). The first component is a software infrastructure development activity that will be performed by a team whose expertise spans the design and development of mathematical algorithms and software frameworks, as well as the design and development of compilers, runtime systems, and tools that enable one to obtain high performance from emerging multicore and heterogeneous architectures. The second component is an outreach activity, in which algorithms, libraries, and software frameworks developed by the institute will be customized and integrated into simulation codes for stakeholder application domains. At the heart of this activity will be collaborations and partnerships, in which the institute will provide one or more software developers to collaborate with application scientists over a period of months to years to develop a new simulation capability or enhance an existing one.\r\n\r\nThe design of this institute will be carried out through a series of workshops, each focused on one of five stakeholder science domains that have been identified as using these motifs and that play a central role in various NSF Grand Challenge problems, with participation of both representatives of the science domain and the the relevant mathematics and computer science communities. In addition, there will be a final workshop that will bring together the relevant mathematics and computer science experts to identify cross-cutting themes. These information obtained from these workshops will be used by the project to develop the final conceptual design of the institute, in the form of a document that includes the input from all of the workshops and our analysis of how this leads to a  design of a software institute.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Mellor-Crummey",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "John M Mellor-Crummey",
   "pi_email_addr": "johnmc@rice.edu",
   "nsf_id": "000195018",
   "pi_start_date": "2012-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Vivek",
   "pi_last_name": "Sarkar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vivek Sarkar",
   "pi_email_addr": "vsarkar@rice.edu",
   "nsf_id": "000334688",
   "pi_start_date": "2012-08-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "125300",
   "pgm_ele_name": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ele_code": "140700",
   "pgm_ele_name": "CFS-Combustion & Fire Systems"
  },
  {
   "pgm_ele_code": "141500",
   "pgm_ele_name": "PMP-Particul&MultiphaseProcess"
  },
  {
   "pgm_ele_code": "144300",
   "pgm_ele_name": "FD-Fluid Dynamics"
  },
  {
   "pgm_ele_code": "755300",
   "pgm_ele_name": "PHYSICS AT THE INFO FRONTIER"
  },
  {
   "pgm_ele_code": "769900",
   "pgm_ele_name": "Integrat & Collab Ed & Rsearch"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "056E",
   "pgm_ref_txt": "Multiphase flow"
  },
  {
   "pgm_ref_code": "058E",
   "pgm_ref_txt": "Newtonian fluids"
  },
  {
   "pgm_ref_code": "1253",
   "pgm_ref_txt": "OFFICE OF MULTIDISCIPLINARY AC"
  },
  {
   "pgm_ref_code": "1407",
   "pgm_ref_txt": "COMBUSTION, FIRE, & PLASMA SYS"
  },
  {
   "pgm_ref_code": "1415",
   "pgm_ref_txt": "PARTICULATE &MULTIPHASE PROCES"
  },
  {
   "pgm_ref_code": "1443",
   "pgm_ref_txt": "FLUID DYNAMICS"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "7483",
   "pgm_ref_txt": "PHYSICS OF THE UNIVERSE"
  },
  {
   "pgm_ref_code": "7553",
   "pgm_ref_txt": "PHYSICS AT THE INFO FRONTIER"
  },
  {
   "pgm_ref_code": "7699",
   "pgm_ref_txt": "ICER"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8211",
   "pgm_ref_txt": "S2I2 - Scient Sftwre Innovat Insts"
  },
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 62035.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Scientific applications developers have been confronted with a transition from single-core processors with homogeneous memory to processors with multiple cores, heterogeneous performance, and complex multilevel memory hierarchies with non-uniform behavior. These changes have reached a point where the current approaches to developing scientific simulation software are no longer viable for producing efficient codes for next generation parallel systems. Our proposed solution is to develop&nbsp;a shared software infrastructure to support multiple scientific domains that possess a common mathematical structure in their models. We focus on two algorithmic motifs: structured grids (including adaptive mesh refinement) and particles. Our approach to determining the feasibility of such an effort was to hold a series of workshops in five scientific areas of importance to NSF that use these motifs &ndash; combustion for engineering systems, astrophysics and cosmology, plasma physics and kinetics, spatial modeling in biology, and climate modeling. Across these areas, we found a sufficiently large common set of requirements that would justify the development of a common software infrastructure. Findings include the need to design new discretization algorithms that are better suited for the next generation of processors, and the development of a software stack based on Domain-Specific Languages (DSLs) that are extensions to widely-used programming languages. Such a software stack would raise the semantic level of the user programs to that of the mathematical description of the discretizations, thus enabling platform-specific optimizations to be performed by compilers, rather than in user code. This effort could be could be executed by a software institute consisting of three components of roughly equal level of effort and funding. One is the development of the DSL-based software stack, including compiler and runtime support, as well as productivity tools to support debugging and performance analysis. Another would the development of mathematical libraries and frameworks using this software stack that would support the new discretization algorithms required to obtain high performance on the next generation of platforms. Finally, the institute would include several activities devoted to the dissemination of the software: the development of general training materials &ndash; user guides, example applications, short courses &ndash; to bring new users up to speed on how to use this software effectively; and the execution of targeted applications campaigns that would develop new scientific simulation capabilities in collaboration with various scientific communities.</p>\n<p><span><br /></span></p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p><span><br /></span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/13/2015<br>\n\t\t\t\t\tModified by: John&nbsp;M&nbsp;Mellor-Crummey</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nScientific applications developers have been confronted with a transition from single-core processors with homogeneous memory to processors with multiple cores, heterogeneous performance, and complex multilevel memory hierarchies with non-uniform behavior. These changes have reached a point where the current approaches to developing scientific simulation software are no longer viable for producing efficient codes for next generation parallel systems. Our proposed solution is to develop a shared software infrastructure to support multiple scientific domains that possess a common mathematical structure in their models. We focus on two algorithmic motifs: structured grids (including adaptive mesh refinement) and particles. Our approach to determining the feasibility of such an effort was to hold a series of workshops in five scientific areas of importance to NSF that use these motifs &ndash; combustion for engineering systems, astrophysics and cosmology, plasma physics and kinetics, spatial modeling in biology, and climate modeling. Across these areas, we found a sufficiently large common set of requirements that would justify the development of a common software infrastructure. Findings include the need to design new discretization algorithms that are better suited for the next generation of processors, and the development of a software stack based on Domain-Specific Languages (DSLs) that are extensions to widely-used programming languages. Such a software stack would raise the semantic level of the user programs to that of the mathematical description of the discretizations, thus enabling platform-specific optimizations to be performed by compilers, rather than in user code. This effort could be could be executed by a software institute consisting of three components of roughly equal level of effort and funding. One is the development of the DSL-based software stack, including compiler and runtime support, as well as productivity tools to support debugging and performance analysis. Another would the development of mathematical libraries and frameworks using this software stack that would support the new discretization algorithms required to obtain high performance on the next generation of platforms. Finally, the institute would include several activities devoted to the dissemination of the software: the development of general training materials &ndash; user guides, example applications, short courses &ndash; to bring new users up to speed on how to use this software effectively; and the execution of targeted applications campaigns that would develop new scientific simulation capabilities in collaboration with various scientific communities.\n\n\n\n\n \n\n \n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 01/13/2015\n\n\t\t\t\t\tSubmitted by: John M Mellor-Crummey"
 }
}