{
 "awd_id": "1206291",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Smart Cameras Getting Smarter: Detecting High-level Events Across Battery-powered Wireless Embedded Smart Cameras",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2011-08-06",
 "awd_exp_date": "2017-12-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2012-03-01",
 "awd_max_amd_letter_date": "2014-08-26",
 "awd_abstract_narration": "Visual surveillance is widely used in military and commercial applications, and public transportation scenarios. There are an estimated 30 million surveillance cameras in the U.S. capturing 4 billion hours of footage a week. The large amounts of video data generated by these cameras necessitate automatic detection of semantically high-level events, since the attention of a human operator watching multiple video feeds decreases over time. In addition, requiring cameras to have access to electrical outlets and to have wired links hinders system flexibility in terms of the number and placement of cameras, incurs significant costs, and limits possible applications and mobility. Embedded smart cameras allow us to deploy many spatially-distributed cameras interconnected by wireless links. A smart camera combines sensing, processing, and communication on a single embedded platform. Yet, it has very limited energy and processing power. The objective in this project is to build a battery-powered, self-adapting, wireless embedded smart camera system for the detection of semantically high-level events, which can span multiple overlapping or non-overlapping camera views, in a scalable and energy-efficient manner, and remove the dependence on wired links. Resource-aware and distributed object tracking and event detection algorithms, and self-adapting decision methodologies will be developed to decrease the energy consumption, and increase the battery-life of cameras. \r\n\r\nThe outcomes are expected to have important positive impact, because they will fundamentally transform video surveillance and event detection solutions by addressing the privacy issues simultaneously. This technology can be used across disciplines and in wide-ranging areas, including next-generation cyber-physical systems, military and commercial applications, traffic analysis, health care and wildlife monitoring. Outreach to younger children is planned through the deployment of this project at the local Children?s Zoo and Museum.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Senem",
   "pi_last_name": "Velipasalar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Senem Velipasalar",
   "pi_email_addr": "svelipas@syr.edu",
   "nsf_id": "000111075",
   "pi_start_date": "2012-03-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Syracuse University",
  "inst_street_address": "900 S CROUSE AVE",
  "inst_street_address_2": "",
  "inst_city_name": "SYRACUSE",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "3154432807",
  "inst_zip_code": "13244",
  "inst_country_name": "United States",
  "cong_dist_code": "22",
  "st_cong_dist_code": "NY22",
  "org_lgl_bus_name": "SYRACUSE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "C4BXLBC11LC6"
 },
 "perf_inst": {
  "perf_inst_name": "Syracuse University",
  "perf_str_addr": "OSP 113 Bowne Hall",
  "perf_city_name": "Syracuse",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "132441200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "22",
  "perf_st_cong_dist": "NY22",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 223316.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 176684.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>During this project, research was conducted on different mobile camera applications, more specifically applications involving wearable cameras, and cameras installed inside vehicles and on drones.</p>\n<p>Elderly healthcare plays an integral part in improving the quality of life of citizens. Falls among the elderly are a major concern for both families and medical professionals, since falls are considered to be the eighth leading cause of death in the U.S., and are known to cause serious damages.</p>\n<p>We developed a fall detection and activity classification system using wearable cameras<strong>.</strong>&nbsp;We took a different view compared to existing vision-based activity monitoring systems by applying a reverse approach. Since the camera is worn by the subject, as opposed to static sensors installed at certain rooms, monitoring is not limited to confined areas, and extends to wherever the subject may travel. Furthermore, since the captured images are not of the subject, as opposed to static cameras watching the subject, the privacy concerns of the subjects are alleviated. The frames are not transmitted to anywhere, but processed locally onboard. Only if a fall occurs, an alarm signal may be sent to the emergency response personnel with the option to send the images during and after a fall. The images of surroundings may help locate the subject. After developing a camera-based fall detection method, we continued to improve our system by using multiple sensor modalities, namely the camera and accelerometer sensors.&nbsp;Proposed algorithm was tested on actual smart phones, equipped with cameras and accelerometers, to show its real time processing capability to be used in daily activity monitoring systems. Results showed that fusing camera and accelerometer data not only increases the detection rate, but also decreases the number of false alarms compared to only accelerometer-based or only camera-based systems. This work resulted in a patent titled \"Automatic Detection by a Wearable Camera\", which was issued in February 2017.</p>\n<p>As another application of wearable cameras, we focused on accurate and reliable footstep counting,&nbsp;which can be used for measuring activity levels, calculating traveled distance, and indoor navigation.&nbsp;Most of the existing approaches for step counting rely only on the accelerometer data, and thus are prone to over-counting. Moreover, most existing devices calculate the traveled distance based on the counted number of steps and a preset stride length. We developed an autonomous method for counting footsteps, and tracking and calculating stride length by&nbsp;using accelerometer, gravity sensor and camera data from smart phones. Instead of using a preset step length, the proposed method calculates the distance traveled with each step by using the camera data. If camera is tilted, the angle data from the gravity sensor is used to account for camera geometry. We compared the proposed approach with accelerometer-based apps. The proposed method increases the accuracy significantly, and provides the lowest average error rate for both the number of steps and the traveled distance across different subjects.</p>\n<p>We also worked on performing fine-grain activity classification from wearable cameras combined with other sensors. Accelerometer-only systems, although computationally efficient, are limited in the variety and complexity of the activities that they can detect. For instance, we can detect a sitting event by using accelerometer data, but cannot determine whether the user has sat on a chair or sofa, or what type of environment the user is in. To detect activities with more detail and context, we presented an autonomous method using both accelerometer and camera data obtained from a smart phone.</p>\n<p>While performing the aforementioned studies, we also prepared a comprehensive survey on activity detection and classification using wearable sensors, which was published in IEEE Sensors Journal in January 2017. We were honored to receive a letter from the President of the IEEE Sensors Council indicating that our paper was one of the&nbsp;top 25 most downloaded papers in the months of January-September 2017. Included in this count are all Sensors Journal papers published since the Journal's foundation.&nbsp;</p>\n<p>Our work on human activity classification, object detection and scene understanding, from wearable devices with cameras,&nbsp;has the potential to make impact on monitoring and assisting visually impaired people and elderly people, and assisting police officers wearing body cams. This work also has potential uses in wildlife monitoring, robotics, and unmanned aerial and ground vehicles. The proposed work could promote interdisciplinary research on related topics.</p>\n<p>We also proposed a traffic standards-based method for detecting the status of traffic lights without relying on GPS, lidar, radar information, or prior (map-based) knowledge. The algorithm can be ported over to an embedded smart camera platform and used as a windshield-mounted driver-assistance device by individuals with color-vision deficiency. Moreover, we proposed a method for traffic sign detection from lower-quality and noisy mobile videos. Accurate traffic sign detection is an important task for autonomous driving and driver assistance purposes. It has potential applications in transportation engineering.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/09/2018<br>\n\t\t\t\t\tModified by: Senem&nbsp;Velipasalar</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDuring this project, research was conducted on different mobile camera applications, more specifically applications involving wearable cameras, and cameras installed inside vehicles and on drones.\n\nElderly healthcare plays an integral part in improving the quality of life of citizens. Falls among the elderly are a major concern for both families and medical professionals, since falls are considered to be the eighth leading cause of death in the U.S., and are known to cause serious damages.\n\nWe developed a fall detection and activity classification system using wearable cameras. We took a different view compared to existing vision-based activity monitoring systems by applying a reverse approach. Since the camera is worn by the subject, as opposed to static sensors installed at certain rooms, monitoring is not limited to confined areas, and extends to wherever the subject may travel. Furthermore, since the captured images are not of the subject, as opposed to static cameras watching the subject, the privacy concerns of the subjects are alleviated. The frames are not transmitted to anywhere, but processed locally onboard. Only if a fall occurs, an alarm signal may be sent to the emergency response personnel with the option to send the images during and after a fall. The images of surroundings may help locate the subject. After developing a camera-based fall detection method, we continued to improve our system by using multiple sensor modalities, namely the camera and accelerometer sensors. Proposed algorithm was tested on actual smart phones, equipped with cameras and accelerometers, to show its real time processing capability to be used in daily activity monitoring systems. Results showed that fusing camera and accelerometer data not only increases the detection rate, but also decreases the number of false alarms compared to only accelerometer-based or only camera-based systems. This work resulted in a patent titled \"Automatic Detection by a Wearable Camera\", which was issued in February 2017.\n\nAs another application of wearable cameras, we focused on accurate and reliable footstep counting, which can be used for measuring activity levels, calculating traveled distance, and indoor navigation. Most of the existing approaches for step counting rely only on the accelerometer data, and thus are prone to over-counting. Moreover, most existing devices calculate the traveled distance based on the counted number of steps and a preset stride length. We developed an autonomous method for counting footsteps, and tracking and calculating stride length by using accelerometer, gravity sensor and camera data from smart phones. Instead of using a preset step length, the proposed method calculates the distance traveled with each step by using the camera data. If camera is tilted, the angle data from the gravity sensor is used to account for camera geometry. We compared the proposed approach with accelerometer-based apps. The proposed method increases the accuracy significantly, and provides the lowest average error rate for both the number of steps and the traveled distance across different subjects.\n\nWe also worked on performing fine-grain activity classification from wearable cameras combined with other sensors. Accelerometer-only systems, although computationally efficient, are limited in the variety and complexity of the activities that they can detect. For instance, we can detect a sitting event by using accelerometer data, but cannot determine whether the user has sat on a chair or sofa, or what type of environment the user is in. To detect activities with more detail and context, we presented an autonomous method using both accelerometer and camera data obtained from a smart phone.\n\nWhile performing the aforementioned studies, we also prepared a comprehensive survey on activity detection and classification using wearable sensors, which was published in IEEE Sensors Journal in January 2017. We were honored to receive a letter from the President of the IEEE Sensors Council indicating that our paper was one of the top 25 most downloaded papers in the months of January-September 2017. Included in this count are all Sensors Journal papers published since the Journal's foundation. \n\nOur work on human activity classification, object detection and scene understanding, from wearable devices with cameras, has the potential to make impact on monitoring and assisting visually impaired people and elderly people, and assisting police officers wearing body cams. This work also has potential uses in wildlife monitoring, robotics, and unmanned aerial and ground vehicles. The proposed work could promote interdisciplinary research on related topics.\n\nWe also proposed a traffic standards-based method for detecting the status of traffic lights without relying on GPS, lidar, radar information, or prior (map-based) knowledge. The algorithm can be ported over to an embedded smart camera platform and used as a windshield-mounted driver-assistance device by individuals with color-vision deficiency. Moreover, we proposed a method for traffic sign detection from lower-quality and noisy mobile videos. Accurate traffic sign detection is an important task for autonomous driving and driver assistance purposes. It has potential applications in transportation engineering.\n\n\t\t\t\t\tLast Modified: 04/09/2018\n\n\t\t\t\t\tSubmitted by: Senem Velipasalar"
 }
}