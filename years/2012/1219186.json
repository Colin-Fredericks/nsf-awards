{
 "awd_id": "1219186",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF:Small: Benchmarking of Transient and Intermittent Errors and Their Application to Microarchitecture",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 399999.0,
 "awd_amount": 399999.0,
 "awd_min_amd_letter_date": "2012-07-03",
 "awd_max_amd_letter_date": "2012-07-03",
 "awd_abstract_narration": "Computing infrastructure has been a driving force for our socio-economic progress in the past several decades. From drug discovery to space exploration, every scientific and engineering domain relies on computer systems to accurately analyze complex datasets. Historically, computational accuracy has been taken for granted in all these disciplines, but this notion is changing. While rapidly shrinking transistor dimensions lead to exponential power and performance benefits, the trend is also creating several unwanted side effects in computer system reliability. There are two types of errors that will become prevalent in the near future: (1) multi-bit soft errors where alpha particles and neutrons cause multiple bits to flip at the same time, and (2) intermittent errors that occur due to stress accumulation over the lifetime of a computer. Thus it is critical to benchmark the impact of these errors on the lifetime of a computer chip. Only when the impact is accurately measured is it possible to judiciously deploy solutions to improve reliability. Since any protection scheme comes with a cost, it is necessary to understand when a particular protection scheme being considered, such as parity or single-error-correcting double-error-detecting code, is too much or too little. \r\n\r\nThis project presents two solutions for benchmarking multi-bit soft errors and intermittent errors.  This project will develop a unified methodology to benchmark the impacts of single-bit and multi-bit soft errors on caches protected with an arbitrary protection scheme, such as an inter-leaved, block-level or word-level error correcting code.  Such a benchmarking framework will significantly enhance a computer designer's ability to objectively evaluate the performance, power, and reliability tradeoffs of various protection schemes proposed for protecting caches.       \r\n\r\nThis research also develops a methodology to benchmark the vulnerability of an instruction set architecture (ISA) to intermittent errors. Each instruction in an ISA specification is enhanced to quantify the amount of stress that it is expected to cause on the underlying microarchitecture of a chip. The stress level information from the ISA is combined with operating conditions of the chip to continuously monitor intermittent error probability during application execution.  Any unwanted degradation in chip reliability is then tackled by software exception handlers, which trigger redundant execution of vulnerable code. \r\n\r\nBroader societal impact will result from these research solutions. Benchmarking is essential to objectively evaluate the cost-benefit tradeoffs of various solutions currently being proposed to tackle reliability concerns. Without benchmarking, building a system to meet reliability specifications is a guessing game. By providing the right set of tools to initiate just-in-time error correction and recovery mechanisms, a computer designer can significantly lower the cost of providing reliable computations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Murali",
   "pi_last_name": "Annavaram",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Murali Annavaram",
   "pi_email_addr": "annavara@usc.edu",
   "nsf_id": "000496713",
   "pi_start_date": "2012-07-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Michel",
   "pi_last_name": "Dubois",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michel Dubois",
   "pi_email_addr": "dubois@paris.usc.edu",
   "nsf_id": "000381644",
   "pi_start_date": "2012-07-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "837 West Downey Way, STO 315",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900891147",
  "perf_ctry_code": "US",
  "perf_cong_dist": "33",
  "perf_st_cong_dist": "CA33",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 399999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>As computing devices shrink in size the amount of charge stored in a memory bit is extremely small. This charge indicates whether that bit holds a \"0\" or \"1\". Since the amount of charge is small these bit cellls are becoming increasingly susceptible to alpha particle (any other particles) strikes. When an alpha particle strikes a bit cell then it can flip the content of that bit. Hence, a \"0\" may be flipped to \"1\" or vice-versa.&nbsp; This bit flip is a transient problem, namely when a new value is written into the bit cell the new value is now stored without any error. Hence, we call these errors as soft errors. But when the bit cell that is impacted by soft error&nbsp; is read before the bit is overwritten  then it may lead to application level errors.</p>\n<p>A second class of errors are called intermittent errors. These are the errors&nbsp; that may occur only under specific operating conditions, such as high temperature or at a high frequency operation. Unlike soft errors these errors do not disappear by simply overwriting the old value. Instead, these errors disappear if the operating condition changes. Generally  lowering the temperature of the chip causes the error to disappear.</p>\n<p>Our first objective in this proposal is to understand how soft errors and intermittent errors impact processor's lifetime. As a first step we built an analytical tool to accurately measure the impact of multi-bit soft errors on the overall system reliability. These are errors where a single bit may be repeatedly hit by alpha particles, or multiple bits may be hit by a single alpha strike. With extremely large cache structures that hold millions of memory bits it is possible for data to reside in these caches for extremely long time. As such, a bit may in fact be hit multiple times, called temporal multi-bit errors. Due to extremely small device dimensions a single hit may in fact flip multiple bits at the same time, called spatial multi-bit errors. Our prior work in this space developed a tool called PARMA that quantified the impact of&nbsp; single bit upsets. As part of this grant we built the PARMA+ tool to extend&nbsp; PARMA to multi bit upsets.</p>\n<p>PARMA+ is based on based on probabiliity theory of soft error benchmarking. It models various multi-bit errror patterns, including when an error spans two different protection regions, and when multiple error patterns overlap with each other. PARMA+ models these issues in three steps. The first step is to identify all possible fault patterns and compute their probabilities of occurrence, given the probability of sinlge event upset. The second step computes the probability distribution of multi bit upsets automatically by simulating a wide range of overlapping error patterns. Finally, it computes the number of failures in a billion hours of opeation of a cache (FIT rate) from the automatically generated probability distributions.&nbsp;</p>\n<p>The second objective of the work targets tackling intermmitent errors. For this work, we developed a metric called Vulnerability to Intermittent Faults (VIF). VIF measures how each instruction in an ISA can have differing levels of stress related impacts on the underlying hardware. By benchmarking each instruction in an ISA over a specific microarchitecture implementation VIF enables designers to understand how typical applications cause hardware degradations over time. We analyzed the&nbsp; VIF metric for a broad range of instructions from the SPARC ISA using the OpenSPARC processor implementation. We also charcterized how different inputs for each instruction can alter the VIF metric. Based on this charcaterization we quantified how diferent compiler optimizations can result in different system vulnerabilities. We showed that while optimized and unoptimized code may suffer similar vulnerability on average, in practice a highly optimized code exhibits much higher standard deviation. Hence, optimized code may cause unexpected vulnerabilities if the hardware is protected only for the average case.&nbsp;</p>\n<p>While not part of the original grant we also developed microarchitectural solutions to improve the reliability of graphics processing units (GPUs) as a follow on to our work.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/21/2016<br>\n\t\t\t\t\tModified by: Murali&nbsp;Annavaram</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAs computing devices shrink in size the amount of charge stored in a memory bit is extremely small. This charge indicates whether that bit holds a \"0\" or \"1\". Since the amount of charge is small these bit cellls are becoming increasingly susceptible to alpha particle (any other particles) strikes. When an alpha particle strikes a bit cell then it can flip the content of that bit. Hence, a \"0\" may be flipped to \"1\" or vice-versa.  This bit flip is a transient problem, namely when a new value is written into the bit cell the new value is now stored without any error. Hence, we call these errors as soft errors. But when the bit cell that is impacted by soft error  is read before the bit is overwritten  then it may lead to application level errors.\n\nA second class of errors are called intermittent errors. These are the errors  that may occur only under specific operating conditions, such as high temperature or at a high frequency operation. Unlike soft errors these errors do not disappear by simply overwriting the old value. Instead, these errors disappear if the operating condition changes. Generally  lowering the temperature of the chip causes the error to disappear.\n\nOur first objective in this proposal is to understand how soft errors and intermittent errors impact processor's lifetime. As a first step we built an analytical tool to accurately measure the impact of multi-bit soft errors on the overall system reliability. These are errors where a single bit may be repeatedly hit by alpha particles, or multiple bits may be hit by a single alpha strike. With extremely large cache structures that hold millions of memory bits it is possible for data to reside in these caches for extremely long time. As such, a bit may in fact be hit multiple times, called temporal multi-bit errors. Due to extremely small device dimensions a single hit may in fact flip multiple bits at the same time, called spatial multi-bit errors. Our prior work in this space developed a tool called PARMA that quantified the impact of  single bit upsets. As part of this grant we built the PARMA+ tool to extend  PARMA to multi bit upsets.\n\nPARMA+ is based on based on probabiliity theory of soft error benchmarking. It models various multi-bit errror patterns, including when an error spans two different protection regions, and when multiple error patterns overlap with each other. PARMA+ models these issues in three steps. The first step is to identify all possible fault patterns and compute their probabilities of occurrence, given the probability of sinlge event upset. The second step computes the probability distribution of multi bit upsets automatically by simulating a wide range of overlapping error patterns. Finally, it computes the number of failures in a billion hours of opeation of a cache (FIT rate) from the automatically generated probability distributions. \n\nThe second objective of the work targets tackling intermmitent errors. For this work, we developed a metric called Vulnerability to Intermittent Faults (VIF). VIF measures how each instruction in an ISA can have differing levels of stress related impacts on the underlying hardware. By benchmarking each instruction in an ISA over a specific microarchitecture implementation VIF enables designers to understand how typical applications cause hardware degradations over time. We analyzed the  VIF metric for a broad range of instructions from the SPARC ISA using the OpenSPARC processor implementation. We also charcterized how different inputs for each instruction can alter the VIF metric. Based on this charcaterization we quantified how diferent compiler optimizations can result in different system vulnerabilities. We showed that while optimized and unoptimized code may suffer similar vulnerability on average, in practice a highly optimized code exhibits much higher standard deviation. Hence, optimized code may cause unexpected vulnerabilities if the hardware is protected only for the average case. \n\nWhile not part of the original grant we also developed microarchitectural solutions to improve the reliability of graphics processing units (GPUs) as a follow on to our work.\n\n \n\n\t\t\t\t\tLast Modified: 10/21/2016\n\n\t\t\t\t\tSubmitted by: Murali Annavaram"
 }
}