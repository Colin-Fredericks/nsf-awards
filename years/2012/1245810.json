{
 "awd_id": "1245810",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CC-NIE Integration: Bringing SDN based Private Cloud to University Research",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2013-01-01",
 "awd_exp_date": "2015-12-31",
 "tot_intn_awd_amt": 900000.0,
 "awd_amount": 900000.0,
 "awd_min_amd_letter_date": "2012-09-14",
 "awd_max_amd_letter_date": "2012-09-14",
 "awd_abstract_narration": "Scientific computing groups on university campuses are often limited by insufficient access to computing cycles, storage capacity, and network bandwidth in a flexible and cost effective way. This prevents them from scaling their applications and taking on grand-challenge problems. Moreover, the current network infrastructure is built with proprietary switches, routers and with proliferating appliances such as firewalls and load balancers. As a result, the IT operation group faces increasing complexity, cost, and lack of control of the existing infrastructure. This closed infrastructure also prevents network researchers from exploring new ideas at scale in a production setting.\r\n\r\nThis project leverages two disruptive technologies that the Web infrastructure and \"a cloud\" are exploiting: multi-tenancy virtualized clusters and Software Defined Networks (SDN). Multi-tenancy virtualized clusters enable scale-out designs with very flexible resource use and the best cost performance. SDN makes it possible to customize infrastructure and to eliminate unnecessary complexity and costs.\r\n\r\nThis project builds an SDN-based Private Cloud to bring these two disruptive technologies to the campus and with them the scale, flexibility, and cost performance. With this private cloud, scientific computing groups are able to share the physical infrastructure while simultaneously customizing computing and networking for their applications. Radar remote sensing and biological computing applications are used as example scientific computing applications to demonstrate value of this infrastructure. The SDN based Private Cloud will impact various communities on a campus and will become a showcase for other campuses and thus help them exploit the latest trends in networking and computing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nick",
   "pi_last_name": "McKeown",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Nick W McKeown",
   "pi_email_addr": "nickm@ee.stanford.edu",
   "nsf_id": "000487607",
   "pi_start_date": "2012-09-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Howard",
   "pi_last_name": "Zebker",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Howard A Zebker",
   "pi_email_addr": "zebker@stanford.edu",
   "nsf_id": "000465355",
   "pi_start_date": "2012-09-14",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tsachy",
   "pi_last_name": "Weissman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tsachy Weissman",
   "pi_email_addr": "tsachy@stanford.edu",
   "nsf_id": "000488863",
   "pi_start_date": "2012-09-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "340 Panama Street",
  "perf_city_name": "Stanford",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943054100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 900000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project we addressed four technical topics aimed at improving the use of cloud resources for supporting scientific computation.&nbsp; These include network infrastructure, compute infrastructure, cloud orchestration, and demonstrating application development.</p>\n<p>In the network infrastructure area, we improved the capability for monitoring and verification of distributed sensors and central data storage.&nbsp; We also extended the coverage of 100Gb connectivity in our network and provided methods for dedicated 40Gb links into up to 20 buildings.&nbsp; For the compute infrastructure we added computational nodes and dedicated storage in order to provide a solution to problems caused by IO and space limitations as a result of growing data set sizes.&nbsp; Cloud implementation software was improved by further developing a method using OpenNetworking and OpenCloud management software on one of the clusters.</p>\n<p>We demonstrated the increased capabilities of the upgraded hardware and software environments by experimental implementations of two application areas that benefit from high-performance computational cloud resources:&nbsp; computational biology and interferometric radar signal processing.&nbsp; For the computational biology application we were able to decrease the time necessary to index genomic data. We successfully decreased the start to finish processing time from more than 24 hours to 90 minutes. This was achieved through a combination of OS changes and optimizations as well as the addition of faster storage to the compute cluster. For the interferometric radar system application we moved a new set of radar processing algorithms to a cloud environment. Part of this includes creating virtual images for use in the distributed compute environment. This has allowed us to deploy multiple instances much faster and with a minimal number of steps. Work is continues on the interferometric radar system application. Initial porting of processing algorithms, of which significant parts are written in FORTRAN, to a new environment was successful.&nbsp; We continue to optimize the code for the distributed environment, since the very large number of needed computations can lead to undesired long execution times. Because modern satellite radar systems acquire huge volumes of data every day, the cluster environment offers the best way to pre-sift the data so that human researchers will have a chance to evaluate known signals. We have applied these signals to algorithms for retrieving small surface deformation over time at sites containing operating water system aquifers, where we hope to observe cm-level changes due to the underground flow of water. This should yield more efficient management approaches for our scarce water resource</p>\n<p>Our collaboration between university IT staff and the application researchers has shown a need for a messaging infrastructure that makes each application more scalable. This has shown the need for better message queue applications, not just for controller communication or job processing, but also because they may be necessary more often than not when re-architecting work flows and applications for researchers.</p>\n<p>An important part of this work is developing opportunities for training and professional development of the nation&rsquo;s future work force.&nbsp; Firstly this involves graduate student training, and several graduate students have worked on this project. They received not only their usual training in their domain of expertise but they also gained experience with both state of the art software defined networking and cloud technologies. The graduate students are Jan Stepinski, who worked on the radar image processing application. This work has supported his graduate school training so that he can learn radar signal processing and how it can be implemented in cloud environments. This will be a useful skill for his re...",
  "por_txt_cntn": "\nIn this project we addressed four technical topics aimed at improving the use of cloud resources for supporting scientific computation.  These include network infrastructure, compute infrastructure, cloud orchestration, and demonstrating application development.\n\nIn the network infrastructure area, we improved the capability for monitoring and verification of distributed sensors and central data storage.  We also extended the coverage of 100Gb connectivity in our network and provided methods for dedicated 40Gb links into up to 20 buildings.  For the compute infrastructure we added computational nodes and dedicated storage in order to provide a solution to problems caused by IO and space limitations as a result of growing data set sizes.  Cloud implementation software was improved by further developing a method using OpenNetworking and OpenCloud management software on one of the clusters.\n\nWe demonstrated the increased capabilities of the upgraded hardware and software environments by experimental implementations of two application areas that benefit from high-performance computational cloud resources:  computational biology and interferometric radar signal processing.  For the computational biology application we were able to decrease the time necessary to index genomic data. We successfully decreased the start to finish processing time from more than 24 hours to 90 minutes. This was achieved through a combination of OS changes and optimizations as well as the addition of faster storage to the compute cluster. For the interferometric radar system application we moved a new set of radar processing algorithms to a cloud environment. Part of this includes creating virtual images for use in the distributed compute environment. This has allowed us to deploy multiple instances much faster and with a minimal number of steps. Work is continues on the interferometric radar system application. Initial porting of processing algorithms, of which significant parts are written in FORTRAN, to a new environment was successful.  We continue to optimize the code for the distributed environment, since the very large number of needed computations can lead to undesired long execution times. Because modern satellite radar systems acquire huge volumes of data every day, the cluster environment offers the best way to pre-sift the data so that human researchers will have a chance to evaluate known signals. We have applied these signals to algorithms for retrieving small surface deformation over time at sites containing operating water system aquifers, where we hope to observe cm-level changes due to the underground flow of water. This should yield more efficient management approaches for our scarce water resource\n\nOur collaboration between university IT staff and the application researchers has shown a need for a messaging infrastructure that makes each application more scalable. This has shown the need for better message queue applications, not just for controller communication or job processing, but also because they may be necessary more often than not when re-architecting work flows and applications for researchers.\n\nAn important part of this work is developing opportunities for training and professional development of the nation\u00c6s future work force.  Firstly this involves graduate student training, and several graduate students have worked on this project. They received not only their usual training in their domain of expertise but they also gained experience with both state of the art software defined networking and cloud technologies. The graduate students are Jan Stepinski, who worked on the radar image processing application. This work has supported his graduate school training so that he can learn radar signal processing and how it can be implemented in cloud environments. This will be a useful skill for his research and future career needs.  Students Mikel Hernaez and Dmitri Serguei Pavlichin similarly worked on the computational biology a..."
 }
}