{
 "awd_id": "1239341",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Collaborative Research: SensEye: An Architecture for Ubiquitous, Real-Time Visual Context Ssensing and Inference",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 700000.0,
 "awd_amount": 732000.0,
 "awd_min_amd_letter_date": "2012-09-10",
 "awd_max_amd_letter_date": "2014-06-11",
 "awd_abstract_narration": "Continuous real-time tracking of the eye and field-of-view of an individual is profoundly important to understanding how humans perceive and interact with the physical world.  This work advances both the technology and engineering of cyber-physical systems by designing an innovative paradigm involving next-generation computational eyeglasses that interact with a user's mobile phone to provide the capability for real-time visual context sensing and inference. This research integrates novel research into low-power embedded systems, image representation, image processing and machine learning, and mobile sensing and inference, to advance the state-of-art in continuous sensing for CPS applications. The activity addresses several fundamental research challenges including: 1) design of novel, highly integrated, computational eyeglasses for tracking eye movements, the visual field of a user, and head movement patterns, all in real-time; 2) a unified compressive signal processing framework that optimizes sensing and estimation, while enabling re-targeting of the device to perform a broad range of tasks depending on the needs of an application; 3) design of a novel real-time visual context sensing system that extracts high-level contexts of interest from compressed data representations; and 4) a layer of intelligence that combines contexts extracted from the computational eyeglass together with contexts obtained from the mobile phone to improve energy-efficiency and sensing accuracy.\r\n\r\nThis technology can revolutionize a range of disciplines including transportation, healthcare, behavioral science and market research. Continuous monitoring of the eye and field-of-view of an individual can enable detection of hazardous behaviors such as drowsiness while driving, mental health issues such as schizophrenia, addictive behavior and substance abuse, neurological disease progression, head injuries, and others. The research provides the foundations for such applications through the design of a prototype platform together with real-time sensor processing algorithms, and making these systems available through open source venues for broader use. Outreach for this project includes demonstrations of the device at science fairs for high-school students, and integration of the platform into undergraduate and graduate courses.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Deepak",
   "pi_last_name": "Ganesan",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Deepak K Ganesan",
   "pi_email_addr": "dganesan@cs.umass.edu",
   "nsf_id": "000486260",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Marco",
   "pi_last_name": "Duarte",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Marco F Duarte",
   "pi_email_addr": "mduarte@ecs.umass.edu",
   "nsf_id": "000600744",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Marlin",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin M Marlin",
   "pi_email_addr": "marlin@cs.umass.edu",
   "nsf_id": "000611228",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039264",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 700000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-ea076461-c5be-367f-7546-5482d5fc1f68\"> </span></p>\n<p dir=\"ltr\"><span>This report represents our efforts at addressing the problem of continuous real-time tracking of the state of the eye (eye movements, pupil dilation) at low power on a wearable eyeglass. Tackling this problem is profoundly important to understanding how humans perceive and interact with the physical world, and can help in driving safety, personal health monitoring, and hands-free displays.</span></p>\n<p dir=\"ltr\"><span>The following key outcomes resulted from this award:</span></p>\n<p>* We designed an ultra-low power eye tracking algorithm that reduces power consumption for continuous eye tracking by only sampling a small number of pixels per image, and thereby tremendously reducing the power consumed for sampling and processing of pixels. This resulting eye tracker operates at a low power of tens of milliwatts, while achieving high rate tracking of the eye, thereby paving the way for low-power wearable eye trackers.</p>\n<p dir=\"ltr\"><span>* We developed mathematical models to select the optimal subset of the dimensions of the image that preserves the geometric structure present in the original data. This masking method implements a form of compressive sensing that reduces power consumption in emerging imaging sensor platforms. Our results show that the manifold structure is preserved through the data-dependent masking process, even for modest mask sizes.</span></p>\n<p dir=\"ltr\"><span>* We developed a learning pipeline for training our models to each user by requiring minimal user burden. Our training process allows us to track pupil center and pupil dilation with no need for user input, thereby reducing burden on adapting to a new user.</span></p>\n<p dir=\"ltr\"><span>* We developed methods for trading off power consumption for robustness to varying illumination conditions including well-lit and poorly-lit indoor and outdoor situations. We developed adaptive algorithms that adjust to more challenging illumination scenarios by sensing more pixels and using more complex computational models.</span></p>\n<div><span>Webpage:&nbsp;http://sensors.cs.umass.edu/projects/eyeglass/</span></div>\n<div><span><br /></span></div>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/22/2015<br>\n\t\t\t\t\tModified by: Deepak&nbsp;K&nbsp;Ganesan</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2015/1239341/1239341_10212730_1450722161750_ishadow--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2015/1239341/1239341_10212730_1450722161750_ishadow--rgov-800width.jpg\" title=\"iShadow prototype\"><img src=\"/por/images/Reports/POR/2015/1239341/1239341_10212730_1450722161750_ishadow--rgov-66x44.jpg\" alt=\"iShadow prototype\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">iShadow prototype</div>\n<div class=\"imageCredit\">Addison Mayberry</div>\n<div class=\"imageSubmitted\">Deepak&nbsp;K&nbsp;Ganesan</div>\n<div class=\"imageTitle\">iShadow prototype</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThis report represents our efforts at addressing the problem of continuous real-time tracking of the state of the eye (eye movements, pupil dilation) at low power on a wearable eyeglass. Tackling this problem is profoundly important to understanding how humans perceive and interact with the physical world, and can help in driving safety, personal health monitoring, and hands-free displays.\nThe following key outcomes resulted from this award:\n\n* We designed an ultra-low power eye tracking algorithm that reduces power consumption for continuous eye tracking by only sampling a small number of pixels per image, and thereby tremendously reducing the power consumed for sampling and processing of pixels. This resulting eye tracker operates at a low power of tens of milliwatts, while achieving high rate tracking of the eye, thereby paving the way for low-power wearable eye trackers.\n* We developed mathematical models to select the optimal subset of the dimensions of the image that preserves the geometric structure present in the original data. This masking method implements a form of compressive sensing that reduces power consumption in emerging imaging sensor platforms. Our results show that the manifold structure is preserved through the data-dependent masking process, even for modest mask sizes.\n* We developed a learning pipeline for training our models to each user by requiring minimal user burden. Our training process allows us to track pupil center and pupil dilation with no need for user input, thereby reducing burden on adapting to a new user.\n* We developed methods for trading off power consumption for robustness to varying illumination conditions including well-lit and poorly-lit indoor and outdoor situations. We developed adaptive algorithms that adjust to more challenging illumination scenarios by sensing more pixels and using more complex computational models.\nWebpage: http://sensors.cs.umass.edu/projects/eyeglass/\n\n\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/22/2015\n\n\t\t\t\t\tSubmitted by: Deepak K Ganesan"
 }
}