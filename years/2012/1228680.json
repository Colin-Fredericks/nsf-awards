{
 "awd_id": "1228680",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collabortive Research: Software Institute for Abstractions and Methodologies for HPC Simulation Codes on Future Architectures",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rudolf Eigenmann",
 "awd_eff_date": "2012-09-15",
 "awd_exp_date": "2015-02-28",
 "tot_intn_awd_amt": 52344.0,
 "awd_amount": 52344.0,
 "awd_min_amd_letter_date": "2012-09-10",
 "awd_max_amd_letter_date": "2012-09-10",
 "awd_abstract_narration": "Large, complex, multi-scale, multi-physics simulation codes, running on high performance computing (HPC) platforms, are essential to advancing science and engineering research in disciplines such as lattice field theory, astrophysics and cosmology, computational fluid dynamics/fluid structure interaction,and high energy density physics. Progress in computational science together with the adoption of high-level frameworks and modular development have produced widely used community simulation software specific to individual communities. These state-of-the-art codes have been under development and optimization for several years and currently simulate multi-scale, multi-physics phenomena with unprecedented fidelity on petascale platforms. Currently each of these codes have solvers with varied performance characteristics, but all face challenges because of changing hardware architecture. Efforts underway to cope with these challenges, are largely fragmented. While it is true that the scientific codes used in various domains differ significantly from one another, many solutions are likely to be conceptually similar, even if they differ in details. The goal of the proposed conceptualization project, Software Institute for Methodologies and Abstractions for Codes (SIMAC) is to find common abstractions and frameworks applicable across a broad range of applications through cooperation, coordination and interdisciplinary interactions among the participants. The core group of participating codes includes FLASH (astrophysics, cosmology, CFD, HEDP), Cactus (CFD, numerical relativity, and quantum relativity), the code suite used by the Lattice QCD community, and Enzo (cosmology). \r\n\r\nThe proposed collaborative research will produce benefit beyond the four simulation codes and collaborating institutions by exploring: a common software infrastructure applicable to a broad range of science and engineering application domains; an engagement model between computer science research and application development; a multidisciplinary immersion program for research, education and training of students, postdoctoral fellows and visitors on future platform architectures.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Brower",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Richard C Brower",
   "pi_email_addr": "brower@bu.edu",
   "nsf_id": "000460814",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Claudio",
   "pi_last_name": "Rebbi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Claudio Rebbi",
   "pi_email_addr": "rebbi@bu.edu",
   "nsf_id": "000454046",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Center for Computational Science, Boston University",
  "perf_str_addr": "3 Cummington Street",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022152406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8211",
   "pgm_ref_txt": "S2I2 - Scient Sftwre Innovat Insts"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 52344.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The vital &nbsp;HPC intersection: Physics &nbsp;^ Algorithms ^ Hardware</p>\n<p>The SIMAC project held 3 workshops. &nbsp;Each explored different aspects of &nbsp;the challenge of HPC &nbsp;Simulations Codes on FutureArchitectures: Software Engineer (at U. Chicago), Industrial Collaboration (NCSA) and High Performance Algorithms (Boston U.) &nbsp;This third SIMAC workshop focused on the design of Numerical Algorithms and Software Frameworks to accommodate the increasingly complex environment of multi-scale physics and complex heterogeneous HPC architectures. Algorithmic examples &nbsp;included Multigrid (MG), Domain Decomposition (DD) solvers and Adaptive MeshRefinement (AMR). Hardware examples included GPU and PHI heterogeneous architectures. The goal was to explore existing collaborative teams and frameworks that seek to respond to this disruptive technological landscape and suggest new or improved methods required to keep pacewith the evolution of Extreme scale computing.<br /><br />The great challenge and vast potential benefits to science and industry as we approach Exascale computing is to link together the end to end disciplinary chain from Physics to Software to Hardware. The weakest link can be disruptive, ending in a failure to deliver. At Boston University, the workshop focussed on algorithms. Algorithms are the &nbsp;mathematical means to capture the correct physics in the language of computers.<br /><br />Historically advances in algorithms have kept pace with the fabulous and celebrated exponential speed of hardare (e.g. Moore'ss law, double every 18 months). The challenge at the Exascale are now sever. As the computer power advances, exposing more and more physical scales, advances in &nbsp;algorithms depend more and more require an intimate knowledge of the application. At the same time high performance parallel computers are becoming complex with heterogeneous nodes (CPU + GPU etc) and deep layers of &nbsp;memory. The workshop sees the need for &nbsp;more resources to build &nbsp;a community to bring these two aspect together. &nbsp;The new bread computational scientist must combine skills in mathematics, physics and software to define solution and to seek specialist in the separate discpines when needed. A properly constructed Software Institute can be of immense &nbsp;value at this intersection with a true balance between the concerns and interests of the community of stakeholders form Computer Science, Application Science and Industrial Partners. &nbsp;Like the three branches of good government (Courts/Congress/Executive) here too three branches (Physics/Algorithms/Hardware) &nbsp;are required to provide for general welfair of high performance computing.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/31/2015<br>\n\t\t\t\t\tModified by: Richard&nbsp;C&nbsp;Brower</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2015/1228680/1228680_10212825_1433100361667_lattice_2014_MG_GPU--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2015/1228680/1228680_10212825_1433100361667_lattice_2014_MG_GPU--rgov-800width.jpg\" title=\"Mapping Algorithms to Harware\"><img src=\"/por/images/Reports/POR/2015/1228680/1228680_10212825_1433100361667_lattice_2014_MG_GPU--rgov-66x44.jpg\" alt=\"Mapping Algorithms to Harware\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Advances require physics, algorithms and software mapping. Here is an example of Multigrid for QCD mapped to the C...",
  "por_txt_cntn": "\n                 The vital  HPC intersection: Physics  ^ Algorithms ^ Hardware\n\nThe SIMAC project held 3 workshops.  Each explored different aspects of  the challenge of HPC  Simulations Codes on FutureArchitectures: Software Engineer (at U. Chicago), Industrial Collaboration (NCSA) and High Performance Algorithms (Boston U.)  This third SIMAC workshop focused on the design of Numerical Algorithms and Software Frameworks to accommodate the increasingly complex environment of multi-scale physics and complex heterogeneous HPC architectures. Algorithmic examples  included Multigrid (MG), Domain Decomposition (DD) solvers and Adaptive MeshRefinement (AMR). Hardware examples included GPU and PHI heterogeneous architectures. The goal was to explore existing collaborative teams and frameworks that seek to respond to this disruptive technological landscape and suggest new or improved methods required to keep pacewith the evolution of Extreme scale computing.\n\nThe great challenge and vast potential benefits to science and industry as we approach Exascale computing is to link together the end to end disciplinary chain from Physics to Software to Hardware. The weakest link can be disruptive, ending in a failure to deliver. At Boston University, the workshop focussed on algorithms. Algorithms are the  mathematical means to capture the correct physics in the language of computers.\n\nHistorically advances in algorithms have kept pace with the fabulous and celebrated exponential speed of hardare (e.g. Moore'ss law, double every 18 months). The challenge at the Exascale are now sever. As the computer power advances, exposing more and more physical scales, advances in  algorithms depend more and more require an intimate knowledge of the application. At the same time high performance parallel computers are becoming complex with heterogeneous nodes (CPU + GPU etc) and deep layers of  memory. The workshop sees the need for  more resources to build  a community to bring these two aspect together.  The new bread computational scientist must combine skills in mathematics, physics and software to define solution and to seek specialist in the separate discpines when needed. A properly constructed Software Institute can be of immense  value at this intersection with a true balance between the concerns and interests of the community of stakeholders form Computer Science, Application Science and Industrial Partners.  Like the three branches of good government (Courts/Congress/Executive) here too three branches (Physics/Algorithms/Hardware)  are required to provide for general welfair of high performance computing.\n\n\t\t\t\t\tLast Modified: 05/31/2015\n\n\t\t\t\t\tSubmitted by: Richard C Brower"
 }
}