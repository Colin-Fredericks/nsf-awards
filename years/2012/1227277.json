{
 "awd_id": "1227277",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "NRI-Large: Collaborative Research: Multilateral Manipulation by Human-Robot Collaborative Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2017-09-30",
 "tot_intn_awd_amt": 586942.0,
 "awd_amount": 639942.0,
 "awd_min_amd_letter_date": "2012-09-11",
 "awd_max_amd_letter_date": "2015-05-18",
 "awd_abstract_narration": "This project addresses a large space of manipulation problems that are repetitive, injury-causing, or dangerous for humans to perform, yet are currently impossible to reliably achieve with purely autonomous robots. These problems generally require dexterity, complex perception, and complex physical interaction. Yet, many such problems can be reliably addressed with human/robot collaborative (HRC) systems, where one or more humans provide needed perception and adaptability, working with one or more robot systems that provide speed, precision, accuracy, and dexterity at an appropriate scale, combining these complementary capabilities.\r\n\r\nThe project focuses on multilateral manipulation, which arises when a human controls one or more robot manipulators in partnership with one or more additional controllers (humans or autonomous agents). Complex operations in surgery and manufacturing can benefit from the extra degrees of freedom provided by more than two hands, and training often depends on hands-on interaction between expert and apprentice. Example applications include surgical operations, which typically involve several physicians and assistants, and other medical tasks such as turning a patient in bed and wrapping a cast to constrain a hand. Multilateral manipulation also applies in manufacturing, for example for threading wires or cables, aligning gaskets to obtain a tight seal, and in many household situations, such as folding tablecloths, wrapping packages, and zipping overfilled suitcases so they will fit inside diabolically-designed overhead airline compartments. Multilateral manipulation often arises with deformable materials or multi-jointed objects with more than six degrees of freedom (DOF). The extra DOFs in materials introduce challenges such as computational complexity, but they also can accommodate minor inconsistencies through redundancy and provide system damping. This project advances the fundamental science of multilateral manipulation guided by specific applications from surgery and manufacturing.\r\n\r\nBroader Impacts: Multilateral manipulation systems have the potential to improve healthcare, improve American competitiveness and product quality in manufacturing, and open the door to new service robot applications in the home. The project will be guided by an Advisory Board of experts from industry and medical practice. Project results will be disseminated through yearly conference workshops, open-source software tools integrated into common robotics software environments such as Robot Operating System (ROS), and the investigators' research and course webpages, to encourage integration of our approach into research projects and courses at many institutions. Outreach programs, public lab tours, and mentoring of minority students will broaden participation of underrepresented groups in engineering. These activities will encourage participation in STEM activities and provide student and postdoctoral researchers with mentoring experience.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Hager",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Hager",
   "pi_email_addr": "hager@cs.jhu.edu",
   "nsf_id": "000385453",
   "pi_start_date": "2012-09-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Johns Hopkins University",
  "inst_street_address": "3400 N CHARLES ST",
  "inst_street_address_2": "",
  "inst_city_name": "BALTIMORE",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "4439971898",
  "inst_zip_code": "212182608",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MD07",
  "org_lgl_bus_name": "THE JOHNS HOPKINS UNIVERSITY",
  "org_prnt_uei_num": "GS4PNKTRNKL3",
  "org_uei_num": "FTMTDMBR29C7"
 },
 "perf_inst": {
  "perf_inst_name": "Johns Hopkins University",
  "perf_str_addr": "3400 N. Charles ST",
  "perf_city_name": "Baltimore",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "212182683",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MD07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "729800",
   "pgm_ele_name": "International Research Collab"
  },
  {
   "pgm_ele_code": "748400",
   "pgm_ele_name": "IIS Special Projects"
  },
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5936",
   "pgm_ref_txt": "GERMANY (F.R.G.)"
  },
  {
   "pgm_ref_code": "7484",
   "pgm_ref_txt": "IIS SPECIAL PROJECTS"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 290404.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 296538.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 53000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>During this project, we have pursued several lines of research. First, we have developed a variety of advanced methods for segmentation and modeling of time-series data captured during both medical (surgical) and manufacturing tasks. Related to this, we have developed a software platform to support collaborative task performance (CoSTAR) which allows for both graphical and demonstration-based robot programming. Along with CoSTAR, we have developed robot perception methods that specifically support collaborative manipulation, and we have developed task and motion planning methods that support adaptation of user-defined/demonstrated plans to new environments.</span></p>\n<p>There are a number of significant results arising from this project. &nbsp;Specifically:</p>\n<p>1) We have developed a new method for spatio-temporal activity modeling. This method, temporal convolution networks (TCN), provides state-of-the art segmentation of complex tasks into phases and smaller components. TCN has been shown to work on a variety of data streams, including tool motion, video data, accelerometer data, and combinations thereof.</p>\n<p>2) We have developed new methods for object detection and pose estimation in clutter, driven by advances in deep learning. These methods, which operate from RGBD (video plus depth) data make use of deep networks for initial segmentation, followed by a geometric alignment to provide precise pose information. This system has been extended to operate from multiple views, and to provide near real-time segmentation and pose on video streams.</p>\n<p>3) We have developed a series of methods to support acquisition of user-demonstrated task performance data, which is then used to support task and motion planning. The user demonstration provides \"hints\" as to how the task is performed, and the motion planning then produces motion plans that are geometrically correct, but which make use of these hints to enhance the quality of the paths and the speed by which they are produced.</p>\n<p>4) We have developed a fully featured, modular, open-source software system that allows a user to graphically define a task structure, and which integrates the above capabilities to support advanced task programming.</p>\n<p>&nbsp;<span>There are several achievements that have \"spun out\" from the results above. First, our CoSTAR system was an entry in the 2016 KUKA innovation competition, and won the competition. Second, we have created a new company, Ready Robotics, which has just received series A funding for a human-machine collaborative system for manufacturing. Finally, this project has supported a growing collaboration with the Technical University of Munich which now has complementary research efforts underway.&nbsp;</span></p>\n<p><span>Finally, this project has provided training for numerous undergraduate and graduate students, it has engaged participants from both medicine and manufacturing, and it has provided a platform for future innovative work in this area.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/27/2018<br>\n\t\t\t\t\tModified by: Gregory&nbsp;D&nbsp;Hager</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDuring this project, we have pursued several lines of research. First, we have developed a variety of advanced methods for segmentation and modeling of time-series data captured during both medical (surgical) and manufacturing tasks. Related to this, we have developed a software platform to support collaborative task performance (CoSTAR) which allows for both graphical and demonstration-based robot programming. Along with CoSTAR, we have developed robot perception methods that specifically support collaborative manipulation, and we have developed task and motion planning methods that support adaptation of user-defined/demonstrated plans to new environments.\n\nThere are a number of significant results arising from this project.  Specifically:\n\n1) We have developed a new method for spatio-temporal activity modeling. This method, temporal convolution networks (TCN), provides state-of-the art segmentation of complex tasks into phases and smaller components. TCN has been shown to work on a variety of data streams, including tool motion, video data, accelerometer data, and combinations thereof.\n\n2) We have developed new methods for object detection and pose estimation in clutter, driven by advances in deep learning. These methods, which operate from RGBD (video plus depth) data make use of deep networks for initial segmentation, followed by a geometric alignment to provide precise pose information. This system has been extended to operate from multiple views, and to provide near real-time segmentation and pose on video streams.\n\n3) We have developed a series of methods to support acquisition of user-demonstrated task performance data, which is then used to support task and motion planning. The user demonstration provides \"hints\" as to how the task is performed, and the motion planning then produces motion plans that are geometrically correct, but which make use of these hints to enhance the quality of the paths and the speed by which they are produced.\n\n4) We have developed a fully featured, modular, open-source software system that allows a user to graphically define a task structure, and which integrates the above capabilities to support advanced task programming.\n\n There are several achievements that have \"spun out\" from the results above. First, our CoSTAR system was an entry in the 2016 KUKA innovation competition, and won the competition. Second, we have created a new company, Ready Robotics, which has just received series A funding for a human-machine collaborative system for manufacturing. Finally, this project has supported a growing collaboration with the Technical University of Munich which now has complementary research efforts underway. \n\nFinally, this project has provided training for numerous undergraduate and graduate students, it has engaged participants from both medicine and manufacturing, and it has provided a platform for future innovative work in this area.\n\n\t\t\t\t\tLast Modified: 02/27/2018\n\n\t\t\t\t\tSubmitted by: Gregory D Hager"
 }
}