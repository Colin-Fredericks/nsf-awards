{
 "awd_id": "1218981",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: MapReduce Workload Management",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 299971.0,
 "awd_amount": 315971.0,
 "awd_min_amd_letter_date": "2012-08-15",
 "awd_max_amd_letter_date": "2014-05-07",
 "awd_abstract_narration": "Researchers and decision makers in diverse fields such as \r\nfraud detection, genome sequencing, and datacenter\r\nmanagement need to process many terabytes of data every day. \r\nMany fields are turning to MapReduce systems to process such \r\ngrowing datasets. Consequently, the relatively young MapReduce\r\necosystem has to support complex workloads that include \r\ndeclarative queries for report generation, MapReduce \r\nprograms for machine learning tasks, and large job workflows.\r\nFurthermore, elastic and pay-as-you-go cloud platforms pose novel \r\nchallenges and opportunities for MapReduce workload management.\r\n\r\nThis project is building the Hadoop AutoAdmin system for \r\nautomating MapReduce workload management. To the PI's knowledge, \r\nHadoop AutoAdmin is the first system to address this challenging \r\nproblem that will become increasingly important as a broad class \r\nof users adopt MapReduce. Hadoop AutoAdmin has three research \r\nthrusts. The first thrust is to understand and characterize the \r\nbehavior of MapReduce workloads based on a comprehensive empirical \r\nstudy involving workloads and data from multiple application domains \r\nas well as different cluster configurations on the cloud. The second \r\nthrust is to develop an easy-to-use and efficient warehouse to \r\nstore, retrieve, and visualize the diverse forms of workload \r\nmonitoring data. The models and insights from these activities will \r\ndrive the third thrust of developing end-to-end algorithms for \r\nworkload management.\r\n\r\nThis project can have significant impact in areas of national \r\nimportance like security and healthcare that are inundated with \r\ndata. Hadoop AutoAdmin will improve worker productivity, system \r\nutilization, and cost-effectiveness of cloud platforms. The \r\ntechnical contributions will be disseminated broadly and the \r\nsystem released publicly.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shivnath",
   "pi_last_name": "Babu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shivnath Babu",
   "pi_email_addr": "shivnath@cs.duke.edu",
   "nsf_id": "000488390",
   "pi_start_date": "2012-08-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 299971.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-905ca75b-e49b-15fd-3599-dfd6dfc0b288\"> <span id=\"docs-internal-guid-fcda6d93-e4ae-1b18-5084-c2ad6c0a7c7e\">\n<p dir=\"ltr\"><span>Data-parallel jobs running on frameworks such as MapReduce, Spark, and Tez form the dominant and growing category of workloads in data-intensive cluster computing across many disciplines of science and engineering. Many businesses, governments, as well as scientific researchers are using such workloads to process the large datasets that they generate. However, there is limited understanding of how these workloads behave and how to troubleshoot performance and reliability issues for these workloads. The focus of this project has been to develop solutions to make the operators and users of data-parallel workloads like MapReduce more productive in how they use and benefit from these workloads.</span></p>\n<p dir=\"ltr\"><span> The project has made significant contributions foundationally and empirically along four topics: </span></p>\n<p dir=\"ltr\"><span>(1) Workload-generation techniques to create simple to complex MapReduce workloads on a cluster; (2) Data-analysis techniques to analyze and understand how these workloads behave; (3) Workload-modeling techniques to model the behavior of these workloads; and (4) Workload-optimization techniques to tune these workloads to the cluster size, workload type, performance, and infrastructure. </span></p>\n<p dir=\"ltr\"><span>The strength of these contributions comes from the wide spectrum along which MapReduce workloads were explored: (a) Scale of processing: where clusters of many different sizes were studied to understand how MapReduce workloads work at different types of scale; (b) Diversity of processing: where workloads of different types ranging from SQL to matrix processing were studied; (c) Performance requirements: where MapReduce workloads being used for batch processing, real-time stream processing, graph processing, etc., were studied; and (d) Infrastructure used: where MapReduce workloads on the elastic cloud as well as multi-tenant on-premises clusters were studied. </span></p>\n<span>The project has generated operational insights from a real-life MapReduce cluster used by analysts, data scientists, and statisticians working on different mission-critical activities at a large enterprise. This fast-growing Hadoop cluster currently stores approximately 30 petabytes of data across more than 1000 nodes. Almost all the types of challenges considered in this project are seen in this cluster. Operational experiences from this large multi-tenant cluster were used to showcase challenges that operators of data-parallel workloads face, and how the work from this project benefits them. Multiple demonstrations of the contributions of this project have been given in venues like VLDB. Talks at research conferences as well as at popular venues like the Hadoop and Spark Summit have been given. The project has also contributed to two PhD dissertations in USA and Europe. </span></span>\n<p dir=\"ltr\">&nbsp;</p>\n</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/09/2016<br>\n\t\t\t\t\tModified by: Shivnath&nbsp;Babu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n \nData-parallel jobs running on frameworks such as MapReduce, Spark, and Tez form the dominant and growing category of workloads in data-intensive cluster computing across many disciplines of science and engineering. Many businesses, governments, as well as scientific researchers are using such workloads to process the large datasets that they generate. However, there is limited understanding of how these workloads behave and how to troubleshoot performance and reliability issues for these workloads. The focus of this project has been to develop solutions to make the operators and users of data-parallel workloads like MapReduce more productive in how they use and benefit from these workloads.\n The project has made significant contributions foundationally and empirically along four topics: \n(1) Workload-generation techniques to create simple to complex MapReduce workloads on a cluster; (2) Data-analysis techniques to analyze and understand how these workloads behave; (3) Workload-modeling techniques to model the behavior of these workloads; and (4) Workload-optimization techniques to tune these workloads to the cluster size, workload type, performance, and infrastructure. \nThe strength of these contributions comes from the wide spectrum along which MapReduce workloads were explored: (a) Scale of processing: where clusters of many different sizes were studied to understand how MapReduce workloads work at different types of scale; (b) Diversity of processing: where workloads of different types ranging from SQL to matrix processing were studied; (c) Performance requirements: where MapReduce workloads being used for batch processing, real-time stream processing, graph processing, etc., were studied; and (d) Infrastructure used: where MapReduce workloads on the elastic cloud as well as multi-tenant on-premises clusters were studied. \nThe project has generated operational insights from a real-life MapReduce cluster used by analysts, data scientists, and statisticians working on different mission-critical activities at a large enterprise. This fast-growing Hadoop cluster currently stores approximately 30 petabytes of data across more than 1000 nodes. Almost all the types of challenges considered in this project are seen in this cluster. Operational experiences from this large multi-tenant cluster were used to showcase challenges that operators of data-parallel workloads face, and how the work from this project benefits them. Multiple demonstrations of the contributions of this project have been given in venues like VLDB. Talks at research conferences as well as at popular venues like the Hadoop and Spark Summit have been given. The project has also contributed to two PhD dissertations in USA and Europe. \n \n\n\n\t\t\t\t\tLast Modified: 12/09/2016\n\n\t\t\t\t\tSubmitted by: Shivnath Babu"
 }
}