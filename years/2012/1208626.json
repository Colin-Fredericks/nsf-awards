{
 "awd_id": "1208626",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI-Small: Measuring Unconstrained Grasp Forces Using Fingernail Imaging",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 917999.0,
 "awd_amount": 917999.0,
 "awd_min_amd_letter_date": "2012-07-18",
 "awd_max_amd_letter_date": "2012-07-18",
 "awd_abstract_narration": "This project develops the technology for unconstrained measurement of human grasp forces. Measurement of multi-fingered grasp forces typically requires a human to grasp an object at predefined sensor locations or to wear instrumented gloves that impede haptic sensations. The objective of this project is to characterize the ability to estimate three-dimensional grasp forces at the fingertips by measuring the color change of the fingernail.  This fingernail imaging technique allows the human subject to freely choose where to place the fingers on the object, allowing for completely unconstrained multi-finger grasping. A magnetic levitation device is used to apply a range of 3-D forces to the human fingertip while collecting images of the fingernail.  Various image processing techniques are being explored to register the fingernail images to a standard template, and various mathematical models relating pixel intensity to force are being investigated to determine an optimal method.  A robotic motion-tracking technique is being implemented to keep the fingers in view of the camera as the hand moves during grasping experiments. The fingernail imaging technique is first validated using constrained grasping experiments, and then applied to unconstrained grasping experiments.\r\n\r\nThis research enables a co-robot to detect the individual finger forces of a human partner using a technique that does not interfere with the human's haptic sense. A co-robot trained with the appropriate calibration data could recognize and emulate or adapt to a human partner's grasp forces, measured using only vision.  Research efforts are being integrated into the Robotics education and outreach at the University of Utah.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Mascaro",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen A Mascaro",
   "pi_email_addr": "smascaro@mech.utah.edu",
   "nsf_id": "000172180",
   "pi_start_date": "2012-07-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Hollerbach",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "John M Hollerbach",
   "pi_email_addr": "jmh@cs.utah.edu",
   "nsf_id": "000326004",
   "pi_start_date": "2012-07-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Utah",
  "inst_street_address": "201 PRESIDENTS CIR",
  "inst_street_address_2": "",
  "inst_city_name": "SALT LAKE CITY",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8015816903",
  "inst_zip_code": "841129049",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "UT01",
  "org_lgl_bus_name": "UNIVERSITY OF UTAH",
  "org_prnt_uei_num": "",
  "org_uei_num": "LL8GLEVH6MG3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Utah",
  "perf_str_addr": "",
  "perf_city_name": "Salt Lake City",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "841128930",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "UT01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 917999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to characterize the ability of a fingernail imaging system to sense precision grasp force when a human grasps an object. Traditionally, in order to measure grasp force, the human would have wear a sensor glove that restricts their natural haptic sense of touch, or force sensors would have to be placed in pre-specified locations on the grasped object, constraining the human to grasp the object at those locations. In either case, the human cannot grasp the object in a natural manner, and so the measured grasp forces are somewhat artificial.</p>\n<p>Fingernail imaging differs from other methods of sensing fingerpad force in that it measures the contact force without restricting the haptic senses or constraining how the human grasps the object. The idea of fingernail imaging is to exploit the fact that the coloration of the fingernail can be correlated to grasp force, and can be imaged via cameras that do not interfere with the human?s grasping. Thus, using fingernail imaging to measure precision grasping force would simplify the detection of human grasp force. This would facilitate interaction between robots and human partners when the measurement of such grasp forces is required, as in machine learning situations or rehabilitative environments.</p>\n<p>The first principal objective of this research was to determine the capacity of fingernail imaging to measure arbitrary three-dimensional touch force at the fingertip. Prior research had shown that fingernail imaging could either be used to estimate touch force in a single direction only, or used to estimate the direction of the force only. A significant outcome of this project was to demonstrate that we can use fingernail imaging to predict full 3-D force magnitudes at the fingertip (normal force straight down against the surface, plus two axes of shear forces) with a root-mean-square error of about 0.5 N in a range of 0-6 N, which is about 8% error. In order to achieve this outcome, we developed a robust calibration method using a magnetic levitation haptic device to apply various combinations of 3D forces to the human fingertip while imaging the fingernail with a digital camera hovering above the hand. The system is capable of tracking a desired 3D force trajectory with a force tracking error of only 0.05 N. Using these datasets, we form what we call an EigenNail model that correlates the 3D forces with principal components of color change in the fingernail images. We have demonstrated that this technique works equally well on the thumb, index, middle, and ring fingers.</p>\n<p>The second principal objective of this research was to determine the accuracy of the imaging and vision components of the system. Prior research involved 3D registration of fingernail images, which was slow and required a stereo camera system. A significant outcome of this project was to demonstrate that Active Appearance Models (an image processing technique originally developed for facial tracking) can be successfully applied to registering fingernail images. This 2D-to-2D registration algorithm requires a single camera and was capable of rapidly and correctly registering all images with a pixel intensity RMS error of less than 0.006 (within an intensity scale of 0-1). A second significant outcome was to demonstrate that we can mount cameras on a pair of open-architecture Kinova robot arms and use Image-Based Visual Servoing to track the hand while grasping an object and moving it around a workspace.</p>\n<p>Finally, the third principal objective of this research was to investigate the accuracy of fingernail imaging for measuring precision grasp force. In this objective, we performed multi-digit grasp force measurement while human subjects grasped both instrumented objects (humans were constrained to grasp objects at force sensor locations) and uninstrumented objects (humans can grasp object without constraint). We found that we can successfully track a moving fingernail during a grasping task in a workspace while estimating 3-DOF grasp forces with approximately 0.5 N accuracy, which is comparable accuracy to prior experiments with a static finger. We have confirmed that force synergies between unconstrained and constrained grasping differ significantly. When the subject can place their fingers in a more natural fashion on the object, they distribute the finger forces differently than if they are forced to place their fingers in certain locations. This justifies the importance of unconstrained grasp force measurement for understanding how humans naturally grasp objects. Finally, we have characterized the robustness of our force estimation against variations in surface curvature. We repeated our fingernail imaging calibration and validation experiments using a variety of 2-D surface curvatures and found that for very sharp curvatures (radius&lt;6mm), then force estimation accuracy does deteriorate, but for curvatures of radius&gt;6mm, the force estimation accuracy is quite robust. This means that fingernail imaging is a useful technique for measuring unconstrained grasp forces on a variety of objects with different surface curvatures.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/06/2019<br>\n\t\t\t\t\tModified by: Stephen&nbsp;A&nbsp;Mascaro</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551910125707_instrumentedobject--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551910125707_instrumentedobject--rgov-800width.jpg\" title=\"Fingernail Imaging with an Instrumented Object\"><img src=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551910125707_instrumentedobject--rgov-66x44.jpg\" alt=\"Fingernail Imaging with an Instrumented Object\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The 6-DOF Kinova robot with digital camera tracks the hand using visual servoing. Force estimates from fingernail imaging are compared with force measurements from the instrumented object.</div>\n<div class=\"imageCredit\">Stephen Mascaro, University of Utah</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Stephen&nbsp;A&nbsp;Mascaro</div>\n<div class=\"imageTitle\">Fingernail Imaging with an Instrumented Object</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901681802_CalibratioSetup--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901681802_CalibratioSetup--rgov-800width.jpg\" title=\"Calibration Setup\"><img src=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901681802_CalibratioSetup--rgov-66x44.jpg\" alt=\"Calibration Setup\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">To calibrate a model to predict forces based on fingernail coloration, a magnetic levitation device is used to apply various force trajectories to the human finger while imaging the finger with a digital camera</div>\n<div class=\"imageCredit\">Stephen Mascaro, University of Utah</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Stephen&nbsp;A&nbsp;Mascaro</div>\n<div class=\"imageTitle\">Calibration Setup</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901964291_flowchart--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901964291_flowchart--rgov-800width.jpg\" title=\"Fingernail Imaging Methodology\"><img src=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901964291_flowchart--rgov-66x44.jpg\" alt=\"Fingernail Imaging Methodology\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">After collecting fingernail images, an Active Appearance Model is used to register the 2D images. Then an EigenNail Model is used to predict 3D fingertip forces.</div>\n<div class=\"imageCredit\">Stephen Mascaro, University of Utah</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Stephen&nbsp;A&nbsp;Mascaro</div>\n<div class=\"imageTitle\">Fingernail Imaging Methodology</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551902143257_forces--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551902143257_forces--rgov-800width.jpg\" title=\"Force Estimation Results\"><img src=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551902143257_forces--rgov-66x44.jpg\" alt=\"Force Estimation Results\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This figures shows sample force estimation results from a constrained grasping experiment. Estimated normal and shear forces (from fingernail imaging) are compared with measured normal and shear forces (from instrumented object).</div>\n<div class=\"imageCredit\">Stephen Mascaro, University of Utah</div>\n<div class=\"imageSubmitted\">Stephen&nbsp;A&nbsp;Mascaro</div>\n<div class=\"imageTitle\">Force Estimation Results</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901558878_color--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901558878_color--rgov-800width.jpg\" title=\"Fingernail Color Patterns\"><img src=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551901558878_color--rgov-66x44.jpg\" alt=\"Fingernail Color Patterns\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The fingernail exhibits different coloration patterns for both normal and shear force applied at the fingertip in different directions.</div>\n<div class=\"imageCredit\">Stephen Mascaro, University of Utah</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Stephen&nbsp;A&nbsp;Mascaro</div>\n<div class=\"imageTitle\">Fingernail Color Patterns</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551910262764_uninstrumentedobject--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551910262764_uninstrumentedobject--rgov-800width.jpg\" title=\"Unconstrained Grasp Force Measurement\"><img src=\"/por/images/Reports/POR/2019/1208626/1208626_10191646_1551910262764_uninstrumentedobject--rgov-66x44.jpg\" alt=\"Unconstrained Grasp Force Measurement\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Using fingernail imaging, we can estimate grasp forces while allowing a human subject to grasp any object in an unconstrained manner.</div>\n<div class=\"imageCredit\">Stephen Mascaro, University of Utah</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Stephen&nbsp;A&nbsp;Mascaro</div>\n<div class=\"imageTitle\">Unconstrained Grasp Force Measurement</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to characterize the ability of a fingernail imaging system to sense precision grasp force when a human grasps an object. Traditionally, in order to measure grasp force, the human would have wear a sensor glove that restricts their natural haptic sense of touch, or force sensors would have to be placed in pre-specified locations on the grasped object, constraining the human to grasp the object at those locations. In either case, the human cannot grasp the object in a natural manner, and so the measured grasp forces are somewhat artificial.\n\nFingernail imaging differs from other methods of sensing fingerpad force in that it measures the contact force without restricting the haptic senses or constraining how the human grasps the object. The idea of fingernail imaging is to exploit the fact that the coloration of the fingernail can be correlated to grasp force, and can be imaged via cameras that do not interfere with the human?s grasping. Thus, using fingernail imaging to measure precision grasping force would simplify the detection of human grasp force. This would facilitate interaction between robots and human partners when the measurement of such grasp forces is required, as in machine learning situations or rehabilitative environments.\n\nThe first principal objective of this research was to determine the capacity of fingernail imaging to measure arbitrary three-dimensional touch force at the fingertip. Prior research had shown that fingernail imaging could either be used to estimate touch force in a single direction only, or used to estimate the direction of the force only. A significant outcome of this project was to demonstrate that we can use fingernail imaging to predict full 3-D force magnitudes at the fingertip (normal force straight down against the surface, plus two axes of shear forces) with a root-mean-square error of about 0.5 N in a range of 0-6 N, which is about 8% error. In order to achieve this outcome, we developed a robust calibration method using a magnetic levitation haptic device to apply various combinations of 3D forces to the human fingertip while imaging the fingernail with a digital camera hovering above the hand. The system is capable of tracking a desired 3D force trajectory with a force tracking error of only 0.05 N. Using these datasets, we form what we call an EigenNail model that correlates the 3D forces with principal components of color change in the fingernail images. We have demonstrated that this technique works equally well on the thumb, index, middle, and ring fingers.\n\nThe second principal objective of this research was to determine the accuracy of the imaging and vision components of the system. Prior research involved 3D registration of fingernail images, which was slow and required a stereo camera system. A significant outcome of this project was to demonstrate that Active Appearance Models (an image processing technique originally developed for facial tracking) can be successfully applied to registering fingernail images. This 2D-to-2D registration algorithm requires a single camera and was capable of rapidly and correctly registering all images with a pixel intensity RMS error of less than 0.006 (within an intensity scale of 0-1). A second significant outcome was to demonstrate that we can mount cameras on a pair of open-architecture Kinova robot arms and use Image-Based Visual Servoing to track the hand while grasping an object and moving it around a workspace.\n\nFinally, the third principal objective of this research was to investigate the accuracy of fingernail imaging for measuring precision grasp force. In this objective, we performed multi-digit grasp force measurement while human subjects grasped both instrumented objects (humans were constrained to grasp objects at force sensor locations) and uninstrumented objects (humans can grasp object without constraint). We found that we can successfully track a moving fingernail during a grasping task in a workspace while estimating 3-DOF grasp forces with approximately 0.5 N accuracy, which is comparable accuracy to prior experiments with a static finger. We have confirmed that force synergies between unconstrained and constrained grasping differ significantly. When the subject can place their fingers in a more natural fashion on the object, they distribute the finger forces differently than if they are forced to place their fingers in certain locations. This justifies the importance of unconstrained grasp force measurement for understanding how humans naturally grasp objects. Finally, we have characterized the robustness of our force estimation against variations in surface curvature. We repeated our fingernail imaging calibration and validation experiments using a variety of 2-D surface curvatures and found that for very sharp curvatures (radius&lt;6mm), then force estimation accuracy does deteriorate, but for curvatures of radius&gt;6mm, the force estimation accuracy is quite robust. This means that fingernail imaging is a useful technique for measuring unconstrained grasp forces on a variety of objects with different surface curvatures.\n\n \n\n\t\t\t\t\tLast Modified: 03/06/2019\n\n\t\t\t\t\tSubmitted by: Stephen A Mascaro"
 }
}