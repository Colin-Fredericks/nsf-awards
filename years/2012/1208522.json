{
 "awd_id": "1208522",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI-Small: A Biologically Plausible Architecture for Robotic Vision",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 1150000.0,
 "awd_amount": 1150000.0,
 "awd_min_amd_letter_date": "2012-08-27",
 "awd_max_amd_letter_date": "2012-08-27",
 "awd_abstract_narration": "The objective of this research is to develop a generic robotic vision architecture that is both biologically plausible and jointly optimal, in a decision theoretic sense, for attention, object tracking, object recognition, and action recognition, in both static and dynamic environments. The research is motivated by the observation that all these problems are solved by biological vision with very homogeneous neural computations. The approach is to exploit a mapping of accepted computational models of visual cortex into the elementary computations of statistical learning and inference in order to derive unified algorithms for all tasks. \r\n\r\nIntellectual merit: the proposed unification of vision tasks is novel and of paramount importance for robotics, since it is computationally infeasible for a robot to implement a large set of disjoint vision algorithms. It will also exploit task synergies, producing algorithms that leverage the solution of one task to improve performance on another. This will likely enable overall better performance of vision systems. Finally, the project will produce novel insights on the structure of the visual world, and how it can be leveraged by robotic vision, by introducing new models for natural image statistics. \r\n\r\nBroader impacts: The research has applicability in manufacturing, intelligent systems, health care, homeland security, etc.  The expected theoretical insights are likely to be of wide application in statistics (models of feature dependence), neuroscience (models of neural computation), and computer vision (synergistic models). Educationally, the project provides an exciting opportunity for the involvement of undergraduates in research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nuno",
   "pi_last_name": "Vasconcelos",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Nuno M Vasconcelos",
   "pi_email_addr": "nuno@ece.ucsd.edu",
   "nsf_id": "000104017",
   "pi_start_date": "2012-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930407",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 1150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigated biologically plausible, i.e. inspired by human vision, algorithms for computer vision. A number of activities were developed, including 1) the design of deep learning algorithms for the recognition and detection of objects, scenes, action and activities as well object tracking and saliency modeling, 2) evaluation of the biological plausibility of these algorithms though psychophysics, implementation with biologically plausible networks, or both, 3) evaluation of the performance of these algorithms in comparison to the state of the art in computer vision, and 4) development of computationally efficient versions of the algorithms, suitable for implementation in low-complexity hardware. In all cases, the algorithms were shown to advance the state of the art in the problems considered, as attested by publication of papers in the most prestigious journals and conferences in computer vision. Overall, the project resulted in the publication of 34 papers. This work was performed by several PhD students, who graduated or are in the process of graduating with thesis derived from the work performed for the project. The algorithms and theory produced by the project have application in many problems of societal intertest, including robots, smart cars, health care, medical imaging, surveillance and military applications, etc.</p>\n<p>Figure 1 illustrates one of the significant outcomes of the project, which a deep learning architecture for classifying images of scenes, e.g. offices, classrooms, etc. The figure shows the scene labels predicted by the system for images of 7 scenes. The bars shown on the right of each scene indicate the degree of confidence of the system for each of the labels in its vocabulary. The five labels of strongest score are shown in the bubbles attached to the large bars. These are the interpretations that the system found most suitable for the images. Note that besides assigning the largest confidence to the true label (something that, over the whole dataset, the system does more than 80% of the times), most of the five labels selected are labels that ?make sense? to some extent. This is a consequence of the biologically plausibility of the architecture. Not only the system thinks in a way that mimics what people do, but also tends to produce similar errors. This type of scene classification is important for autonomous systems, such a robots, which need to recognize where they are, as they move in the 3D world. It also has applications to problems like multimedia management &nbsp;and search, surveillance systems, or wildfire detection and prevention, &nbsp;among many others. More examples of scene classification are available from <a href=\"http://svcl.ucsd.edu/projects/Obj_to_Scene_Transfer/\">http://svcl.ucsd.edu/projects/Obj_to_Scene_Transfer/</a>.</p>\n<p>Figure 2 illustrates another significant contribution of the project. This is a system that performs action recognition and is capable of distinguishing actions that differ in a very subtle manner. In this case, the system is used to classify videos of Olympic sports by event, e.g. ?long jump? vs ?hurdle race?. The system starts by inferring the occurrence of atomic events, such as ?running,? ?jumping,? or ?landing? in the video. A second layer of processing, then infers the labels for the entire video from the sequence of outputs of these atomic event detectors. This is performed by a sophisticated machine learning system, known as the binary dynamic system, introduced by the project, which could be also applied to tasks such as natural language understanding or robotic control. A more detailed explanation, as well as video examples can be found in <a href=\"http://svcl.ucsd.edu/projects/attdyn/index.html\">http://svcl.ucsd.edu/projects/attdyn/index.html</a>. Action understanding systems have applications in robotics, healthcare, surveillance, entertainment and gaming systems, among many others. &nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2018<br>\n\t\t\t\t\tModified by: Nuno&nbsp;M&nbsp;Vasconcelos</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1208522/1208522_10206281_1543622756518_Slide2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1208522/1208522_10206281_1543622756518_Slide2--rgov-800width.jpg\" title=\"Figure 2\"><img src=\"/por/images/Reports/POR/2018/1208522/1208522_10206281_1543622756518_Slide2--rgov-66x44.jpg\" alt=\"Figure 2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Top: frames from videos depicting two actions. Bottom: atomic even detections through time, for each video</div>\n<div class=\"imageCredit\">Nuno Vasconcelos</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Nuno&nbsp;M&nbsp;Vasconcelos</div>\n<div class=\"imageTitle\">Figure 2</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1208522/1208522_10206281_1543622707311_Slide1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1208522/1208522_10206281_1543622707311_Slide1--rgov-800width.jpg\" title=\"Figure 1\"><img src=\"/por/images/Reports/POR/2018/1208522/1208522_10206281_1543622707311_Slide1--rgov-66x44.jpg\" alt=\"Figure 1\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Left: image. Right: scene labels produced by a scene classification system developed in the project</div>\n<div class=\"imageCredit\">Nuno Vasconcelos</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Nuno&nbsp;M&nbsp;Vasconcelos</div>\n<div class=\"imageTitle\">Figure 1</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project investigated biologically plausible, i.e. inspired by human vision, algorithms for computer vision. A number of activities were developed, including 1) the design of deep learning algorithms for the recognition and detection of objects, scenes, action and activities as well object tracking and saliency modeling, 2) evaluation of the biological plausibility of these algorithms though psychophysics, implementation with biologically plausible networks, or both, 3) evaluation of the performance of these algorithms in comparison to the state of the art in computer vision, and 4) development of computationally efficient versions of the algorithms, suitable for implementation in low-complexity hardware. In all cases, the algorithms were shown to advance the state of the art in the problems considered, as attested by publication of papers in the most prestigious journals and conferences in computer vision. Overall, the project resulted in the publication of 34 papers. This work was performed by several PhD students, who graduated or are in the process of graduating with thesis derived from the work performed for the project. The algorithms and theory produced by the project have application in many problems of societal intertest, including robots, smart cars, health care, medical imaging, surveillance and military applications, etc.\n\nFigure 1 illustrates one of the significant outcomes of the project, which a deep learning architecture for classifying images of scenes, e.g. offices, classrooms, etc. The figure shows the scene labels predicted by the system for images of 7 scenes. The bars shown on the right of each scene indicate the degree of confidence of the system for each of the labels in its vocabulary. The five labels of strongest score are shown in the bubbles attached to the large bars. These are the interpretations that the system found most suitable for the images. Note that besides assigning the largest confidence to the true label (something that, over the whole dataset, the system does more than 80% of the times), most of the five labels selected are labels that ?make sense? to some extent. This is a consequence of the biologically plausibility of the architecture. Not only the system thinks in a way that mimics what people do, but also tends to produce similar errors. This type of scene classification is important for autonomous systems, such a robots, which need to recognize where they are, as they move in the 3D world. It also has applications to problems like multimedia management  and search, surveillance systems, or wildfire detection and prevention,  among many others. More examples of scene classification are available from http://svcl.ucsd.edu/projects/Obj_to_Scene_Transfer/.\n\nFigure 2 illustrates another significant contribution of the project. This is a system that performs action recognition and is capable of distinguishing actions that differ in a very subtle manner. In this case, the system is used to classify videos of Olympic sports by event, e.g. ?long jump? vs ?hurdle race?. The system starts by inferring the occurrence of atomic events, such as ?running,? ?jumping,? or ?landing? in the video. A second layer of processing, then infers the labels for the entire video from the sequence of outputs of these atomic event detectors. This is performed by a sophisticated machine learning system, known as the binary dynamic system, introduced by the project, which could be also applied to tasks such as natural language understanding or robotic control. A more detailed explanation, as well as video examples can be found in http://svcl.ucsd.edu/projects/attdyn/index.html. Action understanding systems have applications in robotics, healthcare, surveillance, entertainment and gaming systems, among many others.  \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/30/2018\n\n\t\t\t\t\tSubmitted by: Nuno M Vasconcelos"
 }
}