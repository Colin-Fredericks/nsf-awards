{
 "awd_id": "1229176",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Acquisition of Infant/Robot Grasp Learning Instrumentation",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 312844.0,
 "awd_amount": 312844.0,
 "awd_min_amd_letter_date": "2012-08-18",
 "awd_max_amd_letter_date": "2016-08-06",
 "awd_abstract_narration": "Proposal #:\t12-29176\r\nPI(s):\t\tParker, Lynn E.\r\n\t\tArel, Itamar; Corbetta, Daniela; MAcLennan, Bruce J.; Reynolds, Gregory D.\r\nInstitution:\tUniversity of Tennessee - Knoxville\r\nTitle: \t\tMRI/Acq.: Infant/Robot Grasp Learning Instrumentation\r\nProject Proposed:\r\nThis project, acquiring equipment to create an instrument for social robot learning from human infant studies, aims to transform the development of human-interactive robots by applying fundamental scientific paradigms from developmental psychology studies in infants to new ways of designing autonomous robots. Specifically, the research utilizes data from human infant perceptual-motor learning studies in reaching and grasping to build comprehensive models of human sensorimotor control that achieve embodied learning like that exhibited by infants. These models will then be instantiated in a robotic setting to create a machine that can autonomously acquire shared object manipulation skills through bottom-up and top-down processes mimicking human infant learning. The equipment will facilitate multiple research thrusts within the grand scope of understanding and improving human-robot interaction. The thrusts include: \r\n-\tStudy of biologically-plausible visual attention mechanisms, \r\n-\tAdaptive visually-guided motor skills, and \r\n-\tInference of human state. \r\nInfant visual search patterns, from which models of perception will be developed, will be acquired via the eye tracking system. The robotic arm system will serve as a platform for extensive studies on perceptual motor skills. The instrumentation should enable the following outcomes: \r\n-\tYield robotic systems able to learn to physically interact with humans through shared object manipulation. These systems will learn skills that allow handling of previously unseen objects.\r\n-\tFacilitate human-robot interaction, especially regarding manipulation, grasping, and handling capabilities, as well as eye-tracking, to better study the role of vision in infant grasp and reach learning. \r\n-\tContribute significantly to the integration of two bodies of research ? psychology and engineering ? coupled through the computational models built. These computational models will provide a two-way exchange of ideas between psychology and engineering, \r\n-\tLead both to the design of new experiments in psychology and to new mechanisms for interactive robots. \r\nThese systems will learn skills that allow handling of previously unseen objects. Facilitating robot interaction will permit to better study the role of vision in infant grasp and reach learning. These computational models will provide a two-way exchange of ideas between psychology and engineering leading to the design of new experiments in psychology and to new mechanisms for interactive robots.\r\nBroader Impacts\r\nThe instrument, initially used by 7 faculty in 3 departments across 2 colleges, will offer new opportunities for cross-disciplinary training in the fields of cognitive psychology, developmental cognitive neuroscience, computer science, computer engineering, electrical engineering, and mechanical engineering, for both graduate and undergraduate students. The enabled research is also expected to offer significant practical societal impact, as many potential applications of human-robot interaction involve the exchange of objects (e.g., an assistive robot picking up a dropped TV remote control for a disabled person, a delivery robot handing a package to a human, or a therapeutic robot handing a toy to an autistic child). The work may have implications for addressing developmental problems in children, emanating from the increased understanding of the perceptual and motor learning processes in infants. Moreover, the instrumentation provides experiental and cross-disciplinary opportunities that buttress classroom theory, thus providing a positive impact to education, students, faculty, K-12 teachers, museums, etc.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Lynne",
   "pi_last_name": "Parker",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Lynne E Parker",
   "pi_email_addr": "leparker@utk.edu",
   "nsf_id": "000346469",
   "pi_start_date": "2012-08-18",
   "pi_end_date": "2014-12-19"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniela",
   "pi_last_name": "Corbetta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniela Corbetta",
   "pi_email_addr": "dcorbett@utk.edu",
   "nsf_id": "000505797",
   "pi_start_date": "2016-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bruce",
   "pi_last_name": "MacLennan",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Bruce J MacLennan",
   "pi_email_addr": "maclennan@cs.utk.edu",
   "nsf_id": "000263509",
   "pi_start_date": "2012-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Itamar",
   "pi_last_name": "Arel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Itamar Arel",
   "pi_email_addr": "itamar@eecs.utk.edu",
   "nsf_id": "000488436",
   "pi_start_date": "2012-08-18",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Daniela",
   "pi_last_name": "Corbetta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniela Corbetta",
   "pi_email_addr": "dcorbett@utk.edu",
   "nsf_id": "000505797",
   "pi_start_date": "2012-08-18",
   "pi_end_date": "2014-12-19"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Reynolds",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory D Reynolds",
   "pi_email_addr": "greynolds@utk.edu",
   "nsf_id": "000539754",
   "pi_start_date": "2012-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "1 Circle Park",
  "perf_city_name": "Knoxville",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960003",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118900",
   "pgm_ele_name": "Major Research Instrumentation"
  },
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  },
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 312844.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>MRI: Acquisition of Infant/Robot Grasp Learning Instrumentation</p>\n<p>This project allowed the acquisition of a state-of-the-art robot and a faster eye-tracker for infant studies on the development of reaching. The robot is equipped with two articulated arms, a head with multiple sensors and cameras for detecting the environment, and tactile hands.&nbsp; It is also mounted on wheels providing mobility.&nbsp; The long term aim of the project was to begin building comprehensive models of human sensorimotor control that achieve learning like that exhibited by infants in their physical and social environment and to instantiate these models in the robot platform. The duration of this award allowed us to attain two most fundamental prerequisite goals of our long-term goal: (1) Developing and implementing fundamental functions in the robot to render it operational at a basic level, and (2) continuing infants studies on perception-action with the aim to better understand the fundamental cognitive and motor mechanisms of learning to reach that could ultimately be used to program autonomous learning in the robot.</p>\n<p><span style=\"text-decoration: underline;\">Robot studies</span>: Outcomes with the robot involved developing multiple software to equip the robot with basic functions for safe interactions with human and objects in its environment. Software has been designed to allow the robot to interpret human actions, recognize new situations, and generate appropriate decision-making that incorporates an evaluation of risk factors when confronted with new situations.&nbsp; Another function implemented related to the recognition of multiple individuals, and algorithms were developed to allow the robot to track and reason about gradual transitions between continuous human activities.&nbsp; Several bio-inspired algorithms were also added to address issues related to human-robot interactions.&nbsp; Much of these functions use 3D human skeletal datasets to predict the activity of others (which can facilitate team work), but also recognize human activity at the semantic level as well as at the basic level of body motion.&nbsp; Finally, the robot has also been used to begin developing models of infant acquisition of reaching competence. The robot has been programmed to exhibit several forms of &ldquo;motor babbling&rdquo;, which are the initial pseudo-random movements newborns perform before they gain full control of their arm for reaching.</p>\n<p><span style=\"text-decoration: underline;\">Infant studies</span>: Much is already known about how infants learn to control their arms to reach for objects in the first year of life, but almost nothing is known about how visual information gathered from the target objects prior to reaching is used in the selection and planning of infants&rsquo; goal-directed actions.&nbsp; The eye-tracker acquired with this grant allowed us to begin fulfilling this important gap.&nbsp; The infant studies have provided two major outcomes in relation to the infant visual behaviors directed at objects prior to reaching for them.&nbsp; First, it was found that at reach onset, when infants attempt their first object-directed movements around 3 to 4 month of age, vision does not play as critical a role as thought for many decades.&nbsp; Novice infant reachers seem to aim for the object target by relying mainly on proprioceptive information rather than visual information to direct their arm in space.&nbsp; We suspect that the &ldquo;motor babbling&rdquo; period preceding the emergence of reaching may provide the primary experience that infants use to plan their first reaching attempts towards objects.&nbsp; Second, as infants become better at reaching and start relying more on vision to plan their actions, we discovered that the duration of visual encoding preceding the action of reaching for the object plays an important role for the selection of action.&nbsp; When 9-months-old infants looked at the object target longer, they were more likely to select a target area visually and to bring their hand in that selected area for grasping, than when they reached right after detecting the object.</p>\n<p>In conclusion, acquisition of the robot and the eye-tracker has enabled fundamental studies of how embodied agents (robots or infants) can learn to control their bodies in order to act competently. This knowledge can contribute to the development of future robots, the design of prosthetic devices, and the mitigation and treatment of developmental impairments. In addition, this equipment is helping to train a new generation of researchers able to use state-of-the-art instruments in developmental psychology and robotics.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/11/2017<br>\n\t\t\t\t\tModified by: Daniela&nbsp;Corbetta</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMRI: Acquisition of Infant/Robot Grasp Learning Instrumentation\n\nThis project allowed the acquisition of a state-of-the-art robot and a faster eye-tracker for infant studies on the development of reaching. The robot is equipped with two articulated arms, a head with multiple sensors and cameras for detecting the environment, and tactile hands.  It is also mounted on wheels providing mobility.  The long term aim of the project was to begin building comprehensive models of human sensorimotor control that achieve learning like that exhibited by infants in their physical and social environment and to instantiate these models in the robot platform. The duration of this award allowed us to attain two most fundamental prerequisite goals of our long-term goal: (1) Developing and implementing fundamental functions in the robot to render it operational at a basic level, and (2) continuing infants studies on perception-action with the aim to better understand the fundamental cognitive and motor mechanisms of learning to reach that could ultimately be used to program autonomous learning in the robot.\n\nRobot studies: Outcomes with the robot involved developing multiple software to equip the robot with basic functions for safe interactions with human and objects in its environment. Software has been designed to allow the robot to interpret human actions, recognize new situations, and generate appropriate decision-making that incorporates an evaluation of risk factors when confronted with new situations.  Another function implemented related to the recognition of multiple individuals, and algorithms were developed to allow the robot to track and reason about gradual transitions between continuous human activities.  Several bio-inspired algorithms were also added to address issues related to human-robot interactions.  Much of these functions use 3D human skeletal datasets to predict the activity of others (which can facilitate team work), but also recognize human activity at the semantic level as well as at the basic level of body motion.  Finally, the robot has also been used to begin developing models of infant acquisition of reaching competence. The robot has been programmed to exhibit several forms of \"motor babbling\", which are the initial pseudo-random movements newborns perform before they gain full control of their arm for reaching.\n\nInfant studies: Much is already known about how infants learn to control their arms to reach for objects in the first year of life, but almost nothing is known about how visual information gathered from the target objects prior to reaching is used in the selection and planning of infants? goal-directed actions.  The eye-tracker acquired with this grant allowed us to begin fulfilling this important gap.  The infant studies have provided two major outcomes in relation to the infant visual behaviors directed at objects prior to reaching for them.  First, it was found that at reach onset, when infants attempt their first object-directed movements around 3 to 4 month of age, vision does not play as critical a role as thought for many decades.  Novice infant reachers seem to aim for the object target by relying mainly on proprioceptive information rather than visual information to direct their arm in space.  We suspect that the \"motor babbling\" period preceding the emergence of reaching may provide the primary experience that infants use to plan their first reaching attempts towards objects.  Second, as infants become better at reaching and start relying more on vision to plan their actions, we discovered that the duration of visual encoding preceding the action of reaching for the object plays an important role for the selection of action.  When 9-months-old infants looked at the object target longer, they were more likely to select a target area visually and to bring their hand in that selected area for grasping, than when they reached right after detecting the object.\n\nIn conclusion, acquisition of the robot and the eye-tracker has enabled fundamental studies of how embodied agents (robots or infants) can learn to control their bodies in order to act competently. This knowledge can contribute to the development of future robots, the design of prosthetic devices, and the mitigation and treatment of developmental impairments. In addition, this equipment is helping to train a new generation of researchers able to use state-of-the-art instruments in developmental psychology and robotics.\n\n \n\n\t\t\t\t\tLast Modified: 12/11/2017\n\n\t\t\t\t\tSubmitted by: Daniela Corbetta"
 }
}