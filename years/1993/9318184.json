{
 "awd_id": "9318184",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Understanding Human Joint Mechanics through Advanced        Computational Models",
 "cfda_num": "47.049",
 "org_code": "03040300",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Michael Steuerwalt",
 "awd_eff_date": "1993-09-01",
 "awd_exp_date": "2000-05-31",
 "tot_intn_awd_amt": 2550000.0,
 "awd_amount": 2518000.0,
 "awd_min_amd_letter_date": "1993-09-15",
 "awd_max_amd_letter_date": "1997-07-15",
 "awd_abstract_narration": "9318163  Bielak       The Grand Challenge Application Groups competition provides  one mechanism for the support of multidisciplinary teams of  scientists and engineers to meet the goals of the High Performance  Computing and Communications (HPCC) Initiative in Fiscal Year 1993.   The ideal proposal provided not only the opportunity to achieve  significant progress on  (1) a fundamental problem in science or  engineering whose solution    could be advanced by applying high  performance computing    techniques and resources, or (2) enabling  technologies which facilitate those advances.  but also significant interactions between scientific and  computational activities, usually involving mathematical, computer  or computational scientist, that would have impact in high  performance computational activities beyond the specific scientific  or engineering problem area(s) or discipline being studied.  The  main objective of the proposed research is to develop and  demonstrate the capability for predicting, by computer   simulation the ground motion of large basins during strong  earthquakes, and to use this capability to study the seismic  response of the Greater Los  Angeles Basin. The proposed research seeks to: 1. Develop  three-dimensional models of large-scale, heterogeneous basins that  take into account earthquake source, propagation path, and site  conditions; 2. Develop nonlinear models for sedimentary basins that  experience sufficiently strong ground motion; 3. Develop  unstructured  mesh methods and associated fast parallel solvers, enabling the  study  of much larger basins; 4. Develop software tools for the automatic  mapping of the computations associated with large unstructured mesh  problems on parallel computers; 5. Characterize the computation and  communication requirements of unstructured mesh problems, and make  a  set of recommendations for the design of future parallel systems.  While the proposed work is motivated by an interest in gaining a  better understanding of strong se ismic motion in large basins, the  algorithms and software tools developed will be applicable to a  wide  range of applications that require unstructured meshes.  This award  is being supported by the Advanced Projects Research Agency as well  as NSF programs in engineering, atmospheric and computer sciences. 9318183  Davis       The Grand Challenge Application Groups competition provides  one mechanism for the support of multidisciplinary teams of  scientists and engineers to meet the goals of the High Performance  Computing and Communications (HPCC) Initiative in Fiscal Year 1993.   The ideal proposal provided not only the opportunity to achieve  significant progress on  (1) a fundamental problem in science or  engineering whose solution    could be advanced by applying high  performance computing techniques and resources, or (2) enabling  technologies which facilitate those advances but also significant  interactions between scientific and computational activities,  usually involving mathematical, computer or computational  scientist, that would have impact in high performance computational  activities beyond the specific scientific or engineering problem  area(s) or discipline being studied.  The investigators will study  the application of high performance parallel computing to a class  of scientifically important and computationally demanding problems  in remote sensing-land cover dynamics problems including generating  improved fine spatial resolution data for the global carbon cycle,  hydrological modeling and global ecological responses to climate  changes and human activity.  The research is collaborative,  including scientist from the University of Maryland, University of  Indiana, University of news Hampshire and NASA s Goddard Space  Center.   The award will combine research on       -new analysis procedures for remotely sensed data       -the integration of multispectral, multiresolution and       multitemporal image data sets into a unified global data  structure      based on hie rarchical data structures (i.e.,  quadtrees)       -the utilization of these hierarchical, parallel data  structures for      the representation of spatial data (maps and  products developed  from image analysis) and the development of a  spatial data base   system with a sophisticated query language that  scientist can use to     control the application of biophysical  models to global data sets       -run-time support for constructing scalable and parallel  solutions      to problems involving the manipulation of irregular  data structures     such as quadtrees       -parallel I/O,especially novel methods for mapping large  arrays and     quadtrees onto parallel disks and disk systems,  and  for accessing  them using low overhead bulk transfers  The development work will be conducted on a 32 processor Connection  Machine CM5, installed at the University of Maryland, and on an IBM  SP1 which we propose to obtain as part of the program.  This award  is being supported by the Advanced Projects Research Agency as well  as NSF programs in geological, biological, and computer sciences.                                        9318145  Messina       The Grand Challenge Application Groups competition provides  one mechanism for the support of multidisciplinary teams of  scientists and engineers to meet the goals of the High Performance  Computing and Communications (HPCC) Initiative in Fiscal Year 1993.   The ideal proposal provided not only the opportunity to achieve  significant progress on  (1) a fundamental problem in science or  engineering whose solution    could be advanced by applying high  performance computing techniques and resources, or (2) enabling  technologies which facilitate those advances.  but also significant interactions between scientific and  computational activities, usually involving mathematical, computer  or computational scientist, that would have impact in high  performance computational activities beyond the specific scientific  or engineering problem area(s) or discipline  being studied.  This  multi-disciplinary project will investigate and develop strategies  for efficient implementation of I/O intensive applications in  computational science and engineering.  Scalable parallel I/O  approaches will be pursued by a team of computer scientists and  applications scientists who will work together to:    * Characterize the I/O behavior of specific application programs  running    on large massively parallelcomputers    * Abstract and define I/O models (templates)    * Define application-level methodologies for efficient parallel  I/O    * Implement and test application level I/O tools on large-scale           computers  The Pablo performance analysis environment will provide the  foundation for the performance instrumentation and analysis. The  application programs are already fully operational on advanced  architecture  systems and their authors are all co-investigators in this project.  The principal computers used will be the Intel Touchstone Delta and  Paragon systems at Caltech, each with over 500 computational nodes.  Five  application areas will be included: fluid dynamics, chemistry,  astronomy, neuroscience, and modelling of materials-processing  plasmas. The parallel programs for these applications cover a range  of I/O patterns  and volume, and the techniques that will be developed in this  project will be of relevance to a broad spectrum of engineering and  science applications. In addition, by overcoming their current I/O  limitations, the specific applications targeted in this award will  achieve significant new science and engineering results. By the end  of the project, sustained teraFLOPS computers will become  available. The project will  devise and implement general methods for scalable I/O using today's  advanced computers, immediately apply those methods to carry out  unprecedented applications in several fields, and use the",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Spilker",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Robert L Spilker",
   "pi_email_addr": "spilker@rpi.edu",
   "nsf_id": "000389341",
   "pi_start_date": "1993-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Joseph",
   "pi_last_name": "Flaherty",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Joseph E Flaherty",
   "pi_email_addr": "flaherje@cs.rpi.edu",
   "nsf_id": "000119057",
   "pi_start_date": "1993-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Shephard",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Mark S Shephard",
   "pi_email_addr": "shephard@rpi.edu",
   "nsf_id": "000157587",
   "pi_start_date": "1993-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Boleslaw",
   "pi_last_name": "Szymanski",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Boleslaw K Szymanski",
   "pi_email_addr": "szymansk@cs.rpi.edu",
   "nsf_id": "000204604",
   "pi_start_date": "1993-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Holmes",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Mark H Holmes",
   "pi_email_addr": "holmes@rpi.edu",
   "nsf_id": "000387755",
   "pi_start_date": "1993-09-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rensselaer Polytechnic Institute",
  "inst_street_address": "110 8TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "TROY",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "5182766000",
  "inst_zip_code": "121803590",
  "inst_country_name": "United States",
  "cong_dist_code": "20",
  "st_cong_dist_code": "NY20",
  "org_lgl_bus_name": "RENSSELAER POLYTECHNIC INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "U5WBFKEBLMX3"
 },
 "perf_inst": {
  "perf_inst_name": "Rensselaer Polytechnic Institute",
  "perf_str_addr": "110 8TH ST",
  "perf_city_name": "TROY",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "121803590",
  "perf_ctry_code": "US",
  "perf_cong_dist": "20",
  "perf_st_cong_dist": "NY20",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  },
  {
   "pgm_ele_code": "151800",
   "pgm_ele_name": "CONTROL, NETWORKS, & COMP INTE"
  },
  {
   "pgm_ele_code": "286500",
   "pgm_ele_name": "NUMERIC, SYMBOLIC & GEO COMPUT"
  },
  {
   "pgm_ele_code": "406600",
   "pgm_ele_name": "PART FOR ADVANCED COMP INFRA"
  },
  {
   "pgm_ele_code": "408000",
   "pgm_ele_name": "ADVANCED COMP RESEARCH PROGRAM"
  },
  {
   "pgm_ele_code": "534500",
   "pgm_ele_name": "Engineering of Biomed Systems"
  },
  {
   "pgm_ele_code": "919900",
   "pgm_ele_name": "Unallocated Program Costs"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9216",
   "pgm_ref_txt": "ADVANCED SOFTWARE TECH & ALGOR"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0193",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0193",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0194",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0194",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0195",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0195",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0196",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0196",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0197",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0197",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 1993,
   "fund_oblg_amt": 550000.0
  },
  {
   "fund_oblg_fiscal_yr": 1994,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 1995,
   "fund_oblg_amt": 468000.0
  },
  {
   "fund_oblg_fiscal_yr": 1996,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 1997,
   "fund_oblg_amt": 500000.0
  }
 ],
 "por": null
}