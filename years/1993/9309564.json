{
 "awd_id": "9309564",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Properties of Transsaccadic Memory",
 "cfda_num": "47.075",
 "org_code": "04040500",
 "po_phone": "7032924833",
 "po_email": "jyoung@nsf.gov",
 "po_sign_block_name": "Jasmine Young",
 "awd_eff_date": "1993-10-15",
 "awd_exp_date": "1997-09-30",
 "tot_intn_awd_amt": 194587.0,
 "awd_amount": 194587.0,
 "awd_min_amd_letter_date": "1993-09-24",
 "awd_max_amd_letter_date": "1995-05-19",
 "awd_abstract_narration": "9309564  IRWIN    Our eyes move from object to object in the world several times each  second.  We scan the world by means of saccades--fast eye movements  that are separated by brief fixations during which the eyes are  relatively still.  Each eye movement causes visual stimulation to  be swept across the retinas, producing a blur or smear that is not  ordinarily perceived because vision is suppressed during saccades.   Because of this suppression, we acquire visual information from the  world only during fixations, when the eyes are still.  For this  reason, our visual information about the world is registered in  isolated glimpses that are separated in time.  Furthermore, the  contents of these isolated glimpses are not identical, because  different regions of the world fall on different parts of the  retina when the eyes change position.  Despite this rapidly  changing and discontinuous visual input, we ordinarily perceive the  world as unified, stable, and continuous.  How the perceptual  system accomplishes this has puzzled psychologists and vision  researchers for over a century.  One frequent hypothesis is that  the contents of individual eye fixations are combined in memory  across eye movements to produce a coherent mental representation of  the visual environment.  This research will investigate the  characteristics of this hypothetical process.  In particular, the  research will determine how much information is remembered from one  eye fixation to the next, whether some kinds of information are  remembered better than others, and how this information is stored  and combined across eye movements.    The experimental procedure will involve having individual observers  view visual displays on a computer terminal screen while their eye  movements are monitored by an eyetracking device.  During selected  eye movements, the visual display will disappear and the observer's  memory for that display will be assessed.  For example, the  observer may indicate whether a second display is identi cal to or  different from the first display, attempt to report some particular  information that was present in the first display, or name an  indicated object in a new display as rapidly as possible.  These  responses will reveal what aspects of the original display are  stored in memory, maintained across an eye movement, and used to  relate successive eye fixations to each other.    The question of how people perceive a continuous visual world  across eye movements is a classic problem in perception.  The  research will advance the solution to this problem.  In addition,  the research will lead to the development of more sophisticated  theories of spatial representation and spatial cognition in  general.  From a practical standpoint, the research will help human  factors engineers design visual displays, and tasks that require  multiple eye fixations on a display, to take best advantage of an  operator's ability or inability to combine different kinds of  visual information across eye movements.    ***",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Irwin",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "David E Irwin",
   "pi_email_addr": "dirwin@s.psych.uiuc.edu",
   "nsf_id": "000319891",
   "pi_start_date": "1993-10-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "118000",
   "pgm_ele_name": "HUMAN COGNITION & PERCEPTION"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "0000",
   "pgm_ref_txt": "UNASSIGNED"
  },
  {
   "pgm_ref_code": "OTHR",
   "pgm_ref_txt": "OTHER RESEARCH OR EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0193",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0193",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0194",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0194",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0195",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0195",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 1993,
   "fund_oblg_amt": 61724.0
  },
  {
   "fund_oblg_fiscal_yr": 1994,
   "fund_oblg_amt": 64811.0
  },
  {
   "fund_oblg_fiscal_yr": 1995,
   "fund_oblg_amt": 68052.0
  }
 ],
 "por": null
}