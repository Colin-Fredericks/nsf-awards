{
 "awd_id": "8910073",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Tree-Structured Neural Networks and Learning Algorithms",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Paul Werbos",
 "awd_eff_date": "1989-12-15",
 "awd_exp_date": "1992-05-31",
 "tot_intn_awd_amt": 60000.0,
 "awd_amount": 60000.0,
 "awd_min_amd_letter_date": "1989-12-06",
 "awd_max_amd_letter_date": "1989-12-06",
 "awd_abstract_narration": "The most serious problem with back propagation and other learnng                algorithms for training multilayer perceptrons is that the amount of            training which is required scales too fast with the size of the                 multilayer preceptron.  The project will attempt to develop a learning          algorithm for a neural network architecture with multiple small                 mulitlayer preceptrons connected in such a way as to exhibit better             scaling properties.  The P.I. will consider a tree-structured                   classifier which uses small multilayer preceptrons at its decision              nodes to extract nonlinear combinations of features.  The first phase           of the proposed research will focus on learning algorithms for the              multilayer preceptrons at the nodes of the tree. Suitable back                  propagation-type algorithms will be developed for those purpose.  The           back propagation algorithm is a generalization of is a gradient                 descent algorithm frequently used a linear adaptive filtering with a            mean-square error criteria.  Back propagation-type algorithms will be           developed which local minima by applying the author's own work on               globally convergent gradient descent algorithms, which is a form of             continuous stat simulated annealing.  Back propagation-type algorithms          will be developed which minimize non-mean-square error criteria by              generalizing gradient descent algorithms frequently used for linear             adaptive classifiers with a preceptron, relaxation, or minimum                  probability of error criteria.  Back propagation-type algorithms will           also be developed which are capable of unsupervised learning.  The              second phase of the proposed work will focus on the learning algorithm          for the overall tree.  Here in conjunction with a back propagation-             type algorithm training the multilayer perceptrons at the nodes of the          tree, we will develop adaptive tree growing and pruning algorithms.             The proposed work should provide an important tool for application of           neural networks to hard large-scale problems in speech and image                recognition, where current best systems are far from equaling human             performance.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Saul",
   "pi_last_name": "Gelfand",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Saul B Gelfand",
   "pi_email_addr": "gelfand@ecn.purdue.edu",
   "nsf_id": "000125346",
   "pi_start_date": "1989-12-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue Research Foundation",
  "inst_street_address": "1281 WIN HENTSCHEL BLVD STE 2500",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3174946200",
  "inst_zip_code": "479064353",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE RESEARCH FOUNDATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "C1G1LGYJF5G5"
 },
 "perf_inst": {
  "perf_inst_name": "DATA NOT AVAILABLE",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "",
  "perf_st_name": "RI REQUIRED",
  "perf_zip_code": "",
  "perf_ctry_code": "",
  "perf_cong_dist": "",
  "perf_st_cong_dist": "",
  "perf_ctry_name": "",
  "perf_ctry_flag": "0"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "149400",
   "pgm_ele_name": "NEUROENGINEERING"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "",
   "fund_name": "",
   "fund_symb_id": ""
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 1990,
   "fund_oblg_amt": 60000.0
  }
 ],
 "por": null
}